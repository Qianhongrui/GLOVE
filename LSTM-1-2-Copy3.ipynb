{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Includes\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"10\",\n",
    "    \"11\",\n",
    "    \"12\",\n",
    "    \"13\",\n",
    "    \"14\",\n",
    "    \"15\",\n",
    "    \"16\",\n",
    "    \"17\",\n",
    "    \"18\",\n",
    "    \"19\",\n",
    "    \"20\",\n",
    "    \"21\",\n",
    "    \"22\",\n",
    "    \"23\",\n",
    "    \"24\",\n",
    "    \"25\",\n",
    "    \"26\",\n",
    "    \"27\",\n",
    "    \"28\",\n",
    "    \"29\",\n",
    "    \"30\",\n",
    "    \"31\",\n",
    "    \"32\",\n",
    "    \"33\",\n",
    "    \"34\",\n",
    "    \"35\",\n",
    "    \"36\",\n",
    "    \"37\",\n",
    "    \"38\",\n",
    "    \"39\",\n",
    "    \"40\",\n",
    "    \"41\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"add\", \n",
    "    \"bill\", \n",
    "    \"bit\", \n",
    "    \"coffee\", \n",
    "    \"cup\", \n",
    "    \"drinks\",\n",
    "    \"eat\",\n",
    "    \"give\",\n",
    "    \"good\",\n",
    "    \"greentea\",\n",
    "    \"half\",\n",
    "    \"hot\",\n",
    "    \"howmany\",\n",
    "    \"ice\",\n",
    "    \"me\",\n",
    "    \"menu\",\n",
    "    \"money\",\n",
    "    \"no\",\n",
    "    \"one\",\n",
    "    \"straw\",\n",
    "    \"sugar\",\n",
    "    \"szcancel\",\n",
    "    \"thanks\",\n",
    "    \"think\",\n",
    "    \"toilet\",\n",
    "    \"waiter\",\n",
    "    \"want\",\n",
    "    \"where\",\n",
    "    \"you\",\n",
    "    \"zstart\",\n",
    "    \"0\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"10\",\n",
    "    \"20\",\n",
    "    \"30\",\n",
    "    \"40\",\n",
    "    \"50\",\n",
    "    \"60\",\n",
    "    \"70\",\n",
    "    \"80\",\n",
    "    \"90\",\n",
    "    \"100\",\n",
    "    \"liquor\", #酒\n",
    "    \"wine\",\n",
    "    \"tapwater\", #自来水\n",
    "    \"milk\",\n",
    "    \"hotchocolate\",\n",
    "    \"juice\",\n",
    "    \"beer\",\n",
    "    \"coke\",\n",
    "    \"whitetea\",\n",
    "    \"mineralwater\",\n",
    "    \"redwine\",\n",
    "    \"blacktea\",\n",
    "    \"total\",\n",
    "    \"napkin\",\n",
    "    \"this\",\n",
    "    \"breakfast\",\n",
    "    \"goodbye\",\n",
    "    \"have\",\n",
    "    \"tip\",\n",
    "    \"like\",\n",
    "    \"lunch\",\n",
    "    \"wei\", #位\n",
    "    \"dinner\",\n",
    "    \"recommend\",\n",
    "    \"spoon\",\n",
    "    \"what\",\n",
    "    \"person\",\n",
    "    \"please\",\n",
    "    \"take\",\n",
    "    \"without\",\n",
    "    \"look\",\n",
    "    \"or\",\n",
    "    \"welcome\",\n",
    "    \"and\",\n",
    "    \"drink\",\n",
    "    \"sorry\",\n",
    "    \"tableware\", #餐具\n",
    "    \"glass\",\n",
    "    \"help\",\n",
    "    \"1oclock\",\n",
    "    \"2oclock\",\n",
    "    \"3oclock\",\n",
    "    \"4oclock\",\n",
    "    \"5oclock\",\n",
    "    \"6oclock\",\n",
    "    \"7oclock\",\n",
    "    \"8oclock\",\n",
    "    \"9oclock\",\n",
    "    \"10oclock\",\n",
    "    \"11oclock\",\n",
    "    \"12oclock\",\n",
    "    \"reservation\"\n",
    "] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by downloading the data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Dataset_1_2_1/train/Inertial Signals/0.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/1.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/2.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/3.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/4.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/5.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/6.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/7.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/8.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/9.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/10.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/11.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/12.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/13.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/14.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/15.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/16.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/17.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/18.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/19.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/20.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/21.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/22.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/23.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/24.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/25.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/26.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/27.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/28.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/29.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/30.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/31.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/32.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/33.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/34.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/35.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/36.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/37.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/38.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/39.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/40.txt\n",
      "data/Dataset_1_2_1/train/Inertial Signals/41.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/0.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/1.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/2.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/3.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/4.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/5.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/6.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/7.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/8.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/9.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/10.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/11.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/12.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/13.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/14.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/15.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/16.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/17.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/18.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/19.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/20.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/21.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/22.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/23.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/24.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/25.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/26.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/27.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/28.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/29.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/30.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/31.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/32.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/33.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/34.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/35.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/36.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/37.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/38.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/39.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/40.txt\n",
      "data/Dataset_1_2_1/test/Inertial Signals/41.txt\n"
     ]
    }
   ],
   "source": [
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "DATASET_PATH = \"data/Dataset_1_2_1/\"\n",
    "\n",
    "\n",
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        \n",
    "        print(signal_type_path)\n",
    "        \n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \".txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \".txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal Parameters:\n",
    "\n",
    "Here are some core parameter definitions for the training. \n",
    "\n",
    "For example, the whole neural network's structure could be summarised by enumerating those parameters and the fact that two LSTM are used one on top of another (stacked) output-to-input as hidden layers through time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(3358, 128, 42) (3358, 1) -5.095609 50.289627\n",
      "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 2947 testing series\n",
    "n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
    "\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 64 # Hidden layer num of features\n",
    "n_classes = 101 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.00001\n",
    "lambda_loss_amount = 0.001\n",
    "training_iters = training_data_count * 2500 # Loop 300 times on the dataset\n",
    "batch_size = 1500\n",
    "display_iter = 1000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset, some of the credits goes to \n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # ReLU activation, thanks to Yu Zhao for adding this improvement here:\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many-to-one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_, n_classes=n_classes):\n",
    "    # Function to encode neural one-hot output labels from number indexes \n",
    "    # e.g.: \n",
    "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
    "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get serious and build the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-86e187028df0>:22: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-86e187028df0>:24: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-86e187028df0>:26: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000287BCEB6C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B70E30B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000287B9C65EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-6-cf6b5ef931ba>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input],name='input_x')\n",
    "y = tf.placeholder(tf.float32, [None, n_classes],name='inpu_y')\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooray, now train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1500:   Batch Loss = 14.782017, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.77249526977539, Accuracy = 0.012507445178925991\n",
      "Training iter #3000:   Batch Loss = 14.843967, Accuracy = 0.012000000104308128\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.767631530761719, Accuracy = 0.012507445178925991\n",
      "Training iter #6000:   Batch Loss = 14.757189, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.75874137878418, Accuracy = 0.012507445178925991\n",
      "Training iter #9000:   Batch Loss = 14.808714, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.750635147094727, Accuracy = 0.012507445178925991\n",
      "Training iter #12000:   Batch Loss = 14.797735, Accuracy = 0.004000000189989805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.742091178894043, Accuracy = 0.012507445178925991\n",
      "Training iter #15000:   Batch Loss = 14.715063, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.733450889587402, Accuracy = 0.012507445178925991\n",
      "Training iter #18000:   Batch Loss = 14.611853, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.725150108337402, Accuracy = 0.012507445178925991\n",
      "Training iter #21000:   Batch Loss = 14.687437, Accuracy = 0.01133333332836628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.717278480529785, Accuracy = 0.012507445178925991\n",
      "Training iter #24000:   Batch Loss = 14.711696, Accuracy = 0.0033333334140479565\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.709964752197266, Accuracy = 0.012507445178925991\n",
      "Training iter #27000:   Batch Loss = 14.658747, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.702651023864746, Accuracy = 0.012507445178925991\n",
      "Training iter #30000:   Batch Loss = 14.723042, Accuracy = 0.008666666224598885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.694880485534668, Accuracy = 0.013103037141263485\n",
      "Training iter #33000:   Batch Loss = 14.694685, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.687171936035156, Accuracy = 0.013103037141263485\n",
      "Training iter #36000:   Batch Loss = 14.688845, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.679609298706055, Accuracy = 0.013103037141263485\n",
      "Training iter #39000:   Batch Loss = 14.864838, Accuracy = 0.004666666500270367\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.672185897827148, Accuracy = 0.013103037141263485\n",
      "Training iter #42000:   Batch Loss = 14.622827, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.664684295654297, Accuracy = 0.013103037141263485\n",
      "Training iter #45000:   Batch Loss = 14.533459, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.657541275024414, Accuracy = 0.013103037141263485\n",
      "Training iter #48000:   Batch Loss = 14.654470, Accuracy = 0.009333333000540733\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.650379180908203, Accuracy = 0.012507445178925991\n",
      "Training iter #51000:   Batch Loss = 14.627065, Accuracy = 0.0033333334140479565\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.643549919128418, Accuracy = 0.01220964826643467\n",
      "Training iter #54000:   Batch Loss = 14.664745, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.636548042297363, Accuracy = 0.01220964826643467\n",
      "Training iter #57000:   Batch Loss = 14.630605, Accuracy = 0.009333333000540733\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.629461288452148, Accuracy = 0.012507445178925991\n",
      "Training iter #60000:   Batch Loss = 14.656679, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.622432708740234, Accuracy = 0.01220964826643467\n",
      "Training iter #63000:   Batch Loss = 14.640061, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.615571022033691, Accuracy = 0.01220964826643467\n",
      "Training iter #66000:   Batch Loss = 14.607331, Accuracy = 0.004000000189989805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.608467102050781, Accuracy = 0.01220964826643467\n",
      "Training iter #69000:   Batch Loss = 14.524093, Accuracy = 0.019333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.601256370544434, Accuracy = 0.01220964826643467\n",
      "Training iter #72000:   Batch Loss = 14.439043, Accuracy = 0.019333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.594210624694824, Accuracy = 0.011911852285265923\n",
      "Training iter #75000:   Batch Loss = 14.609278, Accuracy = 0.009333333000540733\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.587516784667969, Accuracy = 0.01220964826643467\n",
      "Training iter #78000:   Batch Loss = 14.547857, Accuracy = 0.0033333334140479565\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.580881118774414, Accuracy = 0.013103037141263485\n",
      "Training iter #81000:   Batch Loss = 14.629343, Accuracy = 0.019999999552965164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.573944091796875, Accuracy = 0.013103037141263485\n",
      "Training iter #84000:   Batch Loss = 14.559457, Accuracy = 0.012000000104308128\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.56722640991211, Accuracy = 0.013400834053754807\n",
      "Training iter #87000:   Batch Loss = 14.659775, Accuracy = 0.014000000432133675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.560525894165039, Accuracy = 0.0139964260160923\n",
      "Training iter #90000:   Batch Loss = 14.576078, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.553966522216797, Accuracy = 0.0139964260160923\n",
      "Training iter #93000:   Batch Loss = 14.580883, Accuracy = 0.005333333276212215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.547508239746094, Accuracy = 0.0139964260160923\n",
      "Training iter #96000:   Batch Loss = 14.572475, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.540807723999023, Accuracy = 0.0139964260160923\n",
      "Training iter #99000:   Batch Loss = 14.412687, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.533990859985352, Accuracy = 0.0139964260160923\n",
      "Training iter #102000:   Batch Loss = 14.494624, Accuracy = 0.01066666655242443\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.527441024780273, Accuracy = 0.0139964260160923\n",
      "Training iter #105000:   Batch Loss = 14.378897, Accuracy = 0.007333333138376474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.52103042602539, Accuracy = 0.014294222928583622\n",
      "Training iter #108000:   Batch Loss = 14.493543, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.514659881591797, Accuracy = 0.015485407784581184\n",
      "Training iter #111000:   Batch Loss = 14.466599, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.508125305175781, Accuracy = 0.015485407784581184\n",
      "Training iter #114000:   Batch Loss = 14.637777, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.501611709594727, Accuracy = 0.01608099974691868\n",
      "Training iter #117000:   Batch Loss = 14.571083, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.495481491088867, Accuracy = 0.01608099974691868\n",
      "Training iter #120000:   Batch Loss = 14.431405, Accuracy = 0.01066666655242443\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.489248275756836, Accuracy = 0.01608099974691868\n",
      "Training iter #123000:   Batch Loss = 14.482399, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.482807159423828, Accuracy = 0.01608099974691868\n",
      "Training iter #126000:   Batch Loss = 14.331215, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.476249694824219, Accuracy = 0.01608099974691868\n",
      "Training iter #129000:   Batch Loss = 14.415516, Accuracy = 0.006666666828095913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.47011947631836, Accuracy = 0.01608099974691868\n",
      "Training iter #132000:   Batch Loss = 14.454256, Accuracy = 0.007333333138376474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.46417236328125, Accuracy = 0.016378797590732574\n",
      "Training iter #135000:   Batch Loss = 14.442257, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.457988739013672, Accuracy = 0.01608099974691868\n",
      "Training iter #138000:   Batch Loss = 14.396130, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.451517105102539, Accuracy = 0.016378797590732574\n",
      "Training iter #141000:   Batch Loss = 14.495909, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.445243835449219, Accuracy = 0.01667659357190132\n",
      "Training iter #144000:   Batch Loss = 14.440886, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.439489364624023, Accuracy = 0.01667659357190132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #147000:   Batch Loss = 14.376858, Accuracy = 0.012666666880249977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.433679580688477, Accuracy = 0.01697438955307007\n",
      "Training iter #150000:   Batch Loss = 14.426044, Accuracy = 0.01066666655242443\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.427679061889648, Accuracy = 0.017272185534238815\n",
      "Training iter #153000:   Batch Loss = 14.309346, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.421730041503906, Accuracy = 0.017272185534238815\n",
      "Training iter #156000:   Batch Loss = 14.355025, Accuracy = 0.007333333138376474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.415919303894043, Accuracy = 0.017569981515407562\n",
      "Training iter #159000:   Batch Loss = 14.548758, Accuracy = 0.007333333138376474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.410045623779297, Accuracy = 0.01786777935922146\n",
      "Training iter #162000:   Batch Loss = 14.373463, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.403999328613281, Accuracy = 0.01786777935922146\n",
      "Training iter #165000:   Batch Loss = 14.307354, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.397829055786133, Accuracy = 0.01786777935922146\n",
      "Training iter #168000:   Batch Loss = 14.453539, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.392045974731445, Accuracy = 0.01786777935922146\n",
      "Training iter #171000:   Batch Loss = 14.208004, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.38644790649414, Accuracy = 0.018463371321558952\n",
      "Training iter #174000:   Batch Loss = 14.327888, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.380799293518066, Accuracy = 0.018463371321558952\n",
      "Training iter #177000:   Batch Loss = 14.448368, Accuracy = 0.01066666655242443\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.375009536743164, Accuracy = 0.0187611673027277\n",
      "Training iter #180000:   Batch Loss = 14.300005, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.369277954101562, Accuracy = 0.019058963283896446\n",
      "Training iter #183000:   Batch Loss = 14.230342, Accuracy = 0.01066666655242443\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.363611221313477, Accuracy = 0.019356759265065193\n",
      "Training iter #186000:   Batch Loss = 14.608318, Accuracy = 0.00800000037997961\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.358112335205078, Accuracy = 0.019356759265065193\n",
      "Training iter #189000:   Batch Loss = 14.397366, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.352523803710938, Accuracy = 0.01965455710887909\n",
      "Training iter #192000:   Batch Loss = 14.262147, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.346965789794922, Accuracy = 0.01965455710887909\n",
      "Training iter #195000:   Batch Loss = 14.467791, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.341663360595703, Accuracy = 0.01965455710887909\n",
      "Training iter #198000:   Batch Loss = 14.260345, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.336427688598633, Accuracy = 0.019952353090047836\n",
      "Training iter #201000:   Batch Loss = 14.123246, Accuracy = 0.030666666105389595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.331024169921875, Accuracy = 0.019952353090047836\n",
      "Training iter #204000:   Batch Loss = 14.318405, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.325490951538086, Accuracy = 0.020250149071216583\n",
      "Training iter #207000:   Batch Loss = 14.284413, Accuracy = 0.012000000104308128\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.32000732421875, Accuracy = 0.019952353090047836\n",
      "Training iter #210000:   Batch Loss = 14.141113, Accuracy = 0.01066666655242443\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.314679145812988, Accuracy = 0.01965455710887909\n",
      "Training iter #213000:   Batch Loss = 14.270535, Accuracy = 0.009999999776482582\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.309404373168945, Accuracy = 0.01965455710887909\n",
      "Training iter #216000:   Batch Loss = 14.404627, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.30396842956543, Accuracy = 0.01965455710887909\n",
      "Training iter #219000:   Batch Loss = 14.219713, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.298255920410156, Accuracy = 0.01965455710887909\n",
      "Training iter #222000:   Batch Loss = 14.398775, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.29301643371582, Accuracy = 0.01965455710887909\n",
      "Training iter #225000:   Batch Loss = 14.173384, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.287883758544922, Accuracy = 0.01965455710887909\n",
      "Training iter #228000:   Batch Loss = 14.272189, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.282654762268066, Accuracy = 0.01965455710887909\n",
      "Training iter #231000:   Batch Loss = 14.210129, Accuracy = 0.012666666880249977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.277349472045898, Accuracy = 0.01965455710887909\n",
      "Training iter #234000:   Batch Loss = 14.218820, Accuracy = 0.014000000432133675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.271921157836914, Accuracy = 0.01965455710887909\n",
      "Training iter #237000:   Batch Loss = 14.091265, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.266916275024414, Accuracy = 0.01965455710887909\n",
      "Training iter #240000:   Batch Loss = 14.186136, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.261861801147461, Accuracy = 0.01965455710887909\n",
      "Training iter #243000:   Batch Loss = 14.272823, Accuracy = 0.020666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.256690979003906, Accuracy = 0.019952353090047836\n",
      "Training iter #246000:   Batch Loss = 14.176588, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.251533508300781, Accuracy = 0.020250149071216583\n",
      "Training iter #249000:   Batch Loss = 14.324825, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.246505737304688, Accuracy = 0.02054794505238533\n",
      "Training iter #252000:   Batch Loss = 14.286537, Accuracy = 0.009999999776482582\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.241576194763184, Accuracy = 0.02054794505238533\n",
      "Training iter #255000:   Batch Loss = 14.204374, Accuracy = 0.029999999329447746\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.23660659790039, Accuracy = 0.02054794505238533\n",
      "Training iter #258000:   Batch Loss = 14.237232, Accuracy = 0.009999999776482582\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.231368064880371, Accuracy = 0.020250149071216583\n",
      "Training iter #261000:   Batch Loss = 14.133505, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.226299285888672, Accuracy = 0.02054794505238533\n",
      "Training iter #264000:   Batch Loss = 14.084318, Accuracy = 0.026000000536441803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.221407890319824, Accuracy = 0.02054794505238533\n",
      "Training iter #267000:   Batch Loss = 14.064547, Accuracy = 0.01133333332836628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.216541290283203, Accuracy = 0.02054794505238533\n",
      "Training iter #270000:   Batch Loss = 14.098022, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.211504936218262, Accuracy = 0.02054794505238533\n",
      "Training iter #273000:   Batch Loss = 14.083775, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.206308364868164, Accuracy = 0.02054794505238533\n",
      "Training iter #276000:   Batch Loss = 14.260729, Accuracy = 0.020666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.201242446899414, Accuracy = 0.020845741033554077\n",
      "Training iter #279000:   Batch Loss = 14.240068, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.196367263793945, Accuracy = 0.020845741033554077\n",
      "Training iter #282000:   Batch Loss = 14.126543, Accuracy = 0.03266666829586029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.191523551940918, Accuracy = 0.021143537014722824\n",
      "Training iter #285000:   Batch Loss = 14.195593, Accuracy = 0.01133333332836628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.186552047729492, Accuracy = 0.021143537014722824\n",
      "Training iter #288000:   Batch Loss = 14.101313, Accuracy = 0.018666666001081467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.181580543518066, Accuracy = 0.021143537014722824\n",
      "Training iter #291000:   Batch Loss = 14.015959, Accuracy = 0.024666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.176776885986328, Accuracy = 0.020845741033554077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #294000:   Batch Loss = 14.077900, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.171911239624023, Accuracy = 0.020845741033554077\n",
      "Training iter #297000:   Batch Loss = 14.162217, Accuracy = 0.01133333332836628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.166937828063965, Accuracy = 0.020845741033554077\n",
      "Training iter #300000:   Batch Loss = 13.973831, Accuracy = 0.019999999552965164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.162130355834961, Accuracy = 0.021143537014722824\n",
      "Training iter #303000:   Batch Loss = 14.260658, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.157777786254883, Accuracy = 0.021143537014722824\n",
      "Training iter #306000:   Batch Loss = 14.132139, Accuracy = 0.02199999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.153092384338379, Accuracy = 0.021143537014722824\n",
      "Training iter #309000:   Batch Loss = 14.016766, Accuracy = 0.03799999877810478\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.148260116577148, Accuracy = 0.021143537014722824\n",
      "Training iter #312000:   Batch Loss = 14.151766, Accuracy = 0.012000000104308128\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.143024444580078, Accuracy = 0.021143537014722824\n",
      "Training iter #315000:   Batch Loss = 14.114857, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.13803482055664, Accuracy = 0.021143537014722824\n",
      "Training iter #318000:   Batch Loss = 14.069130, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.133504867553711, Accuracy = 0.021143537014722824\n",
      "Training iter #321000:   Batch Loss = 14.017799, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.128999710083008, Accuracy = 0.021143537014722824\n",
      "Training iter #324000:   Batch Loss = 14.056980, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.124309539794922, Accuracy = 0.021143537014722824\n",
      "Training iter #327000:   Batch Loss = 13.950184, Accuracy = 0.019999999552965164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.119352340698242, Accuracy = 0.021143537014722824\n",
      "Training iter #330000:   Batch Loss = 14.129581, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.11430835723877, Accuracy = 0.021143537014722824\n",
      "Training iter #333000:   Batch Loss = 14.053965, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.109711647033691, Accuracy = 0.021143537014722824\n",
      "Training iter #336000:   Batch Loss = 13.947841, Accuracy = 0.04333333298563957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.104904174804688, Accuracy = 0.021143537014722824\n",
      "Training iter #339000:   Batch Loss = 14.091129, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.099855422973633, Accuracy = 0.021143537014722824\n",
      "Training iter #342000:   Batch Loss = 14.118122, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.094623565673828, Accuracy = 0.021143537014722824\n",
      "Training iter #345000:   Batch Loss = 14.053067, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.08983039855957, Accuracy = 0.021143537014722824\n",
      "Training iter #348000:   Batch Loss = 14.037037, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.085229873657227, Accuracy = 0.02144133485853672\n",
      "Training iter #351000:   Batch Loss = 14.002388, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.080595016479492, Accuracy = 0.02144133485853672\n",
      "Training iter #354000:   Batch Loss = 13.952629, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.075772285461426, Accuracy = 0.021739130839705467\n",
      "Training iter #357000:   Batch Loss = 14.120157, Accuracy = 0.018666666001081467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.071044921875, Accuracy = 0.021739130839705467\n",
      "Training iter #360000:   Batch Loss = 13.874548, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.066446304321289, Accuracy = 0.021739130839705467\n",
      "Training iter #363000:   Batch Loss = 13.902746, Accuracy = 0.04466666653752327\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.06180477142334, Accuracy = 0.021739130839705467\n",
      "Training iter #366000:   Batch Loss = 14.056814, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.056995391845703, Accuracy = 0.021739130839705467\n",
      "Training iter #369000:   Batch Loss = 14.094915, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.05218505859375, Accuracy = 0.021739130839705467\n",
      "Training iter #372000:   Batch Loss = 13.947721, Accuracy = 0.02866666577756405\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.047607421875, Accuracy = 0.021739130839705467\n",
      "Training iter #375000:   Batch Loss = 13.983025, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.043140411376953, Accuracy = 0.021739130839705467\n",
      "Training iter #378000:   Batch Loss = 13.969412, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.038633346557617, Accuracy = 0.02144133485853672\n",
      "Training iter #381000:   Batch Loss = 13.882870, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.033806800842285, Accuracy = 0.021739130839705467\n",
      "Training iter #384000:   Batch Loss = 14.029000, Accuracy = 0.020666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.028915405273438, Accuracy = 0.022036926820874214\n",
      "Training iter #387000:   Batch Loss = 13.813416, Accuracy = 0.03533333167433739\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.024431228637695, Accuracy = 0.022036926820874214\n",
      "Training iter #390000:   Batch Loss = 14.027334, Accuracy = 0.03533333167433739\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.01991081237793, Accuracy = 0.022036926820874214\n",
      "Training iter #393000:   Batch Loss = 14.004215, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.015254974365234, Accuracy = 0.02233472280204296\n",
      "Training iter #396000:   Batch Loss = 14.111366, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.010388374328613, Accuracy = 0.02233472280204296\n",
      "Training iter #399000:   Batch Loss = 13.955235, Accuracy = 0.02866666577756405\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.005762100219727, Accuracy = 0.02233472280204296\n",
      "Training iter #402000:   Batch Loss = 13.832878, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 14.001255989074707, Accuracy = 0.02233472280204296\n",
      "Training iter #405000:   Batch Loss = 13.913877, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.99681282043457, Accuracy = 0.02233472280204296\n",
      "Training iter #408000:   Batch Loss = 13.814095, Accuracy = 0.019999999552965164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.992149353027344, Accuracy = 0.02233472280204296\n",
      "Training iter #411000:   Batch Loss = 14.019024, Accuracy = 0.018666666001081467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.987573623657227, Accuracy = 0.022632518783211708\n",
      "Training iter #414000:   Batch Loss = 13.793839, Accuracy = 0.04266666620969772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.98318862915039, Accuracy = 0.022632518783211708\n",
      "Training iter #417000:   Batch Loss = 13.856655, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.978899955749512, Accuracy = 0.022632518783211708\n",
      "Training iter #420000:   Batch Loss = 13.873199, Accuracy = 0.018666666001081467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.974557876586914, Accuracy = 0.022632518783211708\n",
      "Training iter #423000:   Batch Loss = 14.148016, Accuracy = 0.013333333656191826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.969898223876953, Accuracy = 0.022632518783211708\n",
      "Training iter #426000:   Batch Loss = 13.963881, Accuracy = 0.029333332553505898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.965546607971191, Accuracy = 0.022632518783211708\n",
      "Training iter #429000:   Batch Loss = 13.845451, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.961197853088379, Accuracy = 0.022632518783211708\n",
      "Training iter #432000:   Batch Loss = 13.928927, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.956781387329102, Accuracy = 0.022930314764380455\n",
      "Training iter #435000:   Batch Loss = 13.785076, Accuracy = 0.018666666001081467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.952186584472656, Accuracy = 0.022930314764380455\n",
      "Training iter #438000:   Batch Loss = 13.831593, Accuracy = 0.019999999552965164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.947467803955078, Accuracy = 0.02322811260819435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #441000:   Batch Loss = 13.789915, Accuracy = 0.04600000008940697\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.943159103393555, Accuracy = 0.02322811260819435\n",
      "Training iter #444000:   Batch Loss = 13.832371, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.939069747924805, Accuracy = 0.02322811260819435\n",
      "Training iter #447000:   Batch Loss = 13.892411, Accuracy = 0.018666666001081467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.934985160827637, Accuracy = 0.02322811260819435\n",
      "Training iter #450000:   Batch Loss = 14.078363, Accuracy = 0.02199999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.930371284484863, Accuracy = 0.02322811260819435\n",
      "Training iter #453000:   Batch Loss = 13.909798, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.92624568939209, Accuracy = 0.02322811260819435\n",
      "Training iter #456000:   Batch Loss = 13.762177, Accuracy = 0.02800000086426735\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.921817779541016, Accuracy = 0.02322811260819435\n",
      "Training iter #459000:   Batch Loss = 13.852603, Accuracy = 0.019333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.917301177978516, Accuracy = 0.02322811260819435\n",
      "Training iter #462000:   Batch Loss = 13.807138, Accuracy = 0.020666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.912866592407227, Accuracy = 0.02322811260819435\n",
      "Training iter #465000:   Batch Loss = 13.850593, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.908591270446777, Accuracy = 0.02322811260819435\n",
      "Training iter #468000:   Batch Loss = 13.813649, Accuracy = 0.04266666620969772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.904464721679688, Accuracy = 0.02322811260819435\n",
      "Training iter #471000:   Batch Loss = 13.747430, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.900224685668945, Accuracy = 0.02322811260819435\n",
      "Training iter #474000:   Batch Loss = 13.857064, Accuracy = 0.01600000075995922\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.89590835571289, Accuracy = 0.023525908589363098\n",
      "Training iter #477000:   Batch Loss = 14.034692, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.891633987426758, Accuracy = 0.023525908589363098\n",
      "Training iter #480000:   Batch Loss = 13.711353, Accuracy = 0.04933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.887528419494629, Accuracy = 0.023823704570531845\n",
      "Training iter #483000:   Batch Loss = 13.692077, Accuracy = 0.02866666577756405\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.883434295654297, Accuracy = 0.024121500551700592\n",
      "Training iter #486000:   Batch Loss = 13.886106, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.879075050354004, Accuracy = 0.02441929653286934\n",
      "Training iter #489000:   Batch Loss = 13.776321, Accuracy = 0.02133333310484886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.87472915649414, Accuracy = 0.02441929653286934\n",
      "Training iter #492000:   Batch Loss = 13.803064, Accuracy = 0.02133333310484886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.870536804199219, Accuracy = 0.02441929653286934\n",
      "Training iter #495000:   Batch Loss = 13.904692, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.866456031799316, Accuracy = 0.02441929653286934\n",
      "Training iter #498000:   Batch Loss = 13.723366, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.862367630004883, Accuracy = 0.02441929653286934\n",
      "Training iter #501000:   Batch Loss = 13.783915, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.858311653137207, Accuracy = 0.024717094376683235\n",
      "Training iter #504000:   Batch Loss = 13.987104, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.854377746582031, Accuracy = 0.024717094376683235\n",
      "Training iter #507000:   Batch Loss = 13.613369, Accuracy = 0.04399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.850542068481445, Accuracy = 0.024717094376683235\n",
      "Training iter #510000:   Batch Loss = 13.571885, Accuracy = 0.03533333167433739\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.846480369567871, Accuracy = 0.024717094376683235\n",
      "Training iter #513000:   Batch Loss = 13.824860, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.84231948852539, Accuracy = 0.024717094376683235\n",
      "Training iter #516000:   Batch Loss = 13.745745, Accuracy = 0.019999999552965164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.838041305541992, Accuracy = 0.02441929653286934\n",
      "Training iter #519000:   Batch Loss = 13.707852, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.83399772644043, Accuracy = 0.024121500551700592\n",
      "Training iter #522000:   Batch Loss = 13.847283, Accuracy = 0.02133333310484886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.829898834228516, Accuracy = 0.024121500551700592\n",
      "Training iter #525000:   Batch Loss = 13.821661, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.825762748718262, Accuracy = 0.02441929653286934\n",
      "Training iter #528000:   Batch Loss = 13.736931, Accuracy = 0.019333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.821603775024414, Accuracy = 0.02441929653286934\n",
      "Training iter #531000:   Batch Loss = 13.978657, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.817590713500977, Accuracy = 0.02441929653286934\n",
      "Training iter #534000:   Batch Loss = 13.651901, Accuracy = 0.047333333641290665\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.813608169555664, Accuracy = 0.02441929653286934\n",
      "Training iter #537000:   Batch Loss = 13.589555, Accuracy = 0.04399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.809593200683594, Accuracy = 0.024717094376683235\n",
      "Training iter #540000:   Batch Loss = 13.772223, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.80549430847168, Accuracy = 0.024717094376683235\n",
      "Training iter #543000:   Batch Loss = 13.770299, Accuracy = 0.02133333310484886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.8013277053833, Accuracy = 0.025014890357851982\n",
      "Training iter #546000:   Batch Loss = 13.666893, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.797338485717773, Accuracy = 0.025014890357851982\n",
      "Training iter #549000:   Batch Loss = 13.668238, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.793359756469727, Accuracy = 0.025014890357851982\n",
      "Training iter #552000:   Batch Loss = 13.768580, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.789400100708008, Accuracy = 0.025014890357851982\n",
      "Training iter #555000:   Batch Loss = 13.721468, Accuracy = 0.019333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.785371780395508, Accuracy = 0.025610482320189476\n",
      "Training iter #558000:   Batch Loss = 13.903436, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.781461715698242, Accuracy = 0.025610482320189476\n",
      "Training iter #561000:   Batch Loss = 13.634346, Accuracy = 0.04933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.777619361877441, Accuracy = 0.025610482320189476\n",
      "Training iter #564000:   Batch Loss = 13.719381, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.773660659790039, Accuracy = 0.025610482320189476\n",
      "Training iter #567000:   Batch Loss = 13.744155, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.769515991210938, Accuracy = 0.025610482320189476\n",
      "Training iter #570000:   Batch Loss = 13.729145, Accuracy = 0.020666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.765249252319336, Accuracy = 0.025908278301358223\n",
      "Training iter #573000:   Batch Loss = 13.586482, Accuracy = 0.03533333167433739\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.76120376586914, Accuracy = 0.025610482320189476\n",
      "Training iter #576000:   Batch Loss = 13.577900, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.757133483886719, Accuracy = 0.025908278301358223\n",
      "Training iter #579000:   Batch Loss = 13.647594, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.75320053100586, Accuracy = 0.026801668107509613\n",
      "Training iter #582000:   Batch Loss = 13.675947, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.749248504638672, Accuracy = 0.02709946408867836\n",
      "Training iter #585000:   Batch Loss = 13.910555, Accuracy = 0.024000000208616257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.74544906616211, Accuracy = 0.027397260069847107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #588000:   Batch Loss = 13.656946, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.74167251586914, Accuracy = 0.027397260069847107\n",
      "Training iter #591000:   Batch Loss = 13.685785, Accuracy = 0.04399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.737854957580566, Accuracy = 0.027397260069847107\n",
      "Training iter #594000:   Batch Loss = 13.753641, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.73404598236084, Accuracy = 0.027397260069847107\n",
      "Training iter #597000:   Batch Loss = 13.703855, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.7301607131958, Accuracy = 0.027397260069847107\n",
      "Training iter #600000:   Batch Loss = 13.582519, Accuracy = 0.03933333232998848\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.72643756866455, Accuracy = 0.027397260069847107\n",
      "Training iter #603000:   Batch Loss = 13.522692, Accuracy = 0.02199999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.72259521484375, Accuracy = 0.027397260069847107\n",
      "Training iter #606000:   Batch Loss = 13.565827, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.718856811523438, Accuracy = 0.027397260069847107\n",
      "Training iter #609000:   Batch Loss = 13.668238, Accuracy = 0.02199999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.715093612670898, Accuracy = 0.02709946408867836\n",
      "Training iter #612000:   Batch Loss = 13.825176, Accuracy = 0.02866666577756405\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.711480140686035, Accuracy = 0.02709946408867836\n",
      "Training iter #615000:   Batch Loss = 13.678075, Accuracy = 0.04333333298563957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.70784854888916, Accuracy = 0.02709946408867836\n",
      "Training iter #618000:   Batch Loss = 13.519236, Accuracy = 0.05066666752099991\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.704106330871582, Accuracy = 0.027397260069847107\n",
      "Training iter #621000:   Batch Loss = 13.742147, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.700325012207031, Accuracy = 0.027397260069847107\n",
      "Training iter #624000:   Batch Loss = 13.650505, Accuracy = 0.018666666001081467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.696505546569824, Accuracy = 0.027397260069847107\n",
      "Training iter #627000:   Batch Loss = 13.548378, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.692876815795898, Accuracy = 0.027397260069847107\n",
      "Training iter #630000:   Batch Loss = 13.484137, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.689218521118164, Accuracy = 0.027397260069847107\n",
      "Training iter #633000:   Batch Loss = 13.629121, Accuracy = 0.024000000208616257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.685544967651367, Accuracy = 0.027695056051015854\n",
      "Training iter #636000:   Batch Loss = 13.574238, Accuracy = 0.02800000086426735\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.681777954101562, Accuracy = 0.027695056051015854\n",
      "Training iter #639000:   Batch Loss = 13.762025, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.678243637084961, Accuracy = 0.0279928520321846\n",
      "Training iter #642000:   Batch Loss = 13.607122, Accuracy = 0.04600000008940697\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.67470932006836, Accuracy = 0.027695056051015854\n",
      "Training iter #645000:   Batch Loss = 13.491362, Accuracy = 0.05400000140070915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.671143531799316, Accuracy = 0.0279928520321846\n",
      "Training iter #648000:   Batch Loss = 13.707062, Accuracy = 0.014000000432133675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.667468070983887, Accuracy = 0.028290649875998497\n",
      "Training iter #651000:   Batch Loss = 13.699097, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.663738250732422, Accuracy = 0.028290649875998497\n",
      "Training iter #654000:   Batch Loss = 13.601952, Accuracy = 0.041999999433755875\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.660181045532227, Accuracy = 0.028588445857167244\n",
      "Training iter #657000:   Batch Loss = 13.456086, Accuracy = 0.02866666577756405\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.656570434570312, Accuracy = 0.028588445857167244\n",
      "Training iter #660000:   Batch Loss = 13.567175, Accuracy = 0.024000000208616257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.652958869934082, Accuracy = 0.028588445857167244\n",
      "Training iter #663000:   Batch Loss = 13.550206, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.649271011352539, Accuracy = 0.028588445857167244\n",
      "Training iter #666000:   Batch Loss = 13.694480, Accuracy = 0.024666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.645672798156738, Accuracy = 0.028588445857167244\n",
      "Training iter #669000:   Batch Loss = 13.473692, Accuracy = 0.05000000074505806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.641969680786133, Accuracy = 0.028588445857167244\n",
      "Training iter #672000:   Batch Loss = 13.426561, Accuracy = 0.058666665107011795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.638379096984863, Accuracy = 0.028588445857167244\n",
      "Training iter #675000:   Batch Loss = 13.587250, Accuracy = 0.017999999225139618\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.634732246398926, Accuracy = 0.02888624183833599\n",
      "Training iter #678000:   Batch Loss = 13.708445, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.631036758422852, Accuracy = 0.029184037819504738\n",
      "Training iter #681000:   Batch Loss = 13.552959, Accuracy = 0.041999999433755875\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.627448081970215, Accuracy = 0.029184037819504738\n",
      "Training iter #684000:   Batch Loss = 13.560867, Accuracy = 0.02199999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.623791694641113, Accuracy = 0.029481833800673485\n",
      "Training iter #687000:   Batch Loss = 13.514484, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.620241165161133, Accuracy = 0.02977963164448738\n",
      "Training iter #690000:   Batch Loss = 13.539772, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.61665153503418, Accuracy = 0.030375223606824875\n",
      "Training iter #693000:   Batch Loss = 13.684661, Accuracy = 0.02133333310484886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.613162994384766, Accuracy = 0.030375223606824875\n",
      "Training iter #696000:   Batch Loss = 13.329439, Accuracy = 0.057999998331069946\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.609628677368164, Accuracy = 0.030375223606824875\n",
      "Training iter #699000:   Batch Loss = 13.449256, Accuracy = 0.0533333346247673\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.606124877929688, Accuracy = 0.030673019587993622\n",
      "Training iter #702000:   Batch Loss = 13.581054, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.602584838867188, Accuracy = 0.03097081556916237\n",
      "Training iter #705000:   Batch Loss = 13.723175, Accuracy = 0.019333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.599002838134766, Accuracy = 0.03097081556916237\n",
      "Training iter #708000:   Batch Loss = 13.567306, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.595516204833984, Accuracy = 0.031268611550331116\n",
      "Training iter #711000:   Batch Loss = 13.382211, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.591968536376953, Accuracy = 0.031268611550331116\n",
      "Training iter #714000:   Batch Loss = 13.488342, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.588460922241211, Accuracy = 0.03156640753149986\n",
      "Training iter #717000:   Batch Loss = 13.478052, Accuracy = 0.024666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.584964752197266, Accuracy = 0.03186420351266861\n",
      "Training iter #720000:   Batch Loss = 13.583351, Accuracy = 0.026000000536441803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.58150863647461, Accuracy = 0.03186420351266861\n",
      "Training iter #723000:   Batch Loss = 13.346181, Accuracy = 0.05999999865889549\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.577997207641602, Accuracy = 0.03186420351266861\n",
      "Training iter #726000:   Batch Loss = 13.480209, Accuracy = 0.04333333298563957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.574508666992188, Accuracy = 0.03156640753149986\n",
      "Training iter #729000:   Batch Loss = 13.562771, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.570320129394531, Accuracy = 0.031268611550331116\n",
      "Training iter #732000:   Batch Loss = 13.708345, Accuracy = 0.015333333052694798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.566757202148438, Accuracy = 0.03156640753149986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #735000:   Batch Loss = 13.501670, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.563222885131836, Accuracy = 0.03186420351266861\n",
      "Training iter #738000:   Batch Loss = 13.335213, Accuracy = 0.03933333232998848\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.559670448303223, Accuracy = 0.03186420351266861\n",
      "Training iter #741000:   Batch Loss = 13.539690, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.556142807006836, Accuracy = 0.03186420351266861\n",
      "Training iter #744000:   Batch Loss = 13.484793, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.55266284942627, Accuracy = 0.03186420351266861\n",
      "Training iter #747000:   Batch Loss = 13.504732, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.54922866821289, Accuracy = 0.03186420351266861\n",
      "Training iter #750000:   Batch Loss = 13.197315, Accuracy = 0.06866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.54580307006836, Accuracy = 0.03216199949383736\n",
      "Training iter #753000:   Batch Loss = 13.391359, Accuracy = 0.041333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.542365074157715, Accuracy = 0.03216199949383736\n",
      "Training iter #756000:   Batch Loss = 13.504698, Accuracy = 0.019999999552965164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.538823127746582, Accuracy = 0.0324597992002964\n",
      "Training iter #759000:   Batch Loss = 13.803047, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.535299301147461, Accuracy = 0.0324597992002964\n",
      "Training iter #762000:   Batch Loss = 13.527927, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.53179931640625, Accuracy = 0.0324597992002964\n",
      "Training iter #765000:   Batch Loss = 13.283180, Accuracy = 0.04333333298563957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.528270721435547, Accuracy = 0.0324597992002964\n",
      "Training iter #768000:   Batch Loss = 13.474835, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.524746894836426, Accuracy = 0.03275759518146515\n",
      "Training iter #771000:   Batch Loss = 13.400364, Accuracy = 0.02866666577756405\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.52127456665039, Accuracy = 0.03275759518146515\n",
      "Training iter #774000:   Batch Loss = 13.428844, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.51786994934082, Accuracy = 0.033055391162633896\n",
      "Training iter #777000:   Batch Loss = 13.281952, Accuracy = 0.05400000140070915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.514387130737305, Accuracy = 0.033055391162633896\n",
      "Training iter #780000:   Batch Loss = 13.368887, Accuracy = 0.04066666588187218\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.510860443115234, Accuracy = 0.03335318714380264\n",
      "Training iter #783000:   Batch Loss = 13.475286, Accuracy = 0.01733333244919777\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.507230758666992, Accuracy = 0.03335318714380264\n",
      "Training iter #786000:   Batch Loss = 13.704617, Accuracy = 0.029333332553505898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.503721237182617, Accuracy = 0.03335318714380264\n",
      "Training iter #789000:   Batch Loss = 13.419800, Accuracy = 0.04866666719317436\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.500199317932129, Accuracy = 0.03365098312497139\n",
      "Training iter #792000:   Batch Loss = 13.272596, Accuracy = 0.04466666653752327\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.496627807617188, Accuracy = 0.03365098312497139\n",
      "Training iter #795000:   Batch Loss = 13.469835, Accuracy = 0.02199999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.49304485321045, Accuracy = 0.03365098312497139\n",
      "Training iter #798000:   Batch Loss = 13.399529, Accuracy = 0.030666666105389595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.489480972290039, Accuracy = 0.03394877910614014\n",
      "Training iter #801000:   Batch Loss = 13.397744, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.486063003540039, Accuracy = 0.03394877910614014\n",
      "Training iter #804000:   Batch Loss = 13.403074, Accuracy = 0.029333332553505898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.482580184936523, Accuracy = 0.03394877910614014\n",
      "Training iter #807000:   Batch Loss = 13.298075, Accuracy = 0.052000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.479032516479492, Accuracy = 0.03394877910614014\n",
      "Training iter #810000:   Batch Loss = 13.406792, Accuracy = 0.01666666753590107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.47546100616455, Accuracy = 0.034246575087308884\n",
      "Training iter #813000:   Batch Loss = 13.680326, Accuracy = 0.029333332553505898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.472049713134766, Accuracy = 0.034246575087308884\n",
      "Training iter #816000:   Batch Loss = 13.221846, Accuracy = 0.07133333384990692\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.46865463256836, Accuracy = 0.034246575087308884\n",
      "Training iter #819000:   Batch Loss = 13.275370, Accuracy = 0.04533333331346512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.46523666381836, Accuracy = 0.03454437106847763\n",
      "Training iter #822000:   Batch Loss = 13.472345, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.461799621582031, Accuracy = 0.03454437106847763\n",
      "Training iter #825000:   Batch Loss = 13.403033, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.458383560180664, Accuracy = 0.03484216704964638\n",
      "Training iter #828000:   Batch Loss = 13.350000, Accuracy = 0.03266666829586029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.454971313476562, Accuracy = 0.03484216704964638\n",
      "Training iter #831000:   Batch Loss = 13.494101, Accuracy = 0.026000000536441803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.451501846313477, Accuracy = 0.03484216704964638\n",
      "Training iter #834000:   Batch Loss = 13.374195, Accuracy = 0.04866666719317436\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.447917938232422, Accuracy = 0.03484216704964638\n",
      "Training iter #837000:   Batch Loss = 13.359280, Accuracy = 0.020666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.444324493408203, Accuracy = 0.035139963030815125\n",
      "Training iter #840000:   Batch Loss = 13.624102, Accuracy = 0.029999999329447746\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.440884590148926, Accuracy = 0.035139963030815125\n",
      "Training iter #843000:   Batch Loss = 13.190834, Accuracy = 0.07133333384990692\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.437482833862305, Accuracy = 0.03573555871844292\n",
      "Training iter #846000:   Batch Loss = 13.095835, Accuracy = 0.06800000369548798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.434045791625977, Accuracy = 0.03573555871844292\n",
      "Training iter #849000:   Batch Loss = 13.443165, Accuracy = 0.026000000536441803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.430574417114258, Accuracy = 0.03633115068078041\n",
      "Training iter #852000:   Batch Loss = 13.405411, Accuracy = 0.02133333310484886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.42718505859375, Accuracy = 0.03662894666194916\n",
      "Training iter #855000:   Batch Loss = 13.273232, Accuracy = 0.03200000151991844\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.423885345458984, Accuracy = 0.03662894666194916\n",
      "Training iter #858000:   Batch Loss = 13.276848, Accuracy = 0.03266666829586029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.420507431030273, Accuracy = 0.03722453862428665\n",
      "Training iter #861000:   Batch Loss = 13.418985, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.41703987121582, Accuracy = 0.03722453862428665\n",
      "Training iter #864000:   Batch Loss = 13.346889, Accuracy = 0.024000000208616257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.413511276245117, Accuracy = 0.0375223346054554\n",
      "Training iter #867000:   Batch Loss = 13.611901, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.410078048706055, Accuracy = 0.0375223346054554\n",
      "Training iter #870000:   Batch Loss = 13.158061, Accuracy = 0.0806666687130928\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.40666675567627, Accuracy = 0.0375223346054554\n",
      "Training iter #873000:   Batch Loss = 13.242918, Accuracy = 0.05999999865889549\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.40322208404541, Accuracy = 0.0375223346054554\n",
      "Training iter #876000:   Batch Loss = 13.344410, Accuracy = 0.02800000086426735\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.399723052978516, Accuracy = 0.037820130586624146\n",
      "Training iter #879000:   Batch Loss = 13.376886, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.39630126953125, Accuracy = 0.037820130586624146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #882000:   Batch Loss = 13.278439, Accuracy = 0.03266666829586029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.393027305603027, Accuracy = 0.037820130586624146\n",
      "Training iter #885000:   Batch Loss = 13.153035, Accuracy = 0.030666666105389595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.389694213867188, Accuracy = 0.037820130586624146\n",
      "Training iter #888000:   Batch Loss = 13.301056, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.386260032653809, Accuracy = 0.0375223346054554\n",
      "Training iter #891000:   Batch Loss = 13.306382, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.382835388183594, Accuracy = 0.037820130586624146\n",
      "Training iter #894000:   Batch Loss = 13.562590, Accuracy = 0.03400000184774399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.379554748535156, Accuracy = 0.037820130586624146\n",
      "Training iter #897000:   Batch Loss = 13.166496, Accuracy = 0.07733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.376333236694336, Accuracy = 0.03811792656779289\n",
      "Training iter #900000:   Batch Loss = 13.254030, Accuracy = 0.06800000369548798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.373075485229492, Accuracy = 0.03841572254896164\n",
      "Training iter #903000:   Batch Loss = 13.341253, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.369729042053223, Accuracy = 0.03841572254896164\n",
      "Training iter #906000:   Batch Loss = 13.330683, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.366455078125, Accuracy = 0.03841572254896164\n",
      "Training iter #909000:   Batch Loss = 13.222433, Accuracy = 0.0533333346247673\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.363231658935547, Accuracy = 0.038713518530130386\n",
      "Training iter #912000:   Batch Loss = 13.042371, Accuracy = 0.030666666105389595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.359962463378906, Accuracy = 0.03901131451129913\n",
      "Training iter #915000:   Batch Loss = 13.182888, Accuracy = 0.03733333200216293\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.356592178344727, Accuracy = 0.03901131451129913\n",
      "Training iter #918000:   Batch Loss = 13.274761, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.353219985961914, Accuracy = 0.03901131451129913\n",
      "Training iter #921000:   Batch Loss = 13.554077, Accuracy = 0.03133333474397659\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.34998607635498, Accuracy = 0.03901131451129913\n",
      "Training iter #924000:   Batch Loss = 13.216694, Accuracy = 0.07066666334867477\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.346765518188477, Accuracy = 0.03901131451129913\n",
      "Training iter #927000:   Batch Loss = 13.210841, Accuracy = 0.07400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.343498229980469, Accuracy = 0.03901131451129913\n",
      "Training iter #930000:   Batch Loss = 13.316236, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.340147972106934, Accuracy = 0.03901131451129913\n",
      "Training iter #933000:   Batch Loss = 13.311045, Accuracy = 0.02266666665673256\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.336885452270508, Accuracy = 0.03901131451129913\n",
      "Training iter #936000:   Batch Loss = 13.139900, Accuracy = 0.052666667848825455\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.333707809448242, Accuracy = 0.03901131451129913\n",
      "Training iter #939000:   Batch Loss = 13.064461, Accuracy = 0.03466666489839554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.330500602722168, Accuracy = 0.039606910198926926\n",
      "Training iter #942000:   Batch Loss = 13.241513, Accuracy = 0.03200000151991844\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.327215194702148, Accuracy = 0.039606910198926926\n",
      "Training iter #945000:   Batch Loss = 13.253332, Accuracy = 0.029999999329447746\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.323915481567383, Accuracy = 0.03930911421775818\n",
      "Training iter #948000:   Batch Loss = 13.506726, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.320747375488281, Accuracy = 0.03901131451129913\n",
      "Training iter #951000:   Batch Loss = 13.201910, Accuracy = 0.06933332979679108\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.317611694335938, Accuracy = 0.03901131451129913\n",
      "Training iter #954000:   Batch Loss = 13.118885, Accuracy = 0.07066666334867477\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.31445026397705, Accuracy = 0.03930911421775818\n",
      "Training iter #957000:   Batch Loss = 13.277203, Accuracy = 0.026000000536441803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.311243057250977, Accuracy = 0.040500298142433167\n",
      "Training iter #960000:   Batch Loss = 13.351405, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.308126449584961, Accuracy = 0.040500298142433167\n",
      "Training iter #963000:   Batch Loss = 13.153358, Accuracy = 0.052000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.305047988891602, Accuracy = 0.040500298142433167\n",
      "Training iter #966000:   Batch Loss = 13.029531, Accuracy = 0.04066666588187218\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.302013397216797, Accuracy = 0.040500298142433167\n",
      "Training iter #969000:   Batch Loss = 13.203878, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.29880142211914, Accuracy = 0.040500298142433167\n",
      "Training iter #972000:   Batch Loss = 13.208447, Accuracy = 0.02800000086426735\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.295526504516602, Accuracy = 0.04079809412360191\n",
      "Training iter #975000:   Batch Loss = 13.389273, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.29249095916748, Accuracy = 0.04079809412360191\n",
      "Training iter #978000:   Batch Loss = 13.116423, Accuracy = 0.07000000029802322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.289508819580078, Accuracy = 0.04079809412360191\n",
      "Training iter #981000:   Batch Loss = 13.082351, Accuracy = 0.07599999755620956\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.286399841308594, Accuracy = 0.04139368608593941\n",
      "Training iter #984000:   Batch Loss = 13.213451, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.283258438110352, Accuracy = 0.041691482067108154\n",
      "Training iter #987000:   Batch Loss = 13.356682, Accuracy = 0.02199999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.280126571655273, Accuracy = 0.0419892780482769\n",
      "Training iter #990000:   Batch Loss = 13.238939, Accuracy = 0.052000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.277120590209961, Accuracy = 0.0419892780482769\n",
      "Training iter #993000:   Batch Loss = 13.050653, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.27413558959961, Accuracy = 0.0419892780482769\n",
      "Training iter #996000:   Batch Loss = 13.218401, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.271072387695312, Accuracy = 0.0419892780482769\n",
      "Training iter #999000:   Batch Loss = 13.203710, Accuracy = 0.024000000208616257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.26801872253418, Accuracy = 0.0419892780482769\n",
      "Training iter #1002000:   Batch Loss = 13.349098, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.265080451965332, Accuracy = 0.041691482067108154\n",
      "Training iter #1005000:   Batch Loss = 12.982628, Accuracy = 0.07999999821186066\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.262198448181152, Accuracy = 0.0419892780482769\n",
      "Training iter #1008000:   Batch Loss = 12.998518, Accuracy = 0.0820000022649765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.259217262268066, Accuracy = 0.04228707402944565\n",
      "Training iter #1011000:   Batch Loss = 13.202204, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.256195068359375, Accuracy = 0.04228707402944565\n",
      "Training iter #1014000:   Batch Loss = 13.359827, Accuracy = 0.02800000086426735\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.253249168395996, Accuracy = 0.042584873735904694\n",
      "Training iter #1017000:   Batch Loss = 13.218490, Accuracy = 0.05533333495259285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.250344276428223, Accuracy = 0.04288266971707344\n",
      "Training iter #1020000:   Batch Loss = 13.084362, Accuracy = 0.04466666653752327\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.24740219116211, Accuracy = 0.04318046569824219\n",
      "Training iter #1023000:   Batch Loss = 13.163382, Accuracy = 0.036666665226221085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.244378089904785, Accuracy = 0.04318046569824219\n",
      "Training iter #1026000:   Batch Loss = 13.171102, Accuracy = 0.02800000086426735\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.241365432739258, Accuracy = 0.043478261679410934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1029000:   Batch Loss = 13.298625, Accuracy = 0.03133333474397659\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.238481521606445, Accuracy = 0.04377605766057968\n",
      "Training iter #1032000:   Batch Loss = 12.885097, Accuracy = 0.0820000022649765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.235605239868164, Accuracy = 0.044371649622917175\n",
      "Training iter #1035000:   Batch Loss = 13.100892, Accuracy = 0.06466666609048843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.232673645019531, Accuracy = 0.044371649622917175\n",
      "Training iter #1038000:   Batch Loss = 13.184526, Accuracy = 0.03400000184774399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.229673385620117, Accuracy = 0.044371649622917175\n",
      "Training iter #1041000:   Batch Loss = 13.388087, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.226789474487305, Accuracy = 0.04466944560408592\n",
      "Training iter #1044000:   Batch Loss = 13.157803, Accuracy = 0.052666667848825455\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.22391414642334, Accuracy = 0.04466944560408592\n",
      "Training iter #1047000:   Batch Loss = 12.921701, Accuracy = 0.06133333221077919\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.22097396850586, Accuracy = 0.04466944560408592\n",
      "Training iter #1050000:   Batch Loss = 13.114790, Accuracy = 0.03933333232998848\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.21793270111084, Accuracy = 0.04496724158525467\n",
      "Training iter #1053000:   Batch Loss = 13.142944, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.214879989624023, Accuracy = 0.045265037566423416\n",
      "Training iter #1056000:   Batch Loss = 13.264616, Accuracy = 0.029999999329447746\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.21186637878418, Accuracy = 0.045265037566423416\n",
      "Training iter #1059000:   Batch Loss = 12.896707, Accuracy = 0.08533333241939545\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.208818435668945, Accuracy = 0.045265037566423416\n",
      "Training iter #1062000:   Batch Loss = 13.069242, Accuracy = 0.06733333319425583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.205719947814941, Accuracy = 0.04556283354759216\n",
      "Training iter #1065000:   Batch Loss = 13.136318, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.202552795410156, Accuracy = 0.04556283354759216\n",
      "Training iter #1068000:   Batch Loss = 13.446856, Accuracy = 0.023333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.19953727722168, Accuracy = 0.04556283354759216\n",
      "Training iter #1071000:   Batch Loss = 13.161714, Accuracy = 0.05533333495259285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.196601867675781, Accuracy = 0.04586062952876091\n",
      "Training iter #1074000:   Batch Loss = 12.980754, Accuracy = 0.06066666543483734\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.193639755249023, Accuracy = 0.04586062952876091\n",
      "Training iter #1077000:   Batch Loss = 13.150003, Accuracy = 0.041999999433755875\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.190617561340332, Accuracy = 0.04586062952876091\n",
      "Training iter #1080000:   Batch Loss = 13.112982, Accuracy = 0.03733333200216293\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.187527656555176, Accuracy = 0.046158429235219955\n",
      "Training iter #1083000:   Batch Loss = 13.089972, Accuracy = 0.03799999877810478\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.18454360961914, Accuracy = 0.046158429235219955\n",
      "Training iter #1086000:   Batch Loss = 12.832464, Accuracy = 0.08733333647251129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.181567192077637, Accuracy = 0.046158429235219955\n",
      "Training iter #1089000:   Batch Loss = 13.021433, Accuracy = 0.06400000303983688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.178628921508789, Accuracy = 0.04675402119755745\n",
      "Training iter #1092000:   Batch Loss = 13.101618, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.175651550292969, Accuracy = 0.047051817178726196\n",
      "Training iter #1095000:   Batch Loss = 13.447857, Accuracy = 0.029999999329447746\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.1727933883667, Accuracy = 0.04675402119755745\n",
      "Training iter #1098000:   Batch Loss = 13.099077, Accuracy = 0.06866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.169998168945312, Accuracy = 0.04675402119755745\n",
      "Training iter #1101000:   Batch Loss = 12.889769, Accuracy = 0.06800000369548798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.167145729064941, Accuracy = 0.04734961315989494\n",
      "Training iter #1104000:   Batch Loss = 13.115458, Accuracy = 0.047333333641290665\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.164260864257812, Accuracy = 0.04734961315989494\n",
      "Training iter #1107000:   Batch Loss = 13.109259, Accuracy = 0.036666665226221085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.161392211914062, Accuracy = 0.04794520512223244\n",
      "Training iter #1110000:   Batch Loss = 13.099934, Accuracy = 0.025333333760499954\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.158625602722168, Accuracy = 0.048243001103401184\n",
      "Training iter #1113000:   Batch Loss = 12.908656, Accuracy = 0.06133333221077919\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.155845642089844, Accuracy = 0.048243001103401184\n",
      "Training iter #1116000:   Batch Loss = 12.941780, Accuracy = 0.07400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.15298080444336, Accuracy = 0.04854079708456993\n",
      "Training iter #1119000:   Batch Loss = 13.129309, Accuracy = 0.03133333474397659\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.150070190429688, Accuracy = 0.04854079708456993\n",
      "Training iter #1122000:   Batch Loss = 13.380207, Accuracy = 0.036666665226221085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.147260665893555, Accuracy = 0.04883859306573868\n",
      "Training iter #1125000:   Batch Loss = 12.970844, Accuracy = 0.08666666597127914\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.144485473632812, Accuracy = 0.04943418875336647\n",
      "Training iter #1128000:   Batch Loss = 12.829976, Accuracy = 0.07199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.141645431518555, Accuracy = 0.04943418875336647\n",
      "Training iter #1131000:   Batch Loss = 13.148989, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.138727188110352, Accuracy = 0.04943418875336647\n",
      "Training iter #1134000:   Batch Loss = 13.063778, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.135780334472656, Accuracy = 0.050029780715703964\n",
      "Training iter #1137000:   Batch Loss = 13.074915, Accuracy = 0.03133333474397659\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.132963180541992, Accuracy = 0.050029780715703964\n",
      "Training iter #1140000:   Batch Loss = 12.999277, Accuracy = 0.03533333167433739\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.130191802978516, Accuracy = 0.050029780715703964\n",
      "Training iter #1143000:   Batch Loss = 12.924683, Accuracy = 0.0820000022649765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.127376556396484, Accuracy = 0.05062537267804146\n",
      "Training iter #1146000:   Batch Loss = 13.036926, Accuracy = 0.04399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.124550819396973, Accuracy = 0.0515187606215477\n",
      "Training iter #1149000:   Batch Loss = 13.337894, Accuracy = 0.035999998450279236\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.121818542480469, Accuracy = 0.051816556602716446\n",
      "Training iter #1152000:   Batch Loss = 12.798789, Accuracy = 0.09333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.119091987609863, Accuracy = 0.051816556602716446\n",
      "Training iter #1155000:   Batch Loss = 12.794334, Accuracy = 0.0793333351612091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.11629867553711, Accuracy = 0.05211435258388519\n",
      "Training iter #1158000:   Batch Loss = 13.106639, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.113462448120117, Accuracy = 0.05241214856505394\n",
      "Training iter #1161000:   Batch Loss = 13.089867, Accuracy = 0.029999999329447746\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.11069107055664, Accuracy = 0.05300774425268173\n",
      "Training iter #1164000:   Batch Loss = 13.055876, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.108054161071777, Accuracy = 0.05330554023385048\n",
      "Training iter #1167000:   Batch Loss = 13.034413, Accuracy = 0.03533333167433739\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.105408668518066, Accuracy = 0.05330554023385048\n",
      "Training iter #1170000:   Batch Loss = 13.030178, Accuracy = 0.05999999865889549\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.102717399597168, Accuracy = 0.05330554023385048\n",
      "Training iter #1173000:   Batch Loss = 13.007277, Accuracy = 0.04933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.099933624267578, Accuracy = 0.053603336215019226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1176000:   Batch Loss = 13.374081, Accuracy = 0.03733333200216293\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.09721851348877, Accuracy = 0.053603336215019226\n",
      "Training iter #1179000:   Batch Loss = 12.817909, Accuracy = 0.09466666728258133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.094537734985352, Accuracy = 0.053603336215019226\n",
      "Training iter #1182000:   Batch Loss = 12.741248, Accuracy = 0.09266666322946548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.091879844665527, Accuracy = 0.053603336215019226\n",
      "Training iter #1185000:   Batch Loss = 13.050682, Accuracy = 0.04533333331346512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.088866233825684, Accuracy = 0.05390113219618797\n",
      "Training iter #1188000:   Batch Loss = 13.106563, Accuracy = 0.02666666731238365\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.086081504821777, Accuracy = 0.05390113219618797\n",
      "Training iter #1191000:   Batch Loss = 12.971450, Accuracy = 0.04066666588187218\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.083410263061523, Accuracy = 0.05419892817735672\n",
      "Training iter #1194000:   Batch Loss = 12.804539, Accuracy = 0.04600000008940697\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.080757141113281, Accuracy = 0.05419892817735672\n",
      "Training iter #1197000:   Batch Loss = 12.999704, Accuracy = 0.06599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.078086853027344, Accuracy = 0.05419892817735672\n",
      "Training iter #1200000:   Batch Loss = 13.037284, Accuracy = 0.04466666653752327\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.074746131896973, Accuracy = 0.054794520139694214\n",
      "Training iter #1203000:   Batch Loss = 13.276911, Accuracy = 0.04333333298563957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.072118759155273, Accuracy = 0.054794520139694214\n",
      "Training iter #1206000:   Batch Loss = 12.775545, Accuracy = 0.10266666859388351\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.069457054138184, Accuracy = 0.05509231612086296\n",
      "Training iter #1209000:   Batch Loss = 12.926764, Accuracy = 0.08799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.066781044006348, Accuracy = 0.05509231612086296\n",
      "Training iter #1212000:   Batch Loss = 12.990349, Accuracy = 0.04600000008940697\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.06406307220459, Accuracy = 0.055687908083200455\n",
      "Training iter #1215000:   Batch Loss = 13.088814, Accuracy = 0.0273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.06138801574707, Accuracy = 0.0559857040643692\n",
      "Training iter #1218000:   Batch Loss = 12.998645, Accuracy = 0.03733333200216293\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.05882740020752, Accuracy = 0.0559857040643692\n",
      "Training iter #1221000:   Batch Loss = 12.748559, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.056264877319336, Accuracy = 0.0559857040643692\n",
      "Training iter #1224000:   Batch Loss = 12.904282, Accuracy = 0.07199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.05367660522461, Accuracy = 0.05687909573316574\n",
      "Training iter #1227000:   Batch Loss = 13.021746, Accuracy = 0.04533333331346512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.0510892868042, Accuracy = 0.05628350377082825\n",
      "Training iter #1230000:   Batch Loss = 13.249346, Accuracy = 0.04333333298563957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.048602104187012, Accuracy = 0.05628350377082825\n",
      "Training iter #1233000:   Batch Loss = 12.891856, Accuracy = 0.0820000022649765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.04612922668457, Accuracy = 0.056581299751996994\n",
      "Training iter #1236000:   Batch Loss = 12.875679, Accuracy = 0.10199999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.043548583984375, Accuracy = 0.05687909573316574\n",
      "Training iter #1239000:   Batch Loss = 13.012614, Accuracy = 0.04399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.040884017944336, Accuracy = 0.05687909573316574\n",
      "Training iter #1242000:   Batch Loss = 13.034009, Accuracy = 0.03333333507180214\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.038305282592773, Accuracy = 0.05687909573316574\n",
      "Training iter #1245000:   Batch Loss = 12.936172, Accuracy = 0.059333331882953644\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.035516738891602, Accuracy = 0.057474687695503235\n",
      "Training iter #1248000:   Batch Loss = 12.688892, Accuracy = 0.03466666489839554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.033039093017578, Accuracy = 0.057474687695503235\n",
      "Training iter #1251000:   Batch Loss = 12.850543, Accuracy = 0.07000000029802322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.030499458312988, Accuracy = 0.057474687695503235\n",
      "Training iter #1254000:   Batch Loss = 13.023617, Accuracy = 0.04399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.027610778808594, Accuracy = 0.058368075639009476\n",
      "Training iter #1257000:   Batch Loss = 13.202866, Accuracy = 0.046666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.024873733520508, Accuracy = 0.05955926328897476\n",
      "Training iter #1260000:   Batch Loss = 12.895531, Accuracy = 0.07533333450555801\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.022343635559082, Accuracy = 0.060154855251312256\n",
      "Training iter #1263000:   Batch Loss = 12.798237, Accuracy = 0.10533333569765091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.019754409790039, Accuracy = 0.060154855251312256\n",
      "Training iter #1266000:   Batch Loss = 12.976878, Accuracy = 0.04800000041723251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.017112731933594, Accuracy = 0.060154855251312256\n",
      "Training iter #1269000:   Batch Loss = 13.005676, Accuracy = 0.03933333232998848\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.014541625976562, Accuracy = 0.060452651232481\n",
      "Training iter #1272000:   Batch Loss = 12.892459, Accuracy = 0.06199999898672104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.01199722290039, Accuracy = 0.06075044721364975\n",
      "Training iter #1275000:   Batch Loss = 12.678890, Accuracy = 0.052000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.009450912475586, Accuracy = 0.06075044721364975\n",
      "Training iter #1278000:   Batch Loss = 12.926928, Accuracy = 0.06066666543483734\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.006800651550293, Accuracy = 0.06075044721364975\n",
      "Training iter #1281000:   Batch Loss = 12.956776, Accuracy = 0.047333333641290665\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.004005432128906, Accuracy = 0.061346039175987244\n",
      "Training iter #1284000:   Batch Loss = 13.173613, Accuracy = 0.04800000041723251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 13.001331329345703, Accuracy = 0.06194163113832474\n",
      "Training iter #1287000:   Batch Loss = 12.850166, Accuracy = 0.07666666805744171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.998505592346191, Accuracy = 0.06194163113832474\n",
      "Training iter #1290000:   Batch Loss = 12.787922, Accuracy = 0.10333333164453506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.995743751525879, Accuracy = 0.062239427119493484\n",
      "Training iter #1293000:   Batch Loss = 12.949320, Accuracy = 0.05000000074505806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.992685317993164, Accuracy = 0.062239427119493484\n",
      "Training iter #1296000:   Batch Loss = 13.054571, Accuracy = 0.03466666489839554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.990096092224121, Accuracy = 0.062239427119493484\n",
      "Training iter #1299000:   Batch Loss = 12.909245, Accuracy = 0.06466666609048843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.987442016601562, Accuracy = 0.062239427119493484\n",
      "Training iter #1302000:   Batch Loss = 12.665773, Accuracy = 0.052000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.984702110290527, Accuracy = 0.06194163113832474\n",
      "Training iter #1305000:   Batch Loss = 12.896370, Accuracy = 0.06666667014360428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.982061386108398, Accuracy = 0.062239427119493484\n",
      "Training iter #1308000:   Batch Loss = 12.952667, Accuracy = 0.04333333298563957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.979469299316406, Accuracy = 0.06194163113832474\n",
      "Training iter #1311000:   Batch Loss = 13.047880, Accuracy = 0.05000000074505806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.977289199829102, Accuracy = 0.062239427119493484\n",
      "Training iter #1314000:   Batch Loss = 12.765495, Accuracy = 0.07733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.975137710571289, Accuracy = 0.06253722310066223\n",
      "Training iter #1317000:   Batch Loss = 12.699670, Accuracy = 0.1133333370089531\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.973272323608398, Accuracy = 0.062239427119493484\n",
      "Training iter #1320000:   Batch Loss = 12.843121, Accuracy = 0.06066666543483734\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.970640182495117, Accuracy = 0.062239427119493484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1323000:   Batch Loss = 13.107225, Accuracy = 0.03400000184774399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.967944145202637, Accuracy = 0.062239427119493484\n",
      "Training iter #1326000:   Batch Loss = 12.881114, Accuracy = 0.06466666609048843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.965314865112305, Accuracy = 0.06253722310066223\n",
      "Training iter #1329000:   Batch Loss = 12.731508, Accuracy = 0.04933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.962690353393555, Accuracy = 0.06283502280712128\n",
      "Training iter #1332000:   Batch Loss = 12.851210, Accuracy = 0.07066666334867477\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.959897994995117, Accuracy = 0.06283502280712128\n",
      "Training iter #1335000:   Batch Loss = 12.960124, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.957002639770508, Accuracy = 0.06283502280712128\n",
      "Training iter #1338000:   Batch Loss = 13.045839, Accuracy = 0.0560000017285347\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.95431900024414, Accuracy = 0.06313281506299973\n",
      "Training iter #1341000:   Batch Loss = 12.563484, Accuracy = 0.08799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.951715469360352, Accuracy = 0.06343061476945877\n",
      "Training iter #1344000:   Batch Loss = 12.697234, Accuracy = 0.10599999874830246\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.949076652526855, Accuracy = 0.06343061476945877\n",
      "Training iter #1347000:   Batch Loss = 12.864224, Accuracy = 0.057999998331069946\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.946252822875977, Accuracy = 0.06372840702533722\n",
      "Training iter #1350000:   Batch Loss = 13.084043, Accuracy = 0.03933333232998848\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.943646430969238, Accuracy = 0.06372840702533722\n",
      "Training iter #1353000:   Batch Loss = 12.893575, Accuracy = 0.06133333221077919\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.94111156463623, Accuracy = 0.06372840702533722\n",
      "Training iter #1356000:   Batch Loss = 12.632355, Accuracy = 0.07066666334867477\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.938461303710938, Accuracy = 0.06343061476945877\n",
      "Training iter #1359000:   Batch Loss = 12.841285, Accuracy = 0.06333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.935613632202148, Accuracy = 0.06402620673179626\n",
      "Training iter #1362000:   Batch Loss = 12.919964, Accuracy = 0.04266666620969772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.933000564575195, Accuracy = 0.06432399898767471\n",
      "Training iter #1365000:   Batch Loss = 12.969866, Accuracy = 0.0560000017285347\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.930866241455078, Accuracy = 0.06462179869413376\n",
      "Training iter #1368000:   Batch Loss = 12.575135, Accuracy = 0.09066666662693024\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.928292274475098, Accuracy = 0.06462179869413376\n",
      "Training iter #1371000:   Batch Loss = 12.795629, Accuracy = 0.09333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.92573356628418, Accuracy = 0.06462179869413376\n",
      "Training iter #1374000:   Batch Loss = 12.876553, Accuracy = 0.058666665107011795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.922934532165527, Accuracy = 0.0649195984005928\n",
      "Training iter #1377000:   Batch Loss = 13.106155, Accuracy = 0.03999999910593033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.920235633850098, Accuracy = 0.06521739065647125\n",
      "Training iter #1380000:   Batch Loss = 12.848469, Accuracy = 0.06533333659172058\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.917656898498535, Accuracy = 0.0655151903629303\n",
      "Training iter #1383000:   Batch Loss = 12.591673, Accuracy = 0.07866666465997696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.915014266967773, Accuracy = 0.06581298261880875\n",
      "Training iter #1386000:   Batch Loss = 12.808385, Accuracy = 0.06266666948795319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.912376403808594, Accuracy = 0.06640857458114624\n",
      "Training iter #1389000:   Batch Loss = 12.893874, Accuracy = 0.046666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.90975570678711, Accuracy = 0.06640857458114624\n",
      "Training iter #1392000:   Batch Loss = 12.916801, Accuracy = 0.05666666850447655\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.907317161560059, Accuracy = 0.06640857458114624\n",
      "Training iter #1395000:   Batch Loss = 12.517653, Accuracy = 0.09733333438634872\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.904806137084961, Accuracy = 0.06640857458114624\n",
      "Training iter #1398000:   Batch Loss = 12.661579, Accuracy = 0.10466666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.902276992797852, Accuracy = 0.06670637428760529\n",
      "Training iter #1401000:   Batch Loss = 12.865479, Accuracy = 0.05400000140070915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.89962387084961, Accuracy = 0.06670637428760529\n",
      "Training iter #1404000:   Batch Loss = 13.194805, Accuracy = 0.036666665226221085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.897138595581055, Accuracy = 0.06700416654348373\n",
      "Training iter #1407000:   Batch Loss = 12.867434, Accuracy = 0.05999999865889549\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.894651412963867, Accuracy = 0.06700416654348373\n",
      "Training iter #1410000:   Batch Loss = 12.585142, Accuracy = 0.07999999821186066\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.892101287841797, Accuracy = 0.06700416654348373\n",
      "Training iter #1413000:   Batch Loss = 12.830317, Accuracy = 0.07066666334867477\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.889477729797363, Accuracy = 0.06700416654348373\n",
      "Training iter #1416000:   Batch Loss = 12.863834, Accuracy = 0.04800000041723251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.886898040771484, Accuracy = 0.06700416654348373\n",
      "Training iter #1419000:   Batch Loss = 12.787413, Accuracy = 0.06199999898672104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.884428024291992, Accuracy = 0.06700416654348373\n",
      "Training iter #1422000:   Batch Loss = 12.569386, Accuracy = 0.07733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.88184642791748, Accuracy = 0.06700416654348373\n",
      "Training iter #1425000:   Batch Loss = 12.662657, Accuracy = 0.09799999743700027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.879148483276367, Accuracy = 0.06700416654348373\n",
      "Training iter #1428000:   Batch Loss = 12.805977, Accuracy = 0.0573333315551281\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.876346588134766, Accuracy = 0.06700416654348373\n",
      "Training iter #1431000:   Batch Loss = 13.166069, Accuracy = 0.0533333346247673\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.873781204223633, Accuracy = 0.06789755821228027\n",
      "Training iter #1434000:   Batch Loss = 12.742057, Accuracy = 0.09200000017881393\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.871256828308105, Accuracy = 0.06819535791873932\n",
      "Training iter #1437000:   Batch Loss = 12.540405, Accuracy = 0.08933333307504654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.868659019470215, Accuracy = 0.06849315017461777\n",
      "Training iter #1440000:   Batch Loss = 12.785428, Accuracy = 0.06933332979679108\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.866033554077148, Accuracy = 0.06879094988107681\n",
      "Training iter #1443000:   Batch Loss = 12.853373, Accuracy = 0.05533333495259285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.863500595092773, Accuracy = 0.06879094988107681\n",
      "Training iter #1446000:   Batch Loss = 12.816854, Accuracy = 0.06066666543483734\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.861166000366211, Accuracy = 0.06849315017461777\n",
      "Training iter #1449000:   Batch Loss = 12.641804, Accuracy = 0.06199999898672104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.858745574951172, Accuracy = 0.06879094988107681\n",
      "Training iter #1452000:   Batch Loss = 12.623347, Accuracy = 0.12266666442155838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.856224060058594, Accuracy = 0.06908874213695526\n",
      "Training iter #1455000:   Batch Loss = 12.807535, Accuracy = 0.06199999898672104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.853861808776855, Accuracy = 0.0699821338057518\n",
      "Training iter #1458000:   Batch Loss = 13.117240, Accuracy = 0.05000000074505806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.85148811340332, Accuracy = 0.0699821338057518\n",
      "Training iter #1461000:   Batch Loss = 12.584806, Accuracy = 0.11733333021402359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.849143028259277, Accuracy = 0.0699821338057518\n",
      "Training iter #1464000:   Batch Loss = 12.512987, Accuracy = 0.09200000017881393\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.846648216247559, Accuracy = 0.06968433409929276\n",
      "Training iter #1467000:   Batch Loss = 12.839474, Accuracy = 0.04933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.843670845031738, Accuracy = 0.06968433409929276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1470000:   Batch Loss = 12.853609, Accuracy = 0.046666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.841108322143555, Accuracy = 0.0699821338057518\n",
      "Training iter #1473000:   Batch Loss = 12.787749, Accuracy = 0.06666667014360428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.83868408203125, Accuracy = 0.0699821338057518\n",
      "Training iter #1476000:   Batch Loss = 12.725461, Accuracy = 0.04533333331346512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.83626651763916, Accuracy = 0.0705777257680893\n",
      "Training iter #1479000:   Batch Loss = 12.621250, Accuracy = 0.13066667318344116\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.833744049072266, Accuracy = 0.0705777257680893\n",
      "Training iter #1482000:   Batch Loss = 12.739216, Accuracy = 0.06666667014360428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.831180572509766, Accuracy = 0.0705777257680893\n",
      "Training iter #1485000:   Batch Loss = 13.054159, Accuracy = 0.052000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.828727722167969, Accuracy = 0.07087551802396774\n",
      "Training iter #1488000:   Batch Loss = 12.502781, Accuracy = 0.1353333294391632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.826335906982422, Accuracy = 0.07087551802396774\n",
      "Training iter #1491000:   Batch Loss = 12.383893, Accuracy = 0.11400000005960464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.823844909667969, Accuracy = 0.0705777257680893\n",
      "Training iter #1494000:   Batch Loss = 12.809381, Accuracy = 0.054666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.821290969848633, Accuracy = 0.0705777257680893\n",
      "Training iter #1497000:   Batch Loss = 12.823202, Accuracy = 0.03866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.818805694580078, Accuracy = 0.0705777257680893\n",
      "Training iter #1500000:   Batch Loss = 12.715218, Accuracy = 0.06599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.81645679473877, Accuracy = 0.07087551802396774\n",
      "Training iter #1503000:   Batch Loss = 12.628807, Accuracy = 0.054666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.814094543457031, Accuracy = 0.07117331773042679\n",
      "Training iter #1506000:   Batch Loss = 12.730651, Accuracy = 0.09799999743700027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.8116455078125, Accuracy = 0.07117331773042679\n",
      "Training iter #1509000:   Batch Loss = 12.783012, Accuracy = 0.06599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.809146881103516, Accuracy = 0.07176890969276428\n",
      "Training iter #1512000:   Batch Loss = 13.091171, Accuracy = 0.05533333495259285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.806787490844727, Accuracy = 0.07176890969276428\n",
      "Training iter #1515000:   Batch Loss = 12.517364, Accuracy = 0.1353333294391632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.804491996765137, Accuracy = 0.07176890969276428\n",
      "Training iter #1518000:   Batch Loss = 12.511600, Accuracy = 0.11866666376590729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.802074432373047, Accuracy = 0.07206670939922333\n",
      "Training iter #1521000:   Batch Loss = 12.742716, Accuracy = 0.06400000303983688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.799549102783203, Accuracy = 0.07206670939922333\n",
      "Training iter #1524000:   Batch Loss = 12.819357, Accuracy = 0.047333333641290665\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.797143936157227, Accuracy = 0.07206670939922333\n",
      "Training iter #1527000:   Batch Loss = 12.735220, Accuracy = 0.06533333659172058\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.794816970825195, Accuracy = 0.07266230136156082\n",
      "Training iter #1530000:   Batch Loss = 12.477556, Accuracy = 0.05400000140070915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.792869567871094, Accuracy = 0.07266230136156082\n",
      "Training iter #1533000:   Batch Loss = 12.634541, Accuracy = 0.10000000149011612\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.79046630859375, Accuracy = 0.07236450165510178\n",
      "Training iter #1536000:   Batch Loss = 12.765217, Accuracy = 0.06599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.787964820861816, Accuracy = 0.07296009361743927\n",
      "Training iter #1539000:   Batch Loss = 13.022073, Accuracy = 0.06533333659172058\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.78547477722168, Accuracy = 0.07355568557977676\n",
      "Training iter #1542000:   Batch Loss = 12.473812, Accuracy = 0.13866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.783102035522461, Accuracy = 0.07355568557977676\n",
      "Training iter #1545000:   Batch Loss = 12.595936, Accuracy = 0.12066666781902313\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.780630111694336, Accuracy = 0.07355568557977676\n",
      "Training iter #1548000:   Batch Loss = 12.689524, Accuracy = 0.06133333221077919\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.777937889099121, Accuracy = 0.07415127754211426\n",
      "Training iter #1551000:   Batch Loss = 12.801725, Accuracy = 0.04866666719317436\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.775588989257812, Accuracy = 0.0744490772485733\n",
      "Training iter #1554000:   Batch Loss = 12.656010, Accuracy = 0.09133332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.773218154907227, Accuracy = 0.0744490772485733\n",
      "Training iter #1557000:   Batch Loss = 12.407627, Accuracy = 0.058666665107011795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.770822525024414, Accuracy = 0.0744490772485733\n",
      "Training iter #1560000:   Batch Loss = 12.591669, Accuracy = 0.09533333033323288\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.76838493347168, Accuracy = 0.07474686950445175\n",
      "Training iter #1563000:   Batch Loss = 12.760187, Accuracy = 0.06466666609048843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.76596736907959, Accuracy = 0.07474686950445175\n",
      "Training iter #1566000:   Batch Loss = 13.006973, Accuracy = 0.07266666740179062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.76357650756836, Accuracy = 0.0744490772485733\n",
      "Training iter #1569000:   Batch Loss = 12.539026, Accuracy = 0.12266666442155838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.761268615722656, Accuracy = 0.07474686950445175\n",
      "Training iter #1572000:   Batch Loss = 12.575438, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.758888244628906, Accuracy = 0.0750446692109108\n",
      "Training iter #1575000:   Batch Loss = 12.673145, Accuracy = 0.07133333384990692\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.756387710571289, Accuracy = 0.07534246891736984\n",
      "Training iter #1578000:   Batch Loss = 12.808619, Accuracy = 0.05000000074505806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.754301071166992, Accuracy = 0.07534246891736984\n",
      "Training iter #1581000:   Batch Loss = 12.615307, Accuracy = 0.10066666454076767\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.75209903717041, Accuracy = 0.07564026117324829\n",
      "Training iter #1584000:   Batch Loss = 12.406261, Accuracy = 0.0533333346247673\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.749807357788086, Accuracy = 0.07564026117324829\n",
      "Training iter #1587000:   Batch Loss = 12.568428, Accuracy = 0.09333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.747411727905273, Accuracy = 0.07564026117324829\n",
      "Training iter #1590000:   Batch Loss = 12.759878, Accuracy = 0.06466666609048843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.745040893554688, Accuracy = 0.07593806087970734\n",
      "Training iter #1593000:   Batch Loss = 12.901575, Accuracy = 0.08133333176374435\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.742751121520996, Accuracy = 0.07593806087970734\n",
      "Training iter #1596000:   Batch Loss = 12.562721, Accuracy = 0.1066666692495346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.740479469299316, Accuracy = 0.07623585313558578\n",
      "Training iter #1599000:   Batch Loss = 12.475957, Accuracy = 0.13600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.738153457641602, Accuracy = 0.07653365284204483\n",
      "Training iter #1602000:   Batch Loss = 12.646049, Accuracy = 0.0806666687130928\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.735760688781738, Accuracy = 0.07683144509792328\n",
      "Training iter #1605000:   Batch Loss = 12.804313, Accuracy = 0.05000000074505806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.733409881591797, Accuracy = 0.07683144509792328\n",
      "Training iter #1608000:   Batch Loss = 12.581100, Accuracy = 0.09466666728258133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.731033325195312, Accuracy = 0.07712924480438232\n",
      "Training iter #1611000:   Batch Loss = 12.345249, Accuracy = 0.07400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.728736877441406, Accuracy = 0.07712924480438232\n",
      "Training iter #1614000:   Batch Loss = 12.601095, Accuracy = 0.09200000017881393\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.726387023925781, Accuracy = 0.07712924480438232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1617000:   Batch Loss = 12.746740, Accuracy = 0.06400000303983688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.724004745483398, Accuracy = 0.07742703706026077\n",
      "Training iter #1620000:   Batch Loss = 12.831650, Accuracy = 0.0846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.721616744995117, Accuracy = 0.07742703706026077\n",
      "Training iter #1623000:   Batch Loss = 12.533091, Accuracy = 0.10866666585206985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.719346046447754, Accuracy = 0.07742703706026077\n",
      "Training iter #1626000:   Batch Loss = 12.464268, Accuracy = 0.1393333375453949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.716994285583496, Accuracy = 0.07742703706026077\n",
      "Training iter #1629000:   Batch Loss = 12.625830, Accuracy = 0.0806666687130928\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.714599609375, Accuracy = 0.07742703706026077\n",
      "Training iter #1632000:   Batch Loss = 12.839903, Accuracy = 0.05133333429694176\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.712181091308594, Accuracy = 0.07742703706026077\n",
      "Training iter #1635000:   Batch Loss = 12.660842, Accuracy = 0.09266666322946548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.709917068481445, Accuracy = 0.07712924480438232\n",
      "Training iter #1638000:   Batch Loss = 12.360707, Accuracy = 0.07533333450555801\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.707733154296875, Accuracy = 0.07712924480438232\n",
      "Training iter #1641000:   Batch Loss = 12.571634, Accuracy = 0.09133332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.70541000366211, Accuracy = 0.07712924480438232\n",
      "Training iter #1644000:   Batch Loss = 12.722079, Accuracy = 0.058666665107011795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.703018188476562, Accuracy = 0.07712924480438232\n",
      "Training iter #1647000:   Batch Loss = 12.780317, Accuracy = 0.08866667002439499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.700824737548828, Accuracy = 0.07742703706026077\n",
      "Training iter #1650000:   Batch Loss = 12.420999, Accuracy = 0.11266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.698781967163086, Accuracy = 0.07742703706026077\n",
      "Training iter #1653000:   Batch Loss = 12.430380, Accuracy = 0.14399999380111694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.696680068969727, Accuracy = 0.07832042872905731\n",
      "Training iter #1656000:   Batch Loss = 12.532410, Accuracy = 0.08799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.694345474243164, Accuracy = 0.07861822843551636\n",
      "Training iter #1659000:   Batch Loss = 12.874948, Accuracy = 0.057999998331069946\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.692073822021484, Accuracy = 0.0789160206913948\n",
      "Training iter #1662000:   Batch Loss = 12.617321, Accuracy = 0.09399999678134918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.689905166625977, Accuracy = 0.0795116126537323\n",
      "Training iter #1665000:   Batch Loss = 12.424820, Accuracy = 0.07400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.68772029876709, Accuracy = 0.07980941236019135\n",
      "Training iter #1668000:   Batch Loss = 12.569672, Accuracy = 0.09533333033323288\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.685422897338867, Accuracy = 0.0801072046160698\n",
      "Training iter #1671000:   Batch Loss = 12.731113, Accuracy = 0.05400000140070915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.683008193969727, Accuracy = 0.0801072046160698\n",
      "Training iter #1674000:   Batch Loss = 12.779797, Accuracy = 0.09000000357627869\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.680763244628906, Accuracy = 0.0801072046160698\n",
      "Training iter #1677000:   Batch Loss = 12.282724, Accuracy = 0.12733332812786102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.678472518920898, Accuracy = 0.0801072046160698\n",
      "Training iter #1680000:   Batch Loss = 12.462113, Accuracy = 0.13333334028720856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.676141738891602, Accuracy = 0.0801072046160698\n",
      "Training iter #1683000:   Batch Loss = 12.565295, Accuracy = 0.08933333307504654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.67365550994873, Accuracy = 0.08040500432252884\n",
      "Training iter #1686000:   Batch Loss = 12.861780, Accuracy = 0.059333331882953644\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.671419143676758, Accuracy = 0.08070279657840729\n",
      "Training iter #1689000:   Batch Loss = 12.658603, Accuracy = 0.08799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.669211387634277, Accuracy = 0.08070279657840729\n",
      "Training iter #1692000:   Batch Loss = 12.285395, Accuracy = 0.10066666454076767\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.666982650756836, Accuracy = 0.08100059628486633\n",
      "Training iter #1695000:   Batch Loss = 12.500889, Accuracy = 0.09333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.664661407470703, Accuracy = 0.08129838854074478\n",
      "Training iter #1698000:   Batch Loss = 12.702211, Accuracy = 0.057999998331069946\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.66231918334961, Accuracy = 0.08189398795366287\n",
      "Training iter #1701000:   Batch Loss = 12.681343, Accuracy = 0.08533333241939545\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.660118103027344, Accuracy = 0.08189398795366287\n",
      "Training iter #1704000:   Batch Loss = 12.290560, Accuracy = 0.12999999523162842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.65794563293457, Accuracy = 0.08189398795366287\n",
      "Training iter #1707000:   Batch Loss = 12.481571, Accuracy = 0.12600000202655792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.655768394470215, Accuracy = 0.08189398795366287\n",
      "Training iter #1710000:   Batch Loss = 12.560504, Accuracy = 0.08933333307504654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.653444290161133, Accuracy = 0.08248957991600037\n",
      "Training iter #1713000:   Batch Loss = 12.903934, Accuracy = 0.058666665107011795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.651254653930664, Accuracy = 0.08248957991600037\n",
      "Training iter #1716000:   Batch Loss = 12.606543, Accuracy = 0.09466666728258133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.648982048034668, Accuracy = 0.08248957991600037\n",
      "Training iter #1719000:   Batch Loss = 12.304714, Accuracy = 0.09733333438634872\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.646697044372559, Accuracy = 0.08278737217187881\n",
      "Training iter #1722000:   Batch Loss = 12.535982, Accuracy = 0.09466666728258133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.64427375793457, Accuracy = 0.08248957991600037\n",
      "Training iter #1725000:   Batch Loss = 12.694714, Accuracy = 0.06333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.641944885253906, Accuracy = 0.08248957991600037\n",
      "Training iter #1728000:   Batch Loss = 12.577296, Accuracy = 0.09200000017881393\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.639707565307617, Accuracy = 0.08248957991600037\n",
      "Training iter #1731000:   Batch Loss = 12.189831, Accuracy = 0.1446666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.637537002563477, Accuracy = 0.08278737217187881\n",
      "Training iter #1734000:   Batch Loss = 12.397396, Accuracy = 0.12866666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.635295867919922, Accuracy = 0.08308517187833786\n",
      "Training iter #1737000:   Batch Loss = 12.556787, Accuracy = 0.0806666687130928\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.633037567138672, Accuracy = 0.08338296413421631\n",
      "Training iter #1740000:   Batch Loss = 12.960075, Accuracy = 0.06400000303983688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.630826950073242, Accuracy = 0.08338296413421631\n",
      "Training iter #1743000:   Batch Loss = 12.545656, Accuracy = 0.10533333569765091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.628761291503906, Accuracy = 0.08278737217187881\n",
      "Training iter #1746000:   Batch Loss = 12.301774, Accuracy = 0.10066666454076767\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.626598358154297, Accuracy = 0.08308517187833786\n",
      "Training iter #1749000:   Batch Loss = 12.535479, Accuracy = 0.09799999743700027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.624311447143555, Accuracy = 0.08338296413421631\n",
      "Training iter #1752000:   Batch Loss = 12.633926, Accuracy = 0.06599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.622071266174316, Accuracy = 0.08338296413421631\n",
      "Training iter #1755000:   Batch Loss = 12.550306, Accuracy = 0.09133332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.619945526123047, Accuracy = 0.08368076384067535\n",
      "Training iter #1758000:   Batch Loss = 12.291346, Accuracy = 0.1133333370089531\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.617862701416016, Accuracy = 0.08427635580301285\n",
      "Training iter #1761000:   Batch Loss = 12.382210, Accuracy = 0.12333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.615921974182129, Accuracy = 0.08487194776535034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1764000:   Batch Loss = 12.569157, Accuracy = 0.0793333351612091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.613733291625977, Accuracy = 0.08487194776535034\n",
      "Training iter #1767000:   Batch Loss = 12.882426, Accuracy = 0.06066666543483734\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.611544609069824, Accuracy = 0.08576533943414688\n",
      "Training iter #1770000:   Batch Loss = 12.440523, Accuracy = 0.1120000034570694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.609403610229492, Accuracy = 0.08606313169002533\n",
      "Training iter #1773000:   Batch Loss = 12.250954, Accuracy = 0.10999999940395355\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.607183456420898, Accuracy = 0.08784990757703781\n",
      "Training iter #1776000:   Batch Loss = 12.558516, Accuracy = 0.08133333176374435\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.604913711547852, Accuracy = 0.08814770728349686\n",
      "Training iter #1779000:   Batch Loss = 12.618719, Accuracy = 0.07266666740179062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.602673530578613, Accuracy = 0.0884455069899559\n",
      "Training iter #1782000:   Batch Loss = 12.542723, Accuracy = 0.09733333438634872\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.600472450256348, Accuracy = 0.08874329924583435\n",
      "Training iter #1785000:   Batch Loss = 12.398811, Accuracy = 0.0860000029206276\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.598260879516602, Accuracy = 0.08874329924583435\n",
      "Training iter #1788000:   Batch Loss = 12.320187, Accuracy = 0.16333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.595925331115723, Accuracy = 0.08874329924583435\n",
      "Training iter #1791000:   Batch Loss = 12.494110, Accuracy = 0.09200000017881393\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.593555450439453, Accuracy = 0.0890410989522934\n",
      "Training iter #1794000:   Batch Loss = 12.862695, Accuracy = 0.06800000369548798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.5909423828125, Accuracy = 0.0890410989522934\n",
      "Training iter #1797000:   Batch Loss = 12.249765, Accuracy = 0.16333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.588776588439941, Accuracy = 0.08933889120817184\n",
      "Training iter #1800000:   Batch Loss = 12.232002, Accuracy = 0.11933333426713943\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.58658218383789, Accuracy = 0.08933889120817184\n",
      "Training iter #1803000:   Batch Loss = 12.592131, Accuracy = 0.06666667014360428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.584309577941895, Accuracy = 0.08963669091463089\n",
      "Training iter #1806000:   Batch Loss = 12.619955, Accuracy = 0.07000000029802322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.582019805908203, Accuracy = 0.08993448317050934\n",
      "Training iter #1809000:   Batch Loss = 12.532963, Accuracy = 0.10599999874830246\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.579814910888672, Accuracy = 0.08993448317050934\n",
      "Training iter #1812000:   Batch Loss = 12.466924, Accuracy = 0.06533333659172058\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.577573776245117, Accuracy = 0.09023228287696838\n",
      "Training iter #1815000:   Batch Loss = 12.395861, Accuracy = 0.15066666901111603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.575273513793945, Accuracy = 0.09053007513284683\n",
      "Training iter #1818000:   Batch Loss = 12.502020, Accuracy = 0.08866667002439499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.57291030883789, Accuracy = 0.09053007513284683\n",
      "Training iter #1821000:   Batch Loss = 12.852852, Accuracy = 0.07599999755620956\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.570598602294922, Accuracy = 0.09053007513284683\n",
      "Training iter #1824000:   Batch Loss = 12.218609, Accuracy = 0.15199999511241913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.56831169128418, Accuracy = 0.09053007513284683\n",
      "Training iter #1827000:   Batch Loss = 12.136060, Accuracy = 0.15399999916553497\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.566020965576172, Accuracy = 0.09053007513284683\n",
      "Training iter #1830000:   Batch Loss = 12.534132, Accuracy = 0.08399999886751175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.563667297363281, Accuracy = 0.09082787483930588\n",
      "Training iter #1833000:   Batch Loss = 12.617560, Accuracy = 0.06666667014360428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.56139087677002, Accuracy = 0.09112566709518433\n",
      "Training iter #1836000:   Batch Loss = 12.494690, Accuracy = 0.09866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.559225082397461, Accuracy = 0.09112566709518433\n",
      "Training iter #1839000:   Batch Loss = 12.240896, Accuracy = 0.08266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.557136535644531, Accuracy = 0.09112566709518433\n",
      "Training iter #1842000:   Batch Loss = 12.413650, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.555005073547363, Accuracy = 0.09142346680164337\n",
      "Training iter #1845000:   Batch Loss = 12.537941, Accuracy = 0.08533333241939545\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.55306625366211, Accuracy = 0.09142346680164337\n",
      "Training iter #1848000:   Batch Loss = 12.791016, Accuracy = 0.0806666687130928\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.550924301147461, Accuracy = 0.09142346680164337\n",
      "Training iter #1851000:   Batch Loss = 12.186518, Accuracy = 0.1666666716337204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.548694610595703, Accuracy = 0.09142346680164337\n",
      "Training iter #1854000:   Batch Loss = 12.311504, Accuracy = 0.1433333307504654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.546422958374023, Accuracy = 0.09142346680164337\n",
      "Training iter #1857000:   Batch Loss = 12.448869, Accuracy = 0.08666666597127914\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.544151306152344, Accuracy = 0.09112566709518433\n",
      "Training iter #1860000:   Batch Loss = 12.599543, Accuracy = 0.07533333450555801\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.54189682006836, Accuracy = 0.09201905876398087\n",
      "Training iter #1863000:   Batch Loss = 12.489138, Accuracy = 0.09933333098888397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.539663314819336, Accuracy = 0.09231685847043991\n",
      "Training iter #1866000:   Batch Loss = 12.166399, Accuracy = 0.0833333358168602\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.537569046020508, Accuracy = 0.09261465072631836\n",
      "Training iter #1869000:   Batch Loss = 12.345462, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.535391807556152, Accuracy = 0.09231685847043991\n",
      "Training iter #1872000:   Batch Loss = 12.537270, Accuracy = 0.08666666597127914\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.533246040344238, Accuracy = 0.09231685847043991\n",
      "Training iter #1875000:   Batch Loss = 12.732229, Accuracy = 0.08666666597127914\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.531293869018555, Accuracy = 0.09261465072631836\n",
      "Training iter #1878000:   Batch Loss = 12.290562, Accuracy = 0.13733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.529045104980469, Accuracy = 0.09261465072631836\n",
      "Training iter #1881000:   Batch Loss = 12.277990, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.526845932006836, Accuracy = 0.09321024268865585\n",
      "Training iter #1884000:   Batch Loss = 12.396570, Accuracy = 0.09933333098888397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.524620056152344, Accuracy = 0.0935080423951149\n",
      "Training iter #1887000:   Batch Loss = 12.554632, Accuracy = 0.07800000160932541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.522453308105469, Accuracy = 0.0935080423951149\n",
      "Training iter #1890000:   Batch Loss = 12.465851, Accuracy = 0.109333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.520406723022461, Accuracy = 0.0935080423951149\n",
      "Training iter #1893000:   Batch Loss = 12.126923, Accuracy = 0.0820000022649765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.518372535705566, Accuracy = 0.0935080423951149\n",
      "Training iter #1896000:   Batch Loss = 12.267839, Accuracy = 0.12800000607967377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.516247749328613, Accuracy = 0.09321024268865585\n",
      "Training iter #1899000:   Batch Loss = 12.507019, Accuracy = 0.08266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.51411247253418, Accuracy = 0.09321024268865585\n",
      "Training iter #1902000:   Batch Loss = 12.689373, Accuracy = 0.09933333098888397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.511922836303711, Accuracy = 0.0935080423951149\n",
      "Training iter #1905000:   Batch Loss = 12.318945, Accuracy = 0.12133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.509712219238281, Accuracy = 0.09410363435745239\n",
      "Training iter #1908000:   Batch Loss = 12.255074, Accuracy = 0.1586666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.507497787475586, Accuracy = 0.09410363435745239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1911000:   Batch Loss = 12.376822, Accuracy = 0.10000000149011612\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.505083084106445, Accuracy = 0.09380583465099335\n",
      "Training iter #1914000:   Batch Loss = 12.555315, Accuracy = 0.07533333450555801\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.502545356750488, Accuracy = 0.09380583465099335\n",
      "Training iter #1917000:   Batch Loss = 12.395544, Accuracy = 0.12266666442155838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.500299453735352, Accuracy = 0.09380583465099335\n",
      "Training iter #1920000:   Batch Loss = 12.137800, Accuracy = 0.09066666662693024\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.497989654541016, Accuracy = 0.09410363435745239\n",
      "Training iter #1923000:   Batch Loss = 12.377216, Accuracy = 0.11266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.495512008666992, Accuracy = 0.09410363435745239\n",
      "Training iter #1926000:   Batch Loss = 12.496288, Accuracy = 0.09266666322946548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.493183135986328, Accuracy = 0.09499701857566833\n",
      "Training iter #1929000:   Batch Loss = 12.666157, Accuracy = 0.10533333569765091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.49106216430664, Accuracy = 0.09529481828212738\n",
      "Training iter #1932000:   Batch Loss = 12.290804, Accuracy = 0.12399999797344208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.489056587219238, Accuracy = 0.09529481828212738\n",
      "Training iter #1935000:   Batch Loss = 12.227905, Accuracy = 0.1599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.486936569213867, Accuracy = 0.09559261798858643\n",
      "Training iter #1938000:   Batch Loss = 12.330328, Accuracy = 0.1106666699051857\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.484762191772461, Accuracy = 0.09589041024446487\n",
      "Training iter #1941000:   Batch Loss = 12.571026, Accuracy = 0.06866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.482627868652344, Accuracy = 0.09589041024446487\n",
      "Training iter #1944000:   Batch Loss = 12.373199, Accuracy = 0.11933333426713943\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.48033332824707, Accuracy = 0.09618820995092392\n",
      "Training iter #1947000:   Batch Loss = 12.083234, Accuracy = 0.10400000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.478211402893066, Accuracy = 0.09618820995092392\n",
      "Training iter #1950000:   Batch Loss = 12.317564, Accuracy = 0.1133333370089531\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.476163864135742, Accuracy = 0.09648600220680237\n",
      "Training iter #1953000:   Batch Loss = 12.487057, Accuracy = 0.08799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.474004745483398, Accuracy = 0.09648600220680237\n",
      "Training iter #1956000:   Batch Loss = 12.539145, Accuracy = 0.109333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.471841812133789, Accuracy = 0.09648600220680237\n",
      "Training iter #1959000:   Batch Loss = 12.254234, Accuracy = 0.12600000202655792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.469676971435547, Accuracy = 0.09648600220680237\n",
      "Training iter #1962000:   Batch Loss = 12.192732, Accuracy = 0.1693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.46748161315918, Accuracy = 0.09678380191326141\n",
      "Training iter #1965000:   Batch Loss = 12.286158, Accuracy = 0.11733333021402359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.465250968933105, Accuracy = 0.09708159416913986\n",
      "Training iter #1968000:   Batch Loss = 12.606848, Accuracy = 0.06599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.463037490844727, Accuracy = 0.09678380191326141\n",
      "Training iter #1971000:   Batch Loss = 12.418634, Accuracy = 0.11266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.460838317871094, Accuracy = 0.09708159416913986\n",
      "Training iter #1974000:   Batch Loss = 12.147350, Accuracy = 0.10000000149011612\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.458660125732422, Accuracy = 0.09708159416913986\n",
      "Training iter #1977000:   Batch Loss = 12.316072, Accuracy = 0.11533333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.456457138061523, Accuracy = 0.09827277809381485\n",
      "Training iter #1980000:   Batch Loss = 12.525475, Accuracy = 0.07066666334867477\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.454268455505371, Accuracy = 0.0985705778002739\n",
      "Training iter #1983000:   Batch Loss = 12.549923, Accuracy = 0.11266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.45210075378418, Accuracy = 0.09976176172494888\n",
      "Training iter #1986000:   Batch Loss = 12.091372, Accuracy = 0.14666666090488434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.449853897094727, Accuracy = 0.10005956143140793\n",
      "Training iter #1989000:   Batch Loss = 12.156738, Accuracy = 0.1706666648387909\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.447540283203125, Accuracy = 0.10005956143140793\n",
      "Training iter #1992000:   Batch Loss = 12.267905, Accuracy = 0.11733333021402359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.445084571838379, Accuracy = 0.10005956143140793\n",
      "Training iter #1995000:   Batch Loss = 12.598407, Accuracy = 0.07333333045244217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.442692756652832, Accuracy = 0.09976176172494888\n",
      "Training iter #1998000:   Batch Loss = 12.391737, Accuracy = 0.10400000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.440564155578613, Accuracy = 0.09976176172494888\n",
      "Training iter #2001000:   Batch Loss = 12.121450, Accuracy = 0.10999999940395355\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.438425064086914, Accuracy = 0.09976176172494888\n",
      "Training iter #2004000:   Batch Loss = 12.304879, Accuracy = 0.1066666692495346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.43623161315918, Accuracy = 0.09976176172494888\n",
      "Training iter #2007000:   Batch Loss = 12.494347, Accuracy = 0.07400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.434189796447754, Accuracy = 0.10035735368728638\n",
      "Training iter #2010000:   Batch Loss = 12.438747, Accuracy = 0.11533333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.432083129882812, Accuracy = 0.10035735368728638\n",
      "Training iter #2013000:   Batch Loss = 12.018257, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.429939270019531, Accuracy = 0.10035735368728638\n",
      "Training iter #2016000:   Batch Loss = 12.269884, Accuracy = 0.15066666901111603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.427801132202148, Accuracy = 0.10035735368728638\n",
      "Training iter #2019000:   Batch Loss = 12.298273, Accuracy = 0.11800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.425665855407715, Accuracy = 0.10035735368728638\n",
      "Training iter #2022000:   Batch Loss = 12.645212, Accuracy = 0.07599999755620956\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.423629760742188, Accuracy = 0.10035735368728638\n",
      "Training iter #2025000:   Batch Loss = 12.410637, Accuracy = 0.10266666859388351\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.421595573425293, Accuracy = 0.10035735368728638\n",
      "Training iter #2028000:   Batch Loss = 12.038376, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.419588088989258, Accuracy = 0.10065515339374542\n",
      "Training iter #2031000:   Batch Loss = 12.246992, Accuracy = 0.1146666631102562\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.417503356933594, Accuracy = 0.10125074535608292\n",
      "Training iter #2034000:   Batch Loss = 12.463703, Accuracy = 0.0820000022649765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.41543960571289, Accuracy = 0.10154853761196136\n",
      "Training iter #2037000:   Batch Loss = 12.407937, Accuracy = 0.11133333295583725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.413406372070312, Accuracy = 0.10154853761196136\n",
      "Training iter #2040000:   Batch Loss = 12.012545, Accuracy = 0.15733332931995392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.411344528198242, Accuracy = 0.10184633731842041\n",
      "Training iter #2043000:   Batch Loss = 12.156172, Accuracy = 0.1613333374261856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.409178733825684, Accuracy = 0.10214413702487946\n",
      "Training iter #2046000:   Batch Loss = 12.268963, Accuracy = 0.10533333569765091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.406946182250977, Accuracy = 0.10214413702487946\n",
      "Training iter #2049000:   Batch Loss = 12.698809, Accuracy = 0.07533333450555801\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.405027389526367, Accuracy = 0.1024419292807579\n",
      "Training iter #2052000:   Batch Loss = 12.393435, Accuracy = 0.10999999940395355\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.403072357177734, Accuracy = 0.10214413702487946\n",
      "Training iter #2055000:   Batch Loss = 12.069504, Accuracy = 0.12800000607967377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.401056289672852, Accuracy = 0.1024419292807579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2058000:   Batch Loss = 12.274445, Accuracy = 0.11866666376590729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.398937225341797, Accuracy = 0.1024419292807579\n",
      "Training iter #2061000:   Batch Loss = 12.425608, Accuracy = 0.08399999886751175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.396856307983398, Accuracy = 0.10273972898721695\n",
      "Training iter #2064000:   Batch Loss = 12.263085, Accuracy = 0.12266666442155838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.394779205322266, Accuracy = 0.1030375212430954\n",
      "Training iter #2067000:   Batch Loss = 12.014595, Accuracy = 0.15399999916553497\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.392712593078613, Accuracy = 0.10333532094955444\n",
      "Training iter #2070000:   Batch Loss = 12.131788, Accuracy = 0.15000000596046448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.390604019165039, Accuracy = 0.10393091291189194\n",
      "Training iter #2073000:   Batch Loss = 12.300465, Accuracy = 0.09733333438634872\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.388513565063477, Accuracy = 0.10363311320543289\n",
      "Training iter #2076000:   Batch Loss = 12.678581, Accuracy = 0.0846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.386478424072266, Accuracy = 0.10393091291189194\n",
      "Training iter #2079000:   Batch Loss = 12.304174, Accuracy = 0.12333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.384449005126953, Accuracy = 0.10452650487422943\n",
      "Training iter #2082000:   Batch Loss = 12.008797, Accuracy = 0.1366666704416275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.382381439208984, Accuracy = 0.10452650487422943\n",
      "Training iter #2085000:   Batch Loss = 12.270708, Accuracy = 0.11533333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.380241394042969, Accuracy = 0.10482429713010788\n",
      "Training iter #2088000:   Batch Loss = 12.424433, Accuracy = 0.08733333647251129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.378131866455078, Accuracy = 0.10512209683656693\n",
      "Training iter #2091000:   Batch Loss = 12.335269, Accuracy = 0.11800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.376058578491211, Accuracy = 0.10571768879890442\n",
      "Training iter #2094000:   Batch Loss = 12.077392, Accuracy = 0.13600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.374013900756836, Accuracy = 0.10571768879890442\n",
      "Training iter #2097000:   Batch Loss = 12.047226, Accuracy = 0.17266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.371866226196289, Accuracy = 0.10571768879890442\n",
      "Training iter #2100000:   Batch Loss = 12.290764, Accuracy = 0.10333333164453506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.369701385498047, Accuracy = 0.10571768879890442\n",
      "Training iter #2103000:   Batch Loss = 12.649027, Accuracy = 0.07866666465997696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.367534637451172, Accuracy = 0.10601548850536346\n",
      "Training iter #2106000:   Batch Loss = 12.134922, Accuracy = 0.15666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.365416526794434, Accuracy = 0.10631328076124191\n",
      "Training iter #2109000:   Batch Loss = 11.981089, Accuracy = 0.1353333294391632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.36314582824707, Accuracy = 0.10631328076124191\n",
      "Training iter #2112000:   Batch Loss = 12.334877, Accuracy = 0.08399999886751175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.360923767089844, Accuracy = 0.10601548850536346\n",
      "Training iter #2115000:   Batch Loss = 12.388304, Accuracy = 0.08866667002439499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.358797073364258, Accuracy = 0.10601548850536346\n",
      "Training iter #2118000:   Batch Loss = 12.330446, Accuracy = 0.1106666699051857\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.356786727905273, Accuracy = 0.10601548850536346\n",
      "Training iter #2121000:   Batch Loss = 12.178610, Accuracy = 0.09133332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.354719161987305, Accuracy = 0.10661108046770096\n",
      "Training iter #2124000:   Batch Loss = 12.068874, Accuracy = 0.18666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.352582931518555, Accuracy = 0.10661108046770096\n",
      "Training iter #2127000:   Batch Loss = 12.253148, Accuracy = 0.1133333370089531\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.350492477416992, Accuracy = 0.10661108046770096\n",
      "Training iter #2130000:   Batch Loss = 12.603224, Accuracy = 0.09066666662693024\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.348348617553711, Accuracy = 0.1069088727235794\n",
      "Training iter #2133000:   Batch Loss = 12.019135, Accuracy = 0.1706666648387909\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.34628963470459, Accuracy = 0.1069088727235794\n",
      "Training iter #2136000:   Batch Loss = 11.892929, Accuracy = 0.15533334016799927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.344160079956055, Accuracy = 0.1069088727235794\n",
      "Training iter #2139000:   Batch Loss = 12.297789, Accuracy = 0.09266666322946548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.342000961303711, Accuracy = 0.1069088727235794\n",
      "Training iter #2142000:   Batch Loss = 12.364296, Accuracy = 0.08399999886751175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.339973449707031, Accuracy = 0.1075044646859169\n",
      "Training iter #2145000:   Batch Loss = 12.325833, Accuracy = 0.11533333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.337966918945312, Accuracy = 0.1075044646859169\n",
      "Training iter #2148000:   Batch Loss = 12.139233, Accuracy = 0.10266666859388351\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.335988998413086, Accuracy = 0.10780226439237595\n",
      "Training iter #2151000:   Batch Loss = 12.158082, Accuracy = 0.15800000727176666\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.333940505981445, Accuracy = 0.10780226439237595\n",
      "Training iter #2154000:   Batch Loss = 12.274244, Accuracy = 0.1080000028014183\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.33197021484375, Accuracy = 0.1081000566482544\n",
      "Training iter #2157000:   Batch Loss = 12.589873, Accuracy = 0.10000000149011612\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.329950332641602, Accuracy = 0.10839785635471344\n",
      "Training iter #2160000:   Batch Loss = 11.994496, Accuracy = 0.164000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.327951431274414, Accuracy = 0.10869564861059189\n",
      "Training iter #2163000:   Batch Loss = 11.947856, Accuracy = 0.16200000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.325963973999023, Accuracy = 0.10899344831705093\n",
      "Training iter #2166000:   Batch Loss = 12.244257, Accuracy = 0.10466666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.323881149291992, Accuracy = 0.10929124802350998\n",
      "Training iter #2169000:   Batch Loss = 12.371609, Accuracy = 0.09133332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.32168960571289, Accuracy = 0.10929124802350998\n",
      "Training iter #2172000:   Batch Loss = 12.297760, Accuracy = 0.1146666631102562\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.319374084472656, Accuracy = 0.10929124802350998\n",
      "Training iter #2175000:   Batch Loss = 11.979361, Accuracy = 0.09933333098888397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.316936492919922, Accuracy = 0.11018463224172592\n",
      "Training iter #2178000:   Batch Loss = 12.111673, Accuracy = 0.15666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.314840316772461, Accuracy = 0.11078022420406342\n",
      "Training iter #2181000:   Batch Loss = 12.299969, Accuracy = 0.10333333164453506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.312761306762695, Accuracy = 0.11078022420406342\n",
      "Training iter #2184000:   Batch Loss = 12.537020, Accuracy = 0.10066666454076767\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.310720443725586, Accuracy = 0.11107802391052246\n",
      "Training iter #2187000:   Batch Loss = 11.997946, Accuracy = 0.17133332788944244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.308691024780273, Accuracy = 0.11137581616640091\n",
      "Training iter #2190000:   Batch Loss = 12.086855, Accuracy = 0.14800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.306600570678711, Accuracy = 0.11167361587285995\n",
      "Training iter #2193000:   Batch Loss = 12.174025, Accuracy = 0.1120000034570694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.304388046264648, Accuracy = 0.11167361587285995\n",
      "Training iter #2196000:   Batch Loss = 12.346987, Accuracy = 0.09666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.302218437194824, Accuracy = 0.1119714081287384\n",
      "Training iter #2199000:   Batch Loss = 12.258883, Accuracy = 0.12999999523162842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.300102233886719, Accuracy = 0.1119714081287384\n",
      "Training iter #2202000:   Batch Loss = 11.903048, Accuracy = 0.10999999940395355\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.297981262207031, Accuracy = 0.1125670075416565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2205000:   Batch Loss = 12.094084, Accuracy = 0.15066666901111603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.295787811279297, Accuracy = 0.11286479979753494\n",
      "Training iter #2208000:   Batch Loss = 12.314288, Accuracy = 0.10466666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.29364013671875, Accuracy = 0.11286479979753494\n",
      "Training iter #2211000:   Batch Loss = 12.513998, Accuracy = 0.1080000028014183\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.2915620803833, Accuracy = 0.11286479979753494\n",
      "Training iter #2214000:   Batch Loss = 12.042998, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.28958511352539, Accuracy = 0.11316259950399399\n",
      "Training iter #2217000:   Batch Loss = 12.064426, Accuracy = 0.16066665947437286\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.287473678588867, Accuracy = 0.11316259950399399\n",
      "Training iter #2220000:   Batch Loss = 12.166530, Accuracy = 0.11866666376590729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.28532600402832, Accuracy = 0.11346039175987244\n",
      "Training iter #2223000:   Batch Loss = 12.356073, Accuracy = 0.09133332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.283212661743164, Accuracy = 0.11346039175987244\n",
      "Training iter #2226000:   Batch Loss = 12.219028, Accuracy = 0.13466666638851166\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.281204223632812, Accuracy = 0.11346039175987244\n",
      "Training iter #2229000:   Batch Loss = 11.891341, Accuracy = 0.10066666454076767\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.279180526733398, Accuracy = 0.11346039175987244\n",
      "Training iter #2232000:   Batch Loss = 12.039765, Accuracy = 0.15533334016799927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.277082443237305, Accuracy = 0.11405598372220993\n",
      "Training iter #2235000:   Batch Loss = 12.341754, Accuracy = 0.10533333569765091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.275087356567383, Accuracy = 0.11494937539100647\n",
      "Training iter #2238000:   Batch Loss = 12.433625, Accuracy = 0.12466666847467422\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.2730712890625, Accuracy = 0.11494937539100647\n",
      "Training iter #2241000:   Batch Loss = 12.074719, Accuracy = 0.13733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.271174430847168, Accuracy = 0.11494937539100647\n",
      "Training iter #2244000:   Batch Loss = 11.985898, Accuracy = 0.16733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.26921272277832, Accuracy = 0.11554496735334396\n",
      "Training iter #2247000:   Batch Loss = 12.133905, Accuracy = 0.12133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.267171859741211, Accuracy = 0.11614055931568146\n",
      "Training iter #2250000:   Batch Loss = 12.304955, Accuracy = 0.09200000017881393\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.265125274658203, Accuracy = 0.11614055931568146\n",
      "Training iter #2253000:   Batch Loss = 12.167944, Accuracy = 0.1340000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.262964248657227, Accuracy = 0.11614055931568146\n",
      "Training iter #2256000:   Batch Loss = 11.855981, Accuracy = 0.12200000137090683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.260812759399414, Accuracy = 0.11614055931568146\n",
      "Training iter #2259000:   Batch Loss = 12.118231, Accuracy = 0.13733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.258609771728516, Accuracy = 0.11614055931568146\n",
      "Training iter #2262000:   Batch Loss = 12.330632, Accuracy = 0.10599999874830246\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.256537437438965, Accuracy = 0.11614055931568146\n",
      "Training iter #2265000:   Batch Loss = 12.366908, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.254576683044434, Accuracy = 0.1164383590221405\n",
      "Training iter #2268000:   Batch Loss = 12.083296, Accuracy = 0.1393333375453949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.252632141113281, Accuracy = 0.11673615127801895\n",
      "Training iter #2271000:   Batch Loss = 11.994888, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.250642776489258, Accuracy = 0.117033950984478\n",
      "Training iter #2274000:   Batch Loss = 12.099106, Accuracy = 0.12666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.248584747314453, Accuracy = 0.11762954294681549\n",
      "Training iter #2277000:   Batch Loss = 12.356647, Accuracy = 0.09399999678134918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.246766090393066, Accuracy = 0.11762954294681549\n",
      "Training iter #2280000:   Batch Loss = 12.211188, Accuracy = 0.13333334028720856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.24491024017334, Accuracy = 0.11762954294681549\n",
      "Training iter #2283000:   Batch Loss = 11.839169, Accuracy = 0.12533333897590637\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.243003845214844, Accuracy = 0.11762954294681549\n",
      "Training iter #2286000:   Batch Loss = 12.066062, Accuracy = 0.1379999965429306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.240915298461914, Accuracy = 0.11762954294681549\n",
      "Training iter #2289000:   Batch Loss = 12.300447, Accuracy = 0.10466666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.238785743713379, Accuracy = 0.11792733520269394\n",
      "Training iter #2292000:   Batch Loss = 12.292027, Accuracy = 0.13333334028720856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.236783981323242, Accuracy = 0.11762954294681549\n",
      "Training iter #2295000:   Batch Loss = 11.981871, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.234965324401855, Accuracy = 0.11792733520269394\n",
      "Training iter #2298000:   Batch Loss = 11.931643, Accuracy = 0.18333333730697632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.233089447021484, Accuracy = 0.11792733520269394\n",
      "Training iter #2301000:   Batch Loss = 11.985261, Accuracy = 0.14666666090488434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.230859756469727, Accuracy = 0.11852292716503143\n",
      "Training iter #2304000:   Batch Loss = 12.388891, Accuracy = 0.09600000083446503\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.22887897491455, Accuracy = 0.11882072687149048\n",
      "Training iter #2307000:   Batch Loss = 12.168812, Accuracy = 0.12533333897590637\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.226848602294922, Accuracy = 0.11882072687149048\n",
      "Training iter #2310000:   Batch Loss = 11.930063, Accuracy = 0.11733333021402359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.224905014038086, Accuracy = 0.11941631883382797\n",
      "Training iter #2313000:   Batch Loss = 12.053042, Accuracy = 0.1420000046491623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.223089218139648, Accuracy = 0.11971411854028702\n",
      "Training iter #2316000:   Batch Loss = 12.359681, Accuracy = 0.09866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.221113204956055, Accuracy = 0.12001191079616547\n",
      "Training iter #2319000:   Batch Loss = 12.288843, Accuracy = 0.13600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.219169616699219, Accuracy = 0.12030971050262451\n",
      "Training iter #2322000:   Batch Loss = 11.805109, Accuracy = 0.16466666758060455\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.217255592346191, Accuracy = 0.120905302464962\n",
      "Training iter #2325000:   Batch Loss = 11.957573, Accuracy = 0.17733334004878998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.215276718139648, Accuracy = 0.12120309472084045\n",
      "Training iter #2328000:   Batch Loss = 12.047081, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.213188171386719, Accuracy = 0.12120309472084045\n",
      "Training iter #2331000:   Batch Loss = 12.423134, Accuracy = 0.09666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.211140632629395, Accuracy = 0.1215008944272995\n",
      "Training iter #2334000:   Batch Loss = 12.231112, Accuracy = 0.11533333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.209026336669922, Accuracy = 0.12179868668317795\n",
      "Training iter #2337000:   Batch Loss = 11.788390, Accuracy = 0.14666666090488434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.20692253112793, Accuracy = 0.1215008944272995\n",
      "Training iter #2340000:   Batch Loss = 12.033714, Accuracy = 0.12999999523162842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.204935073852539, Accuracy = 0.1215008944272995\n",
      "Training iter #2343000:   Batch Loss = 12.298723, Accuracy = 0.09933333098888397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.20302677154541, Accuracy = 0.12179868668317795\n",
      "Training iter #2346000:   Batch Loss = 12.178230, Accuracy = 0.1379999965429306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.201171875, Accuracy = 0.1215008944272995\n",
      "Training iter #2349000:   Batch Loss = 11.805852, Accuracy = 0.1666666716337204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.199220657348633, Accuracy = 0.12179868668317795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2352000:   Batch Loss = 11.993902, Accuracy = 0.1679999977350235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.197269439697266, Accuracy = 0.12269207835197449\n",
      "Training iter #2355000:   Batch Loss = 12.080149, Accuracy = 0.13866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.195127487182617, Accuracy = 0.12239427864551544\n",
      "Training iter #2358000:   Batch Loss = 12.431719, Accuracy = 0.10000000149011612\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.193052291870117, Accuracy = 0.12298987805843353\n",
      "Training iter #2361000:   Batch Loss = 12.148657, Accuracy = 0.12533333897590637\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.190969467163086, Accuracy = 0.12298987805843353\n",
      "Training iter #2364000:   Batch Loss = 11.807439, Accuracy = 0.14666666090488434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.18903923034668, Accuracy = 0.12298987805843353\n",
      "Training iter #2367000:   Batch Loss = 12.058180, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.186975479125977, Accuracy = 0.12358547002077103\n",
      "Training iter #2370000:   Batch Loss = 12.311165, Accuracy = 0.09933333098888397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.184895515441895, Accuracy = 0.12358547002077103\n",
      "Training iter #2373000:   Batch Loss = 12.120464, Accuracy = 0.13333334028720856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.182950973510742, Accuracy = 0.12358547002077103\n",
      "Training iter #2376000:   Batch Loss = 11.691001, Accuracy = 0.18199999630451202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.181022644042969, Accuracy = 0.12358547002077103\n",
      "Training iter #2379000:   Batch Loss = 11.909035, Accuracy = 0.17466667294502258\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.178945541381836, Accuracy = 0.12358547002077103\n",
      "Training iter #2382000:   Batch Loss = 12.076817, Accuracy = 0.12733332812786102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.176814079284668, Accuracy = 0.12358547002077103\n",
      "Training iter #2385000:   Batch Loss = 12.545938, Accuracy = 0.09600000083446503\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.174799919128418, Accuracy = 0.12418106198310852\n",
      "Training iter #2388000:   Batch Loss = 12.147322, Accuracy = 0.12266666442155838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.172821998596191, Accuracy = 0.12388326227664948\n",
      "Training iter #2391000:   Batch Loss = 11.759823, Accuracy = 0.15066666901111603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.170859336853027, Accuracy = 0.12388326227664948\n",
      "Training iter #2394000:   Batch Loss = 12.075825, Accuracy = 0.140666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.168846130371094, Accuracy = 0.12447885423898697\n",
      "Training iter #2397000:   Batch Loss = 12.216713, Accuracy = 0.10599999874830246\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.166879653930664, Accuracy = 0.12447885423898697\n",
      "Training iter #2400000:   Batch Loss = 12.064121, Accuracy = 0.140666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.164941787719727, Accuracy = 0.12447885423898697\n",
      "Training iter #2403000:   Batch Loss = 11.785627, Accuracy = 0.164000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.162948608398438, Accuracy = 0.12447885423898697\n",
      "Training iter #2406000:   Batch Loss = 11.876322, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.16092300415039, Accuracy = 0.12477665394544601\n",
      "Training iter #2409000:   Batch Loss = 12.060124, Accuracy = 0.12666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.158855438232422, Accuracy = 0.12477665394544601\n",
      "Training iter #2412000:   Batch Loss = 12.453779, Accuracy = 0.10466666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.156841278076172, Accuracy = 0.12477665394544601\n",
      "Training iter #2415000:   Batch Loss = 12.006032, Accuracy = 0.14800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.15483283996582, Accuracy = 0.12567004561424255\n",
      "Training iter #2418000:   Batch Loss = 11.765393, Accuracy = 0.1446666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.152813911437988, Accuracy = 0.12626563012599945\n",
      "Training iter #2421000:   Batch Loss = 12.074805, Accuracy = 0.1313333362340927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.150606155395508, Accuracy = 0.1265634298324585\n",
      "Training iter #2424000:   Batch Loss = 12.222111, Accuracy = 0.10533333569765091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.14840030670166, Accuracy = 0.12745681405067444\n",
      "Training iter #2427000:   Batch Loss = 12.096179, Accuracy = 0.1433333307504654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.146097183227539, Accuracy = 0.12775461375713348\n",
      "Training iter #2430000:   Batch Loss = 11.893351, Accuracy = 0.12666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.14402961730957, Accuracy = 0.12775461375713348\n",
      "Training iter #2433000:   Batch Loss = 11.794833, Accuracy = 0.21199999749660492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.142108917236328, Accuracy = 0.12805241346359253\n",
      "Training iter #2436000:   Batch Loss = 12.043352, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.1403169631958, Accuracy = 0.12835021317005157\n",
      "Training iter #2439000:   Batch Loss = 12.436909, Accuracy = 0.11133333295583725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.138557434082031, Accuracy = 0.12864799797534943\n",
      "Training iter #2442000:   Batch Loss = 11.847517, Accuracy = 0.18199999630451202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.13674545288086, Accuracy = 0.12864799797534943\n",
      "Training iter #2445000:   Batch Loss = 11.748595, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.134866714477539, Accuracy = 0.12864799797534943\n",
      "Training iter #2448000:   Batch Loss = 12.095437, Accuracy = 0.1106666699051857\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.132774353027344, Accuracy = 0.12924359738826752\n",
      "Training iter #2451000:   Batch Loss = 12.195618, Accuracy = 0.10466666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.130630493164062, Accuracy = 0.12954139709472656\n",
      "Training iter #2454000:   Batch Loss = 12.078089, Accuracy = 0.14866666495800018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.12848949432373, Accuracy = 0.12894579768180847\n",
      "Training iter #2457000:   Batch Loss = 11.956650, Accuracy = 0.10466666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.126310348510742, Accuracy = 0.12894579768180847\n",
      "Training iter #2460000:   Batch Loss = 11.903265, Accuracy = 0.20399999618530273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.124286651611328, Accuracy = 0.12954139709472656\n",
      "Training iter #2463000:   Batch Loss = 12.044785, Accuracy = 0.13333334028720856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.122296333312988, Accuracy = 0.1298391968011856\n",
      "Training iter #2466000:   Batch Loss = 12.366550, Accuracy = 0.12200000137090683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.120439529418945, Accuracy = 0.12924359738826752\n",
      "Training iter #2469000:   Batch Loss = 11.761462, Accuracy = 0.17666666209697723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.118855476379395, Accuracy = 0.12924359738826752\n",
      "Training iter #2472000:   Batch Loss = 11.664118, Accuracy = 0.1733333319425583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.117055892944336, Accuracy = 0.1298391968011856\n",
      "Training iter #2475000:   Batch Loss = 12.092554, Accuracy = 0.11933333426713943\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.115177154541016, Accuracy = 0.12954139709472656\n",
      "Training iter #2478000:   Batch Loss = 12.179585, Accuracy = 0.10400000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.11319351196289, Accuracy = 0.1310303807258606\n",
      "Training iter #2481000:   Batch Loss = 12.064600, Accuracy = 0.14666666090488434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.111612319946289, Accuracy = 0.13073258101940155\n",
      "Training iter #2484000:   Batch Loss = 11.807805, Accuracy = 0.12800000607967377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.10986328125, Accuracy = 0.1310303807258606\n",
      "Training iter #2487000:   Batch Loss = 11.952550, Accuracy = 0.17800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.107932090759277, Accuracy = 0.1316259652376175\n",
      "Training iter #2490000:   Batch Loss = 12.087580, Accuracy = 0.12866666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.105877876281738, Accuracy = 0.1316259652376175\n",
      "Training iter #2493000:   Batch Loss = 12.362421, Accuracy = 0.12333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.103696823120117, Accuracy = 0.1316259652376175\n",
      "Training iter #2496000:   Batch Loss = 11.741983, Accuracy = 0.1860000044107437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.10165786743164, Accuracy = 0.13222156465053558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2499000:   Batch Loss = 11.808053, Accuracy = 0.1693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.099567413330078, Accuracy = 0.13192376494407654\n",
      "Training iter #2502000:   Batch Loss = 11.982655, Accuracy = 0.1420000046491623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.09695816040039, Accuracy = 0.13222156465053558\n",
      "Training iter #2505000:   Batch Loss = 12.172592, Accuracy = 0.1080000028014183\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.094886779785156, Accuracy = 0.13251934945583344\n",
      "Training iter #2508000:   Batch Loss = 12.083132, Accuracy = 0.14866666495800018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.092878341674805, Accuracy = 0.13341274857521057\n",
      "Training iter #2511000:   Batch Loss = 11.672782, Accuracy = 0.12666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.090848922729492, Accuracy = 0.13371054828166962\n",
      "Training iter #2514000:   Batch Loss = 11.866239, Accuracy = 0.17599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.088871955871582, Accuracy = 0.13371054828166962\n",
      "Training iter #2517000:   Batch Loss = 12.082189, Accuracy = 0.1313333362340927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.086830139160156, Accuracy = 0.1343061327934265\n",
      "Training iter #2520000:   Batch Loss = 12.296932, Accuracy = 0.13333334028720856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.08479118347168, Accuracy = 0.1343061327934265\n",
      "Training iter #2523000:   Batch Loss = 11.755014, Accuracy = 0.1733333319425583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.082108497619629, Accuracy = 0.1354973167181015\n",
      "Training iter #2526000:   Batch Loss = 11.845326, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.079984664916992, Accuracy = 0.1354973167181015\n",
      "Training iter #2529000:   Batch Loss = 11.921641, Accuracy = 0.15199999511241913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.077960014343262, Accuracy = 0.1354973167181015\n",
      "Training iter #2532000:   Batch Loss = 12.135135, Accuracy = 0.11400000005960464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.075973510742188, Accuracy = 0.1354973167181015\n",
      "Training iter #2535000:   Batch Loss = 12.036853, Accuracy = 0.15600000321865082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.074119567871094, Accuracy = 0.1354973167181015\n",
      "Training iter #2538000:   Batch Loss = 11.620374, Accuracy = 0.1326666623353958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.072259902954102, Accuracy = 0.1360929161310196\n",
      "Training iter #2541000:   Batch Loss = 11.812944, Accuracy = 0.17533333599567413\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.070292472839355, Accuracy = 0.1360929161310196\n",
      "Training iter #2544000:   Batch Loss = 12.073917, Accuracy = 0.1340000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.068159103393555, Accuracy = 0.13639071583747864\n",
      "Training iter #2547000:   Batch Loss = 12.289009, Accuracy = 0.14399999380111694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.066120147705078, Accuracy = 0.13698630034923553\n",
      "Training iter #2550000:   Batch Loss = 11.808546, Accuracy = 0.15933333337306976\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.064156532287598, Accuracy = 0.13728410005569458\n",
      "Training iter #2553000:   Batch Loss = 11.829524, Accuracy = 0.17599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.062277793884277, Accuracy = 0.13817748427391052\n",
      "Training iter #2556000:   Batch Loss = 11.900241, Accuracy = 0.15199999511241913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.060356140136719, Accuracy = 0.1387730836868286\n",
      "Training iter #2559000:   Batch Loss = 12.112556, Accuracy = 0.11266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.058469772338867, Accuracy = 0.13817748427391052\n",
      "Training iter #2562000:   Batch Loss = 11.933997, Accuracy = 0.16599999368190765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.05651569366455, Accuracy = 0.13847528398036957\n",
      "Training iter #2565000:   Batch Loss = 11.626638, Accuracy = 0.12733332812786102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.054494857788086, Accuracy = 0.1387730836868286\n",
      "Training iter #2568000:   Batch Loss = 11.882297, Accuracy = 0.1653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.052251815795898, Accuracy = 0.1387730836868286\n",
      "Training iter #2571000:   Batch Loss = 12.117075, Accuracy = 0.1326666623353958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.050117492675781, Accuracy = 0.13907086849212646\n",
      "Training iter #2574000:   Batch Loss = 12.219692, Accuracy = 0.15333333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.048112869262695, Accuracy = 0.13907086849212646\n",
      "Training iter #2577000:   Batch Loss = 11.820310, Accuracy = 0.15066666901111603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.046199798583984, Accuracy = 0.1393686681985855\n",
      "Training iter #2580000:   Batch Loss = 11.789302, Accuracy = 0.17800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.044222831726074, Accuracy = 0.13966646790504456\n",
      "Training iter #2583000:   Batch Loss = 11.849470, Accuracy = 0.1653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.042169570922852, Accuracy = 0.13966646790504456\n",
      "Training iter #2586000:   Batch Loss = 12.145239, Accuracy = 0.1133333370089531\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.040173530578613, Accuracy = 0.1399642676115036\n",
      "Training iter #2589000:   Batch Loss = 11.920532, Accuracy = 0.15533334016799927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.03831672668457, Accuracy = 0.14026206731796265\n",
      "Training iter #2592000:   Batch Loss = 11.585937, Accuracy = 0.140666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.036369323730469, Accuracy = 0.1405598521232605\n",
      "Training iter #2595000:   Batch Loss = 11.869102, Accuracy = 0.16200000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.034429550170898, Accuracy = 0.14085765182971954\n",
      "Training iter #2598000:   Batch Loss = 12.119536, Accuracy = 0.12666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.032415390014648, Accuracy = 0.14085765182971954\n",
      "Training iter #2601000:   Batch Loss = 12.112273, Accuracy = 0.15800000727176666\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.030510902404785, Accuracy = 0.14085765182971954\n",
      "Training iter #2604000:   Batch Loss = 11.797293, Accuracy = 0.15666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.028566360473633, Accuracy = 0.1411554515361786\n",
      "Training iter #2607000:   Batch Loss = 11.780273, Accuracy = 0.18666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.026527404785156, Accuracy = 0.1411554515361786\n",
      "Training iter #2610000:   Batch Loss = 11.811757, Accuracy = 0.1720000058412552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.024433135986328, Accuracy = 0.14145325124263763\n",
      "Training iter #2613000:   Batch Loss = 12.157467, Accuracy = 0.1146666631102562\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.02262020111084, Accuracy = 0.14145325124263763\n",
      "Training iter #2616000:   Batch Loss = 11.973880, Accuracy = 0.1446666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.02078628540039, Accuracy = 0.14234663546085358\n",
      "Training iter #2619000:   Batch Loss = 11.647546, Accuracy = 0.13600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.019022941589355, Accuracy = 0.14324001967906952\n",
      "Training iter #2622000:   Batch Loss = 11.885315, Accuracy = 0.15666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.016949653625488, Accuracy = 0.14324001967906952\n",
      "Training iter #2625000:   Batch Loss = 12.123160, Accuracy = 0.12066666781902313\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.015045166015625, Accuracy = 0.14353781938552856\n",
      "Training iter #2628000:   Batch Loss = 12.087687, Accuracy = 0.16466666758060455\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.013288497924805, Accuracy = 0.1438356190919876\n",
      "Training iter #2631000:   Batch Loss = 11.691875, Accuracy = 0.17666666209697723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.011482238769531, Accuracy = 0.1438356190919876\n",
      "Training iter #2634000:   Batch Loss = 11.687808, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.009615898132324, Accuracy = 0.1438356190919876\n",
      "Training iter #2637000:   Batch Loss = 11.783414, Accuracy = 0.17599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.007390975952148, Accuracy = 0.14472900331020355\n",
      "Training iter #2640000:   Batch Loss = 12.171965, Accuracy = 0.11999999731779099\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.00508975982666, Accuracy = 0.1450268030166626\n",
      "Training iter #2643000:   Batch Loss = 11.998586, Accuracy = 0.13199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.003148078918457, Accuracy = 0.14621798694133759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2646000:   Batch Loss = 11.653177, Accuracy = 0.1353333294391632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 12.001033782958984, Accuracy = 0.14621798694133759\n",
      "Training iter #2649000:   Batch Loss = 11.840095, Accuracy = 0.164000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.999103546142578, Accuracy = 0.14681358635425568\n",
      "Training iter #2652000:   Batch Loss = 12.117798, Accuracy = 0.11533333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.997251510620117, Accuracy = 0.14651578664779663\n",
      "Training iter #2655000:   Batch Loss = 12.029954, Accuracy = 0.16333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.995441436767578, Accuracy = 0.14651578664779663\n",
      "Training iter #2658000:   Batch Loss = 11.565042, Accuracy = 0.19066666066646576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.993557929992676, Accuracy = 0.14711137115955353\n",
      "Training iter #2661000:   Batch Loss = 11.763750, Accuracy = 0.18733333051204681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.99162483215332, Accuracy = 0.14740917086601257\n",
      "Training iter #2664000:   Batch Loss = 11.829293, Accuracy = 0.17800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.989717483520508, Accuracy = 0.14740917086601257\n",
      "Training iter #2667000:   Batch Loss = 12.226881, Accuracy = 0.11999999731779099\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.987902641296387, Accuracy = 0.14711137115955353\n",
      "Training iter #2670000:   Batch Loss = 11.973727, Accuracy = 0.12933333218097687\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.98639965057373, Accuracy = 0.14740917086601257\n",
      "Training iter #2673000:   Batch Loss = 11.534890, Accuracy = 0.16066665947437286\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.98456859588623, Accuracy = 0.14740917086601257\n",
      "Training iter #2676000:   Batch Loss = 11.779198, Accuracy = 0.17000000178813934\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.982609748840332, Accuracy = 0.14770697057247162\n",
      "Training iter #2679000:   Batch Loss = 12.100500, Accuracy = 0.11933333426713943\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.980682373046875, Accuracy = 0.14800477027893066\n",
      "Training iter #2682000:   Batch Loss = 11.978191, Accuracy = 0.1599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.978696823120117, Accuracy = 0.14860035479068756\n",
      "Training iter #2685000:   Batch Loss = 11.563013, Accuracy = 0.19599999487400055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.976835250854492, Accuracy = 0.1494937390089035\n",
      "Training iter #2688000:   Batch Loss = 11.731297, Accuracy = 0.18933333456516266\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.975085258483887, Accuracy = 0.14979153871536255\n",
      "Training iter #2691000:   Batch Loss = 11.824368, Accuracy = 0.1693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.972675323486328, Accuracy = 0.1494937390089035\n",
      "Training iter #2694000:   Batch Loss = 12.285206, Accuracy = 0.12133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.970636367797852, Accuracy = 0.1494937390089035\n",
      "Training iter #2697000:   Batch Loss = 11.959248, Accuracy = 0.13866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.968751907348633, Accuracy = 0.1494937390089035\n",
      "Training iter #2700000:   Batch Loss = 11.617085, Accuracy = 0.15266667306423187\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.966920852661133, Accuracy = 0.14979153871536255\n",
      "Training iter #2703000:   Batch Loss = 11.828543, Accuracy = 0.17466667294502258\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.965046882629395, Accuracy = 0.1500893384218216\n",
      "Training iter #2706000:   Batch Loss = 12.066910, Accuracy = 0.11599999666213989\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.96316909790039, Accuracy = 0.15038713812828064\n",
      "Training iter #2709000:   Batch Loss = 11.835850, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.961241722106934, Accuracy = 0.15038713812828064\n",
      "Training iter #2712000:   Batch Loss = 11.500254, Accuracy = 0.20200000703334808\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.959299087524414, Accuracy = 0.15068493783473969\n",
      "Training iter #2715000:   Batch Loss = 11.673642, Accuracy = 0.19733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.957419395446777, Accuracy = 0.15068493783473969\n",
      "Training iter #2718000:   Batch Loss = 11.816663, Accuracy = 0.1586666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.955476760864258, Accuracy = 0.15128052234649658\n",
      "Training iter #2721000:   Batch Loss = 12.317236, Accuracy = 0.12733332812786102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.953546524047852, Accuracy = 0.15217390656471252\n",
      "Training iter #2724000:   Batch Loss = 11.868398, Accuracy = 0.15199999511241913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.951597213745117, Accuracy = 0.15306730568408966\n",
      "Training iter #2727000:   Batch Loss = 11.550035, Accuracy = 0.16733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.949572563171387, Accuracy = 0.1533651053905487\n",
      "Training iter #2730000:   Batch Loss = 11.847996, Accuracy = 0.1706666648387909\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.947614669799805, Accuracy = 0.1539606899023056\n",
      "Training iter #2733000:   Batch Loss = 12.054766, Accuracy = 0.12066666781902313\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.945645332336426, Accuracy = 0.1545562893152237\n",
      "Training iter #2736000:   Batch Loss = 11.888647, Accuracy = 0.16866666078567505\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.943757057189941, Accuracy = 0.15485407412052155\n",
      "Training iter #2739000:   Batch Loss = 11.569423, Accuracy = 0.17733334004878998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.94184684753418, Accuracy = 0.15485407412052155\n",
      "Training iter #2742000:   Batch Loss = 11.627149, Accuracy = 0.21066667139530182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.939925193786621, Accuracy = 0.15485407412052155\n",
      "Training iter #2745000:   Batch Loss = 11.872582, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.937975883483887, Accuracy = 0.15485407412052155\n",
      "Training iter #2748000:   Batch Loss = 12.213713, Accuracy = 0.12666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.936046600341797, Accuracy = 0.15544967353343964\n",
      "Training iter #2751000:   Batch Loss = 11.743692, Accuracy = 0.17533333599567413\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.934150695800781, Accuracy = 0.15574747323989868\n",
      "Training iter #2754000:   Batch Loss = 11.526381, Accuracy = 0.16733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.932181358337402, Accuracy = 0.15664085745811462\n",
      "Training iter #2757000:   Batch Loss = 11.898298, Accuracy = 0.1459999978542328\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.93013858795166, Accuracy = 0.15664085745811462\n",
      "Training iter #2760000:   Batch Loss = 11.984755, Accuracy = 0.1379999965429306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.928147315979004, Accuracy = 0.15664085745811462\n",
      "Training iter #2763000:   Batch Loss = 11.904920, Accuracy = 0.1613333374261856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.92625617980957, Accuracy = 0.15693865716457367\n",
      "Training iter #2766000:   Batch Loss = 11.662208, Accuracy = 0.14533333480358124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.924361228942871, Accuracy = 0.15723645687103271\n",
      "Training iter #2769000:   Batch Loss = 11.617347, Accuracy = 0.23399999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.922447204589844, Accuracy = 0.1578320413827896\n",
      "Training iter #2772000:   Batch Loss = 11.810950, Accuracy = 0.1706666648387909\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.920465469360352, Accuracy = 0.1584276407957077\n",
      "Training iter #2775000:   Batch Loss = 12.192348, Accuracy = 0.12999999523162842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.918457984924316, Accuracy = 0.1584276407957077\n",
      "Training iter #2778000:   Batch Loss = 11.588338, Accuracy = 0.19599999487400055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.916533470153809, Accuracy = 0.15872542560100555\n",
      "Training iter #2781000:   Batch Loss = 11.490333, Accuracy = 0.18199999630451202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.914594650268555, Accuracy = 0.15872542560100555\n",
      "Training iter #2784000:   Batch Loss = 11.863136, Accuracy = 0.1433333307504654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.912603378295898, Accuracy = 0.1590232253074646\n",
      "Training iter #2787000:   Batch Loss = 11.985771, Accuracy = 0.1393333375453949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.910700798034668, Accuracy = 0.1596188247203827\n",
      "Training iter #2790000:   Batch Loss = 11.926029, Accuracy = 0.16466666758060455\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.908916473388672, Accuracy = 0.16110780835151672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2793000:   Batch Loss = 11.684997, Accuracy = 0.13066667318344116\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.907057762145996, Accuracy = 0.16170339286327362\n",
      "Training iter #2796000:   Batch Loss = 11.709376, Accuracy = 0.20733332633972168\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.905136108398438, Accuracy = 0.16170339286327362\n",
      "Training iter #2799000:   Batch Loss = 11.822929, Accuracy = 0.164000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.903371810913086, Accuracy = 0.1628945767879486\n",
      "Training iter #2802000:   Batch Loss = 12.202347, Accuracy = 0.13600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.90151596069336, Accuracy = 0.1628945767879486\n",
      "Training iter #2805000:   Batch Loss = 11.538933, Accuracy = 0.19733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.89967155456543, Accuracy = 0.16319237649440765\n",
      "Training iter #2808000:   Batch Loss = 11.475576, Accuracy = 0.1940000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.897692680358887, Accuracy = 0.1634901762008667\n",
      "Training iter #2811000:   Batch Loss = 11.811328, Accuracy = 0.1653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.895759582519531, Accuracy = 0.16378797590732574\n",
      "Training iter #2814000:   Batch Loss = 11.982490, Accuracy = 0.1366666704416275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.893850326538086, Accuracy = 0.1646813601255417\n",
      "Training iter #2817000:   Batch Loss = 11.876115, Accuracy = 0.17533333599567413\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.891976356506348, Accuracy = 0.1646813601255417\n",
      "Training iter #2820000:   Batch Loss = 11.467980, Accuracy = 0.15266667306423187\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.8900785446167, Accuracy = 0.1646813601255417\n",
      "Training iter #2823000:   Batch Loss = 11.683975, Accuracy = 0.19599999487400055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.888129234313965, Accuracy = 0.1646813601255417\n",
      "Training iter #2826000:   Batch Loss = 11.875156, Accuracy = 0.15666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.88625717163086, Accuracy = 0.16587254405021667\n",
      "Training iter #2829000:   Batch Loss = 12.127726, Accuracy = 0.14800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.884416580200195, Accuracy = 0.16587254405021667\n",
      "Training iter #2832000:   Batch Loss = 11.521413, Accuracy = 0.21066667139530182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.882659912109375, Accuracy = 0.16646812856197357\n",
      "Training iter #2835000:   Batch Loss = 11.647514, Accuracy = 0.18066667020320892\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.880661964416504, Accuracy = 0.16706372797489166\n",
      "Training iter #2838000:   Batch Loss = 11.710897, Accuracy = 0.18266665935516357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.878462791442871, Accuracy = 0.1673615276813507\n",
      "Training iter #2841000:   Batch Loss = 11.981391, Accuracy = 0.13600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.876484870910645, Accuracy = 0.1673615276813507\n",
      "Training iter #2844000:   Batch Loss = 11.934896, Accuracy = 0.1666666716337204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.874625205993652, Accuracy = 0.16765932738780975\n",
      "Training iter #2847000:   Batch Loss = 11.398494, Accuracy = 0.1599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.872714042663574, Accuracy = 0.16825491189956665\n",
      "Training iter #2850000:   Batch Loss = 11.642065, Accuracy = 0.1979999989271164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.870838165283203, Accuracy = 0.16825491189956665\n",
      "Training iter #2853000:   Batch Loss = 11.882574, Accuracy = 0.164000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.86911678314209, Accuracy = 0.16825491189956665\n",
      "Training iter #2856000:   Batch Loss = 12.073851, Accuracy = 0.15333333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.867481231689453, Accuracy = 0.16885051131248474\n",
      "Training iter #2859000:   Batch Loss = 11.614428, Accuracy = 0.18199999630451202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.866022109985352, Accuracy = 0.1691482961177826\n",
      "Training iter #2862000:   Batch Loss = 11.607826, Accuracy = 0.1926666647195816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.86425495147705, Accuracy = 0.1691482961177826\n",
      "Training iter #2865000:   Batch Loss = 11.721888, Accuracy = 0.18666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.862079620361328, Accuracy = 0.1691482961177826\n",
      "Training iter #2868000:   Batch Loss = 11.930708, Accuracy = 0.13466666638851166\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.860309600830078, Accuracy = 0.16944609582424164\n",
      "Training iter #2871000:   Batch Loss = 11.851053, Accuracy = 0.1706666648387909\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.858467102050781, Accuracy = 0.16944609582424164\n",
      "Training iter #2874000:   Batch Loss = 11.400148, Accuracy = 0.16066665947437286\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.85666561126709, Accuracy = 0.17033949494361877\n",
      "Training iter #2877000:   Batch Loss = 11.597107, Accuracy = 0.20466665923595428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.854938507080078, Accuracy = 0.17033949494361877\n",
      "Training iter #2880000:   Batch Loss = 11.921310, Accuracy = 0.15800000727176666\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.853010177612305, Accuracy = 0.17093507945537567\n",
      "Training iter #2883000:   Batch Loss = 12.058026, Accuracy = 0.16599999368190765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.851080894470215, Accuracy = 0.17123287916183472\n",
      "Training iter #2886000:   Batch Loss = 11.636211, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.849096298217773, Accuracy = 0.17123287916183472\n",
      "Training iter #2889000:   Batch Loss = 11.580297, Accuracy = 0.19066666066646576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.847101211547852, Accuracy = 0.17123287916183472\n",
      "Training iter #2892000:   Batch Loss = 11.667598, Accuracy = 0.1966666728258133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.845154762268066, Accuracy = 0.1718284636735916\n",
      "Training iter #2895000:   Batch Loss = 11.913304, Accuracy = 0.14133332669734955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.843292236328125, Accuracy = 0.1718284636735916\n",
      "Training iter #2898000:   Batch Loss = 11.790929, Accuracy = 0.17266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.841522216796875, Accuracy = 0.17212626338005066\n",
      "Training iter #2901000:   Batch Loss = 11.383909, Accuracy = 0.16733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.839715957641602, Accuracy = 0.17272186279296875\n",
      "Training iter #2904000:   Batch Loss = 11.687206, Accuracy = 0.18933333456516266\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.837871551513672, Accuracy = 0.17331744730472565\n",
      "Training iter #2907000:   Batch Loss = 11.908587, Accuracy = 0.15533334016799927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.836020469665527, Accuracy = 0.1736152470111847\n",
      "Training iter #2910000:   Batch Loss = 11.994405, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.834274291992188, Accuracy = 0.17391304671764374\n",
      "Training iter #2913000:   Batch Loss = 11.625427, Accuracy = 0.17800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.832520484924316, Accuracy = 0.17391304671764374\n",
      "Training iter #2916000:   Batch Loss = 11.591478, Accuracy = 0.1913333386182785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.830694198608398, Accuracy = 0.17480643093585968\n",
      "Training iter #2919000:   Batch Loss = 11.626572, Accuracy = 0.21066667139530182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.828810691833496, Accuracy = 0.17510423064231873\n",
      "Training iter #2922000:   Batch Loss = 11.930576, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.826980590820312, Accuracy = 0.17540203034877777\n",
      "Training iter #2925000:   Batch Loss = 11.786672, Accuracy = 0.1626666635274887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.825225830078125, Accuracy = 0.17599761486053467\n",
      "Training iter #2928000:   Batch Loss = 11.348032, Accuracy = 0.1693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.823413848876953, Accuracy = 0.17659321427345276\n",
      "Training iter #2931000:   Batch Loss = 11.632335, Accuracy = 0.19333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.82155990600586, Accuracy = 0.17659321427345276\n",
      "Training iter #2934000:   Batch Loss = 11.924304, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.819747924804688, Accuracy = 0.17659321427345276\n",
      "Training iter #2937000:   Batch Loss = 11.889527, Accuracy = 0.18333333730697632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.818004608154297, Accuracy = 0.17659321427345276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2940000:   Batch Loss = 11.592524, Accuracy = 0.18466666340827942\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.816215515136719, Accuracy = 0.17659321427345276\n",
      "Training iter #2943000:   Batch Loss = 11.519076, Accuracy = 0.20866666734218597\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.814393997192383, Accuracy = 0.17718879878520966\n",
      "Training iter #2946000:   Batch Loss = 11.541262, Accuracy = 0.2293333262205124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.812515258789062, Accuracy = 0.1780821979045868\n",
      "Training iter #2949000:   Batch Loss = 12.001243, Accuracy = 0.1393333375453949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.810686111450195, Accuracy = 0.17837998270988464\n",
      "Training iter #2952000:   Batch Loss = 11.728535, Accuracy = 0.15533334016799927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.808956146240234, Accuracy = 0.17897558212280273\n",
      "Training iter #2955000:   Batch Loss = 11.451448, Accuracy = 0.15466666221618652\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.807032585144043, Accuracy = 0.17897558212280273\n",
      "Training iter #2958000:   Batch Loss = 11.643882, Accuracy = 0.195333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.805181503295898, Accuracy = 0.17897558212280273\n",
      "Training iter #2961000:   Batch Loss = 11.957773, Accuracy = 0.14399999380111694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.803363800048828, Accuracy = 0.17927338182926178\n",
      "Training iter #2964000:   Batch Loss = 11.895460, Accuracy = 0.18333333730697632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.801565170288086, Accuracy = 0.18016676604747772\n",
      "Training iter #2967000:   Batch Loss = 11.376580, Accuracy = 0.22066666185855865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.799728393554688, Accuracy = 0.18046456575393677\n",
      "Training iter #2970000:   Batch Loss = 11.517365, Accuracy = 0.20866666734218597\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.797886848449707, Accuracy = 0.1807623654603958\n",
      "Training iter #2973000:   Batch Loss = 11.582190, Accuracy = 0.2280000001192093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.79600715637207, Accuracy = 0.18106015026569366\n",
      "Training iter #2976000:   Batch Loss = 11.994622, Accuracy = 0.1446666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.794149398803711, Accuracy = 0.18106015026569366\n",
      "Training iter #2979000:   Batch Loss = 11.777906, Accuracy = 0.14533333480358124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.792362213134766, Accuracy = 0.18106015026569366\n",
      "Training iter #2982000:   Batch Loss = 11.354321, Accuracy = 0.1733333319425583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.7906494140625, Accuracy = 0.18165574967861176\n",
      "Training iter #2985000:   Batch Loss = 11.606233, Accuracy = 0.19333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.788883209228516, Accuracy = 0.18284693360328674\n",
      "Training iter #2988000:   Batch Loss = 11.926332, Accuracy = 0.13866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.787056922912598, Accuracy = 0.1831447333097458\n",
      "Training iter #2991000:   Batch Loss = 11.794988, Accuracy = 0.19333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.785314559936523, Accuracy = 0.18374031782150269\n",
      "Training iter #2994000:   Batch Loss = 11.355556, Accuracy = 0.22333332896232605\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.783527374267578, Accuracy = 0.18374031782150269\n",
      "Training iter #2997000:   Batch Loss = 11.584386, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.781550407409668, Accuracy = 0.18374031782150269\n",
      "Training iter #3000000:   Batch Loss = 11.639642, Accuracy = 0.21266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.779573440551758, Accuracy = 0.18374031782150269\n",
      "Training iter #3003000:   Batch Loss = 12.027149, Accuracy = 0.15333333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.777787208557129, Accuracy = 0.18463371694087982\n",
      "Training iter #3006000:   Batch Loss = 11.748216, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.775940895080566, Accuracy = 0.18463371694087982\n",
      "Training iter #3009000:   Batch Loss = 11.351236, Accuracy = 0.18466666340827942\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.774076461791992, Accuracy = 0.18463371694087982\n",
      "Training iter #3012000:   Batch Loss = 11.582102, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.772210121154785, Accuracy = 0.18493150174617767\n",
      "Training iter #3015000:   Batch Loss = 11.904948, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.77049732208252, Accuracy = 0.18493150174617767\n",
      "Training iter #3018000:   Batch Loss = 11.741897, Accuracy = 0.1979999989271164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.76868724822998, Accuracy = 0.1858249008655548\n",
      "Training iter #3021000:   Batch Loss = 11.286426, Accuracy = 0.23133333027362823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.766796112060547, Accuracy = 0.1858249008655548\n",
      "Training iter #3024000:   Batch Loss = 11.464787, Accuracy = 0.21799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.764752388000488, Accuracy = 0.1858249008655548\n",
      "Training iter #3027000:   Batch Loss = 11.653312, Accuracy = 0.20333333313465118\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.76283073425293, Accuracy = 0.18612268567085266\n",
      "Training iter #3030000:   Batch Loss = 12.119626, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.761058807373047, Accuracy = 0.1858249008655548\n",
      "Training iter #3033000:   Batch Loss = 11.723825, Accuracy = 0.15600000321865082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.759302139282227, Accuracy = 0.1864204853773117\n",
      "Training iter #3036000:   Batch Loss = 11.352213, Accuracy = 0.18133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.75747299194336, Accuracy = 0.18731388449668884\n",
      "Training iter #3039000:   Batch Loss = 11.656535, Accuracy = 0.20999999344348907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.755561828613281, Accuracy = 0.18731388449668884\n",
      "Training iter #3042000:   Batch Loss = 11.863672, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.753622055053711, Accuracy = 0.1876116693019867\n",
      "Training iter #3045000:   Batch Loss = 11.637114, Accuracy = 0.21133333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.751836776733398, Accuracy = 0.18820726871490479\n",
      "Training iter #3048000:   Batch Loss = 11.338329, Accuracy = 0.2240000069141388\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.750078201293945, Accuracy = 0.18820726871490479\n",
      "Training iter #3051000:   Batch Loss = 11.456244, Accuracy = 0.22066666185855865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.748355865478516, Accuracy = 0.18850506842136383\n",
      "Training iter #3054000:   Batch Loss = 11.625708, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.746601104736328, Accuracy = 0.18880285322666168\n",
      "Training iter #3057000:   Batch Loss = 12.087379, Accuracy = 0.14666666090488434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.744861602783203, Accuracy = 0.18939845263957977\n",
      "Training iter #3060000:   Batch Loss = 11.589609, Accuracy = 0.18533332645893097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.74324893951416, Accuracy = 0.18939845263957977\n",
      "Training iter #3063000:   Batch Loss = 11.331823, Accuracy = 0.1860000044107437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.741510391235352, Accuracy = 0.18969625234603882\n",
      "Training iter #3066000:   Batch Loss = 11.634061, Accuracy = 0.2160000056028366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.739562034606934, Accuracy = 0.19029183685779572\n",
      "Training iter #3069000:   Batch Loss = 11.843795, Accuracy = 0.1599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.73764419555664, Accuracy = 0.19058963656425476\n",
      "Training iter #3072000:   Batch Loss = 11.716450, Accuracy = 0.20399999618530273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.735737800598145, Accuracy = 0.1908874362707138\n",
      "Training iter #3075000:   Batch Loss = 11.401417, Accuracy = 0.19733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.733823776245117, Accuracy = 0.1908874362707138\n",
      "Training iter #3078000:   Batch Loss = 11.407300, Accuracy = 0.2446666657924652\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.73190689086914, Accuracy = 0.1926742047071457\n",
      "Training iter #3081000:   Batch Loss = 11.648534, Accuracy = 0.20666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.72994613647461, Accuracy = 0.19297200441360474\n",
      "Training iter #3084000:   Batch Loss = 12.030308, Accuracy = 0.15399999916553497\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.72812557220459, Accuracy = 0.19297200441360474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3087000:   Batch Loss = 11.459381, Accuracy = 0.20933333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.726308822631836, Accuracy = 0.19297200441360474\n",
      "Training iter #3090000:   Batch Loss = 11.300913, Accuracy = 0.19333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.724481582641602, Accuracy = 0.19356760382652283\n",
      "Training iter #3093000:   Batch Loss = 11.679595, Accuracy = 0.1886666715145111\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.722606658935547, Accuracy = 0.19386538863182068\n",
      "Training iter #3096000:   Batch Loss = 11.829727, Accuracy = 0.15800000727176666\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.72077751159668, Accuracy = 0.19416318833827972\n",
      "Training iter #3099000:   Batch Loss = 11.691061, Accuracy = 0.21533332765102386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.71902847290039, Accuracy = 0.19416318833827972\n",
      "Training iter #3102000:   Batch Loss = 11.450472, Accuracy = 0.16733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.717244148254395, Accuracy = 0.19475878775119781\n",
      "Training iter #3105000:   Batch Loss = 11.440871, Accuracy = 0.2553333342075348\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.715384483337402, Accuracy = 0.19773675501346588\n",
      "Training iter #3108000:   Batch Loss = 11.613762, Accuracy = 0.21799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.71347427368164, Accuracy = 0.19863013923168182\n",
      "Training iter #3111000:   Batch Loss = 11.972417, Accuracy = 0.16333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.711609840393066, Accuracy = 0.19863013923168182\n",
      "Training iter #3114000:   Batch Loss = 11.360865, Accuracy = 0.24266666173934937\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.709762573242188, Accuracy = 0.19863013923168182\n",
      "Training iter #3117000:   Batch Loss = 11.213114, Accuracy = 0.23266667127609253\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.707955360412598, Accuracy = 0.19892793893814087\n",
      "Training iter #3120000:   Batch Loss = 11.662551, Accuracy = 0.1913333386182785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.706153869628906, Accuracy = 0.19892793893814087\n",
      "Training iter #3123000:   Batch Loss = 11.781355, Accuracy = 0.1613333374261856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.70431900024414, Accuracy = 0.19952352344989777\n",
      "Training iter #3126000:   Batch Loss = 11.675404, Accuracy = 0.21266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.702513694763184, Accuracy = 0.19952352344989777\n",
      "Training iter #3129000:   Batch Loss = 11.376679, Accuracy = 0.18266665935516357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.700658798217773, Accuracy = 0.1998213231563568\n",
      "Training iter #3132000:   Batch Loss = 11.528354, Accuracy = 0.2280000001192093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.698762893676758, Accuracy = 0.20011912286281586\n",
      "Training iter #3135000:   Batch Loss = 11.672476, Accuracy = 0.20800000429153442\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.696783065795898, Accuracy = 0.20071470737457275\n",
      "Training iter #3138000:   Batch Loss = 12.001293, Accuracy = 0.1586666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.694694519042969, Accuracy = 0.20071470737457275\n",
      "Training iter #3141000:   Batch Loss = 11.349810, Accuracy = 0.25333333015441895\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.692895889282227, Accuracy = 0.20131030678749084\n",
      "Training iter #3144000:   Batch Loss = 11.345450, Accuracy = 0.22066666185855865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.690879821777344, Accuracy = 0.2016081064939499\n",
      "Training iter #3147000:   Batch Loss = 11.596170, Accuracy = 0.20466665923595428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.689016342163086, Accuracy = 0.20190589129924774\n",
      "Training iter #3150000:   Batch Loss = 11.783644, Accuracy = 0.16333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.687215805053711, Accuracy = 0.20190589129924774\n",
      "Training iter #3153000:   Batch Loss = 11.716629, Accuracy = 0.20866666734218597\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.685503959655762, Accuracy = 0.2022036910057068\n",
      "Training iter #3156000:   Batch Loss = 11.211021, Accuracy = 0.1899999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.683746337890625, Accuracy = 0.20250149071216583\n",
      "Training iter #3159000:   Batch Loss = 11.464452, Accuracy = 0.2253333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.68197250366211, Accuracy = 0.20309707522392273\n",
      "Training iter #3162000:   Batch Loss = 11.686562, Accuracy = 0.20466665923595428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.68016242980957, Accuracy = 0.20369267463684082\n",
      "Training iter #3165000:   Batch Loss = 11.941299, Accuracy = 0.1693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.678375244140625, Accuracy = 0.20369267463684082\n",
      "Training iter #3168000:   Batch Loss = 11.311127, Accuracy = 0.2553333342075348\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.676593780517578, Accuracy = 0.20399047434329987\n",
      "Training iter #3171000:   Batch Loss = 11.435109, Accuracy = 0.20733332633972168\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.674736976623535, Accuracy = 0.20458605885505676\n",
      "Training iter #3174000:   Batch Loss = 11.499449, Accuracy = 0.22466666996479034\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.672810554504395, Accuracy = 0.20458605885505676\n",
      "Training iter #3177000:   Batch Loss = 11.773044, Accuracy = 0.16866666078567505\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.67093563079834, Accuracy = 0.20458605885505676\n",
      "Training iter #3180000:   Batch Loss = 11.681339, Accuracy = 0.2160000056028366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.669172286987305, Accuracy = 0.2048838585615158\n",
      "Training iter #3183000:   Batch Loss = 11.175304, Accuracy = 0.20399999618530273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.667305946350098, Accuracy = 0.2048838585615158\n",
      "Training iter #3186000:   Batch Loss = 11.440925, Accuracy = 0.22733333706855774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.665459632873535, Accuracy = 0.2048838585615158\n",
      "Training iter #3189000:   Batch Loss = 11.693509, Accuracy = 0.20200000703334808\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.663477897644043, Accuracy = 0.20518165826797485\n",
      "Training iter #3192000:   Batch Loss = 11.919030, Accuracy = 0.18066667020320892\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.661630630493164, Accuracy = 0.20577724277973175\n",
      "Training iter #3195000:   Batch Loss = 11.360800, Accuracy = 0.23800000548362732\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.659828186035156, Accuracy = 0.2060750424861908\n",
      "Training iter #3198000:   Batch Loss = 11.409298, Accuracy = 0.21533332765102386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.657978057861328, Accuracy = 0.2066706418991089\n",
      "Training iter #3201000:   Batch Loss = 11.483164, Accuracy = 0.23466666042804718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.656082153320312, Accuracy = 0.2066706418991089\n",
      "Training iter #3204000:   Batch Loss = 11.757211, Accuracy = 0.16733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.654302597045898, Accuracy = 0.20696842670440674\n",
      "Training iter #3207000:   Batch Loss = 11.578094, Accuracy = 0.22866666316986084\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.652520179748535, Accuracy = 0.20756402611732483\n",
      "Training iter #3210000:   Batch Loss = 11.186937, Accuracy = 0.1940000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.650697708129883, Accuracy = 0.20786182582378387\n",
      "Training iter #3213000:   Batch Loss = 11.437405, Accuracy = 0.23800000548362732\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.648819923400879, Accuracy = 0.20815962553024292\n",
      "Training iter #3216000:   Batch Loss = 11.741127, Accuracy = 0.18733333051204681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.646955490112305, Accuracy = 0.20875521004199982\n",
      "Training iter #3219000:   Batch Loss = 11.802505, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.645203590393066, Accuracy = 0.20964859426021576\n",
      "Training iter #3222000:   Batch Loss = 11.387700, Accuracy = 0.23733332753181458\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.643485069274902, Accuracy = 0.2105419933795929\n",
      "Training iter #3225000:   Batch Loss = 11.361006, Accuracy = 0.21933333575725555\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.641671180725098, Accuracy = 0.21083977818489075\n",
      "Training iter #3228000:   Batch Loss = 11.442801, Accuracy = 0.2486666738986969\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.639738082885742, Accuracy = 0.2111375778913498\n",
      "Training iter #3231000:   Batch Loss = 11.768183, Accuracy = 0.17133332788944244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.637937545776367, Accuracy = 0.2111375778913498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3234000:   Batch Loss = 11.537146, Accuracy = 0.218666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.636216163635254, Accuracy = 0.2111375778913498\n",
      "Training iter #3237000:   Batch Loss = 11.127132, Accuracy = 0.21466666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.634469032287598, Accuracy = 0.21173317730426788\n",
      "Training iter #3240000:   Batch Loss = 11.463934, Accuracy = 0.23866666853427887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.63268756866455, Accuracy = 0.21203097701072693\n",
      "Training iter #3243000:   Batch Loss = 11.769947, Accuracy = 0.17733334004878998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.63076400756836, Accuracy = 0.21203097701072693\n",
      "Training iter #3246000:   Batch Loss = 11.744406, Accuracy = 0.20666666328907013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.628868103027344, Accuracy = 0.21203097701072693\n",
      "Training iter #3249000:   Batch Loss = 11.397173, Accuracy = 0.24400000274181366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.627016067504883, Accuracy = 0.21322216093540192\n",
      "Training iter #3252000:   Batch Loss = 11.388861, Accuracy = 0.2226666659116745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.625099182128906, Accuracy = 0.21322216093540192\n",
      "Training iter #3255000:   Batch Loss = 11.432704, Accuracy = 0.25200000405311584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.62314224243164, Accuracy = 0.21322216093540192\n",
      "Training iter #3258000:   Batch Loss = 11.782416, Accuracy = 0.17133332788944244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.621307373046875, Accuracy = 0.21262656152248383\n",
      "Training iter #3261000:   Batch Loss = 11.575956, Accuracy = 0.2213333398103714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.619587898254395, Accuracy = 0.21322216093540192\n",
      "Training iter #3264000:   Batch Loss = 11.159061, Accuracy = 0.22599999606609344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.617867469787598, Accuracy = 0.21322216093540192\n",
      "Training iter #3267000:   Batch Loss = 11.433227, Accuracy = 0.24666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.616085052490234, Accuracy = 0.2144133448600769\n",
      "Training iter #3270000:   Batch Loss = 11.736703, Accuracy = 0.17733334004878998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.614457130432129, Accuracy = 0.21530672907829285\n",
      "Training iter #3273000:   Batch Loss = 11.723311, Accuracy = 0.21799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.612777709960938, Accuracy = 0.2156045287847519\n",
      "Training iter #3276000:   Batch Loss = 11.307156, Accuracy = 0.24400000274181366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.610953330993652, Accuracy = 0.2162001132965088\n",
      "Training iter #3279000:   Batch Loss = 11.319876, Accuracy = 0.23533333837985992\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.609132766723633, Accuracy = 0.2162001132965088\n",
      "Training iter #3282000:   Batch Loss = 11.315561, Accuracy = 0.27133333683013916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.607138633728027, Accuracy = 0.2162001132965088\n",
      "Training iter #3285000:   Batch Loss = 11.841662, Accuracy = 0.16866666078567505\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.60534381866455, Accuracy = 0.21709351241588593\n",
      "Training iter #3288000:   Batch Loss = 11.537382, Accuracy = 0.2266666740179062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.603665351867676, Accuracy = 0.21679571270942688\n",
      "Training iter #3291000:   Batch Loss = 11.209967, Accuracy = 0.21799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.6021089553833, Accuracy = 0.21679571270942688\n",
      "Training iter #3294000:   Batch Loss = 11.438379, Accuracy = 0.24799999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.60047721862793, Accuracy = 0.21739129722118378\n",
      "Training iter #3297000:   Batch Loss = 11.768188, Accuracy = 0.1666666716337204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.598610877990723, Accuracy = 0.21739129722118378\n",
      "Training iter #3300000:   Batch Loss = 11.693478, Accuracy = 0.21533332765102386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.596866607666016, Accuracy = 0.21739129722118378\n",
      "Training iter #3303000:   Batch Loss = 11.144893, Accuracy = 0.2746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.595112800598145, Accuracy = 0.21739129722118378\n",
      "Training iter #3306000:   Batch Loss = 11.342031, Accuracy = 0.23733332753181458\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.593368530273438, Accuracy = 0.21798689663410187\n",
      "Training iter #3309000:   Batch Loss = 11.409126, Accuracy = 0.2593333423137665\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.591581344604492, Accuracy = 0.21917808055877686\n",
      "Training iter #3312000:   Batch Loss = 11.853590, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.589831352233887, Accuracy = 0.21917808055877686\n",
      "Training iter #3315000:   Batch Loss = 11.592912, Accuracy = 0.21666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.588170051574707, Accuracy = 0.21977367997169495\n",
      "Training iter #3318000:   Batch Loss = 11.086574, Accuracy = 0.24666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.586487770080566, Accuracy = 0.22036926448345184\n",
      "Training iter #3321000:   Batch Loss = 11.364662, Accuracy = 0.2606666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.584749221801758, Accuracy = 0.22126266360282898\n",
      "Training iter #3324000:   Batch Loss = 11.765985, Accuracy = 0.1693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.582950592041016, Accuracy = 0.22156044840812683\n",
      "Training iter #3327000:   Batch Loss = 11.565285, Accuracy = 0.23066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.581266403198242, Accuracy = 0.22215604782104492\n",
      "Training iter #3330000:   Batch Loss = 11.153791, Accuracy = 0.2866666615009308\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.57961654663086, Accuracy = 0.22304943203926086\n",
      "Training iter #3333000:   Batch Loss = 11.355242, Accuracy = 0.2240000069141388\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.577889442443848, Accuracy = 0.22304943203926086\n",
      "Training iter #3336000:   Batch Loss = 11.407608, Accuracy = 0.2566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.576061248779297, Accuracy = 0.22304943203926086\n",
      "Training iter #3339000:   Batch Loss = 11.892677, Accuracy = 0.18000000715255737\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.574272155761719, Accuracy = 0.22304943203926086\n",
      "Training iter #3342000:   Batch Loss = 11.534687, Accuracy = 0.23000000417232513\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.572531700134277, Accuracy = 0.2233472317457199\n",
      "Training iter #3345000:   Batch Loss = 11.147657, Accuracy = 0.24066667258739471\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.570796966552734, Accuracy = 0.22364503145217896\n",
      "Training iter #3348000:   Batch Loss = 11.393852, Accuracy = 0.25999999046325684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.569014549255371, Accuracy = 0.22364503145217896\n",
      "Training iter #3351000:   Batch Loss = 11.738123, Accuracy = 0.16733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.567123413085938, Accuracy = 0.22543179988861084\n",
      "Training iter #3354000:   Batch Loss = 11.491338, Accuracy = 0.23199999332427979\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.56535530090332, Accuracy = 0.225134015083313\n",
      "Training iter #3357000:   Batch Loss = 11.045766, Accuracy = 0.30399999022483826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.563632011413574, Accuracy = 0.22543179988861084\n",
      "Training iter #3360000:   Batch Loss = 11.268511, Accuracy = 0.25466665625572205\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.561847686767578, Accuracy = 0.22572959959506989\n",
      "Training iter #3363000:   Batch Loss = 11.405251, Accuracy = 0.23600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.560013771057129, Accuracy = 0.22602739930152893\n",
      "Training iter #3366000:   Batch Loss = 11.962189, Accuracy = 0.18066667020320892\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.558202743530273, Accuracy = 0.22662298381328583\n",
      "Training iter #3369000:   Batch Loss = 11.452566, Accuracy = 0.2473333328962326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.556495666503906, Accuracy = 0.22692078351974487\n",
      "Training iter #3372000:   Batch Loss = 11.130864, Accuracy = 0.23933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.554795265197754, Accuracy = 0.22721858322620392\n",
      "Training iter #3375000:   Batch Loss = 11.437721, Accuracy = 0.2680000066757202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.553070068359375, Accuracy = 0.22721858322620392\n",
      "Training iter #3378000:   Batch Loss = 11.672081, Accuracy = 0.17933332920074463\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.551300048828125, Accuracy = 0.22811196744441986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3381000:   Batch Loss = 11.489246, Accuracy = 0.23666666448116302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.549603462219238, Accuracy = 0.22811196744441986\n",
      "Training iter #3384000:   Batch Loss = 11.115008, Accuracy = 0.2826666533946991\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.547901153564453, Accuracy = 0.2284097671508789\n",
      "Training iter #3387000:   Batch Loss = 11.260712, Accuracy = 0.2540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.546217918395996, Accuracy = 0.2284097671508789\n",
      "Training iter #3390000:   Batch Loss = 11.455834, Accuracy = 0.2240000069141388\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.544448852539062, Accuracy = 0.22870756685733795\n",
      "Training iter #3393000:   Batch Loss = 11.858651, Accuracy = 0.17733334004878998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.542709350585938, Accuracy = 0.22870756685733795\n",
      "Training iter #3396000:   Batch Loss = 11.334641, Accuracy = 0.2720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.54098129272461, Accuracy = 0.229005366563797\n",
      "Training iter #3399000:   Batch Loss = 11.113221, Accuracy = 0.24266666173934937\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.539257049560547, Accuracy = 0.2296009510755539\n",
      "Training iter #3402000:   Batch Loss = 11.451870, Accuracy = 0.2553333342075348\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.537464141845703, Accuracy = 0.23019655048847198\n",
      "Training iter #3405000:   Batch Loss = 11.643244, Accuracy = 0.18933333456516266\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.53564453125, Accuracy = 0.23079213500022888\n",
      "Training iter #3408000:   Batch Loss = 11.499631, Accuracy = 0.22866666316986084\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.53393840789795, Accuracy = 0.23079213500022888\n",
      "Training iter #3411000:   Batch Loss = 11.211386, Accuracy = 0.2566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.532252311706543, Accuracy = 0.23108993470668793\n",
      "Training iter #3414000:   Batch Loss = 11.224220, Accuracy = 0.2906666696071625\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.53048038482666, Accuracy = 0.23108993470668793\n",
      "Training iter #3417000:   Batch Loss = 11.398827, Accuracy = 0.24933333694934845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.528554916381836, Accuracy = 0.23168553411960602\n",
      "Training iter #3420000:   Batch Loss = 11.866042, Accuracy = 0.17399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.52673053741455, Accuracy = 0.23257891833782196\n",
      "Training iter #3423000:   Batch Loss = 11.173317, Accuracy = 0.31333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.524984359741211, Accuracy = 0.23228111863136292\n",
      "Training iter #3426000:   Batch Loss = 11.104342, Accuracy = 0.2486666738986969\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.5232515335083, Accuracy = 0.23257891833782196\n",
      "Training iter #3429000:   Batch Loss = 11.487701, Accuracy = 0.2280000001192093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.521405220031738, Accuracy = 0.232876718044281\n",
      "Training iter #3432000:   Batch Loss = 11.636328, Accuracy = 0.18733333051204681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.519586563110352, Accuracy = 0.23317450284957886\n",
      "Training iter #3435000:   Batch Loss = 11.516285, Accuracy = 0.2293333262205124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.517938613891602, Accuracy = 0.2334723025560379\n",
      "Training iter #3438000:   Batch Loss = 11.234751, Accuracy = 0.21400000154972076\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.516319274902344, Accuracy = 0.23377010226249695\n",
      "Training iter #3441000:   Batch Loss = 11.309257, Accuracy = 0.27533334493637085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.514615058898926, Accuracy = 0.23377010226249695\n",
      "Training iter #3444000:   Batch Loss = 11.430059, Accuracy = 0.2460000067949295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.512843132019043, Accuracy = 0.234067901968956\n",
      "Training iter #3447000:   Batch Loss = 11.839919, Accuracy = 0.19066666066646576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.511150360107422, Accuracy = 0.23436568677425385\n",
      "Training iter #3450000:   Batch Loss = 11.099997, Accuracy = 0.32866665720939636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.509464263916016, Accuracy = 0.2346634864807129\n",
      "Training iter #3453000:   Batch Loss = 11.049025, Accuracy = 0.28066667914390564\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.507793426513672, Accuracy = 0.2346634864807129\n",
      "Training iter #3456000:   Batch Loss = 11.438860, Accuracy = 0.23733332753181458\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.50599479675293, Accuracy = 0.23555688560009003\n",
      "Training iter #3459000:   Batch Loss = 11.611713, Accuracy = 0.19066666066646576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.504195213317871, Accuracy = 0.23615247011184692\n",
      "Training iter #3462000:   Batch Loss = 11.501822, Accuracy = 0.24133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.502669334411621, Accuracy = 0.23585467040538788\n",
      "Training iter #3465000:   Batch Loss = 11.026997, Accuracy = 0.24066667258739471\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.500778198242188, Accuracy = 0.23585467040538788\n",
      "Training iter #3468000:   Batch Loss = 11.299662, Accuracy = 0.27533334493637085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.499061584472656, Accuracy = 0.23645026981830597\n",
      "Training iter #3471000:   Batch Loss = 11.481153, Accuracy = 0.23999999463558197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.49726676940918, Accuracy = 0.23674806952476501\n",
      "Training iter #3474000:   Batch Loss = 11.788530, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.495539665222168, Accuracy = 0.23674806952476501\n",
      "Training iter #3477000:   Batch Loss = 11.089203, Accuracy = 0.34200000762939453\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.493839263916016, Accuracy = 0.23674806952476501\n",
      "Training iter #3480000:   Batch Loss = 11.220531, Accuracy = 0.25333333015441895\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.492155075073242, Accuracy = 0.23674806952476501\n",
      "Training iter #3483000:   Batch Loss = 11.330280, Accuracy = 0.25333333015441895\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.4903564453125, Accuracy = 0.23704585433006287\n",
      "Training iter #3486000:   Batch Loss = 11.607821, Accuracy = 0.19733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.488526344299316, Accuracy = 0.2373436540365219\n",
      "Training iter #3489000:   Batch Loss = 11.527328, Accuracy = 0.23999999463558197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.486794471740723, Accuracy = 0.23704585433006287\n",
      "Training iter #3492000:   Batch Loss = 10.955545, Accuracy = 0.2540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.48505973815918, Accuracy = 0.23704585433006287\n",
      "Training iter #3495000:   Batch Loss = 11.251059, Accuracy = 0.27666667103767395\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.483253479003906, Accuracy = 0.23793925344944\n",
      "Training iter #3498000:   Batch Loss = 11.508924, Accuracy = 0.23133333027362823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.481339454650879, Accuracy = 0.23883263766765594\n",
      "Training iter #3501000:   Batch Loss = 11.699579, Accuracy = 0.21666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.479524612426758, Accuracy = 0.23942823708057404\n",
      "Training iter #3504000:   Batch Loss = 11.166647, Accuracy = 0.30933332443237305\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.477811813354492, Accuracy = 0.23942823708057404\n",
      "Training iter #3507000:   Batch Loss = 11.176811, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.47612476348877, Accuracy = 0.24032162129878998\n",
      "Training iter #3510000:   Batch Loss = 11.281997, Accuracy = 0.2653333246707916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.474349021911621, Accuracy = 0.24061942100524902\n",
      "Training iter #3513000:   Batch Loss = 11.564570, Accuracy = 0.20866666734218597\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.47259521484375, Accuracy = 0.24091720581054688\n",
      "Training iter #3516000:   Batch Loss = 11.497147, Accuracy = 0.24933333694934845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.471138000488281, Accuracy = 0.24091720581054688\n",
      "Training iter #3519000:   Batch Loss = 10.972513, Accuracy = 0.26600000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.469526290893555, Accuracy = 0.24210840463638306\n",
      "Training iter #3522000:   Batch Loss = 11.192147, Accuracy = 0.28733333945274353\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.467874526977539, Accuracy = 0.24270398914813995\n",
      "Training iter #3525000:   Batch Loss = 11.502020, Accuracy = 0.22733333706855774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.4659423828125, Accuracy = 0.243001788854599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3528000:   Batch Loss = 11.697256, Accuracy = 0.218666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.464214324951172, Accuracy = 0.2435973733663559\n",
      "Training iter #3531000:   Batch Loss = 11.195055, Accuracy = 0.28866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.462447166442871, Accuracy = 0.2435973733663559\n",
      "Training iter #3534000:   Batch Loss = 11.175756, Accuracy = 0.27000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.460681915283203, Accuracy = 0.24449077248573303\n",
      "Training iter #3537000:   Batch Loss = 11.230742, Accuracy = 0.27799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.458893775939941, Accuracy = 0.24508635699748993\n",
      "Training iter #3540000:   Batch Loss = 11.578473, Accuracy = 0.2133333384990692\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.457164764404297, Accuracy = 0.24538415670394897\n",
      "Training iter #3543000:   Batch Loss = 11.394908, Accuracy = 0.25733333826065063\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.455554962158203, Accuracy = 0.24538415670394897\n",
      "Training iter #3546000:   Batch Loss = 10.975328, Accuracy = 0.2633333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.453872680664062, Accuracy = 0.24568195641040802\n",
      "Training iter #3549000:   Batch Loss = 11.294853, Accuracy = 0.2720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.452105522155762, Accuracy = 0.24597975611686707\n",
      "Training iter #3552000:   Batch Loss = 11.529715, Accuracy = 0.2213333398103714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.450316429138184, Accuracy = 0.24597975611686707\n",
      "Training iter #3555000:   Batch Loss = 11.647885, Accuracy = 0.22599999606609344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.448591232299805, Accuracy = 0.246873140335083\n",
      "Training iter #3558000:   Batch Loss = 11.173615, Accuracy = 0.30799999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.446890830993652, Accuracy = 0.248064324259758\n",
      "Training iter #3561000:   Batch Loss = 11.176121, Accuracy = 0.27133333683013916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.44517993927002, Accuracy = 0.24836212396621704\n",
      "Training iter #3564000:   Batch Loss = 11.186498, Accuracy = 0.2913333475589752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.443461418151855, Accuracy = 0.24836212396621704\n",
      "Training iter #3567000:   Batch Loss = 11.579015, Accuracy = 0.21400000154972076\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.441719055175781, Accuracy = 0.24836212396621704\n",
      "Training iter #3570000:   Batch Loss = 11.357197, Accuracy = 0.2626666724681854\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.440033912658691, Accuracy = 0.24836212396621704\n",
      "Training iter #3573000:   Batch Loss = 10.902147, Accuracy = 0.28333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.438310623168945, Accuracy = 0.24836212396621704\n",
      "Training iter #3576000:   Batch Loss = 11.228133, Accuracy = 0.2866666615009308\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.436504364013672, Accuracy = 0.24925550818443298\n",
      "Training iter #3579000:   Batch Loss = 11.535501, Accuracy = 0.2133333384990692\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.434791564941406, Accuracy = 0.24955330789089203\n",
      "Training iter #3582000:   Batch Loss = 11.545130, Accuracy = 0.24133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.43313217163086, Accuracy = 0.24985110759735107\n",
      "Training iter #3585000:   Batch Loss = 11.171594, Accuracy = 0.30000001192092896\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.431435585021973, Accuracy = 0.2501488924026489\n",
      "Training iter #3588000:   Batch Loss = 11.152279, Accuracy = 0.2759999930858612\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.429743766784668, Accuracy = 0.2501488924026489\n",
      "Training iter #3591000:   Batch Loss = 11.148926, Accuracy = 0.29466667771339417\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.428017616271973, Accuracy = 0.250744491815567\n",
      "Training iter #3594000:   Batch Loss = 11.623852, Accuracy = 0.21066667139530182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.426345825195312, Accuracy = 0.25104227662086487\n",
      "Training iter #3597000:   Batch Loss = 11.345942, Accuracy = 0.27133333683013916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.424737930297852, Accuracy = 0.25163787603378296\n",
      "Training iter #3600000:   Batch Loss = 10.998705, Accuracy = 0.2746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.422908782958984, Accuracy = 0.2519356906414032\n",
      "Training iter #3603000:   Batch Loss = 11.248250, Accuracy = 0.2939999997615814\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.4212646484375, Accuracy = 0.2513400912284851\n",
      "Training iter #3606000:   Batch Loss = 11.590949, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.419594764709473, Accuracy = 0.2513400912284851\n",
      "Training iter #3609000:   Batch Loss = 11.548195, Accuracy = 0.23866666853427887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.417879104614258, Accuracy = 0.2519356906414032\n",
      "Training iter #3612000:   Batch Loss = 11.018096, Accuracy = 0.32466667890548706\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.416186332702637, Accuracy = 0.2519356906414032\n",
      "Training iter #3615000:   Batch Loss = 11.111362, Accuracy = 0.28600001335144043\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.41448974609375, Accuracy = 0.2525312602519989\n",
      "Training iter #3618000:   Batch Loss = 11.135144, Accuracy = 0.30133333802223206\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.41269302368164, Accuracy = 0.2537224590778351\n",
      "Training iter #3621000:   Batch Loss = 11.626330, Accuracy = 0.21933333575725555\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.410881996154785, Accuracy = 0.2537224590778351\n",
      "Training iter #3624000:   Batch Loss = 11.373953, Accuracy = 0.26466667652130127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.409173011779785, Accuracy = 0.2537224590778351\n",
      "Training iter #3627000:   Batch Loss = 10.974971, Accuracy = 0.26600000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.407389640808105, Accuracy = 0.2549136281013489\n",
      "Training iter #3630000:   Batch Loss = 11.218916, Accuracy = 0.2913333475589752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.405706405639648, Accuracy = 0.254615843296051\n",
      "Training iter #3633000:   Batch Loss = 11.568516, Accuracy = 0.20000000298023224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.40420150756836, Accuracy = 0.2549136281013489\n",
      "Training iter #3636000:   Batch Loss = 11.426729, Accuracy = 0.24933333694934845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.402552604675293, Accuracy = 0.2552114427089691\n",
      "Training iter #3639000:   Batch Loss = 10.953448, Accuracy = 0.3386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.400882720947266, Accuracy = 0.2552114427089691\n",
      "Training iter #3642000:   Batch Loss = 11.190969, Accuracy = 0.2680000066757202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.399218559265137, Accuracy = 0.2552114427089691\n",
      "Training iter #3645000:   Batch Loss = 11.183449, Accuracy = 0.28866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.397504806518555, Accuracy = 0.2549136281013489\n",
      "Training iter #3648000:   Batch Loss = 11.689926, Accuracy = 0.2160000056028366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.395904541015625, Accuracy = 0.2552114427089691\n",
      "Training iter #3651000:   Batch Loss = 11.365909, Accuracy = 0.2613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.394365310668945, Accuracy = 0.2549136281013489\n",
      "Training iter #3654000:   Batch Loss = 10.921732, Accuracy = 0.30000001192092896\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.392748832702637, Accuracy = 0.2549136281013489\n",
      "Training iter #3657000:   Batch Loss = 11.157664, Accuracy = 0.30666667222976685\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.390838623046875, Accuracy = 0.25550922751426697\n",
      "Training iter #3660000:   Batch Loss = 11.551509, Accuracy = 0.20866666734218597\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.389077186584473, Accuracy = 0.25550922751426697\n",
      "Training iter #3663000:   Batch Loss = 11.365285, Accuracy = 0.25200000405311584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.387415885925293, Accuracy = 0.25550922751426697\n",
      "Training iter #3666000:   Batch Loss = 10.914618, Accuracy = 0.32866665720939636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.385704040527344, Accuracy = 0.25550922751426697\n",
      "Training iter #3669000:   Batch Loss = 11.091816, Accuracy = 0.2926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.383894920349121, Accuracy = 0.2558070421218872\n",
      "Training iter #3672000:   Batch Loss = 11.182224, Accuracy = 0.2800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.382328033447266, Accuracy = 0.25670042634010315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3675000:   Batch Loss = 11.756140, Accuracy = 0.21533332765102386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.380701065063477, Accuracy = 0.25670042634010315\n",
      "Training iter #3678000:   Batch Loss = 11.319403, Accuracy = 0.281333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.379068374633789, Accuracy = 0.25670042634010315\n",
      "Training iter #3681000:   Batch Loss = 10.966653, Accuracy = 0.2893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.377619743347168, Accuracy = 0.25670042634010315\n",
      "Training iter #3684000:   Batch Loss = 11.227015, Accuracy = 0.30266666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.375995635986328, Accuracy = 0.25670042634010315\n",
      "Training iter #3687000:   Batch Loss = 11.508577, Accuracy = 0.21266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.374320983886719, Accuracy = 0.25670042634010315\n",
      "Training iter #3690000:   Batch Loss = 11.252823, Accuracy = 0.26733332872390747\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.372727394104004, Accuracy = 0.25670042634010315\n",
      "Training iter #3693000:   Batch Loss = 10.916030, Accuracy = 0.3213333189487457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.37110710144043, Accuracy = 0.25670042634010315\n",
      "Training iter #3696000:   Batch Loss = 11.064590, Accuracy = 0.2993333339691162\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.369367599487305, Accuracy = 0.256998211145401\n",
      "Training iter #3699000:   Batch Loss = 11.226649, Accuracy = 0.2606666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.367572784423828, Accuracy = 0.25729599595069885\n",
      "Training iter #3702000:   Batch Loss = 11.744068, Accuracy = 0.21933333575725555\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.365826606750488, Accuracy = 0.2575938105583191\n",
      "Training iter #3705000:   Batch Loss = 11.214366, Accuracy = 0.30533334612846375\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.364165306091309, Accuracy = 0.2581894099712372\n",
      "Training iter #3708000:   Batch Loss = 10.926666, Accuracy = 0.2966666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.362502098083496, Accuracy = 0.25848719477653503\n",
      "Training iter #3711000:   Batch Loss = 11.238361, Accuracy = 0.3059999942779541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.360864639282227, Accuracy = 0.25848719477653503\n",
      "Training iter #3714000:   Batch Loss = 11.498571, Accuracy = 0.21666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.359186172485352, Accuracy = 0.25848719477653503\n",
      "Training iter #3717000:   Batch Loss = 11.338734, Accuracy = 0.2626666724681854\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.357551574707031, Accuracy = 0.2587849795818329\n",
      "Training iter #3720000:   Batch Loss = 10.959003, Accuracy = 0.30799999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.355902671813965, Accuracy = 0.2587849795818329\n",
      "Training iter #3723000:   Batch Loss = 11.004249, Accuracy = 0.3113333284854889\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.354206085205078, Accuracy = 0.2590827941894531\n",
      "Training iter #3726000:   Batch Loss = 11.244049, Accuracy = 0.2626666724681854\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.352460861206055, Accuracy = 0.2590827941894531\n",
      "Training iter #3729000:   Batch Loss = 11.697224, Accuracy = 0.20999999344348907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.350767135620117, Accuracy = 0.2590827941894531\n",
      "Training iter #3732000:   Batch Loss = 11.084121, Accuracy = 0.33933332562446594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.349112510681152, Accuracy = 0.2590827941894531\n",
      "Training iter #3735000:   Batch Loss = 10.900158, Accuracy = 0.2966666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.347471237182617, Accuracy = 0.2590827941894531\n",
      "Training iter #3738000:   Batch Loss = 11.264728, Accuracy = 0.2759999930858612\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.345813751220703, Accuracy = 0.2590827941894531\n",
      "Training iter #3741000:   Batch Loss = 11.453625, Accuracy = 0.2266666740179062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.34414291381836, Accuracy = 0.2596783936023712\n",
      "Training iter #3744000:   Batch Loss = 11.331211, Accuracy = 0.2606666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.342522621154785, Accuracy = 0.259380578994751\n",
      "Training iter #3747000:   Batch Loss = 11.028671, Accuracy = 0.2653333246707916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.340873718261719, Accuracy = 0.2590827941894531\n",
      "Training iter #3750000:   Batch Loss = 11.044159, Accuracy = 0.3319999873638153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.339150428771973, Accuracy = 0.2590827941894531\n",
      "Training iter #3753000:   Batch Loss = 11.219619, Accuracy = 0.28200000524520874\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.337374687194824, Accuracy = 0.2596783936023712\n",
      "Training iter #3756000:   Batch Loss = 11.665199, Accuracy = 0.22200000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.335661888122559, Accuracy = 0.25997617840766907\n",
      "Training iter #3759000:   Batch Loss = 10.958335, Accuracy = 0.3766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.334009170532227, Accuracy = 0.2602739632129669\n",
      "Training iter #3762000:   Batch Loss = 10.850636, Accuracy = 0.3293333351612091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.33234977722168, Accuracy = 0.260869562625885\n",
      "Training iter #3765000:   Batch Loss = 11.248944, Accuracy = 0.27133333683013916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.330657005310059, Accuracy = 0.26116734743118286\n",
      "Training iter #3768000:   Batch Loss = 11.419376, Accuracy = 0.22333332896232605\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.328940391540527, Accuracy = 0.26176294684410095\n",
      "Training iter #3771000:   Batch Loss = 11.353990, Accuracy = 0.25600001215934753\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.32731819152832, Accuracy = 0.2620607614517212\n",
      "Training iter #3774000:   Batch Loss = 10.993362, Accuracy = 0.24666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.325502395629883, Accuracy = 0.26235854625701904\n",
      "Training iter #3777000:   Batch Loss = 11.109730, Accuracy = 0.31066668033599854\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.324050903320312, Accuracy = 0.26503869891166687\n",
      "Training iter #3780000:   Batch Loss = 11.272992, Accuracy = 0.26866665482521057\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.322373390197754, Accuracy = 0.2653365135192871\n",
      "Training iter #3783000:   Batch Loss = 11.654566, Accuracy = 0.23333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.320751190185547, Accuracy = 0.26622989773750305\n",
      "Training iter #3786000:   Batch Loss = 10.904928, Accuracy = 0.3779999911785126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.319169998168945, Accuracy = 0.26742109656333923\n",
      "Training iter #3789000:   Batch Loss = 10.921641, Accuracy = 0.3153333365917206\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.31749439239502, Accuracy = 0.26682549715042114\n",
      "Training iter #3792000:   Batch Loss = 11.203329, Accuracy = 0.2706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.315712928771973, Accuracy = 0.26742109656333923\n",
      "Training iter #3795000:   Batch Loss = 11.426291, Accuracy = 0.22599999606609344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.313944816589355, Accuracy = 0.2683144807815552\n",
      "Training iter #3798000:   Batch Loss = 11.362115, Accuracy = 0.2553333342075348\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.312223434448242, Accuracy = 0.26801666617393494\n",
      "Training iter #3801000:   Batch Loss = 10.814951, Accuracy = 0.2926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.310466766357422, Accuracy = 0.26801666617393494\n",
      "Training iter #3804000:   Batch Loss = 11.061360, Accuracy = 0.3199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.308710098266602, Accuracy = 0.2683144807815552\n",
      "Training iter #3807000:   Batch Loss = 11.302423, Accuracy = 0.25866666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.307092666625977, Accuracy = 0.26891008019447327\n",
      "Training iter #3810000:   Batch Loss = 11.584661, Accuracy = 0.23999999463558197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.30569076538086, Accuracy = 0.2692078649997711\n",
      "Training iter #3813000:   Batch Loss = 10.921121, Accuracy = 0.37066665291786194\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.304082870483398, Accuracy = 0.2698034644126892\n",
      "Training iter #3816000:   Batch Loss = 11.047524, Accuracy = 0.2933333218097687\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.30247688293457, Accuracy = 0.2703990340232849\n",
      "Training iter #3819000:   Batch Loss = 11.089690, Accuracy = 0.29733332991600037\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.300870895385742, Accuracy = 0.27010124921798706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3822000:   Batch Loss = 11.409315, Accuracy = 0.23733332753181458\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.299245834350586, Accuracy = 0.27010124921798706\n",
      "Training iter #3825000:   Batch Loss = 11.358534, Accuracy = 0.2626666724681854\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.297529220581055, Accuracy = 0.27069684863090515\n",
      "Training iter #3828000:   Batch Loss = 10.764387, Accuracy = 0.33399999141693115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.295812606811523, Accuracy = 0.27129244804382324\n",
      "Training iter #3831000:   Batch Loss = 11.061266, Accuracy = 0.3153333365917206\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.294235229492188, Accuracy = 0.27188801765441895\n",
      "Training iter #3834000:   Batch Loss = 11.343268, Accuracy = 0.25600001215934753\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.292499542236328, Accuracy = 0.27248361706733704\n",
      "Training iter #3837000:   Batch Loss = 11.543375, Accuracy = 0.24199999868869781\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.290763854980469, Accuracy = 0.2721858322620392\n",
      "Training iter #3840000:   Batch Loss = 10.968172, Accuracy = 0.33399999141693115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.288982391357422, Accuracy = 0.2727814316749573\n",
      "Training iter #3843000:   Batch Loss = 11.020932, Accuracy = 0.3059999942779541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.287192344665527, Accuracy = 0.273377001285553\n",
      "Training iter #3846000:   Batch Loss = 11.078371, Accuracy = 0.3073333203792572\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.28528881072998, Accuracy = 0.2736748158931732\n",
      "Training iter #3849000:   Batch Loss = 11.409846, Accuracy = 0.24400000274181366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.283381462097168, Accuracy = 0.2736748158931732\n",
      "Training iter #3852000:   Batch Loss = 11.260748, Accuracy = 0.2840000092983246\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.281582832336426, Accuracy = 0.2736748158931732\n",
      "Training iter #3855000:   Batch Loss = 10.800038, Accuracy = 0.3233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.279837608337402, Accuracy = 0.27397260069847107\n",
      "Training iter #3858000:   Batch Loss = 11.036108, Accuracy = 0.3233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.278036117553711, Accuracy = 0.2742703855037689\n",
      "Training iter #3861000:   Batch Loss = 11.392403, Accuracy = 0.23800000548362732\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.276069641113281, Accuracy = 0.274865984916687\n",
      "Training iter #3864000:   Batch Loss = 11.477622, Accuracy = 0.24533332884311676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.274307250976562, Accuracy = 0.27516379952430725\n",
      "Training iter #3867000:   Batch Loss = 10.991055, Accuracy = 0.3226666748523712\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.272750854492188, Accuracy = 0.27516379952430725\n",
      "Training iter #3870000:   Batch Loss = 10.975311, Accuracy = 0.3226666748523712\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.271167755126953, Accuracy = 0.27516379952430725\n",
      "Training iter #3873000:   Batch Loss = 11.045016, Accuracy = 0.3173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.27000617980957, Accuracy = 0.27575936913490295\n",
      "Training iter #3876000:   Batch Loss = 11.368120, Accuracy = 0.24666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.268759727478027, Accuracy = 0.27635496854782104\n",
      "Training iter #3879000:   Batch Loss = 11.192131, Accuracy = 0.3006666600704193\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.267353057861328, Accuracy = 0.2760571837425232\n",
      "Training iter #3882000:   Batch Loss = 10.773944, Accuracy = 0.3306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.26583194732666, Accuracy = 0.2766527831554413\n",
      "Training iter #3885000:   Batch Loss = 11.091734, Accuracy = 0.31466665863990784\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.264169692993164, Accuracy = 0.277248352766037\n",
      "Training iter #3888000:   Batch Loss = 11.417898, Accuracy = 0.23399999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.262467384338379, Accuracy = 0.277248352766037\n",
      "Training iter #3891000:   Batch Loss = 11.414944, Accuracy = 0.25333333015441895\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.260763168334961, Accuracy = 0.2775461673736572\n",
      "Training iter #3894000:   Batch Loss = 11.006646, Accuracy = 0.3213333189487457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.259065628051758, Accuracy = 0.2775461673736572\n",
      "Training iter #3897000:   Batch Loss = 10.998638, Accuracy = 0.3179999887943268\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.257372856140137, Accuracy = 0.2778439521789551\n",
      "Training iter #3900000:   Batch Loss = 11.001999, Accuracy = 0.3253333270549774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.255596160888672, Accuracy = 0.2778439521789551\n",
      "Training iter #3903000:   Batch Loss = 11.406911, Accuracy = 0.23933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.253746032714844, Accuracy = 0.27814173698425293\n",
      "Training iter #3906000:   Batch Loss = 11.208108, Accuracy = 0.2939999997615814\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.251836776733398, Accuracy = 0.27843955159187317\n",
      "Training iter #3909000:   Batch Loss = 10.752832, Accuracy = 0.3479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.249918937683105, Accuracy = 0.2793329358100891\n",
      "Training iter #3912000:   Batch Loss = 11.033775, Accuracy = 0.33000001311302185\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.247955322265625, Accuracy = 0.2793329358100891\n",
      "Training iter #3915000:   Batch Loss = 11.376136, Accuracy = 0.23866666853427887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.245926856994629, Accuracy = 0.2799285352230072\n",
      "Training iter #3918000:   Batch Loss = 11.354746, Accuracy = 0.2619999945163727\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.24409294128418, Accuracy = 0.2799285352230072\n",
      "Training iter #3921000:   Batch Loss = 10.947895, Accuracy = 0.3166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.242280006408691, Accuracy = 0.28022632002830505\n",
      "Training iter #3924000:   Batch Loss = 10.938993, Accuracy = 0.31933334469795227\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.240531921386719, Accuracy = 0.28022632002830505\n",
      "Training iter #3927000:   Batch Loss = 10.886017, Accuracy = 0.34333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.238643646240234, Accuracy = 0.2805241346359253\n",
      "Training iter #3930000:   Batch Loss = 11.466704, Accuracy = 0.24199999868869781\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.236968040466309, Accuracy = 0.281119704246521\n",
      "Training iter #3933000:   Batch Loss = 11.148945, Accuracy = 0.304666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.235380172729492, Accuracy = 0.281119704246521\n",
      "Training iter #3936000:   Batch Loss = 10.830595, Accuracy = 0.3306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.233654022216797, Accuracy = 0.281119704246521\n",
      "Training iter #3939000:   Batch Loss = 11.034548, Accuracy = 0.32600000500679016\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.231738090515137, Accuracy = 0.2817153036594391\n",
      "Training iter #3942000:   Batch Loss = 11.462826, Accuracy = 0.2213333398103714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.229779243469238, Accuracy = 0.2817153036594391\n",
      "Training iter #3945000:   Batch Loss = 11.332459, Accuracy = 0.2639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.227964401245117, Accuracy = 0.2823109030723572\n",
      "Training iter #3948000:   Batch Loss = 10.748274, Accuracy = 0.36666667461395264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.226007461547852, Accuracy = 0.2823109030723572\n",
      "Training iter #3951000:   Batch Loss = 10.959143, Accuracy = 0.3213333189487457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.224395751953125, Accuracy = 0.28350207209587097\n",
      "Training iter #3954000:   Batch Loss = 10.976326, Accuracy = 0.3386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.222488403320312, Accuracy = 0.28409767150878906\n",
      "Training iter #3957000:   Batch Loss = 11.502077, Accuracy = 0.2433333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.220708847045898, Accuracy = 0.28469327092170715\n",
      "Training iter #3960000:   Batch Loss = 11.211737, Accuracy = 0.28200000524520874\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.218975067138672, Accuracy = 0.28469327092170715\n",
      "Training iter #3963000:   Batch Loss = 10.702283, Accuracy = 0.36000001430511475\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.217245101928711, Accuracy = 0.2855866551399231\n",
      "Training iter #3966000:   Batch Loss = 11.009586, Accuracy = 0.3253333270549774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.21539306640625, Accuracy = 0.2855866551399231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3969000:   Batch Loss = 11.431510, Accuracy = 0.2213333398103714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.213549613952637, Accuracy = 0.2855866551399231\n",
      "Training iter #3972000:   Batch Loss = 11.191771, Accuracy = 0.2933333218097687\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.211867332458496, Accuracy = 0.28588446974754333\n",
      "Training iter #3975000:   Batch Loss = 10.771209, Accuracy = 0.35066667199134827\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.210230827331543, Accuracy = 0.2867778539657593\n",
      "Training iter #3978000:   Batch Loss = 10.981921, Accuracy = 0.2966666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.20862102508545, Accuracy = 0.28648003935813904\n",
      "Training iter #3981000:   Batch Loss = 11.016796, Accuracy = 0.32600000500679016\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.206969261169434, Accuracy = 0.28707563877105713\n",
      "Training iter #3984000:   Batch Loss = 11.526190, Accuracy = 0.2433333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.205490112304688, Accuracy = 0.287373423576355\n",
      "Training iter #3987000:   Batch Loss = 11.124242, Accuracy = 0.3160000145435333\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.204106330871582, Accuracy = 0.287373423576355\n",
      "Training iter #3990000:   Batch Loss = 10.767645, Accuracy = 0.3440000116825104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.202598571777344, Accuracy = 0.287373423576355\n",
      "Training iter #3993000:   Batch Loss = 11.028363, Accuracy = 0.3160000145435333\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.200864791870117, Accuracy = 0.28796902298927307\n",
      "Training iter #3996000:   Batch Loss = 11.418695, Accuracy = 0.22466666996479034\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.199065208435059, Accuracy = 0.2882668375968933\n",
      "Training iter #3999000:   Batch Loss = 11.156000, Accuracy = 0.28066667914390564\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.197265625, Accuracy = 0.28856462240219116\n",
      "Training iter #4002000:   Batch Loss = 10.661965, Accuracy = 0.38466668128967285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.195452690124512, Accuracy = 0.2894580066204071\n",
      "Training iter #4005000:   Batch Loss = 10.890979, Accuracy = 0.3319999873638153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.193747520446777, Accuracy = 0.2900536060333252\n",
      "Training iter #4008000:   Batch Loss = 11.001742, Accuracy = 0.3186666667461395\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.192066192626953, Accuracy = 0.2900536060333252\n",
      "Training iter #4011000:   Batch Loss = 11.639278, Accuracy = 0.22733333706855774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.190370559692383, Accuracy = 0.29094699025154114\n",
      "Training iter #4014000:   Batch Loss = 11.084143, Accuracy = 0.3179999887943268\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.188697814941406, Accuracy = 0.2906492054462433\n",
      "Training iter #4017000:   Batch Loss = 10.707123, Accuracy = 0.35866665840148926\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.187018394470215, Accuracy = 0.29094699025154114\n",
      "Training iter #4020000:   Batch Loss = 11.081532, Accuracy = 0.3213333189487457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.185262680053711, Accuracy = 0.29035139083862305\n",
      "Training iter #4023000:   Batch Loss = 11.314880, Accuracy = 0.23933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.183490753173828, Accuracy = 0.291244775056839\n",
      "Training iter #4026000:   Batch Loss = 11.096195, Accuracy = 0.2953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.181777954101562, Accuracy = 0.2918403744697571\n",
      "Training iter #4029000:   Batch Loss = 10.736164, Accuracy = 0.3619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.180068969726562, Accuracy = 0.2918403744697571\n",
      "Training iter #4032000:   Batch Loss = 10.857676, Accuracy = 0.3413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.178401947021484, Accuracy = 0.292733758687973\n",
      "Training iter #4035000:   Batch Loss = 11.006647, Accuracy = 0.304666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.176628112792969, Accuracy = 0.292733758687973\n",
      "Training iter #4038000:   Batch Loss = 11.521872, Accuracy = 0.2446666657924652\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.174896240234375, Accuracy = 0.292733758687973\n",
      "Training iter #4041000:   Batch Loss = 10.952101, Accuracy = 0.3473333418369293\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.173016548156738, Accuracy = 0.2933293581008911\n",
      "Training iter #4044000:   Batch Loss = 10.730527, Accuracy = 0.35199999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.170868873596191, Accuracy = 0.2939249575138092\n",
      "Training iter #4047000:   Batch Loss = 11.068866, Accuracy = 0.33666667342185974\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.169095993041992, Accuracy = 0.2939249575138092\n",
      "Training iter #4050000:   Batch Loss = 11.300809, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.167293548583984, Accuracy = 0.29481834173202515\n",
      "Training iter #4053000:   Batch Loss = 11.156125, Accuracy = 0.3019999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.165573120117188, Accuracy = 0.29541394114494324\n",
      "Training iter #4056000:   Batch Loss = 10.817762, Accuracy = 0.3346666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.163895606994629, Accuracy = 0.29541394114494324\n",
      "Training iter #4059000:   Batch Loss = 10.809258, Accuracy = 0.36800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.162593841552734, Accuracy = 0.2957117259502411\n",
      "Training iter #4062000:   Batch Loss = 11.046406, Accuracy = 0.3033333420753479\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.160874366760254, Accuracy = 0.2963073253631592\n",
      "Training iter #4065000:   Batch Loss = 11.526537, Accuracy = 0.23266667127609253\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.159090042114258, Accuracy = 0.29660511016845703\n",
      "Training iter #4068000:   Batch Loss = 10.809765, Accuracy = 0.3840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.157198905944824, Accuracy = 0.29660511016845703\n",
      "Training iter #4071000:   Batch Loss = 10.707775, Accuracy = 0.35333332419395447\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.155096054077148, Accuracy = 0.29690292477607727\n",
      "Training iter #4074000:   Batch Loss = 11.072641, Accuracy = 0.30533334612846375\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.15323257446289, Accuracy = 0.29690292477607727\n",
      "Training iter #4077000:   Batch Loss = 11.276964, Accuracy = 0.24933333694934845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.151430130004883, Accuracy = 0.29749852418899536\n",
      "Training iter #4080000:   Batch Loss = 11.128863, Accuracy = 0.3059999942779541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.149772644042969, Accuracy = 0.29809409379959106\n",
      "Training iter #4083000:   Batch Loss = 10.835639, Accuracy = 0.30533334612846375\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.148140907287598, Accuracy = 0.29809409379959106\n",
      "Training iter #4086000:   Batch Loss = 10.906649, Accuracy = 0.35866665840148926\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.146529197692871, Accuracy = 0.29809409379959106\n",
      "Training iter #4089000:   Batch Loss = 11.053939, Accuracy = 0.30666667222976685\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.144720077514648, Accuracy = 0.29809409379959106\n",
      "Training iter #4092000:   Batch Loss = 11.472222, Accuracy = 0.25333333015441895\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.143010139465332, Accuracy = 0.29809409379959106\n",
      "Training iter #4095000:   Batch Loss = 10.711616, Accuracy = 0.4126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.141358375549316, Accuracy = 0.2983919084072113\n",
      "Training iter #4098000:   Batch Loss = 10.655037, Accuracy = 0.39266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.139669418334961, Accuracy = 0.2983919084072113\n",
      "Training iter #4101000:   Batch Loss = 11.064257, Accuracy = 0.2966666519641876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.137855529785156, Accuracy = 0.2983919084072113\n",
      "Training iter #4104000:   Batch Loss = 11.257433, Accuracy = 0.2553333342075348\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.13608455657959, Accuracy = 0.29868969321250916\n",
      "Training iter #4107000:   Batch Loss = 11.146055, Accuracy = 0.30399999022483826\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.134432792663574, Accuracy = 0.2983919084072113\n",
      "Training iter #4110000:   Batch Loss = 10.713573, Accuracy = 0.33533334732055664\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.132844924926758, Accuracy = 0.29868969321250916\n",
      "Training iter #4113000:   Batch Loss = 10.923635, Accuracy = 0.3526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.131210327148438, Accuracy = 0.2983919084072113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4116000:   Batch Loss = 11.117720, Accuracy = 0.29600000381469727\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.129514694213867, Accuracy = 0.2983919084072113\n",
      "Training iter #4119000:   Batch Loss = 11.449534, Accuracy = 0.2593333423137665\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.12781047821045, Accuracy = 0.298987478017807\n",
      "Training iter #4122000:   Batch Loss = 10.694876, Accuracy = 0.4233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.126174926757812, Accuracy = 0.298987478017807\n",
      "Training iter #4125000:   Batch Loss = 10.804646, Accuracy = 0.35333332419395447\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.124584197998047, Accuracy = 0.298987478017807\n",
      "Training iter #4128000:   Batch Loss = 10.957329, Accuracy = 0.3199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.122783660888672, Accuracy = 0.298987478017807\n",
      "Training iter #4131000:   Batch Loss = 11.268432, Accuracy = 0.25600001215934753\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.121014595031738, Accuracy = 0.298987478017807\n",
      "Training iter #4134000:   Batch Loss = 11.176017, Accuracy = 0.3006666600704193\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.119293212890625, Accuracy = 0.29988089203834534\n",
      "Training iter #4137000:   Batch Loss = 10.584594, Accuracy = 0.36933332681655884\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.117650985717773, Accuracy = 0.29988089203834534\n",
      "Training iter #4140000:   Batch Loss = 10.851504, Accuracy = 0.36000001430511475\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.116020202636719, Accuracy = 0.3007742762565613\n",
      "Training iter #4143000:   Batch Loss = 11.131924, Accuracy = 0.2893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.114325523376465, Accuracy = 0.3007742762565613\n",
      "Training iter #4146000:   Batch Loss = 11.362982, Accuracy = 0.2666666805744171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.112680435180664, Accuracy = 0.30107206106185913\n",
      "Training iter #4149000:   Batch Loss = 10.712031, Accuracy = 0.39399999380111694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.111108779907227, Accuracy = 0.3007742762565613\n",
      "Training iter #4152000:   Batch Loss = 10.838705, Accuracy = 0.3386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.109504699707031, Accuracy = 0.3019654452800751\n",
      "Training iter #4155000:   Batch Loss = 10.862054, Accuracy = 0.33533334732055664\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.107683181762695, Accuracy = 0.3016676604747772\n",
      "Training iter #4158000:   Batch Loss = 11.223082, Accuracy = 0.27933332324028015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.10578727722168, Accuracy = 0.30315664410591125\n",
      "Training iter #4161000:   Batch Loss = 11.119168, Accuracy = 0.3240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.104029655456543, Accuracy = 0.3034544289112091\n",
      "Training iter #4164000:   Batch Loss = 10.582324, Accuracy = 0.3840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.102351188659668, Accuracy = 0.3034544289112091\n",
      "Training iter #4167000:   Batch Loss = 10.818548, Accuracy = 0.3619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.100666046142578, Accuracy = 0.30375224351882935\n",
      "Training iter #4170000:   Batch Loss = 11.140353, Accuracy = 0.28333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.099080085754395, Accuracy = 0.3040500283241272\n",
      "Training iter #4173000:   Batch Loss = 11.367573, Accuracy = 0.2540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.097419738769531, Accuracy = 0.3046456277370453\n",
      "Training iter #4176000:   Batch Loss = 10.748837, Accuracy = 0.3733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.095928192138672, Accuracy = 0.30434781312942505\n",
      "Training iter #4179000:   Batch Loss = 10.806470, Accuracy = 0.3606666624546051\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.09445571899414, Accuracy = 0.3040500283241272\n",
      "Training iter #4182000:   Batch Loss = 10.843501, Accuracy = 0.34466665983200073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.092672348022461, Accuracy = 0.3040500283241272\n",
      "Training iter #4185000:   Batch Loss = 11.209515, Accuracy = 0.28600001335144043\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.090932846069336, Accuracy = 0.3040500283241272\n",
      "Training iter #4188000:   Batch Loss = 10.994379, Accuracy = 0.34333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.08925724029541, Accuracy = 0.3046456277370453\n",
      "Training iter #4191000:   Batch Loss = 10.583498, Accuracy = 0.37066665291786194\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.087587356567383, Accuracy = 0.3046456277370453\n",
      "Training iter #4194000:   Batch Loss = 10.887469, Accuracy = 0.3453333377838135\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.085848808288574, Accuracy = 0.3052412271499634\n",
      "Training iter #4197000:   Batch Loss = 11.197763, Accuracy = 0.26600000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.084102630615234, Accuracy = 0.3058367967605591\n",
      "Training iter #4200000:   Batch Loss = 11.296129, Accuracy = 0.2666666805744171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.082442283630371, Accuracy = 0.3067302107810974\n",
      "Training iter #4203000:   Batch Loss = 10.743891, Accuracy = 0.37066665291786194\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.080726623535156, Accuracy = 0.30702799558639526\n",
      "Training iter #4206000:   Batch Loss = 10.793827, Accuracy = 0.3720000088214874\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.078923225402832, Accuracy = 0.3073257803916931\n",
      "Training iter #4209000:   Batch Loss = 10.785787, Accuracy = 0.3606666624546051\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.076798439025879, Accuracy = 0.3079213798046112\n",
      "Training iter #4212000:   Batch Loss = 11.228127, Accuracy = 0.28733333945274353\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.074953079223633, Accuracy = 0.30821916460990906\n",
      "Training iter #4215000:   Batch Loss = 10.973664, Accuracy = 0.3453333377838135\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.073225021362305, Accuracy = 0.30821916460990906\n",
      "Training iter #4218000:   Batch Loss = 10.534733, Accuracy = 0.3813333213329315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.07154655456543, Accuracy = 0.3085169792175293\n",
      "Training iter #4221000:   Batch Loss = 10.869045, Accuracy = 0.3499999940395355\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.069843292236328, Accuracy = 0.30881476402282715\n",
      "Training iter #4224000:   Batch Loss = 11.214952, Accuracy = 0.2593333423137665\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.068052291870117, Accuracy = 0.30941036343574524\n",
      "Training iter #4227000:   Batch Loss = 11.207558, Accuracy = 0.28200000524520874\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.06654167175293, Accuracy = 0.3091125786304474\n",
      "Training iter #4230000:   Batch Loss = 10.745979, Accuracy = 0.36933332681655884\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.064899444580078, Accuracy = 0.3097081482410431\n",
      "Training iter #4233000:   Batch Loss = 10.796694, Accuracy = 0.3633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.063220977783203, Accuracy = 0.3097081482410431\n",
      "Training iter #4236000:   Batch Loss = 10.770426, Accuracy = 0.35733333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.06145191192627, Accuracy = 0.3106015622615814\n",
      "Training iter #4239000:   Batch Loss = 11.253761, Accuracy = 0.281333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.059797286987305, Accuracy = 0.3108993470668793\n",
      "Training iter #4242000:   Batch Loss = 10.962009, Accuracy = 0.3633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.058362007141113, Accuracy = 0.3108993470668793\n",
      "Training iter #4245000:   Batch Loss = 10.597580, Accuracy = 0.3853333294391632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.056757926940918, Accuracy = 0.31149494647979736\n",
      "Training iter #4248000:   Batch Loss = 10.884985, Accuracy = 0.34333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.054845809936523, Accuracy = 0.31149494647979736\n",
      "Training iter #4251000:   Batch Loss = 11.221952, Accuracy = 0.25600001215934753\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.053023338317871, Accuracy = 0.31149494647979736\n",
      "Training iter #4254000:   Batch Loss = 11.191583, Accuracy = 0.2926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.051322937011719, Accuracy = 0.3111971318721771\n",
      "Training iter #4257000:   Batch Loss = 10.660566, Accuracy = 0.390666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.0496187210083, Accuracy = 0.3111971318721771\n",
      "Training iter #4260000:   Batch Loss = 10.720925, Accuracy = 0.37066665291786194\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.047861099243164, Accuracy = 0.31149494647979736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4263000:   Batch Loss = 10.728781, Accuracy = 0.3726666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.046100616455078, Accuracy = 0.3123883306980133\n",
      "Training iter #4266000:   Batch Loss = 11.278328, Accuracy = 0.28866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.044450759887695, Accuracy = 0.3129839301109314\n",
      "Training iter #4269000:   Batch Loss = 11.024397, Accuracy = 0.33000001311302185\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.04275894165039, Accuracy = 0.3129839301109314\n",
      "Training iter #4272000:   Batch Loss = 10.588908, Accuracy = 0.38066667318344116\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.041136741638184, Accuracy = 0.3159618675708771\n",
      "Training iter #4275000:   Batch Loss = 10.833012, Accuracy = 0.3473333418369293\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.039441108703613, Accuracy = 0.3159618675708771\n",
      "Training iter #4278000:   Batch Loss = 11.231481, Accuracy = 0.2473333328962326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.037580490112305, Accuracy = 0.3162596821784973\n",
      "Training iter #4281000:   Batch Loss = 11.099529, Accuracy = 0.30533334612846375\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.035959243774414, Accuracy = 0.3162596821784973\n",
      "Training iter #4284000:   Batch Loss = 10.552192, Accuracy = 0.4326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.034334182739258, Accuracy = 0.3168552815914154\n",
      "Training iter #4287000:   Batch Loss = 10.775922, Accuracy = 0.36266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.032654762268066, Accuracy = 0.31834426522254944\n",
      "Training iter #4290000:   Batch Loss = 10.792427, Accuracy = 0.36666667461395264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.030858993530273, Accuracy = 0.31893983483314514\n",
      "Training iter #4293000:   Batch Loss = 11.353935, Accuracy = 0.2773333191871643\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.029176712036133, Accuracy = 0.3186420500278473\n",
      "Training iter #4296000:   Batch Loss = 10.968879, Accuracy = 0.3466666638851166\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.027514457702637, Accuracy = 0.3192376494407654\n",
      "Training iter #4299000:   Batch Loss = 10.482569, Accuracy = 0.4386666715145111\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.025970458984375, Accuracy = 0.3192376494407654\n",
      "Training iter #4302000:   Batch Loss = 10.783166, Accuracy = 0.36133334040641785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.024341583251953, Accuracy = 0.3192376494407654\n",
      "Training iter #4305000:   Batch Loss = 11.225467, Accuracy = 0.24933333694934845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.022666931152344, Accuracy = 0.31983324885368347\n",
      "Training iter #4308000:   Batch Loss = 11.025990, Accuracy = 0.3186666667461395\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.021074295043945, Accuracy = 0.31983324885368347\n",
      "Training iter #4311000:   Batch Loss = 10.555817, Accuracy = 0.41733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.019464492797852, Accuracy = 0.3201310336589813\n",
      "Training iter #4314000:   Batch Loss = 10.730618, Accuracy = 0.3646666705608368\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.017804145812988, Accuracy = 0.3207266330718994\n",
      "Training iter #4317000:   Batch Loss = 10.787923, Accuracy = 0.36000001430511475\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.015976905822754, Accuracy = 0.32102441787719727\n",
      "Training iter #4320000:   Batch Loss = 11.406910, Accuracy = 0.273333340883255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.014223098754883, Accuracy = 0.3213222026824951\n",
      "Training iter #4323000:   Batch Loss = 10.914458, Accuracy = 0.3713333308696747\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.012491226196289, Accuracy = 0.32221561670303345\n",
      "Training iter #4326000:   Batch Loss = 10.593603, Accuracy = 0.3893333375453949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.010791778564453, Accuracy = 0.32281118631362915\n",
      "Training iter #4329000:   Batch Loss = 10.861624, Accuracy = 0.3499999940395355\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.00901985168457, Accuracy = 0.3231090009212494\n",
      "Training iter #4332000:   Batch Loss = 11.179545, Accuracy = 0.2613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.00743579864502, Accuracy = 0.3231090009212494\n",
      "Training iter #4335000:   Batch Loss = 10.908738, Accuracy = 0.33266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.005683898925781, Accuracy = 0.32340678572654724\n",
      "Training iter #4338000:   Batch Loss = 10.513106, Accuracy = 0.4333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.003924369812012, Accuracy = 0.3237046003341675\n",
      "Training iter #4341000:   Batch Loss = 10.670923, Accuracy = 0.382666677236557\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.001974105834961, Accuracy = 0.3237046003341675\n",
      "Training iter #4344000:   Batch Loss = 10.785122, Accuracy = 0.34933334589004517\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 11.00020980834961, Accuracy = 0.3251935541629791\n",
      "Training iter #4347000:   Batch Loss = 11.445248, Accuracy = 0.2633333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.99850082397461, Accuracy = 0.32549136877059937\n",
      "Training iter #4350000:   Batch Loss = 10.831804, Accuracy = 0.38866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.996896743774414, Accuracy = 0.3257891535758972\n",
      "Training iter #4353000:   Batch Loss = 10.529396, Accuracy = 0.4046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.995342254638672, Accuracy = 0.32668253779411316\n",
      "Training iter #4356000:   Batch Loss = 10.878386, Accuracy = 0.3526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.993632316589355, Accuracy = 0.3269803524017334\n",
      "Training iter #4359000:   Batch Loss = 11.178318, Accuracy = 0.2653333246707916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.99189281463623, Accuracy = 0.3269803524017334\n",
      "Training iter #4362000:   Batch Loss = 10.972997, Accuracy = 0.3306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.990175247192383, Accuracy = 0.32727813720703125\n",
      "Training iter #4365000:   Batch Loss = 10.550942, Accuracy = 0.42399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.988584518432617, Accuracy = 0.32787373661994934\n",
      "Training iter #4368000:   Batch Loss = 10.658644, Accuracy = 0.3840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.9868803024292, Accuracy = 0.32846933603286743\n",
      "Training iter #4371000:   Batch Loss = 10.868419, Accuracy = 0.3226666748523712\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.98498249053955, Accuracy = 0.3281715214252472\n",
      "Training iter #4374000:   Batch Loss = 11.321136, Accuracy = 0.26866665482521057\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.982987403869629, Accuracy = 0.3287671208381653\n",
      "Training iter #4377000:   Batch Loss = 10.708365, Accuracy = 0.41200000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.981424331665039, Accuracy = 0.32906490564346313\n",
      "Training iter #4380000:   Batch Loss = 10.507121, Accuracy = 0.40533334016799927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.97990608215332, Accuracy = 0.3296605050563812\n",
      "Training iter #4383000:   Batch Loss = 10.897470, Accuracy = 0.3413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.978609085083008, Accuracy = 0.3302561044692993\n",
      "Training iter #4386000:   Batch Loss = 11.091988, Accuracy = 0.2840000092983246\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.977161407470703, Accuracy = 0.3302561044692993\n",
      "Training iter #4389000:   Batch Loss = 10.987097, Accuracy = 0.3266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.975615501403809, Accuracy = 0.3302561044692993\n",
      "Training iter #4392000:   Batch Loss = 10.630751, Accuracy = 0.390666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.974016189575195, Accuracy = 0.33114948868751526\n",
      "Training iter #4395000:   Batch Loss = 10.666883, Accuracy = 0.41066667437553406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.972274780273438, Accuracy = 0.3326384723186493\n",
      "Training iter #4398000:   Batch Loss = 10.841415, Accuracy = 0.34066668152809143\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.970460891723633, Accuracy = 0.33293625712394714\n",
      "Training iter #4401000:   Batch Loss = 11.319862, Accuracy = 0.26733332872390747\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.968697547912598, Accuracy = 0.33353185653686523\n",
      "Training iter #4404000:   Batch Loss = 10.578939, Accuracy = 0.46533334255218506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.966947555541992, Accuracy = 0.3341274559497833\n",
      "Training iter #4407000:   Batch Loss = 10.495719, Accuracy = 0.40799999237060547\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.965272903442383, Accuracy = 0.33502084016799927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4410000:   Batch Loss = 10.870735, Accuracy = 0.32733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.963386535644531, Accuracy = 0.33502084016799927\n",
      "Training iter #4413000:   Batch Loss = 11.075242, Accuracy = 0.2866666615009308\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.96155834197998, Accuracy = 0.33561643958091736\n",
      "Training iter #4416000:   Batch Loss = 11.002424, Accuracy = 0.32466667890548706\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.959810256958008, Accuracy = 0.33561643958091736\n",
      "Training iter #4419000:   Batch Loss = 10.633929, Accuracy = 0.36533331871032715\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.958123207092285, Accuracy = 0.33561643958091736\n",
      "Training iter #4422000:   Batch Loss = 10.728539, Accuracy = 0.38999998569488525\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.956341743469238, Accuracy = 0.33561643958091736\n",
      "Training iter #4425000:   Batch Loss = 10.877423, Accuracy = 0.33399999141693115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.954493522644043, Accuracy = 0.3359142243862152\n",
      "Training iter #4428000:   Batch Loss = 11.322289, Accuracy = 0.27266666293144226\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.952702522277832, Accuracy = 0.33621203899383545\n",
      "Training iter #4431000:   Batch Loss = 10.491007, Accuracy = 0.47733333706855774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.950925827026367, Accuracy = 0.33621203899383545\n",
      "Training iter #4434000:   Batch Loss = 10.512537, Accuracy = 0.42266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.94921588897705, Accuracy = 0.33621203899383545\n",
      "Training iter #4437000:   Batch Loss = 10.818871, Accuracy = 0.3240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.947434425354004, Accuracy = 0.33621203899383545\n",
      "Training iter #4440000:   Batch Loss = 11.077879, Accuracy = 0.2926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.94567584991455, Accuracy = 0.3365098237991333\n",
      "Training iter #4443000:   Batch Loss = 10.988905, Accuracy = 0.3233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.943907737731934, Accuracy = 0.33680763840675354\n",
      "Training iter #4446000:   Batch Loss = 10.422239, Accuracy = 0.414000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.942100524902344, Accuracy = 0.3371054232120514\n",
      "Training iter #4449000:   Batch Loss = 10.677463, Accuracy = 0.38866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.94013786315918, Accuracy = 0.3371054232120514\n",
      "Training iter #4452000:   Batch Loss = 10.939051, Accuracy = 0.31466665863990784\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.938103675842285, Accuracy = 0.33740320801734924\n",
      "Training iter #4455000:   Batch Loss = 11.229262, Accuracy = 0.29733332991600037\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.93631649017334, Accuracy = 0.33740320801734924\n",
      "Training iter #4458000:   Batch Loss = 10.512495, Accuracy = 0.47600001096725464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.934575080871582, Accuracy = 0.3388921916484833\n",
      "Training iter #4461000:   Batch Loss = 10.655791, Accuracy = 0.3713333308696747\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.93291187286377, Accuracy = 0.33948779106140137\n",
      "Training iter #4464000:   Batch Loss = 10.685665, Accuracy = 0.36000001430511475\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.93115520477295, Accuracy = 0.34067898988723755\n",
      "Training iter #4467000:   Batch Loss = 11.089787, Accuracy = 0.2980000078678131\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.929457664489746, Accuracy = 0.34067898988723755\n",
      "Training iter #4470000:   Batch Loss = 11.044730, Accuracy = 0.3226666748523712\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.927773475646973, Accuracy = 0.3409767746925354\n",
      "Training iter #4473000:   Batch Loss = 10.362487, Accuracy = 0.4560000002384186\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.926114082336426, Accuracy = 0.3409767746925354\n",
      "Training iter #4476000:   Batch Loss = 10.671299, Accuracy = 0.38999998569488525\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.924356460571289, Accuracy = 0.3409767746925354\n",
      "Training iter #4479000:   Batch Loss = 10.971001, Accuracy = 0.30933332443237305\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.922531127929688, Accuracy = 0.3421679437160492\n",
      "Training iter #4482000:   Batch Loss = 11.163130, Accuracy = 0.29466667771339417\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.920823097229004, Accuracy = 0.3427635431289673\n",
      "Training iter #4485000:   Batch Loss = 10.579989, Accuracy = 0.42933332920074463\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.919112205505371, Accuracy = 0.3427635431289673\n",
      "Training iter #4488000:   Batch Loss = 10.610470, Accuracy = 0.38600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.917457580566406, Accuracy = 0.3430613577365875\n",
      "Training iter #4491000:   Batch Loss = 10.698709, Accuracy = 0.35600000619888306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.915705680847168, Accuracy = 0.3433591425418854\n",
      "Training iter #4494000:   Batch Loss = 11.029247, Accuracy = 0.328000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.91402816772461, Accuracy = 0.3442525267601013\n",
      "Training iter #4497000:   Batch Loss = 10.931825, Accuracy = 0.3499999940395355\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.912391662597656, Accuracy = 0.3442525267601013\n",
      "Training iter #4500000:   Batch Loss = 10.424397, Accuracy = 0.44066667556762695\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.91069221496582, Accuracy = 0.3442525267601013\n",
      "Training iter #4503000:   Batch Loss = 10.643718, Accuracy = 0.3986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.908864974975586, Accuracy = 0.34455034136772156\n",
      "Training iter #4506000:   Batch Loss = 11.008155, Accuracy = 0.2993333339691162\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.906982421875, Accuracy = 0.3448481261730194\n",
      "Training iter #4509000:   Batch Loss = 11.158555, Accuracy = 0.2913333475589752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.905288696289062, Accuracy = 0.3460392951965332\n",
      "Training iter #4512000:   Batch Loss = 10.596752, Accuracy = 0.41999998688697815\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.903769493103027, Accuracy = 0.34752827882766724\n",
      "Training iter #4515000:   Batch Loss = 10.597504, Accuracy = 0.414000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.902265548706055, Accuracy = 0.3472304940223694\n",
      "Training iter #4518000:   Batch Loss = 10.628845, Accuracy = 0.3726666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.900537490844727, Accuracy = 0.3478260934352875\n",
      "Training iter #4521000:   Batch Loss = 11.025119, Accuracy = 0.32600000500679016\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.898839950561523, Accuracy = 0.3472304940223694\n",
      "Training iter #4524000:   Batch Loss = 10.868336, Accuracy = 0.3633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.897177696228027, Accuracy = 0.3478260934352875\n",
      "Training iter #4527000:   Batch Loss = 10.402874, Accuracy = 0.4346666634082794\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.895543098449707, Accuracy = 0.34842169284820557\n",
      "Training iter #4530000:   Batch Loss = 10.724122, Accuracy = 0.36800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.893784523010254, Accuracy = 0.3487194776535034\n",
      "Training iter #4533000:   Batch Loss = 11.022153, Accuracy = 0.29466667771339417\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.891969680786133, Accuracy = 0.3487194776535034\n",
      "Training iter #4536000:   Batch Loss = 11.097203, Accuracy = 0.304666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.89023494720459, Accuracy = 0.34901726245880127\n",
      "Training iter #4539000:   Batch Loss = 10.571137, Accuracy = 0.4286666810512543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.888513565063477, Accuracy = 0.3499106466770172\n",
      "Training iter #4542000:   Batch Loss = 10.623445, Accuracy = 0.40400001406669617\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.88691234588623, Accuracy = 0.35080406069755554\n",
      "Training iter #4545000:   Batch Loss = 10.599514, Accuracy = 0.37933334708213806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.885204315185547, Accuracy = 0.35139963030815125\n",
      "Training iter #4548000:   Batch Loss = 11.033854, Accuracy = 0.32733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.88349723815918, Accuracy = 0.35139963030815125\n",
      "Training iter #4551000:   Batch Loss = 10.844661, Accuracy = 0.3686666786670685\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.881768226623535, Accuracy = 0.35139963030815125\n",
      "Training iter #4554000:   Batch Loss = 10.356616, Accuracy = 0.44200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.880044937133789, Accuracy = 0.3525908291339874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4557000:   Batch Loss = 10.648636, Accuracy = 0.38999998569488525\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.878238677978516, Accuracy = 0.3525908291339874\n",
      "Training iter #4560000:   Batch Loss = 11.023691, Accuracy = 0.28999999165534973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.876445770263672, Accuracy = 0.3531864285469055\n",
      "Training iter #4563000:   Batch Loss = 11.013732, Accuracy = 0.3173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.874783515930176, Accuracy = 0.3531864285469055\n",
      "Training iter #4566000:   Batch Loss = 10.584815, Accuracy = 0.421999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.87313461303711, Accuracy = 0.35407981276512146\n",
      "Training iter #4569000:   Batch Loss = 10.564440, Accuracy = 0.4193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.871539115905762, Accuracy = 0.3549731969833374\n",
      "Training iter #4572000:   Batch Loss = 10.518820, Accuracy = 0.4033333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.869793891906738, Accuracy = 0.35527098178863525\n",
      "Training iter #4575000:   Batch Loss = 11.120476, Accuracy = 0.31333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.868146896362305, Accuracy = 0.3555687963962555\n",
      "Training iter #4578000:   Batch Loss = 10.761559, Accuracy = 0.38733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.866496086120605, Accuracy = 0.35527098178863525\n",
      "Training iter #4581000:   Batch Loss = 10.444652, Accuracy = 0.42800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.86482048034668, Accuracy = 0.35586658120155334\n",
      "Training iter #4584000:   Batch Loss = 10.674314, Accuracy = 0.37933334708213806\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.863086700439453, Accuracy = 0.3561643958091736\n",
      "Training iter #4587000:   Batch Loss = 11.080683, Accuracy = 0.27666667103767395\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.861310005187988, Accuracy = 0.35586658120155334\n",
      "Training iter #4590000:   Batch Loss = 10.999319, Accuracy = 0.32600000500679016\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.85965633392334, Accuracy = 0.3561643958091736\n",
      "Training iter #4593000:   Batch Loss = 10.360204, Accuracy = 0.49133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.858003616333008, Accuracy = 0.3567599654197693\n",
      "Training iter #4596000:   Batch Loss = 10.579134, Accuracy = 0.41600000858306885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.856367111206055, Accuracy = 0.3567599654197693\n",
      "Training iter #4599000:   Batch Loss = 10.568453, Accuracy = 0.40666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.854615211486816, Accuracy = 0.3567599654197693\n",
      "Training iter #4602000:   Batch Loss = 11.119025, Accuracy = 0.3140000104904175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.852922439575195, Accuracy = 0.3567599654197693\n",
      "Training iter #4605000:   Batch Loss = 10.827727, Accuracy = 0.36800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.851249694824219, Accuracy = 0.3570577800273895\n",
      "Training iter #4608000:   Batch Loss = 10.346566, Accuracy = 0.44866666197776794\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.849337577819824, Accuracy = 0.3573555648326874\n",
      "Training iter #4611000:   Batch Loss = 10.630529, Accuracy = 0.38866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.847288131713867, Accuracy = 0.3582489490509033\n",
      "Training iter #4614000:   Batch Loss = 11.068565, Accuracy = 0.2800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.845431327819824, Accuracy = 0.3576533794403076\n",
      "Training iter #4617000:   Batch Loss = 10.883515, Accuracy = 0.35066667199134827\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.843696594238281, Accuracy = 0.35795116424560547\n",
      "Training iter #4620000:   Batch Loss = 10.382612, Accuracy = 0.4646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.841986656188965, Accuracy = 0.35854676365852356\n",
      "Training iter #4623000:   Batch Loss = 10.618535, Accuracy = 0.38866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.840304374694824, Accuracy = 0.35914233326911926\n",
      "Training iter #4626000:   Batch Loss = 10.629768, Accuracy = 0.3919999897480011\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.838562965393066, Accuracy = 0.3594401478767395\n",
      "Training iter #4629000:   Batch Loss = 11.154674, Accuracy = 0.3100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.836908340454102, Accuracy = 0.35973793268203735\n",
      "Training iter #4632000:   Batch Loss = 10.754148, Accuracy = 0.3986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.835287094116211, Accuracy = 0.35914233326911926\n",
      "Training iter #4635000:   Batch Loss = 10.362543, Accuracy = 0.4573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.833646774291992, Accuracy = 0.3600357472896576\n",
      "Training iter #4638000:   Batch Loss = 10.614602, Accuracy = 0.3919999897480011\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.831867218017578, Accuracy = 0.3606313169002533\n",
      "Training iter #4641000:   Batch Loss = 11.042824, Accuracy = 0.2919999957084656\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.83009147644043, Accuracy = 0.3612269163131714\n",
      "Training iter #4644000:   Batch Loss = 10.840819, Accuracy = 0.35600000619888306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.828409194946289, Accuracy = 0.3615247309207916\n",
      "Training iter #4647000:   Batch Loss = 10.319213, Accuracy = 0.4866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.826749801635742, Accuracy = 0.36212030053138733\n",
      "Training iter #4650000:   Batch Loss = 10.501749, Accuracy = 0.4313333332538605\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.82509994506836, Accuracy = 0.36241811513900757\n",
      "Training iter #4653000:   Batch Loss = 10.622791, Accuracy = 0.38199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.823404312133789, Accuracy = 0.36301368474960327\n",
      "Training iter #4656000:   Batch Loss = 11.257885, Accuracy = 0.28999999165534973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.821836471557617, Accuracy = 0.3633114993572235\n",
      "Training iter #4659000:   Batch Loss = 10.685094, Accuracy = 0.4073333442211151\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.82033920288086, Accuracy = 0.36480048298835754\n",
      "Training iter #4662000:   Batch Loss = 10.352299, Accuracy = 0.4519999921321869\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.81872844696045, Accuracy = 0.36480048298835754\n",
      "Training iter #4665000:   Batch Loss = 10.720876, Accuracy = 0.3786666691303253\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.816986083984375, Accuracy = 0.3656938672065735\n",
      "Training iter #4668000:   Batch Loss = 10.991678, Accuracy = 0.29466667771339417\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.81521224975586, Accuracy = 0.3656938672065735\n",
      "Training iter #4671000:   Batch Loss = 10.743016, Accuracy = 0.37400001287460327\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.81348705291748, Accuracy = 0.36599165201187134\n",
      "Training iter #4674000:   Batch Loss = 10.370192, Accuracy = 0.4880000054836273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.811807632446289, Accuracy = 0.3668850362300873\n",
      "Training iter #4677000:   Batch Loss = 10.500179, Accuracy = 0.4313333332538605\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.81019401550293, Accuracy = 0.3671828508377075\n",
      "Training iter #4680000:   Batch Loss = 10.631270, Accuracy = 0.3726666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.808609962463379, Accuracy = 0.36807623505592346\n",
      "Training iter #4683000:   Batch Loss = 11.201707, Accuracy = 0.30133333802223206\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.80705738067627, Accuracy = 0.3683740198612213\n",
      "Training iter #4686000:   Batch Loss = 10.562490, Accuracy = 0.43666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.805496215820312, Accuracy = 0.36867183446884155\n",
      "Training iter #4689000:   Batch Loss = 10.352031, Accuracy = 0.44733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.80389404296875, Accuracy = 0.36926743388175964\n",
      "Training iter #4692000:   Batch Loss = 10.684367, Accuracy = 0.39533331990242004\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.802190780639648, Accuracy = 0.3701608180999756\n",
      "Training iter #4695000:   Batch Loss = 10.960648, Accuracy = 0.31200000643730164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.800460815429688, Accuracy = 0.37045860290527344\n",
      "Training iter #4698000:   Batch Loss = 10.832069, Accuracy = 0.359333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.798702239990234, Accuracy = 0.37045860290527344\n",
      "Training iter #4701000:   Batch Loss = 10.429050, Accuracy = 0.47066667675971985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.797051429748535, Accuracy = 0.37045860290527344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4704000:   Batch Loss = 10.466601, Accuracy = 0.43933331966400146\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.795403480529785, Accuracy = 0.37105420231819153\n",
      "Training iter #4707000:   Batch Loss = 10.687201, Accuracy = 0.3646666705608368\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.793757438659668, Accuracy = 0.3716498017311096\n",
      "Training iter #4710000:   Batch Loss = 11.159694, Accuracy = 0.3033333420753479\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.792121887207031, Accuracy = 0.3722453713417053\n",
      "Training iter #4713000:   Batch Loss = 10.457341, Accuracy = 0.47200000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.790584564208984, Accuracy = 0.37254318594932556\n",
      "Training iter #4716000:   Batch Loss = 10.306056, Accuracy = 0.46266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.789018630981445, Accuracy = 0.37313878536224365\n",
      "Training iter #4719000:   Batch Loss = 10.698315, Accuracy = 0.3726666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.787338256835938, Accuracy = 0.37373435497283936\n",
      "Training iter #4722000:   Batch Loss = 10.932145, Accuracy = 0.31333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.78569221496582, Accuracy = 0.3740321695804596\n",
      "Training iter #4725000:   Batch Loss = 10.797616, Accuracy = 0.3713333308696747\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.78409481048584, Accuracy = 0.3740321695804596\n",
      "Training iter #4728000:   Batch Loss = 10.442341, Accuracy = 0.4493333399295807\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.782546043395996, Accuracy = 0.3740321695804596\n",
      "Training iter #4731000:   Batch Loss = 10.497405, Accuracy = 0.4566666781902313\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.780980110168457, Accuracy = 0.3746277689933777\n",
      "Training iter #4734000:   Batch Loss = 10.673059, Accuracy = 0.37533333897590637\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.779342651367188, Accuracy = 0.3746277689933777\n",
      "Training iter #4737000:   Batch Loss = 11.118241, Accuracy = 0.3153333365917206\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.777727127075195, Accuracy = 0.37492555379867554\n",
      "Training iter #4740000:   Batch Loss = 10.349640, Accuracy = 0.5040000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.776135444641113, Accuracy = 0.3758189380168915\n",
      "Training iter #4743000:   Batch Loss = 10.257589, Accuracy = 0.4860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.774579048156738, Accuracy = 0.3758189380168915\n",
      "Training iter #4746000:   Batch Loss = 10.684410, Accuracy = 0.3619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.77292251586914, Accuracy = 0.37641453742980957\n",
      "Training iter #4749000:   Batch Loss = 10.875822, Accuracy = 0.3306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.771291732788086, Accuracy = 0.37701013684272766\n",
      "Training iter #4752000:   Batch Loss = 10.797535, Accuracy = 0.36399999260902405\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.769670486450195, Accuracy = 0.37820130586624146\n",
      "Training iter #4755000:   Batch Loss = 10.378224, Accuracy = 0.4586666524410248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.768165588378906, Accuracy = 0.37820130586624146\n",
      "Training iter #4758000:   Batch Loss = 10.552336, Accuracy = 0.4300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.766669273376465, Accuracy = 0.3784991204738617\n",
      "Training iter #4761000:   Batch Loss = 10.746447, Accuracy = 0.3580000102519989\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.765083312988281, Accuracy = 0.37820130586624146\n",
      "Training iter #4764000:   Batch Loss = 11.121236, Accuracy = 0.3113333284854889\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.763507843017578, Accuracy = 0.37820130586624146\n",
      "Training iter #4767000:   Batch Loss = 10.352184, Accuracy = 0.515333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.761993408203125, Accuracy = 0.3784991204738617\n",
      "Training iter #4770000:   Batch Loss = 10.396293, Accuracy = 0.44333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.760543823242188, Accuracy = 0.37879690527915955\n",
      "Training iter #4773000:   Batch Loss = 10.603684, Accuracy = 0.38199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.758970260620117, Accuracy = 0.37939250469207764\n",
      "Training iter #4776000:   Batch Loss = 10.894936, Accuracy = 0.3306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.757389068603516, Accuracy = 0.37998807430267334\n",
      "Training iter #4779000:   Batch Loss = 10.847226, Accuracy = 0.35466668009757996\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.75587272644043, Accuracy = 0.37998807430267334\n",
      "Training iter #4782000:   Batch Loss = 10.208175, Accuracy = 0.5146666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.75421142578125, Accuracy = 0.37998807430267334\n",
      "Training iter #4785000:   Batch Loss = 10.489979, Accuracy = 0.437333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.752552032470703, Accuracy = 0.3802858889102936\n",
      "Training iter #4788000:   Batch Loss = 10.781925, Accuracy = 0.3453333377838135\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.750877380371094, Accuracy = 0.3811792731285095\n",
      "Training iter #4791000:   Batch Loss = 11.038774, Accuracy = 0.3233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.749235153198242, Accuracy = 0.3817748725414276\n",
      "Training iter #4794000:   Batch Loss = 10.322095, Accuracy = 0.5046666860580444\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.747661590576172, Accuracy = 0.38207265734672546\n",
      "Training iter #4797000:   Batch Loss = 10.476554, Accuracy = 0.421999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.746082305908203, Accuracy = 0.3823704719543457\n",
      "Training iter #4800000:   Batch Loss = 10.482321, Accuracy = 0.41333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.74437427520752, Accuracy = 0.38207265734672546\n",
      "Training iter #4803000:   Batch Loss = 10.896888, Accuracy = 0.3473333418369293\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.742713928222656, Accuracy = 0.38326385617256165\n",
      "Training iter #4806000:   Batch Loss = 10.813679, Accuracy = 0.36800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.741172790527344, Accuracy = 0.38326385617256165\n",
      "Training iter #4809000:   Batch Loss = 10.233715, Accuracy = 0.5133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.739727020263672, Accuracy = 0.38385942578315735\n",
      "Training iter #4812000:   Batch Loss = 10.480077, Accuracy = 0.4313333332538605\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.738235473632812, Accuracy = 0.38385942578315735\n",
      "Training iter #4815000:   Batch Loss = 10.793125, Accuracy = 0.34599998593330383\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.736695289611816, Accuracy = 0.38445502519607544\n",
      "Training iter #4818000:   Batch Loss = 11.031909, Accuracy = 0.3140000104904175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.735143661499023, Accuracy = 0.3847528398036957\n",
      "Training iter #4821000:   Batch Loss = 10.339437, Accuracy = 0.5006666779518127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.733559608459473, Accuracy = 0.38505062460899353\n",
      "Training iter #4824000:   Batch Loss = 10.429989, Accuracy = 0.45399999618530273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.73199462890625, Accuracy = 0.3853484094142914\n",
      "Training iter #4827000:   Batch Loss = 10.475085, Accuracy = 0.414000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.730317115783691, Accuracy = 0.3859440088272095\n",
      "Training iter #4830000:   Batch Loss = 10.864344, Accuracy = 0.35333332419395447\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.728643417358398, Accuracy = 0.3856462240219116\n",
      "Training iter #4833000:   Batch Loss = 10.689435, Accuracy = 0.38733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.72695541381836, Accuracy = 0.38505062460899353\n",
      "Training iter #4836000:   Batch Loss = 10.240536, Accuracy = 0.49266666173934937\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.725327491760254, Accuracy = 0.3856462240219116\n",
      "Training iter #4839000:   Batch Loss = 10.507401, Accuracy = 0.4233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.723755836486816, Accuracy = 0.3862418234348297\n",
      "Training iter #4842000:   Batch Loss = 10.862366, Accuracy = 0.32866665720939636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.722212791442871, Accuracy = 0.38653960824012756\n",
      "Training iter #4845000:   Batch Loss = 10.926914, Accuracy = 0.3333333432674408\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.720703125, Accuracy = 0.3874329924583435\n",
      "Training iter #4848000:   Batch Loss = 10.344079, Accuracy = 0.5073333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.719289779663086, Accuracy = 0.38773077726364136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4851000:   Batch Loss = 10.420910, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.717863082885742, Accuracy = 0.3880285918712616\n",
      "Training iter #4854000:   Batch Loss = 10.414522, Accuracy = 0.43666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.716333389282227, Accuracy = 0.38832637667655945\n",
      "Training iter #4857000:   Batch Loss = 10.879065, Accuracy = 0.3479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.714807510375977, Accuracy = 0.3892197608947754\n",
      "Training iter #4860000:   Batch Loss = 10.635158, Accuracy = 0.4059999883174896\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.713181495666504, Accuracy = 0.3898153603076935\n",
      "Training iter #4863000:   Batch Loss = 10.187759, Accuracy = 0.5053333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.711583137512207, Accuracy = 0.3898153603076935\n",
      "Training iter #4866000:   Batch Loss = 10.511020, Accuracy = 0.414000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.710025787353516, Accuracy = 0.3904109597206116\n",
      "Training iter #4869000:   Batch Loss = 10.898792, Accuracy = 0.3213333189487457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.70843505859375, Accuracy = 0.39100655913352966\n",
      "Training iter #4872000:   Batch Loss = 10.879338, Accuracy = 0.34200000762939453\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.70681095123291, Accuracy = 0.3904109597206116\n",
      "Training iter #4875000:   Batch Loss = 10.373419, Accuracy = 0.5073333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.704913139343262, Accuracy = 0.39100655913352966\n",
      "Training iter #4878000:   Batch Loss = 10.453562, Accuracy = 0.4580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.703283309936523, Accuracy = 0.39160215854644775\n",
      "Training iter #4881000:   Batch Loss = 10.430497, Accuracy = 0.42800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.701659202575684, Accuracy = 0.39160215854644775\n",
      "Training iter #4884000:   Batch Loss = 10.901871, Accuracy = 0.34599998593330383\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.7000732421875, Accuracy = 0.39160215854644775\n",
      "Training iter #4887000:   Batch Loss = 10.634157, Accuracy = 0.41200000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.69847297668457, Accuracy = 0.39219772815704346\n",
      "Training iter #4890000:   Batch Loss = 10.220419, Accuracy = 0.4933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.696864128112793, Accuracy = 0.39279332756996155\n",
      "Training iter #4893000:   Batch Loss = 10.489247, Accuracy = 0.41733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.69527530670166, Accuracy = 0.3936867117881775\n",
      "Training iter #4896000:   Batch Loss = 10.862520, Accuracy = 0.33133333921432495\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.693608283996582, Accuracy = 0.3942823112010956\n",
      "Training iter #4899000:   Batch Loss = 10.880586, Accuracy = 0.34599998593330383\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.692046165466309, Accuracy = 0.3951756954193115\n",
      "Training iter #4902000:   Batch Loss = 10.299854, Accuracy = 0.5233333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.690507888793945, Accuracy = 0.39547351002693176\n",
      "Training iter #4905000:   Batch Loss = 10.383128, Accuracy = 0.4699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.689064025878906, Accuracy = 0.39547351002693176\n",
      "Training iter #4908000:   Batch Loss = 10.323956, Accuracy = 0.4593333303928375\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.687543869018555, Accuracy = 0.39547351002693176\n",
      "Training iter #4911000:   Batch Loss = 10.966643, Accuracy = 0.33799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.686042785644531, Accuracy = 0.39547351002693176\n",
      "Training iter #4914000:   Batch Loss = 10.622469, Accuracy = 0.3986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.68453598022461, Accuracy = 0.3957712948322296\n",
      "Training iter #4917000:   Batch Loss = 10.239737, Accuracy = 0.4779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.683048248291016, Accuracy = 0.3963668942451477\n",
      "Training iter #4920000:   Batch Loss = 10.481831, Accuracy = 0.41333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.681580543518066, Accuracy = 0.39726027846336365\n",
      "Training iter #4923000:   Batch Loss = 10.904486, Accuracy = 0.3233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.680109977722168, Accuracy = 0.39785587787628174\n",
      "Training iter #4926000:   Batch Loss = 10.814373, Accuracy = 0.3540000021457672\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.67882251739502, Accuracy = 0.3981536626815796\n",
      "Training iter #4929000:   Batch Loss = 10.148190, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.677330017089844, Accuracy = 0.39845144748687744\n",
      "Training iter #4932000:   Batch Loss = 10.412612, Accuracy = 0.4580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.675771713256836, Accuracy = 0.39845144748687744\n",
      "Training iter #4935000:   Batch Loss = 10.412768, Accuracy = 0.4440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.674216270446777, Accuracy = 0.3987492620944977\n",
      "Training iter #4938000:   Batch Loss = 10.999712, Accuracy = 0.3253333270549774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.672690391540527, Accuracy = 0.3987492620944977\n",
      "Training iter #4941000:   Batch Loss = 10.636827, Accuracy = 0.40799999237060547\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.67105770111084, Accuracy = 0.39845144748687744\n",
      "Training iter #4944000:   Batch Loss = 10.123672, Accuracy = 0.5299999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.66947078704834, Accuracy = 0.3987492620944977\n",
      "Training iter #4947000:   Batch Loss = 10.433741, Accuracy = 0.4246666729450226\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.667828559875488, Accuracy = 0.3987492620944977\n",
      "Training iter #4950000:   Batch Loss = 10.916460, Accuracy = 0.32066667079925537\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.666047096252441, Accuracy = 0.39934486150741577\n",
      "Training iter #4953000:   Batch Loss = 10.668362, Accuracy = 0.39533331990242004\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.664352416992188, Accuracy = 0.39904704689979553\n",
      "Training iter #4956000:   Batch Loss = 10.200201, Accuracy = 0.531333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.662698745727539, Accuracy = 0.39934486150741577\n",
      "Training iter #4959000:   Batch Loss = 10.399154, Accuracy = 0.44333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.661091804504395, Accuracy = 0.39934486150741577\n",
      "Training iter #4962000:   Batch Loss = 10.418763, Accuracy = 0.4426666796207428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.659555435180664, Accuracy = 0.4002382457256317\n",
      "Training iter #4965000:   Batch Loss = 11.023020, Accuracy = 0.3293333351612091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.658052444458008, Accuracy = 0.4008338153362274\n",
      "Training iter #4968000:   Batch Loss = 10.543653, Accuracy = 0.43533334136009216\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.656523704528809, Accuracy = 0.4008338153362274\n",
      "Training iter #4971000:   Batch Loss = 10.208483, Accuracy = 0.49533334374427795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.654996871948242, Accuracy = 0.40113162994384766\n",
      "Training iter #4974000:   Batch Loss = 10.462232, Accuracy = 0.41999998688697815\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.653437614440918, Accuracy = 0.40291839838027954\n",
      "Training iter #4977000:   Batch Loss = 10.878393, Accuracy = 0.32199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.651823043823242, Accuracy = 0.40351399779319763\n",
      "Training iter #4980000:   Batch Loss = 10.617729, Accuracy = 0.3933333456516266\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.650123596191406, Accuracy = 0.4038117825984955\n",
      "Training iter #4983000:   Batch Loss = 10.124450, Accuracy = 0.5699999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.64844799041748, Accuracy = 0.4038117825984955\n",
      "Training iter #4986000:   Batch Loss = 10.330338, Accuracy = 0.46799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.646790504455566, Accuracy = 0.4038117825984955\n",
      "Training iter #4989000:   Batch Loss = 10.407298, Accuracy = 0.4386666715145111\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.645307540893555, Accuracy = 0.4044073820114136\n",
      "Training iter #4992000:   Batch Loss = 11.104485, Accuracy = 0.31066668033599854\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.643869400024414, Accuracy = 0.4047051668167114\n",
      "Training iter #4995000:   Batch Loss = 10.456898, Accuracy = 0.4573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.642438888549805, Accuracy = 0.4047051668167114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4998000:   Batch Loss = 10.173079, Accuracy = 0.49799999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.64095687866211, Accuracy = 0.40500298142433167\n",
      "Training iter #5001000:   Batch Loss = 10.501282, Accuracy = 0.4300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.639413833618164, Accuracy = 0.40559858083724976\n",
      "Training iter #5004000:   Batch Loss = 10.805053, Accuracy = 0.3440000116825104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.637975692749023, Accuracy = 0.40738534927368164\n",
      "Training iter #5007000:   Batch Loss = 10.634020, Accuracy = 0.3866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.636503219604492, Accuracy = 0.40798094868659973\n",
      "Training iter #5010000:   Batch Loss = 10.170517, Accuracy = 0.5613333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.634963035583496, Accuracy = 0.40798094868659973\n",
      "Training iter #5013000:   Batch Loss = 10.330743, Accuracy = 0.4673333466053009\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.633384704589844, Accuracy = 0.40798094868659973\n",
      "Training iter #5016000:   Batch Loss = 10.496596, Accuracy = 0.4099999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.631775856018066, Accuracy = 0.4082787334918976\n",
      "Training iter #5019000:   Batch Loss = 10.970423, Accuracy = 0.335999995470047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.630194664001465, Accuracy = 0.4082787334918976\n",
      "Training iter #5022000:   Batch Loss = 10.331228, Accuracy = 0.4866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.628510475158691, Accuracy = 0.4082787334918976\n",
      "Training iter #5025000:   Batch Loss = 10.157123, Accuracy = 0.5040000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.626846313476562, Accuracy = 0.4088743329048157\n",
      "Training iter #5028000:   Batch Loss = 10.487255, Accuracy = 0.43666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.625253677368164, Accuracy = 0.4085765480995178\n",
      "Training iter #5031000:   Batch Loss = 10.760519, Accuracy = 0.3526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.623720169067383, Accuracy = 0.4088743329048157\n",
      "Training iter #5034000:   Batch Loss = 10.634826, Accuracy = 0.39133334159851074\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.622180938720703, Accuracy = 0.4091721177101135\n",
      "Training iter #5037000:   Batch Loss = 10.258812, Accuracy = 0.5113333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.62072467803955, Accuracy = 0.4088743329048157\n",
      "Training iter #5040000:   Batch Loss = 10.329880, Accuracy = 0.4806666672229767\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.61925983428955, Accuracy = 0.40946993231773376\n",
      "Training iter #5043000:   Batch Loss = 10.472420, Accuracy = 0.4320000112056732\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.617870330810547, Accuracy = 0.4097677171230316\n",
      "Training iter #5046000:   Batch Loss = 11.009504, Accuracy = 0.3179999887943268\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.616349220275879, Accuracy = 0.41006550192832947\n",
      "Training iter #5049000:   Batch Loss = 10.207432, Accuracy = 0.5353333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.614832878112793, Accuracy = 0.41006550192832947\n",
      "Training iter #5052000:   Batch Loss = 10.156212, Accuracy = 0.5019999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.61330795288086, Accuracy = 0.41066110134124756\n",
      "Training iter #5055000:   Batch Loss = 10.528809, Accuracy = 0.4026666581630707\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.611778259277344, Accuracy = 0.4109589159488678\n",
      "Training iter #5058000:   Batch Loss = 10.738269, Accuracy = 0.35199999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.610247611999512, Accuracy = 0.41125670075416565\n",
      "Training iter #5061000:   Batch Loss = 10.650779, Accuracy = 0.38999998569488525\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.608641624450684, Accuracy = 0.41125670075416565\n",
      "Training iter #5064000:   Batch Loss = 10.257101, Accuracy = 0.4886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.607060432434082, Accuracy = 0.4115544855594635\n",
      "Training iter #5067000:   Batch Loss = 10.385194, Accuracy = 0.4713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.605485916137695, Accuracy = 0.41125670075416565\n",
      "Training iter #5070000:   Batch Loss = 10.528139, Accuracy = 0.4206666648387909\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.604022026062012, Accuracy = 0.4109589159488678\n",
      "Training iter #5073000:   Batch Loss = 10.987928, Accuracy = 0.328000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.602463722229004, Accuracy = 0.4115544855594635\n",
      "Training iter #5076000:   Batch Loss = 10.120409, Accuracy = 0.5693333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.600910186767578, Accuracy = 0.41185230016708374\n",
      "Training iter #5079000:   Batch Loss = 10.129574, Accuracy = 0.5226666927337646\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.599271774291992, Accuracy = 0.41185230016708374\n",
      "Training iter #5082000:   Batch Loss = 10.467037, Accuracy = 0.4086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.597663879394531, Accuracy = 0.4121500849723816\n",
      "Training iter #5085000:   Batch Loss = 10.720472, Accuracy = 0.3580000102519989\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.596155166625977, Accuracy = 0.4127456843852997\n",
      "Training iter #5088000:   Batch Loss = 10.647514, Accuracy = 0.3880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.594619750976562, Accuracy = 0.4127456843852997\n",
      "Training iter #5091000:   Batch Loss = 10.062367, Accuracy = 0.54666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.593080520629883, Accuracy = 0.4139368534088135\n",
      "Training iter #5094000:   Batch Loss = 10.352516, Accuracy = 0.47999998927116394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.591492652893066, Accuracy = 0.4142346680164337\n",
      "Training iter #5097000:   Batch Loss = 10.582717, Accuracy = 0.40933331847190857\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.589988708496094, Accuracy = 0.41453245282173157\n",
      "Training iter #5100000:   Batch Loss = 10.914218, Accuracy = 0.34599998593330383\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.588485717773438, Accuracy = 0.4154258370399475\n",
      "Training iter #5103000:   Batch Loss = 10.146040, Accuracy = 0.5613333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.586999893188477, Accuracy = 0.41572365164756775\n",
      "Training iter #5106000:   Batch Loss = 10.280996, Accuracy = 0.47066667675971985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.585562705993652, Accuracy = 0.4166170358657837\n",
      "Training iter #5109000:   Batch Loss = 10.355408, Accuracy = 0.4426666796207428\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.584097862243652, Accuracy = 0.4166170358657837\n",
      "Training iter #5112000:   Batch Loss = 10.736050, Accuracy = 0.3606666624546051\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.582625389099121, Accuracy = 0.41691482067108154\n",
      "Training iter #5115000:   Batch Loss = 10.686016, Accuracy = 0.38466668128967285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.581113815307617, Accuracy = 0.4172126352787018\n",
      "Training iter #5118000:   Batch Loss = 9.999323, Accuracy = 0.5759999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.579593658447266, Accuracy = 0.4172126352787018\n",
      "Training iter #5121000:   Batch Loss = 10.322256, Accuracy = 0.4713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.5780611038208, Accuracy = 0.41751042008399963\n",
      "Training iter #5124000:   Batch Loss = 10.636398, Accuracy = 0.39266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.576534271240234, Accuracy = 0.4184038043022156\n",
      "Training iter #5127000:   Batch Loss = 10.809168, Accuracy = 0.36133334040641785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.574995040893555, Accuracy = 0.4192971885204315\n",
      "Training iter #5130000:   Batch Loss = 10.174909, Accuracy = 0.5320000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.573482513427734, Accuracy = 0.4198927879333496\n",
      "Training iter #5133000:   Batch Loss = 10.227779, Accuracy = 0.49533334374427795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.571991920471191, Accuracy = 0.41959500312805176\n",
      "Training iter #5136000:   Batch Loss = 10.312302, Accuracy = 0.4586666524410248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.570517539978027, Accuracy = 0.4204883873462677\n",
      "Training iter #5139000:   Batch Loss = 10.689819, Accuracy = 0.38866665959358215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.569052696228027, Accuracy = 0.42138177156448364\n",
      "Training iter #5142000:   Batch Loss = 10.633471, Accuracy = 0.39800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.567680358886719, Accuracy = 0.42138177156448364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #5145000:   Batch Loss = 10.080166, Accuracy = 0.5400000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.566515922546387, Accuracy = 0.42197737097740173\n",
      "Training iter #5148000:   Batch Loss = 10.271582, Accuracy = 0.4846666753292084\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.56513786315918, Accuracy = 0.42197737097740173\n",
      "Training iter #5151000:   Batch Loss = 10.646967, Accuracy = 0.390666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.56368637084961, Accuracy = 0.42197737097740173\n",
      "Training iter #5154000:   Batch Loss = 10.838439, Accuracy = 0.3466666638851166\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.562170028686523, Accuracy = 0.4228707551956177\n",
      "Training iter #5157000:   Batch Loss = 10.188738, Accuracy = 0.5413333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.560619354248047, Accuracy = 0.4237641394138336\n",
      "Training iter #5160000:   Batch Loss = 10.245041, Accuracy = 0.5146666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.559087753295898, Accuracy = 0.4237641394138336\n",
      "Training iter #5163000:   Batch Loss = 10.236228, Accuracy = 0.476666659116745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.557546615600586, Accuracy = 0.42346635460853577\n",
      "Training iter #5166000:   Batch Loss = 10.710203, Accuracy = 0.3853333294391632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.556111335754395, Accuracy = 0.4237641394138336\n",
      "Training iter #5169000:   Batch Loss = 10.524267, Accuracy = 0.41466665267944336\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.554551124572754, Accuracy = 0.42465752363204956\n",
      "Training iter #5172000:   Batch Loss = 10.055826, Accuracy = 0.5320000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.553080558776855, Accuracy = 0.4249553382396698\n",
      "Training iter #5175000:   Batch Loss = 10.387942, Accuracy = 0.44733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.551567077636719, Accuracy = 0.4249553382396698\n",
      "Training iter #5178000:   Batch Loss = 10.675367, Accuracy = 0.38600000739097595\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.549982070922852, Accuracy = 0.42525312304496765\n",
      "Training iter #5181000:   Batch Loss = 10.788308, Accuracy = 0.35866665840148926\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.548398971557617, Accuracy = 0.4261465072631836\n",
      "Training iter #5184000:   Batch Loss = 10.155287, Accuracy = 0.5586666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.546874046325684, Accuracy = 0.4267421066761017\n",
      "Training iter #5187000:   Batch Loss = 10.254529, Accuracy = 0.518666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.545361518859863, Accuracy = 0.4273377060890198\n",
      "Training iter #5190000:   Batch Loss = 10.206147, Accuracy = 0.48133334517478943\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.543871879577637, Accuracy = 0.4267421066761017\n",
      "Training iter #5193000:   Batch Loss = 10.701797, Accuracy = 0.38733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.542372703552246, Accuracy = 0.42852887511253357\n",
      "Training iter #5196000:   Batch Loss = 10.473734, Accuracy = 0.4346666634082794\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.540864944458008, Accuracy = 0.42852887511253357\n",
      "Training iter #5199000:   Batch Loss = 9.971645, Accuracy = 0.5479999780654907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.539399147033691, Accuracy = 0.42912447452545166\n",
      "Training iter #5202000:   Batch Loss = 10.310432, Accuracy = 0.4620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.537938117980957, Accuracy = 0.4294222891330719\n",
      "Training iter #5205000:   Batch Loss = 10.683072, Accuracy = 0.38333332538604736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.53647518157959, Accuracy = 0.4294222891330719\n",
      "Training iter #5208000:   Batch Loss = 10.695230, Accuracy = 0.3773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.535001754760742, Accuracy = 0.42972007393836975\n",
      "Training iter #5211000:   Batch Loss = 10.187885, Accuracy = 0.5533333420753479\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.533517837524414, Accuracy = 0.4300178587436676\n",
      "Training iter #5214000:   Batch Loss = 10.226880, Accuracy = 0.5253333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.532024383544922, Accuracy = 0.4306134581565857\n",
      "Training iter #5217000:   Batch Loss = 10.165956, Accuracy = 0.4880000054836273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.530561447143555, Accuracy = 0.43150684237480164\n",
      "Training iter #5220000:   Batch Loss = 10.763576, Accuracy = 0.3700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.529106140136719, Accuracy = 0.4321024417877197\n",
      "Training iter #5223000:   Batch Loss = 10.425697, Accuracy = 0.44333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.527626991271973, Accuracy = 0.4324002265930176\n",
      "Training iter #5226000:   Batch Loss = 10.056332, Accuracy = 0.5320000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.526179313659668, Accuracy = 0.43359142541885376\n",
      "Training iter #5229000:   Batch Loss = 10.330913, Accuracy = 0.4580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.524679183959961, Accuracy = 0.43359142541885376\n",
      "Training iter #5232000:   Batch Loss = 10.736477, Accuracy = 0.36933332681655884\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.523179054260254, Accuracy = 0.43359142541885376\n",
      "Training iter #5235000:   Batch Loss = 10.695222, Accuracy = 0.382666677236557\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.521734237670898, Accuracy = 0.4338892102241516\n",
      "Training iter #5238000:   Batch Loss = 10.048281, Accuracy = 0.6073333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.520248413085938, Accuracy = 0.4338892102241516\n",
      "Training iter #5241000:   Batch Loss = 10.212723, Accuracy = 0.5213333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.518744468688965, Accuracy = 0.4338892102241516\n",
      "Training iter #5244000:   Batch Loss = 10.181426, Accuracy = 0.49933332204818726\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.517261505126953, Accuracy = 0.4344848096370697\n",
      "Training iter #5247000:   Batch Loss = 10.771083, Accuracy = 0.3686666786670685\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.515582084655762, Accuracy = 0.4350804090499878\n",
      "Training iter #5250000:   Batch Loss = 10.475963, Accuracy = 0.42266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.513944625854492, Accuracy = 0.4350804090499878\n",
      "Training iter #5253000:   Batch Loss = 10.027371, Accuracy = 0.5353333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.512320518493652, Accuracy = 0.4356760084629059\n",
      "Training iter #5256000:   Batch Loss = 10.290279, Accuracy = 0.4586666524410248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.510725975036621, Accuracy = 0.4365693926811218\n",
      "Training iter #5259000:   Batch Loss = 10.735705, Accuracy = 0.3700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.50930118560791, Accuracy = 0.4362715780735016\n",
      "Training iter #5262000:   Batch Loss = 10.568730, Accuracy = 0.4073333442211151\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.508036613464355, Accuracy = 0.4365693926811218\n",
      "Training iter #5265000:   Batch Loss = 10.020901, Accuracy = 0.5986666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.506820678710938, Accuracy = 0.4368671774864197\n",
      "Training iter #5268000:   Batch Loss = 10.262485, Accuracy = 0.49000000953674316\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.505438804626465, Accuracy = 0.4371649920940399\n",
      "Training iter #5271000:   Batch Loss = 10.224401, Accuracy = 0.492000013589859\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.503986358642578, Accuracy = 0.4377605617046356\n",
      "Training iter #5274000:   Batch Loss = 10.837904, Accuracy = 0.3566666543483734\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.50257682800293, Accuracy = 0.4383561611175537\n",
      "Training iter #5277000:   Batch Loss = 10.415028, Accuracy = 0.4480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.501143455505371, Accuracy = 0.4389517605304718\n",
      "Training iter #5280000:   Batch Loss = 9.990927, Accuracy = 0.5733333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.49964427947998, Accuracy = 0.4401429295539856\n",
      "Training iter #5283000:   Batch Loss = 10.251675, Accuracy = 0.47600001096725464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.498095512390137, Accuracy = 0.44044074416160583\n",
      "Training iter #5286000:   Batch Loss = 10.719204, Accuracy = 0.3700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.496581077575684, Accuracy = 0.4413341283798218\n",
      "Training iter #5289000:   Batch Loss = 10.511830, Accuracy = 0.41600000858306885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.495070457458496, Accuracy = 0.44192972779273987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #5292000:   Batch Loss = 9.982812, Accuracy = 0.6046666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.493515014648438, Accuracy = 0.44192972779273987\n",
      "Training iter #5295000:   Batch Loss = 10.177990, Accuracy = 0.5120000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.491886138916016, Accuracy = 0.44252532720565796\n",
      "Training iter #5298000:   Batch Loss = 10.237902, Accuracy = 0.48533332347869873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.490470886230469, Accuracy = 0.44252532720565796\n",
      "Training iter #5301000:   Batch Loss = 10.903432, Accuracy = 0.3466666638851166\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.489038467407227, Accuracy = 0.4428231120109558\n",
      "Training iter #5304000:   Batch Loss = 10.326511, Accuracy = 0.48133334517478943\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.4876127243042, Accuracy = 0.44312089681625366\n",
      "Training iter #5307000:   Batch Loss = 10.022135, Accuracy = 0.5419999957084656\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.486126899719238, Accuracy = 0.44371649622917175\n",
      "Training iter #5310000:   Batch Loss = 10.338324, Accuracy = 0.46399998664855957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.484654426574707, Accuracy = 0.4440142810344696\n",
      "Training iter #5313000:   Batch Loss = 10.669760, Accuracy = 0.37599998712539673\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.48322868347168, Accuracy = 0.44431209564208984\n",
      "Training iter #5316000:   Batch Loss = 10.422563, Accuracy = 0.4326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.481782913208008, Accuracy = 0.4452054798603058\n",
      "Training iter #5319000:   Batch Loss = 10.002868, Accuracy = 0.596666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.480367660522461, Accuracy = 0.4452054798603058\n",
      "Training iter #5322000:   Batch Loss = 10.158000, Accuracy = 0.5113333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.47891616821289, Accuracy = 0.4452054798603058\n",
      "Training iter #5325000:   Batch Loss = 10.298259, Accuracy = 0.46399998664855957\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.477540969848633, Accuracy = 0.4452054798603058\n",
      "Training iter #5328000:   Batch Loss = 10.887239, Accuracy = 0.3486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.476099014282227, Accuracy = 0.44550326466560364\n",
      "Training iter #5331000:   Batch Loss = 10.231617, Accuracy = 0.5139999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.474664688110352, Accuracy = 0.44609886407852173\n",
      "Training iter #5334000:   Batch Loss = 9.992038, Accuracy = 0.5433333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.47316837310791, Accuracy = 0.4466944634914398\n",
      "Training iter #5337000:   Batch Loss = 10.326425, Accuracy = 0.47466665506362915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.471656799316406, Accuracy = 0.4466944634914398\n",
      "Training iter #5340000:   Batch Loss = 10.644438, Accuracy = 0.3853333294391632\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.47016429901123, Accuracy = 0.4472900629043579\n",
      "Training iter #5343000:   Batch Loss = 10.515429, Accuracy = 0.4113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.468645095825195, Accuracy = 0.4472900629043579\n",
      "Training iter #5346000:   Batch Loss = 10.046916, Accuracy = 0.5826666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.467157363891602, Accuracy = 0.4478856325149536\n",
      "Training iter #5349000:   Batch Loss = 10.127229, Accuracy = 0.515333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.465619087219238, Accuracy = 0.44877904653549194\n",
      "Training iter #5352000:   Batch Loss = 10.338772, Accuracy = 0.46533334255218506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.46419906616211, Accuracy = 0.45056581497192383\n",
      "Training iter #5355000:   Batch Loss = 10.846705, Accuracy = 0.3486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.462735176086426, Accuracy = 0.45056581497192383\n",
      "Training iter #5358000:   Batch Loss = 10.144157, Accuracy = 0.5460000038146973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.461308479309082, Accuracy = 0.4508635997772217\n",
      "Training iter #5361000:   Batch Loss = 9.954268, Accuracy = 0.5553333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.459831237792969, Accuracy = 0.4508635997772217\n",
      "Training iter #5364000:   Batch Loss = 10.322917, Accuracy = 0.4659999907016754\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.458322525024414, Accuracy = 0.4508635997772217\n",
      "Training iter #5367000:   Batch Loss = 10.594622, Accuracy = 0.38999998569488525\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.4568510055542, Accuracy = 0.4517569839954376\n",
      "Training iter #5370000:   Batch Loss = 10.492269, Accuracy = 0.4193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.455395698547363, Accuracy = 0.4517569839954376\n",
      "Training iter #5373000:   Batch Loss = 10.079327, Accuracy = 0.5519999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.453925132751465, Accuracy = 0.4517569839954376\n",
      "Training iter #5376000:   Batch Loss = 10.165316, Accuracy = 0.5233333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.452412605285645, Accuracy = 0.45205479860305786\n",
      "Training iter #5379000:   Batch Loss = 10.342229, Accuracy = 0.4740000069141388\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.451013565063477, Accuracy = 0.4523525834083557\n",
      "Training iter #5382000:   Batch Loss = 10.837388, Accuracy = 0.3526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.449577331542969, Accuracy = 0.45265039801597595\n",
      "Training iter #5385000:   Batch Loss = 10.010398, Accuracy = 0.5993333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.448175430297852, Accuracy = 0.45324596762657166\n",
      "Training iter #5388000:   Batch Loss = 9.935235, Accuracy = 0.5693333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.446764945983887, Accuracy = 0.45265039801597595\n",
      "Training iter #5391000:   Batch Loss = 10.322481, Accuracy = 0.4546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.445316314697266, Accuracy = 0.45324596762657166\n",
      "Training iter #5394000:   Batch Loss = 10.543037, Accuracy = 0.4046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.443880081176758, Accuracy = 0.45413938164711\n",
      "Training iter #5397000:   Batch Loss = 10.510739, Accuracy = 0.41333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.442415237426758, Accuracy = 0.4550327658653259\n",
      "Training iter #5400000:   Batch Loss = 10.058092, Accuracy = 0.5419999957084656\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.440942764282227, Accuracy = 0.4553305506706238\n",
      "Training iter #5403000:   Batch Loss = 10.199033, Accuracy = 0.5073333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.43946361541748, Accuracy = 0.4550327658653259\n",
      "Training iter #5406000:   Batch Loss = 10.408974, Accuracy = 0.4586666524410248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.43808364868164, Accuracy = 0.45562833547592163\n",
      "Training iter #5409000:   Batch Loss = 10.815276, Accuracy = 0.3553333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.436647415161133, Accuracy = 0.4562239348888397\n",
      "Training iter #5412000:   Batch Loss = 9.975344, Accuracy = 0.6100000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.435195922851562, Accuracy = 0.45652174949645996\n",
      "Training iter #5415000:   Batch Loss = 10.007947, Accuracy = 0.5493333339691162\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.433746337890625, Accuracy = 0.4568195343017578\n",
      "Training iter #5418000:   Batch Loss = 10.278785, Accuracy = 0.4586666524410248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.432208061218262, Accuracy = 0.4568195343017578\n",
      "Training iter #5421000:   Batch Loss = 10.573606, Accuracy = 0.39533331990242004\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.430766105651855, Accuracy = 0.45652174949645996\n",
      "Training iter #5424000:   Batch Loss = 10.542413, Accuracy = 0.40400001406669617\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.429296493530273, Accuracy = 0.45771291851997375\n",
      "Training iter #5427000:   Batch Loss = 9.875646, Accuracy = 0.5946666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.427810668945312, Accuracy = 0.4586063027381897\n",
      "Training iter #5430000:   Batch Loss = 10.152323, Accuracy = 0.5173333287239075\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.426372528076172, Accuracy = 0.45890411734580994\n",
      "Training iter #5433000:   Batch Loss = 10.438828, Accuracy = 0.4519999921321869\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.424971580505371, Accuracy = 0.4606908857822418\n",
      "Training iter #5436000:   Batch Loss = 10.714647, Accuracy = 0.38066667318344116\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.423564910888672, Accuracy = 0.4612864851951599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #5439000:   Batch Loss = 9.997780, Accuracy = 0.6046666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.42212963104248, Accuracy = 0.4612864851951599\n",
      "Training iter #5442000:   Batch Loss = 10.121545, Accuracy = 0.5266666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.420669555664062, Accuracy = 0.46158427000045776\n",
      "Training iter #5445000:   Batch Loss = 10.146758, Accuracy = 0.49533334374427795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.419242858886719, Accuracy = 0.461882084608078\n",
      "Training iter #5448000:   Batch Loss = 10.575135, Accuracy = 0.39933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.417696952819824, Accuracy = 0.46217986941337585\n",
      "Training iter #5451000:   Batch Loss = 10.548903, Accuracy = 0.4180000126361847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.416168212890625, Accuracy = 0.4630732536315918\n",
      "Training iter #5454000:   Batch Loss = 9.862347, Accuracy = 0.6119999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.41464614868164, Accuracy = 0.46337106823921204\n",
      "Training iter #5457000:   Batch Loss = 10.165645, Accuracy = 0.5026666522026062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.413237571716309, Accuracy = 0.46337106823921204\n",
      "Training iter #5460000:   Batch Loss = 10.494638, Accuracy = 0.44066667556762695\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.411798477172852, Accuracy = 0.4630732536315918\n",
      "Training iter #5463000:   Batch Loss = 10.676533, Accuracy = 0.3799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.410402297973633, Accuracy = 0.4636688530445099\n",
      "Training iter #5466000:   Batch Loss = 9.993931, Accuracy = 0.6013333201408386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.408853530883789, Accuracy = 0.464264452457428\n",
      "Training iter #5469000:   Batch Loss = 10.090412, Accuracy = 0.5540000200271606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.406980514526367, Accuracy = 0.4648600220680237\n",
      "Training iter #5472000:   Batch Loss = 10.131166, Accuracy = 0.4986666738986969\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.405451774597168, Accuracy = 0.4648600220680237\n",
      "Training iter #5475000:   Batch Loss = 10.557384, Accuracy = 0.41999998688697815\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.403895378112793, Accuracy = 0.4654556214809418\n",
      "Training iter #5478000:   Batch Loss = 10.412388, Accuracy = 0.44600000977516174\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.402323722839355, Accuracy = 0.4663490056991577\n",
      "Training iter #5481000:   Batch Loss = 9.922135, Accuracy = 0.5633333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.400857925415039, Accuracy = 0.46605122089385986\n",
      "Training iter #5484000:   Batch Loss = 10.177821, Accuracy = 0.5080000162124634\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.399421691894531, Accuracy = 0.4663490056991577\n",
      "Training iter #5487000:   Batch Loss = 10.546202, Accuracy = 0.421999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.397891998291016, Accuracy = 0.4669446051120758\n",
      "Training iter #5490000:   Batch Loss = 10.636950, Accuracy = 0.382666677236557\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.396222114562988, Accuracy = 0.46783798933029175\n",
      "Training iter #5493000:   Batch Loss = 10.011209, Accuracy = 0.6046666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.394340515136719, Accuracy = 0.468135803937912\n",
      "Training iter #5496000:   Batch Loss = 10.076698, Accuracy = 0.5806666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.392724990844727, Accuracy = 0.46783798933029175\n",
      "Training iter #5499000:   Batch Loss = 10.086743, Accuracy = 0.5106666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.39138126373291, Accuracy = 0.46843358874320984\n",
      "Training iter #5502000:   Batch Loss = 10.529169, Accuracy = 0.42533332109451294\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.390302658081055, Accuracy = 0.4687313735485077\n",
      "Training iter #5505000:   Batch Loss = 10.338774, Accuracy = 0.4593333303928375\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.388824462890625, Accuracy = 0.4687313735485077\n",
      "Training iter #5508000:   Batch Loss = 9.885900, Accuracy = 0.578000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.387307167053223, Accuracy = 0.46902918815612793\n",
      "Training iter #5511000:   Batch Loss = 10.214231, Accuracy = 0.47999998927116394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.38571548461914, Accuracy = 0.469624787569046\n",
      "Training iter #5514000:   Batch Loss = 10.584484, Accuracy = 0.4126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.384170532226562, Accuracy = 0.4702203571796417\n",
      "Training iter #5517000:   Batch Loss = 10.579985, Accuracy = 0.38466668128967285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.382691383361816, Accuracy = 0.47051817178726196\n",
      "Training iter #5520000:   Batch Loss = 10.038763, Accuracy = 0.6013333201408386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.381267547607422, Accuracy = 0.4708159565925598\n",
      "Training iter #5523000:   Batch Loss = 10.109292, Accuracy = 0.5759999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.379837036132812, Accuracy = 0.47111377120018005\n",
      "Training iter #5526000:   Batch Loss = 10.051388, Accuracy = 0.5246666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.378339767456055, Accuracy = 0.4714115560054779\n",
      "Training iter #5529000:   Batch Loss = 10.557088, Accuracy = 0.41600000858306885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.376839637756348, Accuracy = 0.47170934081077576\n",
      "Training iter #5532000:   Batch Loss = 10.336062, Accuracy = 0.47066667675971985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.375368118286133, Accuracy = 0.4714115560054779\n",
      "Training iter #5535000:   Batch Loss = 9.866024, Accuracy = 0.5846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.373872756958008, Accuracy = 0.47170934081077576\n",
      "Training iter #5538000:   Batch Loss = 10.163906, Accuracy = 0.49533334374427795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.372411727905273, Accuracy = 0.4726027250289917\n",
      "Training iter #5541000:   Batch Loss = 10.547517, Accuracy = 0.4153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.370975494384766, Accuracy = 0.4726027250289917\n",
      "Training iter #5544000:   Batch Loss = 10.532529, Accuracy = 0.39666667580604553\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.369538307189941, Accuracy = 0.47290053963661194\n",
      "Training iter #5547000:   Batch Loss = 9.999043, Accuracy = 0.6053333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.367959976196289, Accuracy = 0.4731983244419098\n",
      "Training iter #5550000:   Batch Loss = 10.062741, Accuracy = 0.5820000171661377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.366336822509766, Accuracy = 0.47290053963661194\n",
      "Training iter #5553000:   Batch Loss = 9.966198, Accuracy = 0.5460000038146973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.364773750305176, Accuracy = 0.4731983244419098\n",
      "Training iter #5556000:   Batch Loss = 10.634254, Accuracy = 0.4033333361148834\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.36332893371582, Accuracy = 0.47349613904953003\n",
      "Training iter #5559000:   Batch Loss = 10.272597, Accuracy = 0.47200000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.361922264099121, Accuracy = 0.4737939238548279\n",
      "Training iter #5562000:   Batch Loss = 9.897879, Accuracy = 0.5753333568572998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.360481262207031, Accuracy = 0.47409170866012573\n",
      "Training iter #5565000:   Batch Loss = 10.150707, Accuracy = 0.4933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.359015464782715, Accuracy = 0.47409170866012573\n",
      "Training iter #5568000:   Batch Loss = 10.629418, Accuracy = 0.4020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.357582092285156, Accuracy = 0.47409170866012573\n",
      "Training iter #5571000:   Batch Loss = 10.502786, Accuracy = 0.4113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.35621452331543, Accuracy = 0.4746873080730438\n",
      "Training iter #5574000:   Batch Loss = 9.817105, Accuracy = 0.6673333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.354780197143555, Accuracy = 0.4746873080730438\n",
      "Training iter #5577000:   Batch Loss = 10.086797, Accuracy = 0.5680000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.353334426879883, Accuracy = 0.47498512268066406\n",
      "Training iter #5580000:   Batch Loss = 10.053406, Accuracy = 0.5320000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.351844787597656, Accuracy = 0.4752829074859619\n",
      "Training iter #5583000:   Batch Loss = 10.664445, Accuracy = 0.3933333456516266\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.350398063659668, Accuracy = 0.47587850689888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #5586000:   Batch Loss = 10.318029, Accuracy = 0.4713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.348921775817871, Accuracy = 0.4764741063117981\n",
      "Training iter #5589000:   Batch Loss = 9.784940, Accuracy = 0.6153333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.347423553466797, Accuracy = 0.47736749053001404\n",
      "Training iter #5592000:   Batch Loss = 10.134831, Accuracy = 0.49799999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.346185684204102, Accuracy = 0.4776652753353119\n",
      "Training iter #5595000:   Batch Loss = 10.612783, Accuracy = 0.4020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.344764709472656, Accuracy = 0.47796306014060974\n",
      "Training iter #5598000:   Batch Loss = 10.363322, Accuracy = 0.4480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.34338092803955, Accuracy = 0.47855865955352783\n",
      "Training iter #5601000:   Batch Loss = 9.876235, Accuracy = 0.625333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.341650009155273, Accuracy = 0.47885647416114807\n",
      "Training iter #5604000:   Batch Loss = 10.084071, Accuracy = 0.5413333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.340163230895996, Accuracy = 0.4791542589664459\n",
      "Training iter #5607000:   Batch Loss = 10.103441, Accuracy = 0.518666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.338689804077148, Accuracy = 0.47885647416114807\n",
      "Training iter #5610000:   Batch Loss = 10.701211, Accuracy = 0.3840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.33709716796875, Accuracy = 0.47855865955352783\n",
      "Training iter #5613000:   Batch Loss = 10.205585, Accuracy = 0.49933332204818726\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.335481643676758, Accuracy = 0.47885647416114807\n",
      "Training iter #5616000:   Batch Loss = 9.868984, Accuracy = 0.6006666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.33388900756836, Accuracy = 0.4794520437717438\n",
      "Training iter #5619000:   Batch Loss = 10.148639, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.332317352294922, Accuracy = 0.479749858379364\n",
      "Training iter #5622000:   Batch Loss = 10.588592, Accuracy = 0.40933331847190857\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.330527305603027, Accuracy = 0.48004764318466187\n",
      "Training iter #5625000:   Batch Loss = 10.345811, Accuracy = 0.4533333480358124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.328849792480469, Accuracy = 0.48123884201049805\n",
      "Training iter #5628000:   Batch Loss = 9.787924, Accuracy = 0.6579999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.327383041381836, Accuracy = 0.4815366268157959\n",
      "Training iter #5631000:   Batch Loss = 10.011824, Accuracy = 0.5580000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.325981140136719, Accuracy = 0.4815366268157959\n",
      "Training iter #5634000:   Batch Loss = 10.090205, Accuracy = 0.5173333287239075\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.324691772460938, Accuracy = 0.4815366268157959\n",
      "Training iter #5637000:   Batch Loss = 10.803945, Accuracy = 0.3540000021457672\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.323344230651855, Accuracy = 0.48183441162109375\n",
      "Training iter #5640000:   Batch Loss = 10.139647, Accuracy = 0.5246666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.321935653686523, Accuracy = 0.482132226228714\n",
      "Training iter #5643000:   Batch Loss = 9.800166, Accuracy = 0.6140000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.320528030395508, Accuracy = 0.48243001103401184\n",
      "Training iter #5646000:   Batch Loss = 10.200394, Accuracy = 0.5013333559036255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.319156646728516, Accuracy = 0.4827278256416321\n",
      "Training iter #5649000:   Batch Loss = 10.479294, Accuracy = 0.4326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.317780494689941, Accuracy = 0.48302561044692993\n",
      "Training iter #5652000:   Batch Loss = 10.296001, Accuracy = 0.4646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.31647777557373, Accuracy = 0.4833233952522278\n",
      "Training iter #5655000:   Batch Loss = 9.840338, Accuracy = 0.6439999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.31509780883789, Accuracy = 0.48302561044692993\n",
      "Training iter #5658000:   Batch Loss = 9.998919, Accuracy = 0.5566666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.313698768615723, Accuracy = 0.4839189946651459\n",
      "Training iter #5661000:   Batch Loss = 10.123133, Accuracy = 0.5120000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.312152862548828, Accuracy = 0.4839189946651459\n",
      "Training iter #5664000:   Batch Loss = 10.677525, Accuracy = 0.382666677236557\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.310466766357422, Accuracy = 0.4842168092727661\n",
      "Training iter #5667000:   Batch Loss = 10.025316, Accuracy = 0.5586666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.309069633483887, Accuracy = 0.4842168092727661\n",
      "Training iter #5670000:   Batch Loss = 9.826895, Accuracy = 0.6033333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.307720184326172, Accuracy = 0.4848123788833618\n",
      "Training iter #5673000:   Batch Loss = 10.173374, Accuracy = 0.512666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.306404113769531, Accuracy = 0.4854079782962799\n",
      "Training iter #5676000:   Batch Loss = 10.441511, Accuracy = 0.4386666715145111\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.305073738098145, Accuracy = 0.4854079782962799\n",
      "Training iter #5679000:   Batch Loss = 10.361860, Accuracy = 0.4586666524410248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.3037109375, Accuracy = 0.4854079782962799\n",
      "Training iter #5682000:   Batch Loss = 9.911065, Accuracy = 0.609333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.30235767364502, Accuracy = 0.48570576310157776\n",
      "Training iter #5685000:   Batch Loss = 9.987048, Accuracy = 0.5653333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.300969123840332, Accuracy = 0.48630136251449585\n",
      "Training iter #5688000:   Batch Loss = 10.182297, Accuracy = 0.5059999823570251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.299654960632324, Accuracy = 0.4865991771221161\n",
      "Training iter #5691000:   Batch Loss = 10.696989, Accuracy = 0.367333322763443\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.298184394836426, Accuracy = 0.4865991771221161\n",
      "Training iter #5694000:   Batch Loss = 9.911570, Accuracy = 0.6053333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.296638488769531, Accuracy = 0.4871947467327118\n",
      "Training iter #5697000:   Batch Loss = 9.805894, Accuracy = 0.6273333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.295133590698242, Accuracy = 0.48749256134033203\n",
      "Training iter #5700000:   Batch Loss = 10.164817, Accuracy = 0.4893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.293680191040039, Accuracy = 0.48749256134033203\n",
      "Training iter #5703000:   Batch Loss = 10.424698, Accuracy = 0.43066665530204773\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.292187690734863, Accuracy = 0.4886837303638458\n",
      "Training iter #5706000:   Batch Loss = 10.321438, Accuracy = 0.468666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.290725708007812, Accuracy = 0.48898154497146606\n",
      "Training iter #5709000:   Batch Loss = 9.913175, Accuracy = 0.5899999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.289297103881836, Accuracy = 0.48898154497146606\n",
      "Training iter #5712000:   Batch Loss = 10.041848, Accuracy = 0.5519999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.287899017333984, Accuracy = 0.48898154497146606\n",
      "Training iter #5715000:   Batch Loss = 10.216393, Accuracy = 0.5040000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.286663055419922, Accuracy = 0.4892793297767639\n",
      "Training iter #5718000:   Batch Loss = 10.669954, Accuracy = 0.37533333897590637\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.285497665405273, Accuracy = 0.489874929189682\n",
      "Training iter #5721000:   Batch Loss = 9.803439, Accuracy = 0.6466666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.2842435836792, Accuracy = 0.49017271399497986\n",
      "Training iter #5724000:   Batch Loss = 9.789491, Accuracy = 0.6473333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.282960891723633, Accuracy = 0.49017271399497986\n",
      "Training iter #5727000:   Batch Loss = 10.147881, Accuracy = 0.48533332347869873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.281661033630371, Accuracy = 0.489874929189682\n",
      "Training iter #5730000:   Batch Loss = 10.402288, Accuracy = 0.43533334136009216\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.279848098754883, Accuracy = 0.4904705286026001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #5733000:   Batch Loss = 10.339872, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.278349876403809, Accuracy = 0.4904705286026001\n",
      "Training iter #5736000:   Batch Loss = 9.810305, Accuracy = 0.6259999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.27699089050293, Accuracy = 0.49136391282081604\n",
      "Training iter #5739000:   Batch Loss = 10.038657, Accuracy = 0.54666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.275613784790039, Accuracy = 0.49136391282081604\n",
      "Training iter #5742000:   Batch Loss = 10.279427, Accuracy = 0.4959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.274245262145996, Accuracy = 0.4916616976261139\n",
      "Training iter #5745000:   Batch Loss = 10.621330, Accuracy = 0.3893333375453949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.272807121276855, Accuracy = 0.49195951223373413\n",
      "Training iter #5748000:   Batch Loss = 9.810913, Accuracy = 0.6506666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.27138900756836, Accuracy = 0.4928528964519501\n",
      "Training iter #5751000:   Batch Loss = 9.915915, Accuracy = 0.6133333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.270030975341797, Accuracy = 0.4928528964519501\n",
      "Training iter #5754000:   Batch Loss = 10.055658, Accuracy = 0.5133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.268726348876953, Accuracy = 0.4931506812572479\n",
      "Training iter #5757000:   Batch Loss = 10.434155, Accuracy = 0.4206666648387909\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.267315864562988, Accuracy = 0.4931506812572479\n",
      "Training iter #5760000:   Batch Loss = 10.379171, Accuracy = 0.46133333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.266012191772461, Accuracy = 0.49344849586486816\n",
      "Training iter #5763000:   Batch Loss = 9.693297, Accuracy = 0.6520000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.264741897583008, Accuracy = 0.493746280670166\n",
      "Training iter #5766000:   Batch Loss = 9.995037, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.26342487335205, Accuracy = 0.49404406547546387\n",
      "Training iter #5769000:   Batch Loss = 10.312176, Accuracy = 0.492000013589859\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.262072563171387, Accuracy = 0.49463966488838196\n",
      "Training iter #5772000:   Batch Loss = 10.514383, Accuracy = 0.414000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.260698318481445, Accuracy = 0.4949374496936798\n",
      "Training iter #5775000:   Batch Loss = 9.814981, Accuracy = 0.6386666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.259329795837402, Accuracy = 0.4949374496936798\n",
      "Training iter #5778000:   Batch Loss = 9.946241, Accuracy = 0.6073333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.257981300354004, Accuracy = 0.4955330491065979\n",
      "Training iter #5781000:   Batch Loss = 9.948660, Accuracy = 0.5366666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.256603240966797, Accuracy = 0.49583086371421814\n",
      "Training iter #5784000:   Batch Loss = 10.393247, Accuracy = 0.43933331966400146\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.255168914794922, Accuracy = 0.4967242479324341\n",
      "Training iter #5787000:   Batch Loss = 10.331269, Accuracy = 0.47733333706855774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.253744125366211, Accuracy = 0.49642643332481384\n",
      "Training iter #5790000:   Batch Loss = 9.759974, Accuracy = 0.6346666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.252357482910156, Accuracy = 0.4973198473453522\n",
      "Training iter #5793000:   Batch Loss = 9.980139, Accuracy = 0.5460000038146973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.251029014587402, Accuracy = 0.4973198473453522\n",
      "Training iter #5796000:   Batch Loss = 10.324113, Accuracy = 0.47466665506362915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.249699592590332, Accuracy = 0.49851101636886597\n",
      "Training iter #5799000:   Batch Loss = 10.538942, Accuracy = 0.39666667580604553\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.248514175415039, Accuracy = 0.49910661578178406\n",
      "Training iter #5802000:   Batch Loss = 9.819900, Accuracy = 0.6433333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.247255325317383, Accuracy = 0.5\n",
      "Training iter #5805000:   Batch Loss = 9.920710, Accuracy = 0.6086666584014893\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.24598503112793, Accuracy = 0.5002977848052979\n",
      "Training iter #5808000:   Batch Loss = 9.928486, Accuracy = 0.5419999957084656\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.24466609954834, Accuracy = 0.49970221519470215\n",
      "Training iter #5811000:   Batch Loss = 10.388819, Accuracy = 0.445333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.243316650390625, Accuracy = 0.5005955696105957\n",
      "Training iter #5814000:   Batch Loss = 10.180054, Accuracy = 0.515333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.24205207824707, Accuracy = 0.5008934140205383\n",
      "Training iter #5817000:   Batch Loss = 9.730561, Accuracy = 0.6439999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.24084186553955, Accuracy = 0.501488983631134\n",
      "Training iter #5820000:   Batch Loss = 10.049561, Accuracy = 0.5320000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.239569664001465, Accuracy = 0.501488983631134\n",
      "Training iter #5823000:   Batch Loss = 10.375745, Accuracy = 0.4546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.23824405670166, Accuracy = 0.5017867684364319\n",
      "Training iter #5826000:   Batch Loss = 10.478328, Accuracy = 0.41600000858306885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.236898422241211, Accuracy = 0.5032757520675659\n",
      "Training iter #5829000:   Batch Loss = 9.808359, Accuracy = 0.6493333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.23556900024414, Accuracy = 0.5032757520675659\n",
      "Training iter #5832000:   Batch Loss = 9.942993, Accuracy = 0.6053333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.23420524597168, Accuracy = 0.5035735368728638\n",
      "Training iter #5835000:   Batch Loss = 9.871425, Accuracy = 0.5619999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.23287582397461, Accuracy = 0.5035735368728638\n",
      "Training iter #5838000:   Batch Loss = 10.392971, Accuracy = 0.445333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.231398582458496, Accuracy = 0.5035735368728638\n",
      "Training iter #5841000:   Batch Loss = 10.151058, Accuracy = 0.531333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.229927062988281, Accuracy = 0.5038713812828064\n",
      "Training iter #5844000:   Batch Loss = 9.674290, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.228528022766113, Accuracy = 0.5044669508934021\n",
      "Training iter #5847000:   Batch Loss = 10.035071, Accuracy = 0.5239999890327454\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.227203369140625, Accuracy = 0.5041691660881042\n",
      "Training iter #5850000:   Batch Loss = 10.406645, Accuracy = 0.4493333399295807\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.225810050964355, Accuracy = 0.5050625205039978\n",
      "Training iter #5853000:   Batch Loss = 10.407386, Accuracy = 0.4320000112056732\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.224383354187012, Accuracy = 0.5050625205039978\n",
      "Training iter #5856000:   Batch Loss = 9.836852, Accuracy = 0.6306666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.223156929016113, Accuracy = 0.5050625205039978\n",
      "Training iter #5859000:   Batch Loss = 9.930784, Accuracy = 0.6066666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.221940994262695, Accuracy = 0.5053603053092957\n",
      "Training iter #5862000:   Batch Loss = 9.867057, Accuracy = 0.5633333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.2206449508667, Accuracy = 0.5056581497192383\n",
      "Training iter #5865000:   Batch Loss = 10.428258, Accuracy = 0.4399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.219189643859863, Accuracy = 0.5053603053092957\n",
      "Training iter #5868000:   Batch Loss = 10.109209, Accuracy = 0.5379999876022339\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.217768669128418, Accuracy = 0.506253719329834\n",
      "Training iter #5871000:   Batch Loss = 9.732122, Accuracy = 0.6520000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.216383934020996, Accuracy = 0.5068492889404297\n",
      "Training iter #5874000:   Batch Loss = 10.034194, Accuracy = 0.527999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.214984893798828, Accuracy = 0.5068492889404297\n",
      "Training iter #5877000:   Batch Loss = 10.414159, Accuracy = 0.44066667556762695\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.213520050048828, Accuracy = 0.5071471333503723\n",
      "Training iter #5880000:   Batch Loss = 10.398434, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.212100982666016, Accuracy = 0.5071471333503723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #5883000:   Batch Loss = 9.765666, Accuracy = 0.656000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.210722923278809, Accuracy = 0.5068492889404297\n",
      "Training iter #5886000:   Batch Loss = 9.879659, Accuracy = 0.6140000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.209391593933105, Accuracy = 0.507742702960968\n",
      "Training iter #5889000:   Batch Loss = 9.836978, Accuracy = 0.5759999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.208060264587402, Accuracy = 0.5086361169815063\n",
      "Training iter #5892000:   Batch Loss = 10.461521, Accuracy = 0.4320000112056732\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.206652641296387, Accuracy = 0.5095294713973999\n",
      "Training iter #5895000:   Batch Loss = 10.157158, Accuracy = 0.5013333559036255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.20521354675293, Accuracy = 0.5110184550285339\n",
      "Training iter #5898000:   Batch Loss = 9.720799, Accuracy = 0.6439999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.20382308959961, Accuracy = 0.5116140842437744\n",
      "Training iter #5901000:   Batch Loss = 9.977251, Accuracy = 0.5419999957084656\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.202454566955566, Accuracy = 0.5116140842437744\n",
      "Training iter #5904000:   Batch Loss = 10.428948, Accuracy = 0.44333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.201019287109375, Accuracy = 0.5119118690490723\n",
      "Training iter #5907000:   Batch Loss = 10.296560, Accuracy = 0.46933332085609436\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.199628829956055, Accuracy = 0.5122096538543701\n",
      "Training iter #5910000:   Batch Loss = 9.705330, Accuracy = 0.6740000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.198259353637695, Accuracy = 0.512507438659668\n",
      "Training iter #5913000:   Batch Loss = 9.917009, Accuracy = 0.596666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.196924209594727, Accuracy = 0.512507438659668\n",
      "Training iter #5916000:   Batch Loss = 9.913415, Accuracy = 0.5613333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.195564270019531, Accuracy = 0.5131030082702637\n",
      "Training iter #5919000:   Batch Loss = 10.546243, Accuracy = 0.4073333442211151\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.194132804870605, Accuracy = 0.5131030082702637\n",
      "Training iter #5922000:   Batch Loss = 10.079663, Accuracy = 0.531333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.192754745483398, Accuracy = 0.5134008526802063\n",
      "Training iter #5925000:   Batch Loss = 9.625407, Accuracy = 0.6793333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.191411972045898, Accuracy = 0.5134008526802063\n",
      "Training iter #5928000:   Batch Loss = 9.940251, Accuracy = 0.5566666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.190164566040039, Accuracy = 0.513996422290802\n",
      "Training iter #5931000:   Batch Loss = 10.426714, Accuracy = 0.44200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.188899993896484, Accuracy = 0.5145919919013977\n",
      "Training iter #5934000:   Batch Loss = 10.236642, Accuracy = 0.49000000953674316\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.187559127807617, Accuracy = 0.513996422290802\n",
      "Training iter #5937000:   Batch Loss = 9.688509, Accuracy = 0.6793333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.18620491027832, Accuracy = 0.5148898363113403\n",
      "Training iter #5940000:   Batch Loss = 9.869729, Accuracy = 0.5946666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.184906005859375, Accuracy = 0.5151876211166382\n",
      "Training iter #5943000:   Batch Loss = 9.922806, Accuracy = 0.5573333501815796\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.183634757995605, Accuracy = 0.5157831907272339\n",
      "Training iter #5946000:   Batch Loss = 10.590850, Accuracy = 0.3919999897480011\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.182296752929688, Accuracy = 0.5157831907272339\n",
      "Training iter #5949000:   Batch Loss = 10.001896, Accuracy = 0.5519999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.180895805358887, Accuracy = 0.5157831907272339\n",
      "Training iter #5952000:   Batch Loss = 9.733057, Accuracy = 0.6466666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.179521560668945, Accuracy = 0.5163788199424744\n",
      "Training iter #5955000:   Batch Loss = 10.034665, Accuracy = 0.5326666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.178248405456543, Accuracy = 0.5169743895530701\n",
      "Training iter #5958000:   Batch Loss = 10.367432, Accuracy = 0.45533332228660583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.176961898803711, Accuracy = 0.5172721743583679\n",
      "Training iter #5961000:   Batch Loss = 10.133636, Accuracy = 0.5173333287239075\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.175628662109375, Accuracy = 0.5181655883789062\n",
      "Training iter #5964000:   Batch Loss = 9.663403, Accuracy = 0.6866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.174227714538574, Accuracy = 0.5181655883789062\n",
      "Training iter #5967000:   Batch Loss = 9.833168, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.172822952270508, Accuracy = 0.5184633731842041\n",
      "Training iter #5970000:   Batch Loss = 9.933971, Accuracy = 0.5460000038146973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.17143440246582, Accuracy = 0.518761157989502\n",
      "Training iter #5973000:   Batch Loss = 10.631584, Accuracy = 0.38466668128967285\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.170049667358398, Accuracy = 0.5190589427947998\n",
      "Training iter #5976000:   Batch Loss = 9.936182, Accuracy = 0.5746666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.168743133544922, Accuracy = 0.5193567872047424\n",
      "Training iter #5979000:   Batch Loss = 9.680474, Accuracy = 0.6539999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.167490005493164, Accuracy = 0.520250141620636\n",
      "Training iter #5982000:   Batch Loss = 10.016684, Accuracy = 0.5413333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.166239738464355, Accuracy = 0.5208457708358765\n",
      "Training iter #5985000:   Batch Loss = 10.361294, Accuracy = 0.4546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.164911270141602, Accuracy = 0.5208457708358765\n",
      "Training iter #5988000:   Batch Loss = 10.208939, Accuracy = 0.4946666657924652\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.16354751586914, Accuracy = 0.5211435556411743\n",
      "Training iter #5991000:   Batch Loss = 9.697826, Accuracy = 0.6773333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.162223815917969, Accuracy = 0.5211435556411743\n",
      "Training iter #5994000:   Batch Loss = 9.839365, Accuracy = 0.5886666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.160907745361328, Accuracy = 0.5214413404464722\n",
      "Training iter #5997000:   Batch Loss = 10.031042, Accuracy = 0.5253333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.159616470336914, Accuracy = 0.5214413404464722\n",
      "Training iter #6000000:   Batch Loss = 10.503514, Accuracy = 0.40933331847190857\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.158235549926758, Accuracy = 0.52173912525177\n",
      "Training iter #6003000:   Batch Loss = 9.825923, Accuracy = 0.6060000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.156864166259766, Accuracy = 0.5223346948623657\n",
      "Training iter #6006000:   Batch Loss = 9.651093, Accuracy = 0.6620000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.155555725097656, Accuracy = 0.5229303240776062\n",
      "Training iter #6009000:   Batch Loss = 10.016042, Accuracy = 0.5460000038146973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.154223442077637, Accuracy = 0.5226325392723083\n",
      "Training iter #6012000:   Batch Loss = 10.273425, Accuracy = 0.47333332896232605\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.152826309204102, Accuracy = 0.5235258936882019\n",
      "Training iter #6015000:   Batch Loss = 10.212874, Accuracy = 0.49399998784065247\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.151446342468262, Accuracy = 0.523228108882904\n",
      "Training iter #6018000:   Batch Loss = 9.748045, Accuracy = 0.6366666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.150089263916016, Accuracy = 0.5229303240776062\n",
      "Training iter #6021000:   Batch Loss = 9.870977, Accuracy = 0.596666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.148733139038086, Accuracy = 0.5235258936882019\n",
      "Training iter #6024000:   Batch Loss = 10.016150, Accuracy = 0.5446666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.147363662719727, Accuracy = 0.5235258936882019\n",
      "Training iter #6027000:   Batch Loss = 10.525384, Accuracy = 0.40066665410995483\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.145967483520508, Accuracy = 0.5235258936882019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6030000:   Batch Loss = 9.720006, Accuracy = 0.6506666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.144577026367188, Accuracy = 0.5238236784934998\n",
      "Training iter #6033000:   Batch Loss = 9.646146, Accuracy = 0.6639999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.143256187438965, Accuracy = 0.5241215229034424\n",
      "Training iter #6036000:   Batch Loss = 10.002957, Accuracy = 0.531333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.142049789428711, Accuracy = 0.5244193077087402\n",
      "Training iter #6039000:   Batch Loss = 10.254416, Accuracy = 0.4753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.140795707702637, Accuracy = 0.5253126621246338\n",
      "Training iter #6042000:   Batch Loss = 10.215198, Accuracy = 0.5006666779518127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.139446258544922, Accuracy = 0.5253126621246338\n",
      "Training iter #6045000:   Batch Loss = 9.761850, Accuracy = 0.624666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.138079643249512, Accuracy = 0.52650386095047\n",
      "Training iter #6048000:   Batch Loss = 9.893465, Accuracy = 0.5913333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.136787414550781, Accuracy = 0.52650386095047\n",
      "Training iter #6051000:   Batch Loss = 10.070257, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.13542652130127, Accuracy = 0.5268016457557678\n",
      "Training iter #6054000:   Batch Loss = 10.525700, Accuracy = 0.40666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.133966445922852, Accuracy = 0.5276950597763062\n",
      "Training iter #6057000:   Batch Loss = 9.621170, Accuracy = 0.6706666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.132431030273438, Accuracy = 0.5288862586021423\n",
      "Training iter #6060000:   Batch Loss = 9.674992, Accuracy = 0.6626666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.131006240844727, Accuracy = 0.5291840434074402\n",
      "Training iter #6063000:   Batch Loss = 9.953718, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.129549980163574, Accuracy = 0.529481828212738\n",
      "Training iter #6066000:   Batch Loss = 10.259498, Accuracy = 0.4699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.128040313720703, Accuracy = 0.5303752422332764\n",
      "Training iter #6069000:   Batch Loss = 10.236797, Accuracy = 0.48399999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.126673698425293, Accuracy = 0.5300773978233337\n",
      "Training iter #6072000:   Batch Loss = 9.572863, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.125481605529785, Accuracy = 0.5303752422332764\n",
      "Training iter #6075000:   Batch Loss = 9.829877, Accuracy = 0.593999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.124308586120605, Accuracy = 0.5303752422332764\n",
      "Training iter #6078000:   Batch Loss = 10.127589, Accuracy = 0.5199999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.123054504394531, Accuracy = 0.5309708118438721\n",
      "Training iter #6081000:   Batch Loss = 10.425956, Accuracy = 0.4413333237171173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.121723175048828, Accuracy = 0.5312685966491699\n",
      "Training iter #6084000:   Batch Loss = 9.666491, Accuracy = 0.668666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.120380401611328, Accuracy = 0.5315663814544678\n",
      "Training iter #6087000:   Batch Loss = 9.790147, Accuracy = 0.6213333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.119222640991211, Accuracy = 0.5321620106697083\n",
      "Training iter #6090000:   Batch Loss = 9.827660, Accuracy = 0.5659999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.117979049682617, Accuracy = 0.532757580280304\n",
      "Training iter #6093000:   Batch Loss = 10.295415, Accuracy = 0.4586666524410248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.116668701171875, Accuracy = 0.532757580280304\n",
      "Training iter #6096000:   Batch Loss = 10.275162, Accuracy = 0.4880000054836273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.115358352661133, Accuracy = 0.5330553650856018\n",
      "Training iter #6099000:   Batch Loss = 9.529845, Accuracy = 0.690666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.114166259765625, Accuracy = 0.5321620106697083\n",
      "Training iter #6102000:   Batch Loss = 9.866464, Accuracy = 0.5826666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.112982749938965, Accuracy = 0.5324597954750061\n",
      "Training iter #6105000:   Batch Loss = 10.174597, Accuracy = 0.5099999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.111759185791016, Accuracy = 0.5321620106697083\n",
      "Training iter #6108000:   Batch Loss = 10.347311, Accuracy = 0.45133334398269653\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.110433578491211, Accuracy = 0.5324597954750061\n",
      "Training iter #6111000:   Batch Loss = 9.668187, Accuracy = 0.6613333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.109079360961914, Accuracy = 0.5330553650856018\n",
      "Training iter #6114000:   Batch Loss = 9.744373, Accuracy = 0.640666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.107751846313477, Accuracy = 0.5336509943008423\n",
      "Training iter #6117000:   Batch Loss = 9.831096, Accuracy = 0.5680000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.106413841247559, Accuracy = 0.5345443487167358\n",
      "Training iter #6120000:   Batch Loss = 10.235735, Accuracy = 0.4893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.105043411254883, Accuracy = 0.5345443487167358\n",
      "Training iter #6123000:   Batch Loss = 10.136606, Accuracy = 0.5193333625793457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.103605270385742, Accuracy = 0.5345443487167358\n",
      "Training iter #6126000:   Batch Loss = 9.625013, Accuracy = 0.6673333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.102226257324219, Accuracy = 0.5351399779319763\n",
      "Training iter #6129000:   Batch Loss = 9.839022, Accuracy = 0.5899999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.100542068481445, Accuracy = 0.5363311767578125\n",
      "Training iter #6132000:   Batch Loss = 10.203528, Accuracy = 0.4973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.099050521850586, Accuracy = 0.5360333323478699\n",
      "Training iter #6135000:   Batch Loss = 10.360279, Accuracy = 0.44200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.097745895385742, Accuracy = 0.5366289615631104\n",
      "Training iter #6138000:   Batch Loss = 9.704222, Accuracy = 0.6506666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.096426010131836, Accuracy = 0.537224531173706\n",
      "Training iter #6141000:   Batch Loss = 9.765718, Accuracy = 0.6320000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.095169067382812, Accuracy = 0.537224531173706\n",
      "Training iter #6144000:   Batch Loss = 9.762645, Accuracy = 0.581333339214325\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.093826293945312, Accuracy = 0.5384157299995422\n",
      "Training iter #6147000:   Batch Loss = 10.236256, Accuracy = 0.4906666576862335\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.092445373535156, Accuracy = 0.5390112996101379\n",
      "Training iter #6150000:   Batch Loss = 10.066992, Accuracy = 0.5366666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.091161727905273, Accuracy = 0.5390112996101379\n",
      "Training iter #6153000:   Batch Loss = 9.578526, Accuracy = 0.6793333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.08991527557373, Accuracy = 0.5396069288253784\n",
      "Training iter #6156000:   Batch Loss = 9.921322, Accuracy = 0.5619999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.088765144348145, Accuracy = 0.5402024984359741\n",
      "Training iter #6159000:   Batch Loss = 10.231474, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.087502479553223, Accuracy = 0.5407980680465698\n",
      "Training iter #6162000:   Batch Loss = 10.305923, Accuracy = 0.45133334398269653\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.086263656616211, Accuracy = 0.540500283241272\n",
      "Training iter #6165000:   Batch Loss = 9.675295, Accuracy = 0.659333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.084968566894531, Accuracy = 0.540500283241272\n",
      "Training iter #6168000:   Batch Loss = 9.793216, Accuracy = 0.6273333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.08365535736084, Accuracy = 0.5407980680465698\n",
      "Training iter #6171000:   Batch Loss = 9.737221, Accuracy = 0.5866666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.082271575927734, Accuracy = 0.5416914820671082\n",
      "Training iter #6174000:   Batch Loss = 10.242596, Accuracy = 0.48533332347869873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.0809326171875, Accuracy = 0.5422870516777039\n",
      "Training iter #6177000:   Batch Loss = 10.026841, Accuracy = 0.5559999942779541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.079610824584961, Accuracy = 0.541989266872406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6180000:   Batch Loss = 9.548181, Accuracy = 0.6880000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.078325271606445, Accuracy = 0.541989266872406\n",
      "Training iter #6183000:   Batch Loss = 9.857061, Accuracy = 0.5826666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.07689380645752, Accuracy = 0.5431804656982422\n",
      "Training iter #6186000:   Batch Loss = 10.235142, Accuracy = 0.4819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.075481414794922, Accuracy = 0.54347825050354\n",
      "Training iter #6189000:   Batch Loss = 10.245955, Accuracy = 0.4713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.074094772338867, Accuracy = 0.54347825050354\n",
      "Training iter #6192000:   Batch Loss = 9.708597, Accuracy = 0.6546666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.072721481323242, Accuracy = 0.54347825050354\n",
      "Training iter #6195000:   Batch Loss = 9.745924, Accuracy = 0.6426666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.071355819702148, Accuracy = 0.54347825050354\n",
      "Training iter #6198000:   Batch Loss = 9.668306, Accuracy = 0.6113333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.069961547851562, Accuracy = 0.5443716645240784\n",
      "Training iter #6201000:   Batch Loss = 10.323948, Accuracy = 0.46266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.068595886230469, Accuracy = 0.5440738797187805\n",
      "Training iter #6204000:   Batch Loss = 9.945826, Accuracy = 0.5566666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.067302703857422, Accuracy = 0.5440738797187805\n",
      "Training iter #6207000:   Batch Loss = 9.603815, Accuracy = 0.6706666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.06608772277832, Accuracy = 0.5440738797187805\n",
      "Training iter #6210000:   Batch Loss = 9.879845, Accuracy = 0.5706666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.064823150634766, Accuracy = 0.5443716645240784\n",
      "Training iter #6213000:   Batch Loss = 10.295167, Accuracy = 0.4620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.063470840454102, Accuracy = 0.5455628633499146\n",
      "Training iter #6216000:   Batch Loss = 10.230785, Accuracy = 0.4860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.062299728393555, Accuracy = 0.5455628633499146\n",
      "Training iter #6219000:   Batch Loss = 9.532055, Accuracy = 0.7266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.06110668182373, Accuracy = 0.5455628633499146\n",
      "Training iter #6222000:   Batch Loss = 9.764430, Accuracy = 0.6326666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.059839248657227, Accuracy = 0.5458606481552124\n",
      "Training iter #6225000:   Batch Loss = 9.741045, Accuracy = 0.593999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.058517456054688, Accuracy = 0.5461584329605103\n",
      "Training iter #6228000:   Batch Loss = 10.334164, Accuracy = 0.4606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.057168006896973, Accuracy = 0.5464562177658081\n",
      "Training iter #6231000:   Batch Loss = 9.991835, Accuracy = 0.5473333597183228\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.055744171142578, Accuracy = 0.546754002571106\n",
      "Training iter #6234000:   Batch Loss = 9.522364, Accuracy = 0.6919999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.054315567016602, Accuracy = 0.546754002571106\n",
      "Training iter #6237000:   Batch Loss = 9.831238, Accuracy = 0.5839999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.052674293518066, Accuracy = 0.5476474165916443\n",
      "Training iter #6240000:   Batch Loss = 10.280333, Accuracy = 0.4646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.051342010498047, Accuracy = 0.5476474165916443\n",
      "Training iter #6243000:   Batch Loss = 10.119136, Accuracy = 0.5133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.049858093261719, Accuracy = 0.5476474165916443\n",
      "Training iter #6246000:   Batch Loss = 9.570686, Accuracy = 0.7053333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.048112869262695, Accuracy = 0.5488386154174805\n",
      "Training iter #6249000:   Batch Loss = 9.797791, Accuracy = 0.6100000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.0465087890625, Accuracy = 0.5491364002227783\n",
      "Training iter #6252000:   Batch Loss = 9.802453, Accuracy = 0.5740000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.045248985290527, Accuracy = 0.5488386154174805\n",
      "Training iter #6255000:   Batch Loss = 10.378842, Accuracy = 0.4440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.043859481811523, Accuracy = 0.5488386154174805\n",
      "Training iter #6258000:   Batch Loss = 9.897081, Accuracy = 0.5820000171661377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.04218864440918, Accuracy = 0.5488386154174805\n",
      "Training iter #6261000:   Batch Loss = 9.560021, Accuracy = 0.6846666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.040855407714844, Accuracy = 0.5491364002227783\n",
      "Training iter #6264000:   Batch Loss = 9.824030, Accuracy = 0.5920000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.039656639099121, Accuracy = 0.5503275990486145\n",
      "Training iter #6267000:   Batch Loss = 10.261404, Accuracy = 0.47200000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.038472175598145, Accuracy = 0.5500297546386719\n",
      "Training iter #6270000:   Batch Loss = 10.082304, Accuracy = 0.5266666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.037432670593262, Accuracy = 0.5503275990486145\n",
      "Training iter #6273000:   Batch Loss = 9.497026, Accuracy = 0.7360000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.036426544189453, Accuracy = 0.5506253838539124\n",
      "Training iter #6276000:   Batch Loss = 9.699340, Accuracy = 0.6353333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.035269737243652, Accuracy = 0.5509231686592102\n",
      "Training iter #6279000:   Batch Loss = 9.791675, Accuracy = 0.5806666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.03413200378418, Accuracy = 0.5515187382698059\n",
      "Training iter #6282000:   Batch Loss = 10.486359, Accuracy = 0.4126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.033015251159668, Accuracy = 0.5518165826797485\n",
      "Training iter #6285000:   Batch Loss = 9.818865, Accuracy = 0.6013333201408386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.031730651855469, Accuracy = 0.5521143674850464\n",
      "Training iter #6288000:   Batch Loss = 9.536743, Accuracy = 0.6893333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.030531883239746, Accuracy = 0.5524121522903442\n",
      "Training iter #6291000:   Batch Loss = 9.921064, Accuracy = 0.5746666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.029304504394531, Accuracy = 0.5533055663108826\n",
      "Training iter #6294000:   Batch Loss = 10.197973, Accuracy = 0.49133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.02804183959961, Accuracy = 0.5530077219009399\n",
      "Training iter #6297000:   Batch Loss = 9.999348, Accuracy = 0.5506666898727417\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.026765823364258, Accuracy = 0.5533055663108826\n",
      "Training iter #6300000:   Batch Loss = 9.543824, Accuracy = 0.7173333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.025640487670898, Accuracy = 0.5533055663108826\n",
      "Training iter #6303000:   Batch Loss = 9.708456, Accuracy = 0.6313333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.024506568908691, Accuracy = 0.5536033511161804\n",
      "Training iter #6306000:   Batch Loss = 9.826166, Accuracy = 0.5733333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.023346900939941, Accuracy = 0.5541989207267761\n",
      "Training iter #6309000:   Batch Loss = 10.409683, Accuracy = 0.42533332109451294\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.021993637084961, Accuracy = 0.5541989207267761\n",
      "Training iter #6312000:   Batch Loss = 9.718257, Accuracy = 0.628000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.020702362060547, Accuracy = 0.5547945499420166\n",
      "Training iter #6315000:   Batch Loss = 9.540273, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.019245147705078, Accuracy = 0.5553901195526123\n",
      "Training iter #6318000:   Batch Loss = 9.877628, Accuracy = 0.5846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.017928123474121, Accuracy = 0.5556879043579102\n",
      "Training iter #6321000:   Batch Loss = 10.165439, Accuracy = 0.5026666522026062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.016767501831055, Accuracy = 0.5583680868148804\n",
      "Training iter #6324000:   Batch Loss = 10.082639, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.015555381774902, Accuracy = 0.5583680868148804\n",
      "Training iter #6327000:   Batch Loss = 9.598863, Accuracy = 0.6966666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.01437759399414, Accuracy = 0.5580703020095825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6330000:   Batch Loss = 9.696197, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.013157844543457, Accuracy = 0.5586658716201782\n",
      "Training iter #6333000:   Batch Loss = 9.890779, Accuracy = 0.5586666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.011940956115723, Accuracy = 0.5589636564254761\n",
      "Training iter #6336000:   Batch Loss = 10.400639, Accuracy = 0.42933332920074463\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.010661125183105, Accuracy = 0.5586658716201782\n",
      "Training iter #6339000:   Batch Loss = 9.635545, Accuracy = 0.6600000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.009368896484375, Accuracy = 0.5583680868148804\n",
      "Training iter #6342000:   Batch Loss = 9.492961, Accuracy = 0.7080000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.008094787597656, Accuracy = 0.5592614412307739\n",
      "Training iter #6345000:   Batch Loss = 9.872824, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.006781578063965, Accuracy = 0.5601548552513123\n",
      "Training iter #6348000:   Batch Loss = 10.132059, Accuracy = 0.5006666779518127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.005457878112793, Accuracy = 0.5610482692718506\n",
      "Training iter #6351000:   Batch Loss = 10.057567, Accuracy = 0.5400000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.004179954528809, Accuracy = 0.5619416236877441\n",
      "Training iter #6354000:   Batch Loss = 9.597607, Accuracy = 0.6853333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.003059387207031, Accuracy = 0.5616438388824463\n",
      "Training iter #6357000:   Batch Loss = 9.721420, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.00192642211914, Accuracy = 0.5619416236877441\n",
      "Training iter #6360000:   Batch Loss = 9.892381, Accuracy = 0.5653333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 10.000723838806152, Accuracy = 0.562239408493042\n",
      "Training iter #6363000:   Batch Loss = 10.367209, Accuracy = 0.437333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.999357223510742, Accuracy = 0.5634306073188782\n",
      "Training iter #6366000:   Batch Loss = 9.509093, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.998154640197754, Accuracy = 0.5634306073188782\n",
      "Training iter #6369000:   Batch Loss = 9.474277, Accuracy = 0.7059999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.996984481811523, Accuracy = 0.5640261769294739\n",
      "Training iter #6372000:   Batch Loss = 9.864998, Accuracy = 0.5706666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.995718002319336, Accuracy = 0.5640261769294739\n",
      "Training iter #6375000:   Batch Loss = 10.089391, Accuracy = 0.5133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.994348526000977, Accuracy = 0.5640261769294739\n",
      "Training iter #6378000:   Batch Loss = 10.048716, Accuracy = 0.5446666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.992937088012695, Accuracy = 0.5640261769294739\n",
      "Training iter #6381000:   Batch Loss = 9.568328, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.99156379699707, Accuracy = 0.5649195909500122\n",
      "Training iter #6384000:   Batch Loss = 9.745005, Accuracy = 0.6273333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.990220069885254, Accuracy = 0.5652173757553101\n",
      "Training iter #6387000:   Batch Loss = 9.966462, Accuracy = 0.5513333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.989080429077148, Accuracy = 0.5652173757553101\n",
      "Training iter #6390000:   Batch Loss = 10.341173, Accuracy = 0.4519999921321869\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.987796783447266, Accuracy = 0.5652173757553101\n",
      "Training iter #6393000:   Batch Loss = 9.528652, Accuracy = 0.7153333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.98641586303711, Accuracy = 0.5652173757553101\n",
      "Training iter #6396000:   Batch Loss = 9.585409, Accuracy = 0.6880000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.98508071899414, Accuracy = 0.5652173757553101\n",
      "Training iter #6399000:   Batch Loss = 9.786137, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.98372745513916, Accuracy = 0.5646218061447144\n",
      "Training iter #6402000:   Batch Loss = 10.124590, Accuracy = 0.49666666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.982418060302734, Accuracy = 0.5646218061447144\n",
      "Training iter #6405000:   Batch Loss = 10.109303, Accuracy = 0.5273333191871643\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.981217384338379, Accuracy = 0.5646218061447144\n",
      "Training iter #6408000:   Batch Loss = 9.415788, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.980125427246094, Accuracy = 0.5643240213394165\n",
      "Training iter #6411000:   Batch Loss = 9.709436, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.978742599487305, Accuracy = 0.5652173757553101\n",
      "Training iter #6414000:   Batch Loss = 10.001825, Accuracy = 0.5433333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.977131843566895, Accuracy = 0.5661107897758484\n",
      "Training iter #6417000:   Batch Loss = 10.246530, Accuracy = 0.4753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.975522994995117, Accuracy = 0.5661107897758484\n",
      "Training iter #6420000:   Batch Loss = 9.480776, Accuracy = 0.7286666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.974133491516113, Accuracy = 0.5664085745811462\n",
      "Training iter #6423000:   Batch Loss = 9.662867, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.972963333129883, Accuracy = 0.5661107897758484\n",
      "Training iter #6426000:   Batch Loss = 9.656734, Accuracy = 0.6140000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.971671104431152, Accuracy = 0.5670041441917419\n",
      "Training iter #6429000:   Batch Loss = 10.134826, Accuracy = 0.5046666860580444\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.970396041870117, Accuracy = 0.5673019886016846\n",
      "Training iter #6432000:   Batch Loss = 10.089711, Accuracy = 0.5400000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.969133377075195, Accuracy = 0.5675997734069824\n",
      "Training iter #6435000:   Batch Loss = 9.469551, Accuracy = 0.6946666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.967948913574219, Accuracy = 0.5678975582122803\n",
      "Training iter #6438000:   Batch Loss = 9.709029, Accuracy = 0.624666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.966731071472168, Accuracy = 0.5678975582122803\n",
      "Training iter #6441000:   Batch Loss = 10.021343, Accuracy = 0.5320000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.965442657470703, Accuracy = 0.5681953430175781\n",
      "Training iter #6444000:   Batch Loss = 10.254631, Accuracy = 0.4713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.964168548583984, Accuracy = 0.5678975582122803\n",
      "Training iter #6447000:   Batch Loss = 9.497802, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.962904930114746, Accuracy = 0.568493127822876\n",
      "Training iter #6450000:   Batch Loss = 9.620133, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.96163272857666, Accuracy = 0.5681953430175781\n",
      "Training iter #6453000:   Batch Loss = 9.654343, Accuracy = 0.6193333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.960287094116211, Accuracy = 0.5681953430175781\n",
      "Training iter #6456000:   Batch Loss = 10.105970, Accuracy = 0.512666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.959070205688477, Accuracy = 0.5693865418434143\n",
      "Training iter #6459000:   Batch Loss = 9.932777, Accuracy = 0.5806666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.957812309265137, Accuracy = 0.56998211145401\n",
      "Training iter #6462000:   Batch Loss = 9.455798, Accuracy = 0.7020000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.95660400390625, Accuracy = 0.5702799558639526\n",
      "Training iter #6465000:   Batch Loss = 9.759225, Accuracy = 0.6179999709129333\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.95536994934082, Accuracy = 0.5705777406692505\n",
      "Training iter #6468000:   Batch Loss = 10.091673, Accuracy = 0.5073333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.954368591308594, Accuracy = 0.5711733102798462\n",
      "Training iter #6471000:   Batch Loss = 10.168566, Accuracy = 0.4893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.953227996826172, Accuracy = 0.5708755254745483\n",
      "Training iter #6474000:   Batch Loss = 9.494391, Accuracy = 0.722000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.952014923095703, Accuracy = 0.5708755254745483\n",
      "Training iter #6477000:   Batch Loss = 9.627800, Accuracy = 0.6793333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.950910568237305, Accuracy = 0.5711733102798462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6480000:   Batch Loss = 9.588694, Accuracy = 0.6366666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.949753761291504, Accuracy = 0.5708755254745483\n",
      "Training iter #6483000:   Batch Loss = 10.114288, Accuracy = 0.5093333125114441\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.948568344116211, Accuracy = 0.571471095085144\n",
      "Training iter #6486000:   Batch Loss = 9.879234, Accuracy = 0.590666651725769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.947308540344238, Accuracy = 0.5720667243003845\n",
      "Training iter #6489000:   Batch Loss = 9.419953, Accuracy = 0.718666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.946123123168945, Accuracy = 0.5720667243003845\n",
      "Training iter #6492000:   Batch Loss = 9.759086, Accuracy = 0.6140000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.944965362548828, Accuracy = 0.5723645091056824\n",
      "Training iter #6495000:   Batch Loss = 10.127905, Accuracy = 0.49533334374427795\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.943934440612793, Accuracy = 0.5726622939109802\n",
      "Training iter #6498000:   Batch Loss = 10.125639, Accuracy = 0.5019999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.942813873291016, Accuracy = 0.5729600787162781\n",
      "Training iter #6501000:   Batch Loss = 9.535347, Accuracy = 0.7006666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.941509246826172, Accuracy = 0.5744490623474121\n",
      "Training iter #6504000:   Batch Loss = 9.653339, Accuracy = 0.6633333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.94017505645752, Accuracy = 0.5741512775421143\n",
      "Training iter #6507000:   Batch Loss = 9.608071, Accuracy = 0.6286666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.938682556152344, Accuracy = 0.5735557079315186\n",
      "Training iter #6510000:   Batch Loss = 10.139248, Accuracy = 0.49933332204818726\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.937328338623047, Accuracy = 0.5735557079315186\n",
      "Training iter #6513000:   Batch Loss = 9.859941, Accuracy = 0.5993333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.936071395874023, Accuracy = 0.5738534927368164\n",
      "Training iter #6516000:   Batch Loss = 9.436623, Accuracy = 0.7093333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.93484115600586, Accuracy = 0.5741512775421143\n",
      "Training iter #6519000:   Batch Loss = 9.744323, Accuracy = 0.612666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.933513641357422, Accuracy = 0.5741512775421143\n",
      "Training iter #6522000:   Batch Loss = 10.105441, Accuracy = 0.4986666738986969\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.932121276855469, Accuracy = 0.5744490623474121\n",
      "Training iter #6525000:   Batch Loss = 10.148754, Accuracy = 0.5053333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.931026458740234, Accuracy = 0.5750446915626526\n",
      "Training iter #6528000:   Batch Loss = 9.479190, Accuracy = 0.718666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.929786682128906, Accuracy = 0.5750446915626526\n",
      "Training iter #6531000:   Batch Loss = 9.595469, Accuracy = 0.6793333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.928518295288086, Accuracy = 0.5753424763679504\n",
      "Training iter #6534000:   Batch Loss = 9.535522, Accuracy = 0.6553333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.927220344543457, Accuracy = 0.5759380459785461\n",
      "Training iter #6537000:   Batch Loss = 10.202429, Accuracy = 0.4886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.925997734069824, Accuracy = 0.576235830783844\n",
      "Training iter #6540000:   Batch Loss = 9.824980, Accuracy = 0.5920000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.924823760986328, Accuracy = 0.576235830783844\n",
      "Training iter #6543000:   Batch Loss = 9.438654, Accuracy = 0.7080000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.923676490783691, Accuracy = 0.576235830783844\n",
      "Training iter #6546000:   Batch Loss = 9.711968, Accuracy = 0.6240000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.922489166259766, Accuracy = 0.5765336751937866\n",
      "Training iter #6549000:   Batch Loss = 10.133091, Accuracy = 0.4873333275318146\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.921218872070312, Accuracy = 0.5765336751937866\n",
      "Training iter #6552000:   Batch Loss = 10.065622, Accuracy = 0.515333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.920037269592285, Accuracy = 0.5765336751937866\n",
      "Training iter #6555000:   Batch Loss = 9.381636, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.918824195861816, Accuracy = 0.5774270296096802\n",
      "Training iter #6558000:   Batch Loss = 9.626619, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.917613983154297, Accuracy = 0.5780226588249207\n",
      "Training iter #6561000:   Batch Loss = 9.624243, Accuracy = 0.6306666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.916365623474121, Accuracy = 0.577724814414978\n",
      "Training iter #6564000:   Batch Loss = 10.246985, Accuracy = 0.4713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.915098190307617, Accuracy = 0.5771292448043823\n",
      "Training iter #6567000:   Batch Loss = 9.801977, Accuracy = 0.6053333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.913722038269043, Accuracy = 0.5774270296096802\n",
      "Training iter #6570000:   Batch Loss = 9.343250, Accuracy = 0.734000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.912264823913574, Accuracy = 0.577724814414978\n",
      "Training iter #6573000:   Batch Loss = 9.680623, Accuracy = 0.637333333492279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.910665512084961, Accuracy = 0.5789160132408142\n",
      "Training iter #6576000:   Batch Loss = 10.153152, Accuracy = 0.4886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.909513473510742, Accuracy = 0.5786182284355164\n",
      "Training iter #6579000:   Batch Loss = 9.931561, Accuracy = 0.5553333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.90843391418457, Accuracy = 0.5792137980461121\n",
      "Training iter #6582000:   Batch Loss = 9.420969, Accuracy = 0.7419999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.907215118408203, Accuracy = 0.5798094272613525\n",
      "Training iter #6585000:   Batch Loss = 9.594042, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.905973434448242, Accuracy = 0.5789160132408142\n",
      "Training iter #6588000:   Batch Loss = 9.646008, Accuracy = 0.6226666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.904722213745117, Accuracy = 0.5789160132408142\n",
      "Training iter #6591000:   Batch Loss = 10.263415, Accuracy = 0.46266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.903453826904297, Accuracy = 0.5804049968719482\n",
      "Training iter #6594000:   Batch Loss = 9.709896, Accuracy = 0.621999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.902193069458008, Accuracy = 0.5815961956977844\n",
      "Training iter #6597000:   Batch Loss = 9.438784, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.900920867919922, Accuracy = 0.5812984108924866\n",
      "Training iter #6600000:   Batch Loss = 9.713711, Accuracy = 0.624666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.899724960327148, Accuracy = 0.5815961956977844\n",
      "Training iter #6603000:   Batch Loss = 10.108685, Accuracy = 0.4986666738986969\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.898517608642578, Accuracy = 0.5815961956977844\n",
      "Training iter #6606000:   Batch Loss = 9.893815, Accuracy = 0.5673333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.897388458251953, Accuracy = 0.5812984108924866\n",
      "Training iter #6609000:   Batch Loss = 9.367008, Accuracy = 0.7633333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.896275520324707, Accuracy = 0.5812984108924866\n",
      "Training iter #6612000:   Batch Loss = 9.557245, Accuracy = 0.6693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.895125389099121, Accuracy = 0.5818939805030823\n",
      "Training iter #6615000:   Batch Loss = 9.636551, Accuracy = 0.6153333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.894279479980469, Accuracy = 0.5821917653083801\n",
      "Training iter #6618000:   Batch Loss = 10.356093, Accuracy = 0.4273333251476288\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.893177032470703, Accuracy = 0.5827873945236206\n",
      "Training iter #6621000:   Batch Loss = 9.634796, Accuracy = 0.6426666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.891951560974121, Accuracy = 0.5833829641342163\n",
      "Training iter #6624000:   Batch Loss = 9.400743, Accuracy = 0.7239999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.89067268371582, Accuracy = 0.5836807489395142\n",
      "Training iter #6627000:   Batch Loss = 9.733529, Accuracy = 0.625333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.889389038085938, Accuracy = 0.5842763781547546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6630000:   Batch Loss = 10.043383, Accuracy = 0.5226666927337646\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.888018608093262, Accuracy = 0.585467517375946\n",
      "Training iter #6633000:   Batch Loss = 9.919554, Accuracy = 0.5673333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.886897087097168, Accuracy = 0.585467517375946\n",
      "Training iter #6636000:   Batch Loss = 9.398678, Accuracy = 0.7519999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.885841369628906, Accuracy = 0.5848719477653503\n",
      "Training iter #6639000:   Batch Loss = 9.573030, Accuracy = 0.6700000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.884700775146484, Accuracy = 0.585467517375946\n",
      "Training iter #6642000:   Batch Loss = 9.730383, Accuracy = 0.5926666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.883467674255371, Accuracy = 0.5863609313964844\n",
      "Training iter #6645000:   Batch Loss = 10.213999, Accuracy = 0.4580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.882115364074707, Accuracy = 0.5866587162017822\n",
      "Training iter #6648000:   Batch Loss = 9.529255, Accuracy = 0.6693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.880880355834961, Accuracy = 0.5866587162017822\n",
      "Training iter #6651000:   Batch Loss = 9.380215, Accuracy = 0.7279999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.879664421081543, Accuracy = 0.5869565010070801\n",
      "Training iter #6654000:   Batch Loss = 9.715635, Accuracy = 0.6293333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.878344535827637, Accuracy = 0.5878499150276184\n",
      "Training iter #6657000:   Batch Loss = 10.001730, Accuracy = 0.5273333191871643\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.877121925354004, Accuracy = 0.5881476998329163\n",
      "Training iter #6660000:   Batch Loss = 9.912055, Accuracy = 0.5633333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.875932693481445, Accuracy = 0.5881476998329163\n",
      "Training iter #6663000:   Batch Loss = 9.460842, Accuracy = 0.7179999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.874774932861328, Accuracy = 0.5887433290481567\n",
      "Training iter #6666000:   Batch Loss = 9.601666, Accuracy = 0.6779999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.87360668182373, Accuracy = 0.5887433290481567\n",
      "Training iter #6669000:   Batch Loss = 9.718027, Accuracy = 0.6026666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.872469902038574, Accuracy = 0.5893388986587524\n",
      "Training iter #6672000:   Batch Loss = 10.274831, Accuracy = 0.44466665387153625\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.871354103088379, Accuracy = 0.5899344682693481\n",
      "Training iter #6675000:   Batch Loss = 9.425581, Accuracy = 0.7126666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.870207786560059, Accuracy = 0.5899344682693481\n",
      "Training iter #6678000:   Batch Loss = 9.385900, Accuracy = 0.7279999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.869000434875488, Accuracy = 0.590232253074646\n",
      "Training iter #6681000:   Batch Loss = 9.748928, Accuracy = 0.6053333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.867720603942871, Accuracy = 0.5908278822898865\n",
      "Training iter #6684000:   Batch Loss = 9.975298, Accuracy = 0.531333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.866464614868164, Accuracy = 0.5911256670951843\n",
      "Training iter #6687000:   Batch Loss = 9.927247, Accuracy = 0.559333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.86526107788086, Accuracy = 0.5914234519004822\n",
      "Training iter #6690000:   Batch Loss = 9.465358, Accuracy = 0.7120000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.864011764526367, Accuracy = 0.59172123670578\n",
      "Training iter #6693000:   Batch Loss = 9.622473, Accuracy = 0.6693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.86281681060791, Accuracy = 0.59172123670578\n",
      "Training iter #6696000:   Batch Loss = 9.776052, Accuracy = 0.5866666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.861669540405273, Accuracy = 0.5920190811157227\n",
      "Training iter #6699000:   Batch Loss = 10.254431, Accuracy = 0.45399999618530273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.860650062561035, Accuracy = 0.5920190811157227\n",
      "Training iter #6702000:   Batch Loss = 9.331506, Accuracy = 0.7440000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.85970687866211, Accuracy = 0.5914234519004822\n",
      "Training iter #6705000:   Batch Loss = 9.389437, Accuracy = 0.7239999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.858575820922852, Accuracy = 0.5923168659210205\n",
      "Training iter #6708000:   Batch Loss = 9.687169, Accuracy = 0.6133333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.857486724853516, Accuracy = 0.5926146507263184\n",
      "Training iter #6711000:   Batch Loss = 9.974736, Accuracy = 0.518666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.856425285339355, Accuracy = 0.5914234519004822\n",
      "Training iter #6714000:   Batch Loss = 9.940428, Accuracy = 0.5540000200271606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.85535717010498, Accuracy = 0.5920190811157227\n",
      "Training iter #6717000:   Batch Loss = 9.317017, Accuracy = 0.7426666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.85419750213623, Accuracy = 0.5926146507263184\n",
      "Training iter #6720000:   Batch Loss = 9.592422, Accuracy = 0.6626666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.85291576385498, Accuracy = 0.5926146507263184\n",
      "Training iter #6723000:   Batch Loss = 9.834240, Accuracy = 0.5786666870117188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.851670265197754, Accuracy = 0.5935080647468567\n",
      "Training iter #6726000:   Batch Loss = 10.156457, Accuracy = 0.4860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.850532531738281, Accuracy = 0.5929124355316162\n",
      "Training iter #6729000:   Batch Loss = 9.367319, Accuracy = 0.7426666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.849328994750977, Accuracy = 0.5929124355316162\n",
      "Training iter #6732000:   Batch Loss = 9.495130, Accuracy = 0.6980000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.848211288452148, Accuracy = 0.5929124355316162\n",
      "Training iter #6735000:   Batch Loss = 9.576030, Accuracy = 0.6386666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.846968650817871, Accuracy = 0.5935080647468567\n",
      "Training iter #6738000:   Batch Loss = 10.000766, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.845747947692871, Accuracy = 0.5932102203369141\n",
      "Training iter #6741000:   Batch Loss = 9.976837, Accuracy = 0.5533333420753479\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.84465217590332, Accuracy = 0.5932102203369141\n",
      "Training iter #6744000:   Batch Loss = 9.278125, Accuracy = 0.7540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.843538284301758, Accuracy = 0.5932102203369141\n",
      "Training iter #6747000:   Batch Loss = 9.584068, Accuracy = 0.6673333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.842351913452148, Accuracy = 0.5929124355316162\n",
      "Training iter #6750000:   Batch Loss = 9.895868, Accuracy = 0.559333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.841166496276855, Accuracy = 0.5932102203369141\n",
      "Training iter #6753000:   Batch Loss = 10.056332, Accuracy = 0.5053333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.839835166931152, Accuracy = 0.5944014191627502\n",
      "Training iter #6756000:   Batch Loss = 9.361250, Accuracy = 0.7353333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.8384428024292, Accuracy = 0.5944014191627502\n",
      "Training iter #6759000:   Batch Loss = 9.459704, Accuracy = 0.7059999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.837265014648438, Accuracy = 0.5949970483779907\n",
      "Training iter #6762000:   Batch Loss = 9.545535, Accuracy = 0.6480000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.83599853515625, Accuracy = 0.5949970483779907\n",
      "Training iter #6765000:   Batch Loss = 9.960257, Accuracy = 0.5346666574478149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.834793090820312, Accuracy = 0.5955926179885864\n",
      "Training iter #6768000:   Batch Loss = 9.915990, Accuracy = 0.5686666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.833587646484375, Accuracy = 0.5967838168144226\n",
      "Training iter #6771000:   Batch Loss = 9.358410, Accuracy = 0.7453333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.832427024841309, Accuracy = 0.5967838168144226\n",
      "Training iter #6774000:   Batch Loss = 9.544339, Accuracy = 0.668666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.831280708312988, Accuracy = 0.5970816016197205\n",
      "Training iter #6777000:   Batch Loss = 9.917426, Accuracy = 0.5479999780654907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.830059051513672, Accuracy = 0.5973793864250183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6780000:   Batch Loss = 10.100426, Accuracy = 0.4906666576862335\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.82882308959961, Accuracy = 0.5976771712303162\n",
      "Training iter #6783000:   Batch Loss = 9.385575, Accuracy = 0.7279999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.827640533447266, Accuracy = 0.5976771712303162\n",
      "Training iter #6786000:   Batch Loss = 9.484968, Accuracy = 0.7053333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.826560974121094, Accuracy = 0.597974956035614\n",
      "Training iter #6789000:   Batch Loss = 9.471189, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.82561206817627, Accuracy = 0.5982728004455566\n",
      "Training iter #6792000:   Batch Loss = 9.975380, Accuracy = 0.5353333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.824605941772461, Accuracy = 0.5982728004455566\n",
      "Training iter #6795000:   Batch Loss = 9.788110, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.823622703552246, Accuracy = 0.597974956035614\n",
      "Training iter #6798000:   Batch Loss = 9.322686, Accuracy = 0.7486666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.82253646850586, Accuracy = 0.597974956035614\n",
      "Training iter #6801000:   Batch Loss = 9.662195, Accuracy = 0.6359999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.821533203125, Accuracy = 0.5985705852508545\n",
      "Training iter #6804000:   Batch Loss = 9.945337, Accuracy = 0.5320000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.820520401000977, Accuracy = 0.5991661548614502\n",
      "Training iter #6807000:   Batch Loss = 10.058232, Accuracy = 0.5006666779518127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.81937026977539, Accuracy = 0.5997617840766907\n",
      "Training iter #6810000:   Batch Loss = 9.355639, Accuracy = 0.7393333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.818194389343262, Accuracy = 0.6000595688819885\n",
      "Training iter #6813000:   Batch Loss = 9.492040, Accuracy = 0.7006666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.817091941833496, Accuracy = 0.6003573536872864\n",
      "Training iter #6816000:   Batch Loss = 9.447415, Accuracy = 0.6713333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.815942764282227, Accuracy = 0.6000595688819885\n",
      "Training iter #6819000:   Batch Loss = 9.965626, Accuracy = 0.5379999876022339\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.814821243286133, Accuracy = 0.5997617840766907\n",
      "Training iter #6822000:   Batch Loss = 9.739906, Accuracy = 0.6226666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.813692092895508, Accuracy = 0.5997617840766907\n",
      "Training iter #6825000:   Batch Loss = 9.258412, Accuracy = 0.7646666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.81251335144043, Accuracy = 0.5997617840766907\n",
      "Training iter #6828000:   Batch Loss = 9.590630, Accuracy = 0.6606666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.81174087524414, Accuracy = 0.6006551384925842\n",
      "Training iter #6831000:   Batch Loss = 9.957079, Accuracy = 0.5260000228881836\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.810674667358398, Accuracy = 0.6009529232978821\n",
      "Training iter #6834000:   Batch Loss = 9.976042, Accuracy = 0.5226666927337646\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.809539794921875, Accuracy = 0.6018463373184204\n",
      "Training iter #6837000:   Batch Loss = 9.392801, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.808359146118164, Accuracy = 0.6015485525131226\n",
      "Training iter #6840000:   Batch Loss = 9.477203, Accuracy = 0.7006666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.807230949401855, Accuracy = 0.6012507677078247\n",
      "Training iter #6843000:   Batch Loss = 9.407669, Accuracy = 0.687333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.806081771850586, Accuracy = 0.6009529232978821\n",
      "Training iter #6846000:   Batch Loss = 10.021505, Accuracy = 0.515999972820282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.804913520812988, Accuracy = 0.6009529232978821\n",
      "Training iter #6849000:   Batch Loss = 9.675386, Accuracy = 0.6353333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.803648948669434, Accuracy = 0.6012507677078247\n",
      "Training iter #6852000:   Batch Loss = 9.316488, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.802207946777344, Accuracy = 0.6015485525131226\n",
      "Training iter #6855000:   Batch Loss = 9.602697, Accuracy = 0.6566666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.800976753234863, Accuracy = 0.6012507677078247\n",
      "Training iter #6858000:   Batch Loss = 9.995715, Accuracy = 0.5113333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.799779891967773, Accuracy = 0.6021441221237183\n",
      "Training iter #6861000:   Batch Loss = 9.981065, Accuracy = 0.5286666750907898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.798680305480957, Accuracy = 0.6018463373184204\n",
      "Training iter #6864000:   Batch Loss = 9.302142, Accuracy = 0.7620000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.797616958618164, Accuracy = 0.6021441221237183\n",
      "Training iter #6867000:   Batch Loss = 9.478495, Accuracy = 0.6966666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.796695709228516, Accuracy = 0.6021441221237183\n",
      "Training iter #6870000:   Batch Loss = 9.453686, Accuracy = 0.6746666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.795822143554688, Accuracy = 0.6024419069290161\n",
      "Training iter #6873000:   Batch Loss = 10.038738, Accuracy = 0.5113333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.79466724395752, Accuracy = 0.6030375361442566\n",
      "Training iter #6876000:   Batch Loss = 9.701660, Accuracy = 0.6206666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.793540000915527, Accuracy = 0.6024419069290161\n",
      "Training iter #6879000:   Batch Loss = 9.297899, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.792304039001465, Accuracy = 0.6027397513389587\n",
      "Training iter #6882000:   Batch Loss = 9.558608, Accuracy = 0.6693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.791139602661133, Accuracy = 0.6030375361442566\n",
      "Training iter #6885000:   Batch Loss = 10.004141, Accuracy = 0.512666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.790006637573242, Accuracy = 0.6036331057548523\n",
      "Training iter #6888000:   Batch Loss = 9.856521, Accuracy = 0.5580000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.78890609741211, Accuracy = 0.6030375361442566\n",
      "Training iter #6891000:   Batch Loss = 9.303883, Accuracy = 0.765999972820282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.787714004516602, Accuracy = 0.6027397513389587\n",
      "Training iter #6894000:   Batch Loss = 9.508118, Accuracy = 0.6853333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.786523818969727, Accuracy = 0.6045265197753906\n",
      "Training iter #6897000:   Batch Loss = 9.493208, Accuracy = 0.6539999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.78544807434082, Accuracy = 0.6045265197753906\n",
      "Training iter #6900000:   Batch Loss = 10.111893, Accuracy = 0.4886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.784408569335938, Accuracy = 0.6036331057548523\n",
      "Training iter #6903000:   Batch Loss = 9.618296, Accuracy = 0.6513333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.78327751159668, Accuracy = 0.6036331057548523\n",
      "Training iter #6906000:   Batch Loss = 9.281544, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.782126426696777, Accuracy = 0.6036331057548523\n",
      "Training iter #6909000:   Batch Loss = 9.544005, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.780991554260254, Accuracy = 0.6033353209495544\n",
      "Training iter #6912000:   Batch Loss = 9.993687, Accuracy = 0.5233333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.779878616333008, Accuracy = 0.6033353209495544\n",
      "Training iter #6915000:   Batch Loss = 9.815462, Accuracy = 0.5720000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.778680801391602, Accuracy = 0.6033353209495544\n",
      "Training iter #6918000:   Batch Loss = 9.252398, Accuracy = 0.7760000228881836\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.777456283569336, Accuracy = 0.6039308905601501\n",
      "Training iter #6921000:   Batch Loss = 9.434717, Accuracy = 0.7020000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.776283264160156, Accuracy = 0.6042287349700928\n",
      "Training iter #6924000:   Batch Loss = 9.507862, Accuracy = 0.6546666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.775151252746582, Accuracy = 0.6045265197753906\n",
      "Training iter #6927000:   Batch Loss = 10.173165, Accuracy = 0.4646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.774053573608398, Accuracy = 0.6042287349700928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6930000:   Batch Loss = 9.526278, Accuracy = 0.671999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.773014068603516, Accuracy = 0.6042287349700928\n",
      "Training iter #6933000:   Batch Loss = 9.302118, Accuracy = 0.7480000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.771869659423828, Accuracy = 0.6045265197753906\n",
      "Training iter #6936000:   Batch Loss = 9.631382, Accuracy = 0.6546666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.770798683166504, Accuracy = 0.6039308905601501\n",
      "Training iter #6939000:   Batch Loss = 9.938581, Accuracy = 0.5379999876022339\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.769838333129883, Accuracy = 0.6042287349700928\n",
      "Training iter #6942000:   Batch Loss = 9.736071, Accuracy = 0.5960000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.768889427185059, Accuracy = 0.6045265197753906\n",
      "Training iter #6945000:   Batch Loss = 9.274487, Accuracy = 0.768666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.76777458190918, Accuracy = 0.6051220893859863\n",
      "Training iter #6948000:   Batch Loss = 9.434033, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.766819953918457, Accuracy = 0.6054198741912842\n",
      "Training iter #6951000:   Batch Loss = 9.569303, Accuracy = 0.6320000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.765912055969238, Accuracy = 0.6060155034065247\n",
      "Training iter #6954000:   Batch Loss = 10.163104, Accuracy = 0.4633333384990692\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.764872550964355, Accuracy = 0.6060155034065247\n",
      "Training iter #6957000:   Batch Loss = 9.445030, Accuracy = 0.6919999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.763714790344238, Accuracy = 0.6066110730171204\n",
      "Training iter #6960000:   Batch Loss = 9.278939, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.762593269348145, Accuracy = 0.6063132882118225\n",
      "Training iter #6963000:   Batch Loss = 9.600041, Accuracy = 0.6579999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.76154613494873, Accuracy = 0.6072066426277161\n",
      "Training iter #6966000:   Batch Loss = 9.909756, Accuracy = 0.5426666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.760475158691406, Accuracy = 0.6075044870376587\n",
      "Training iter #6969000:   Batch Loss = 9.811340, Accuracy = 0.5793333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.759262084960938, Accuracy = 0.6075044870376587\n",
      "Training iter #6972000:   Batch Loss = 9.312542, Accuracy = 0.7553333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.758092880249023, Accuracy = 0.6072066426277161\n",
      "Training iter #6975000:   Batch Loss = 9.422916, Accuracy = 0.7086666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.75695514678955, Accuracy = 0.6069088578224182\n",
      "Training iter #6978000:   Batch Loss = 9.611226, Accuracy = 0.6233333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.755874633789062, Accuracy = 0.6072066426277161\n",
      "Training iter #6981000:   Batch Loss = 10.145763, Accuracy = 0.47333332896232605\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.75479793548584, Accuracy = 0.6072066426277161\n",
      "Training iter #6984000:   Batch Loss = 9.390959, Accuracy = 0.7160000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.753657341003418, Accuracy = 0.6081000566482544\n",
      "Training iter #6987000:   Batch Loss = 9.238546, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.752484321594238, Accuracy = 0.6081000566482544\n",
      "Training iter #6990000:   Batch Loss = 9.585005, Accuracy = 0.6453333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.751382827758789, Accuracy = 0.6083978414535522\n",
      "Training iter #6993000:   Batch Loss = 9.865404, Accuracy = 0.5553333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.750393867492676, Accuracy = 0.6086956262588501\n",
      "Training iter #6996000:   Batch Loss = 9.792710, Accuracy = 0.5820000171661377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.749454498291016, Accuracy = 0.6086956262588501\n",
      "Training iter #6999000:   Batch Loss = 9.318928, Accuracy = 0.7453333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.748504638671875, Accuracy = 0.6095890402793884\n",
      "Training iter #7002000:   Batch Loss = 9.461331, Accuracy = 0.7020000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.747546195983887, Accuracy = 0.6098868250846863\n",
      "Training iter #7005000:   Batch Loss = 9.619281, Accuracy = 0.6213333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.746474266052246, Accuracy = 0.6092912554740906\n",
      "Training iter #7008000:   Batch Loss = 10.131602, Accuracy = 0.47733333706855774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.745317459106445, Accuracy = 0.6101846098899841\n",
      "Training iter #7011000:   Batch Loss = 9.250702, Accuracy = 0.762666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.744214057922363, Accuracy = 0.6104824542999268\n",
      "Training iter #7014000:   Batch Loss = 9.240613, Accuracy = 0.7553333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.74307918548584, Accuracy = 0.6104824542999268\n",
      "Training iter #7017000:   Batch Loss = 9.591731, Accuracy = 0.6453333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.741869926452637, Accuracy = 0.6101846098899841\n",
      "Training iter #7020000:   Batch Loss = 9.821787, Accuracy = 0.5640000104904175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.740705490112305, Accuracy = 0.6104824542999268\n",
      "Training iter #7023000:   Batch Loss = 9.801827, Accuracy = 0.5766666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.73961067199707, Accuracy = 0.6110780239105225\n",
      "Training iter #7026000:   Batch Loss = 9.333411, Accuracy = 0.7480000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.738548278808594, Accuracy = 0.6107802391052246\n",
      "Training iter #7029000:   Batch Loss = 9.472466, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.737523078918457, Accuracy = 0.6104824542999268\n",
      "Training iter #7032000:   Batch Loss = 9.686480, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.736467361450195, Accuracy = 0.6104824542999268\n",
      "Training iter #7035000:   Batch Loss = 10.090088, Accuracy = 0.4893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.73541259765625, Accuracy = 0.6110780239105225\n",
      "Training iter #7038000:   Batch Loss = 9.229279, Accuracy = 0.7720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.73427963256836, Accuracy = 0.6116735935211182\n",
      "Training iter #7041000:   Batch Loss = 9.292224, Accuracy = 0.7433333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.733089447021484, Accuracy = 0.6119714379310608\n",
      "Training iter #7044000:   Batch Loss = 9.553286, Accuracy = 0.6486666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.732072830200195, Accuracy = 0.6122692227363586\n",
      "Training iter #7047000:   Batch Loss = 9.861397, Accuracy = 0.5446666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.7310791015625, Accuracy = 0.6128647923469543\n",
      "Training iter #7050000:   Batch Loss = 9.853594, Accuracy = 0.5673333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.730082511901855, Accuracy = 0.6134604215621948\n",
      "Training iter #7053000:   Batch Loss = 9.189034, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.729087829589844, Accuracy = 0.6131625771522522\n",
      "Training iter #7056000:   Batch Loss = 9.446515, Accuracy = 0.6959999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.72810173034668, Accuracy = 0.6137582063674927\n",
      "Training iter #7059000:   Batch Loss = 9.730132, Accuracy = 0.5920000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.727089881896973, Accuracy = 0.6140559911727905\n",
      "Training iter #7062000:   Batch Loss = 9.990013, Accuracy = 0.5180000066757202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.725915908813477, Accuracy = 0.6134604215621948\n",
      "Training iter #7065000:   Batch Loss = 9.236142, Accuracy = 0.7713333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.724770545959473, Accuracy = 0.6137582063674927\n",
      "Training iter #7068000:   Batch Loss = 9.393530, Accuracy = 0.7086666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.723664283752441, Accuracy = 0.6137582063674927\n",
      "Training iter #7071000:   Batch Loss = 9.418754, Accuracy = 0.6753333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.722579002380371, Accuracy = 0.6137582063674927\n",
      "Training iter #7074000:   Batch Loss = 9.868934, Accuracy = 0.5453333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.721539497375488, Accuracy = 0.6140559911727905\n",
      "Training iter #7077000:   Batch Loss = 9.868490, Accuracy = 0.5706666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.720510482788086, Accuracy = 0.6149493455886841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7080000:   Batch Loss = 9.193591, Accuracy = 0.7760000228881836\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.719350814819336, Accuracy = 0.6149493455886841\n",
      "Training iter #7083000:   Batch Loss = 9.462221, Accuracy = 0.690666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.718374252319336, Accuracy = 0.6149493455886841\n",
      "Training iter #7086000:   Batch Loss = 9.779578, Accuracy = 0.5799999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.71748161315918, Accuracy = 0.6149493455886841\n",
      "Training iter #7089000:   Batch Loss = 9.959148, Accuracy = 0.5213333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.716604232788086, Accuracy = 0.6152471899986267\n",
      "Training iter #7092000:   Batch Loss = 9.220098, Accuracy = 0.7706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.715603828430176, Accuracy = 0.6152471899986267\n",
      "Training iter #7095000:   Batch Loss = 9.367502, Accuracy = 0.7193333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.714524269104004, Accuracy = 0.6149493455886841\n",
      "Training iter #7098000:   Batch Loss = 9.403501, Accuracy = 0.6800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.713401794433594, Accuracy = 0.6149493455886841\n",
      "Training iter #7101000:   Batch Loss = 9.844744, Accuracy = 0.5573333501815796\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.712308883666992, Accuracy = 0.6161405444145203\n",
      "Training iter #7104000:   Batch Loss = 9.717178, Accuracy = 0.6106666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.711268424987793, Accuracy = 0.6161405444145203\n",
      "Training iter #7107000:   Batch Loss = 9.243814, Accuracy = 0.7606666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.710216522216797, Accuracy = 0.6164383292198181\n",
      "Training iter #7110000:   Batch Loss = 9.489677, Accuracy = 0.6826666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.709158897399902, Accuracy = 0.6164383292198181\n",
      "Training iter #7113000:   Batch Loss = 9.837467, Accuracy = 0.5566666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.70810604095459, Accuracy = 0.6158427596092224\n",
      "Training iter #7116000:   Batch Loss = 9.941486, Accuracy = 0.5233333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.706972122192383, Accuracy = 0.6161405444145203\n",
      "Training iter #7119000:   Batch Loss = 9.251184, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.705933570861816, Accuracy = 0.6161405444145203\n",
      "Training iter #7122000:   Batch Loss = 9.366686, Accuracy = 0.7293333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.704846382141113, Accuracy = 0.6164383292198181\n",
      "Training iter #7125000:   Batch Loss = 9.362958, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.703713417053223, Accuracy = 0.6170339584350586\n",
      "Training iter #7128000:   Batch Loss = 9.821022, Accuracy = 0.5646666884422302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.70264720916748, Accuracy = 0.6173317432403564\n",
      "Training iter #7131000:   Batch Loss = 9.635938, Accuracy = 0.6313333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.70159912109375, Accuracy = 0.6182251572608948\n",
      "Training iter #7134000:   Batch Loss = 9.206294, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.700542449951172, Accuracy = 0.6173317432403564\n",
      "Training iter #7137000:   Batch Loss = 9.533090, Accuracy = 0.6786666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.69950008392334, Accuracy = 0.6173317432403564\n",
      "Training iter #7140000:   Batch Loss = 9.878517, Accuracy = 0.5406666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.698454856872559, Accuracy = 0.6179273128509521\n",
      "Training iter #7143000:   Batch Loss = 9.886503, Accuracy = 0.5353333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.697465896606445, Accuracy = 0.6179273128509521\n",
      "Training iter #7146000:   Batch Loss = 9.272129, Accuracy = 0.7506666779518127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.69649600982666, Accuracy = 0.6185229420661926\n",
      "Training iter #7149000:   Batch Loss = 9.396947, Accuracy = 0.7153333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.695455551147461, Accuracy = 0.6182251572608948\n",
      "Training iter #7152000:   Batch Loss = 9.334959, Accuracy = 0.7006666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.69440746307373, Accuracy = 0.6185229420661926\n",
      "Training iter #7155000:   Batch Loss = 9.836597, Accuracy = 0.559333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.693317413330078, Accuracy = 0.6188207268714905\n",
      "Training iter #7158000:   Batch Loss = 9.627404, Accuracy = 0.6439999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.692240715026855, Accuracy = 0.6200119256973267\n",
      "Training iter #7161000:   Batch Loss = 9.185774, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.691143035888672, Accuracy = 0.6203097105026245\n",
      "Training iter #7164000:   Batch Loss = 9.485295, Accuracy = 0.6846666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.690120697021484, Accuracy = 0.6209052801132202\n",
      "Training iter #7167000:   Batch Loss = 9.853004, Accuracy = 0.5433333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.689011573791504, Accuracy = 0.6212031245231628\n",
      "Training iter #7170000:   Batch Loss = 9.854118, Accuracy = 0.5453333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.68789291381836, Accuracy = 0.6215009093284607\n",
      "Training iter #7173000:   Batch Loss = 9.247993, Accuracy = 0.7593333125114441\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.686841011047363, Accuracy = 0.6223942637443542\n",
      "Training iter #7176000:   Batch Loss = 9.359326, Accuracy = 0.7246666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.685754776000977, Accuracy = 0.6226921081542969\n",
      "Training iter #7179000:   Batch Loss = 9.280218, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.684709548950195, Accuracy = 0.6229898929595947\n",
      "Training iter #7182000:   Batch Loss = 9.923932, Accuracy = 0.5426666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.683733940124512, Accuracy = 0.6232876777648926\n",
      "Training iter #7185000:   Batch Loss = 9.545202, Accuracy = 0.659333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.682759284973145, Accuracy = 0.6229898929595947\n",
      "Training iter #7188000:   Batch Loss = 9.197592, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.681551933288574, Accuracy = 0.6232876777648926\n",
      "Training iter #7191000:   Batch Loss = 9.470014, Accuracy = 0.6886666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.680534362792969, Accuracy = 0.6235854625701904\n",
      "Training iter #7194000:   Batch Loss = 9.914837, Accuracy = 0.5253333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.679534912109375, Accuracy = 0.6238832473754883\n",
      "Training iter #7197000:   Batch Loss = 9.811118, Accuracy = 0.559333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.678337097167969, Accuracy = 0.6247766613960266\n",
      "Training iter #7200000:   Batch Loss = 9.140291, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.677083969116211, Accuracy = 0.6247766613960266\n",
      "Training iter #7203000:   Batch Loss = 9.386664, Accuracy = 0.7179999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.675819396972656, Accuracy = 0.6247766613960266\n",
      "Training iter #7206000:   Batch Loss = 9.362762, Accuracy = 0.6899999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.674674034118652, Accuracy = 0.6253722310066223\n",
      "Training iter #7209000:   Batch Loss = 9.948918, Accuracy = 0.5326666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.673672676086426, Accuracy = 0.6268612146377563\n",
      "Training iter #7212000:   Batch Loss = 9.561477, Accuracy = 0.6620000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.672800064086914, Accuracy = 0.6268612146377563\n",
      "Training iter #7215000:   Batch Loss = 9.116814, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.67183780670166, Accuracy = 0.6277546286582947\n",
      "Training iter #7218000:   Batch Loss = 9.450186, Accuracy = 0.6919999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.670880317687988, Accuracy = 0.6280524134635925\n",
      "Training iter #7221000:   Batch Loss = 9.913643, Accuracy = 0.5273333191871643\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.669970512390137, Accuracy = 0.6280524134635925\n",
      "Training iter #7224000:   Batch Loss = 9.687678, Accuracy = 0.5886666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.669044494628906, Accuracy = 0.6274568438529968\n",
      "Training iter #7227000:   Batch Loss = 9.194191, Accuracy = 0.7773333191871643\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.667940139770508, Accuracy = 0.6274568438529968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7230000:   Batch Loss = 9.374463, Accuracy = 0.7086666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.666784286499023, Accuracy = 0.6277546286582947\n",
      "Training iter #7233000:   Batch Loss = 9.410501, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.665667533874512, Accuracy = 0.6280524134635925\n",
      "Training iter #7236000:   Batch Loss = 9.989790, Accuracy = 0.515333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.66457748413086, Accuracy = 0.6280524134635925\n",
      "Training iter #7239000:   Batch Loss = 9.454370, Accuracy = 0.6886666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.663321495056152, Accuracy = 0.6289458274841309\n",
      "Training iter #7242000:   Batch Loss = 9.185738, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.662104606628418, Accuracy = 0.6289458274841309\n",
      "Training iter #7245000:   Batch Loss = 9.475293, Accuracy = 0.6866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.66099739074707, Accuracy = 0.6289458274841309\n",
      "Training iter #7248000:   Batch Loss = 9.891357, Accuracy = 0.5406666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.659982681274414, Accuracy = 0.6292436122894287\n",
      "Training iter #7251000:   Batch Loss = 9.675467, Accuracy = 0.5926666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.658984184265137, Accuracy = 0.6295413970947266\n",
      "Training iter #7254000:   Batch Loss = 9.124103, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.65798568725586, Accuracy = 0.6295413970947266\n",
      "Training iter #7257000:   Batch Loss = 9.327418, Accuracy = 0.7153333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.6569242477417, Accuracy = 0.6310303807258606\n",
      "Training iter #7260000:   Batch Loss = 9.399289, Accuracy = 0.6740000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.655834197998047, Accuracy = 0.6307325959205627\n",
      "Training iter #7263000:   Batch Loss = 10.084827, Accuracy = 0.48133334517478943\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.654834747314453, Accuracy = 0.6310303807258606\n",
      "Training iter #7266000:   Batch Loss = 9.386786, Accuracy = 0.7126666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.653873443603516, Accuracy = 0.6310303807258606\n",
      "Training iter #7269000:   Batch Loss = 9.136553, Accuracy = 0.7879999876022339\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.652900695800781, Accuracy = 0.6322215795516968\n",
      "Training iter #7272000:   Batch Loss = 9.520559, Accuracy = 0.687333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.651936531066895, Accuracy = 0.6319237351417542\n",
      "Training iter #7275000:   Batch Loss = 9.794668, Accuracy = 0.5746666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.65095329284668, Accuracy = 0.6319237351417542\n",
      "Training iter #7278000:   Batch Loss = 9.635229, Accuracy = 0.6060000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.649925231933594, Accuracy = 0.6322215795516968\n",
      "Training iter #7281000:   Batch Loss = 9.152057, Accuracy = 0.7900000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.648856163024902, Accuracy = 0.6316259503364563\n",
      "Training iter #7284000:   Batch Loss = 9.327180, Accuracy = 0.7253333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.647785186767578, Accuracy = 0.6325193643569946\n",
      "Training iter #7287000:   Batch Loss = 9.451159, Accuracy = 0.6613333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.646756172180176, Accuracy = 0.6328171491622925\n",
      "Training iter #7290000:   Batch Loss = 9.975834, Accuracy = 0.5139999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.646005630493164, Accuracy = 0.6331149339675903\n",
      "Training iter #7293000:   Batch Loss = 9.305721, Accuracy = 0.7286666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.645150184631348, Accuracy = 0.6331149339675903\n",
      "Training iter #7296000:   Batch Loss = 9.155613, Accuracy = 0.7806666493415833\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.643473625183105, Accuracy = 0.6334127187728882\n",
      "Training iter #7299000:   Batch Loss = 9.509388, Accuracy = 0.6880000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.642558097839355, Accuracy = 0.6343061327934265\n",
      "Training iter #7302000:   Batch Loss = 9.741873, Accuracy = 0.5853333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.641797065734863, Accuracy = 0.6340083479881287\n",
      "Training iter #7305000:   Batch Loss = 9.684027, Accuracy = 0.5973333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.640876770019531, Accuracy = 0.6343061327934265\n",
      "Training iter #7308000:   Batch Loss = 9.198292, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.639915466308594, Accuracy = 0.6340083479881287\n",
      "Training iter #7311000:   Batch Loss = 9.346752, Accuracy = 0.7246666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.638917922973633, Accuracy = 0.6337105631828308\n",
      "Training iter #7314000:   Batch Loss = 9.495163, Accuracy = 0.6553333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.638004302978516, Accuracy = 0.6337105631828308\n",
      "Training iter #7317000:   Batch Loss = 10.012219, Accuracy = 0.49933332204818726\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.637079238891602, Accuracy = 0.6337105631828308\n",
      "Training iter #7320000:   Batch Loss = 9.205904, Accuracy = 0.7680000066757202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.636112213134766, Accuracy = 0.6343061327934265\n",
      "Training iter #7323000:   Batch Loss = 9.138524, Accuracy = 0.7846666574478149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.635074615478516, Accuracy = 0.6351995468139648\n",
      "Training iter #7326000:   Batch Loss = 9.494897, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.634115219116211, Accuracy = 0.6343061327934265\n",
      "Training iter #7329000:   Batch Loss = 9.729632, Accuracy = 0.5766666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.633262634277344, Accuracy = 0.6343061327934265\n",
      "Training iter #7332000:   Batch Loss = 9.657383, Accuracy = 0.6019999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.632177352905273, Accuracy = 0.6346039175987244\n",
      "Training iter #7335000:   Batch Loss = 9.211380, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.63109016418457, Accuracy = 0.6343061327934265\n",
      "Training iter #7338000:   Batch Loss = 9.363888, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.629993438720703, Accuracy = 0.6349017024040222\n",
      "Training iter #7341000:   Batch Loss = 9.542880, Accuracy = 0.6433333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.628988265991211, Accuracy = 0.6354973316192627\n",
      "Training iter #7344000:   Batch Loss = 9.978958, Accuracy = 0.5146666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.628118515014648, Accuracy = 0.6357951164245605\n",
      "Training iter #7347000:   Batch Loss = 9.098732, Accuracy = 0.7979999780654907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.627191543579102, Accuracy = 0.6357951164245605\n",
      "Training iter #7350000:   Batch Loss = 9.136120, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.626243591308594, Accuracy = 0.6357951164245605\n",
      "Training iter #7353000:   Batch Loss = 9.475031, Accuracy = 0.6826666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.625252723693848, Accuracy = 0.6363906860351562\n",
      "Training iter #7356000:   Batch Loss = 9.720881, Accuracy = 0.5793333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.624335289001465, Accuracy = 0.6366885304450989\n",
      "Training iter #7359000:   Batch Loss = 9.668441, Accuracy = 0.6006666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.623382568359375, Accuracy = 0.6357951164245605\n",
      "Training iter #7362000:   Batch Loss = 9.153292, Accuracy = 0.7786666750907898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.622407913208008, Accuracy = 0.6354973316192627\n",
      "Training iter #7365000:   Batch Loss = 9.361501, Accuracy = 0.7153333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.621366500854492, Accuracy = 0.6357951164245605\n",
      "Training iter #7368000:   Batch Loss = 9.596939, Accuracy = 0.6306666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.620381355285645, Accuracy = 0.6354973316192627\n",
      "Training iter #7371000:   Batch Loss = 9.913042, Accuracy = 0.5406666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.619421005249023, Accuracy = 0.6366885304450989\n",
      "Training iter #7374000:   Batch Loss = 9.111763, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.618419647216797, Accuracy = 0.6369863152503967\n",
      "Training iter #7377000:   Batch Loss = 9.226157, Accuracy = 0.7586666941642761\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.617355346679688, Accuracy = 0.6372841000556946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7380000:   Batch Loss = 9.389204, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.616220474243164, Accuracy = 0.6369863152503967\n",
      "Training iter #7383000:   Batch Loss = 9.748460, Accuracy = 0.5706666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.61523151397705, Accuracy = 0.6375818848609924\n",
      "Training iter #7386000:   Batch Loss = 9.712942, Accuracy = 0.5926666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.614287376403809, Accuracy = 0.6372841000556946\n",
      "Training iter #7389000:   Batch Loss = 9.073915, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.613070487976074, Accuracy = 0.6372841000556946\n",
      "Training iter #7392000:   Batch Loss = 9.336637, Accuracy = 0.734666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.612016677856445, Accuracy = 0.6378796696662903\n",
      "Training iter #7395000:   Batch Loss = 9.644520, Accuracy = 0.6200000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.610970497131348, Accuracy = 0.6378796696662903\n",
      "Training iter #7398000:   Batch Loss = 9.817921, Accuracy = 0.5580000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.60995864868164, Accuracy = 0.6387730836868286\n",
      "Training iter #7401000:   Batch Loss = 9.095983, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.60893726348877, Accuracy = 0.6390708684921265\n",
      "Training iter #7404000:   Batch Loss = 9.262067, Accuracy = 0.7353333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.607867240905762, Accuracy = 0.6393686532974243\n",
      "Training iter #7407000:   Batch Loss = 9.283859, Accuracy = 0.722000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.606752395629883, Accuracy = 0.6396664977073669\n",
      "Training iter #7410000:   Batch Loss = 9.712287, Accuracy = 0.5873333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.605792999267578, Accuracy = 0.6393686532974243\n",
      "Training iter #7413000:   Batch Loss = 9.687565, Accuracy = 0.6053333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.604939460754395, Accuracy = 0.6396664977073669\n",
      "Training iter #7416000:   Batch Loss = 9.136803, Accuracy = 0.7913333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.604068756103516, Accuracy = 0.6393686532974243\n",
      "Training iter #7419000:   Batch Loss = 9.338858, Accuracy = 0.7239999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.603397369384766, Accuracy = 0.6390708684921265\n",
      "Training iter #7422000:   Batch Loss = 9.660772, Accuracy = 0.6046666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.602529525756836, Accuracy = 0.6393686532974243\n",
      "Training iter #7425000:   Batch Loss = 9.848249, Accuracy = 0.5373333096504211\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.601571083068848, Accuracy = 0.6399642825126648\n",
      "Training iter #7428000:   Batch Loss = 9.101414, Accuracy = 0.7879999876022339\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.600621223449707, Accuracy = 0.6402620673179626\n",
      "Training iter #7431000:   Batch Loss = 9.250292, Accuracy = 0.7486666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.599552154541016, Accuracy = 0.6402620673179626\n",
      "Training iter #7434000:   Batch Loss = 9.263827, Accuracy = 0.7326666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.598423957824707, Accuracy = 0.6408576369285583\n",
      "Training iter #7437000:   Batch Loss = 9.705141, Accuracy = 0.5933333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.597430229187012, Accuracy = 0.6408576369285583\n",
      "Training iter #7440000:   Batch Loss = 9.526194, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.59644889831543, Accuracy = 0.6411554217338562\n",
      "Training iter #7443000:   Batch Loss = 9.100383, Accuracy = 0.7973333597183228\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.595418930053711, Accuracy = 0.6414532661437988\n",
      "Training iter #7446000:   Batch Loss = 9.406084, Accuracy = 0.7073333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.594306945800781, Accuracy = 0.6420488357543945\n",
      "Training iter #7449000:   Batch Loss = 9.707164, Accuracy = 0.5879999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.593219757080078, Accuracy = 0.6420488357543945\n",
      "Training iter #7452000:   Batch Loss = 9.801988, Accuracy = 0.5526666641235352\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.592263221740723, Accuracy = 0.6420488357543945\n",
      "Training iter #7455000:   Batch Loss = 9.105736, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.5913667678833, Accuracy = 0.6420488357543945\n",
      "Training iter #7458000:   Batch Loss = 9.271594, Accuracy = 0.7446666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.59036922454834, Accuracy = 0.6423466205596924\n",
      "Training iter #7461000:   Batch Loss = 9.214460, Accuracy = 0.7426666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.589390754699707, Accuracy = 0.6420488357543945\n",
      "Training iter #7464000:   Batch Loss = 9.698820, Accuracy = 0.5933333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.588666915893555, Accuracy = 0.6432400345802307\n",
      "Training iter #7467000:   Batch Loss = 9.499223, Accuracy = 0.6646666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.587759017944336, Accuracy = 0.6429422497749329\n",
      "Training iter #7470000:   Batch Loss = 9.048354, Accuracy = 0.812666654586792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.586719512939453, Accuracy = 0.6447290182113647\n",
      "Training iter #7473000:   Batch Loss = 9.399196, Accuracy = 0.7080000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.58569049835205, Accuracy = 0.6450268030166626\n",
      "Training iter #7476000:   Batch Loss = 9.738784, Accuracy = 0.5786666870117188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.584699630737305, Accuracy = 0.6450268030166626\n",
      "Training iter #7479000:   Batch Loss = 9.748372, Accuracy = 0.5659999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.583745002746582, Accuracy = 0.6447290182113647\n",
      "Training iter #7482000:   Batch Loss = 9.126418, Accuracy = 0.7739999890327454\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.5828218460083, Accuracy = 0.6453245878219604\n",
      "Training iter #7485000:   Batch Loss = 9.264182, Accuracy = 0.7433333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.58182144165039, Accuracy = 0.6450268030166626\n",
      "Training iter #7488000:   Batch Loss = 9.220215, Accuracy = 0.7426666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.580711364746094, Accuracy = 0.6444312334060669\n",
      "Training iter #7491000:   Batch Loss = 9.740570, Accuracy = 0.5846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.579463005065918, Accuracy = 0.6456223726272583\n",
      "Training iter #7494000:   Batch Loss = 9.452668, Accuracy = 0.6793333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.578169822692871, Accuracy = 0.6456223726272583\n",
      "Training iter #7497000:   Batch Loss = 9.084217, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.577190399169922, Accuracy = 0.6459202170372009\n",
      "Training iter #7500000:   Batch Loss = 9.395762, Accuracy = 0.7059999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.576220512390137, Accuracy = 0.6462180018424988\n",
      "Training iter #7503000:   Batch Loss = 9.750049, Accuracy = 0.5680000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.575132369995117, Accuracy = 0.6459202170372009\n",
      "Training iter #7506000:   Batch Loss = 9.747086, Accuracy = 0.5720000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.574103355407715, Accuracy = 0.6459202170372009\n",
      "Training iter #7509000:   Batch Loss = 9.093434, Accuracy = 0.7786666750907898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.573090553283691, Accuracy = 0.6459202170372009\n",
      "Training iter #7512000:   Batch Loss = 9.239239, Accuracy = 0.7519999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.572015762329102, Accuracy = 0.6462180018424988\n",
      "Training iter #7515000:   Batch Loss = 9.192228, Accuracy = 0.7480000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.57088851928711, Accuracy = 0.6462180018424988\n",
      "Training iter #7518000:   Batch Loss = 9.769252, Accuracy = 0.5799999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.569820404052734, Accuracy = 0.6468135714530945\n",
      "Training iter #7521000:   Batch Loss = 9.443985, Accuracy = 0.6766666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.568721771240234, Accuracy = 0.6468135714530945\n",
      "Training iter #7524000:   Batch Loss = 9.081985, Accuracy = 0.7913333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.567633628845215, Accuracy = 0.6471113562583923\n",
      "Training iter #7527000:   Batch Loss = 9.339674, Accuracy = 0.722000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.566922187805176, Accuracy = 0.6471113562583923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7530000:   Batch Loss = 9.765791, Accuracy = 0.5693333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.565980911254883, Accuracy = 0.6468135714530945\n",
      "Training iter #7533000:   Batch Loss = 9.642882, Accuracy = 0.5839999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.564913749694824, Accuracy = 0.6477069854736328\n",
      "Training iter #7536000:   Batch Loss = 9.079697, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.564092636108398, Accuracy = 0.6477069854736328\n",
      "Training iter #7539000:   Batch Loss = 9.261004, Accuracy = 0.7459999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.56298828125, Accuracy = 0.6477069854736328\n",
      "Training iter #7542000:   Batch Loss = 9.272074, Accuracy = 0.7213333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.562173843383789, Accuracy = 0.6480047702789307\n",
      "Training iter #7545000:   Batch Loss = 9.856478, Accuracy = 0.5566666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.56131649017334, Accuracy = 0.6477069854736328\n",
      "Training iter #7548000:   Batch Loss = 9.367921, Accuracy = 0.7053333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.560515403747559, Accuracy = 0.6483025550842285\n",
      "Training iter #7551000:   Batch Loss = 9.007004, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.559646606445312, Accuracy = 0.6488981246948242\n",
      "Training iter #7554000:   Batch Loss = 9.329836, Accuracy = 0.7293333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.558688163757324, Accuracy = 0.6494937539100647\n",
      "Training iter #7557000:   Batch Loss = 9.759829, Accuracy = 0.5713333487510681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.557635307312012, Accuracy = 0.6491959691047668\n",
      "Training iter #7560000:   Batch Loss = 9.596861, Accuracy = 0.6013333201408386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.556476593017578, Accuracy = 0.6497915387153625\n",
      "Training iter #7563000:   Batch Loss = 9.044447, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.555495262145996, Accuracy = 0.6500893235206604\n",
      "Training iter #7566000:   Batch Loss = 9.225255, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.554574012756348, Accuracy = 0.6500893235206604\n",
      "Training iter #7569000:   Batch Loss = 9.283323, Accuracy = 0.718666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.553690910339355, Accuracy = 0.6503871083259583\n",
      "Training iter #7572000:   Batch Loss = 9.893909, Accuracy = 0.5360000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.552830696105957, Accuracy = 0.6506849527359009\n",
      "Training iter #7575000:   Batch Loss = 9.291059, Accuracy = 0.7253333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.55186939239502, Accuracy = 0.6509827375411987\n",
      "Training iter #7578000:   Batch Loss = 9.091640, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.550908088684082, Accuracy = 0.6512805223464966\n",
      "Training iter #7581000:   Batch Loss = 9.415739, Accuracy = 0.7120000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.549989700317383, Accuracy = 0.6512805223464966\n",
      "Training iter #7584000:   Batch Loss = 9.702877, Accuracy = 0.5893333554267883\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.549057960510254, Accuracy = 0.6512805223464966\n",
      "Training iter #7587000:   Batch Loss = 9.512501, Accuracy = 0.625333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.54786205291748, Accuracy = 0.6515783071517944\n",
      "Training iter #7590000:   Batch Loss = 9.024586, Accuracy = 0.8059999942779541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.546819686889648, Accuracy = 0.6515783071517944\n",
      "Training iter #7593000:   Batch Loss = 9.205933, Accuracy = 0.7480000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.545792579650879, Accuracy = 0.6524717211723328\n",
      "Training iter #7596000:   Batch Loss = 9.300725, Accuracy = 0.6940000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.544790267944336, Accuracy = 0.6521739363670349\n",
      "Training iter #7599000:   Batch Loss = 9.941193, Accuracy = 0.518666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.543716430664062, Accuracy = 0.6521739363670349\n",
      "Training iter #7602000:   Batch Loss = 9.246396, Accuracy = 0.737333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.542715072631836, Accuracy = 0.6527695059776306\n",
      "Training iter #7605000:   Batch Loss = 9.056344, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.541793823242188, Accuracy = 0.6527695059776306\n",
      "Training iter #7608000:   Batch Loss = 9.385466, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.540854454040527, Accuracy = 0.6527695059776306\n",
      "Training iter #7611000:   Batch Loss = 9.686417, Accuracy = 0.5960000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.539918899536133, Accuracy = 0.6530672907829285\n",
      "Training iter #7614000:   Batch Loss = 9.576498, Accuracy = 0.6106666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.539182662963867, Accuracy = 0.6530672907829285\n",
      "Training iter #7617000:   Batch Loss = 9.057036, Accuracy = 0.7960000038146973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.538179397583008, Accuracy = 0.6530672907829285\n",
      "Training iter #7620000:   Batch Loss = 9.203382, Accuracy = 0.7540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.537184715270996, Accuracy = 0.6530672907829285\n",
      "Training iter #7623000:   Batch Loss = 9.387716, Accuracy = 0.6759999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.536232948303223, Accuracy = 0.6539607048034668\n",
      "Training iter #7626000:   Batch Loss = 9.842516, Accuracy = 0.5413333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.535234451293945, Accuracy = 0.6533650755882263\n",
      "Training iter #7629000:   Batch Loss = 9.157545, Accuracy = 0.7646666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.534213066101074, Accuracy = 0.6539607048034668\n",
      "Training iter #7632000:   Batch Loss = 9.034107, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.53318977355957, Accuracy = 0.653662919998169\n",
      "Training iter #7635000:   Batch Loss = 9.384180, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.532279014587402, Accuracy = 0.6539607048034668\n",
      "Training iter #7638000:   Batch Loss = 9.606586, Accuracy = 0.6173333525657654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.531299591064453, Accuracy = 0.6542584896087646\n",
      "Training iter #7641000:   Batch Loss = 9.567585, Accuracy = 0.6066666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.530290603637695, Accuracy = 0.655151903629303\n",
      "Training iter #7644000:   Batch Loss = 9.077613, Accuracy = 0.7853333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.529303550720215, Accuracy = 0.655151903629303\n",
      "Training iter #7647000:   Batch Loss = 9.257081, Accuracy = 0.7406666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.528346061706543, Accuracy = 0.6554496884346008\n",
      "Training iter #7650000:   Batch Loss = 9.374233, Accuracy = 0.690666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.52737045288086, Accuracy = 0.6554496884346008\n",
      "Training iter #7653000:   Batch Loss = 9.862805, Accuracy = 0.5360000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.526337623596191, Accuracy = 0.6557474732398987\n",
      "Training iter #7656000:   Batch Loss = 9.064236, Accuracy = 0.7979999780654907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.525373458862305, Accuracy = 0.6560452580451965\n",
      "Training iter #7659000:   Batch Loss = 9.028929, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.52441120147705, Accuracy = 0.6560452580451965\n",
      "Training iter #7662000:   Batch Loss = 9.382454, Accuracy = 0.7006666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.523412704467773, Accuracy = 0.6554496884346008\n",
      "Training iter #7665000:   Batch Loss = 9.598207, Accuracy = 0.6159999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.522414207458496, Accuracy = 0.6557474732398987\n",
      "Training iter #7668000:   Batch Loss = 9.573990, Accuracy = 0.612666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.521371841430664, Accuracy = 0.6560452580451965\n",
      "Training iter #7671000:   Batch Loss = 9.114107, Accuracy = 0.7860000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.520307540893555, Accuracy = 0.6560452580451965\n",
      "Training iter #7674000:   Batch Loss = 9.256216, Accuracy = 0.7379999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.519404411315918, Accuracy = 0.655151903629303\n",
      "Training iter #7677000:   Batch Loss = 9.446678, Accuracy = 0.6673333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.518753051757812, Accuracy = 0.655151903629303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7680000:   Batch Loss = 9.846241, Accuracy = 0.5433333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.517746925354004, Accuracy = 0.6557474732398987\n",
      "Training iter #7683000:   Batch Loss = 8.969332, Accuracy = 0.8220000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.516818046569824, Accuracy = 0.6560452580451965\n",
      "Training iter #7686000:   Batch Loss = 9.058004, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.515741348266602, Accuracy = 0.6557474732398987\n",
      "Training iter #7689000:   Batch Loss = 9.326734, Accuracy = 0.7200000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.514726638793945, Accuracy = 0.6554496884346008\n",
      "Training iter #7692000:   Batch Loss = 9.599918, Accuracy = 0.6060000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.513805389404297, Accuracy = 0.6557474732398987\n",
      "Training iter #7695000:   Batch Loss = 9.614838, Accuracy = 0.5993333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.51296329498291, Accuracy = 0.6557474732398987\n",
      "Training iter #7698000:   Batch Loss = 8.994191, Accuracy = 0.8100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.512090682983398, Accuracy = 0.6560452580451965\n",
      "Training iter #7701000:   Batch Loss = 9.200895, Accuracy = 0.7459999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.511214256286621, Accuracy = 0.6557474732398987\n",
      "Training iter #7704000:   Batch Loss = 9.501683, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.510245323181152, Accuracy = 0.6563430428504944\n",
      "Training iter #7707000:   Batch Loss = 9.762297, Accuracy = 0.5686666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.509224891662598, Accuracy = 0.6560452580451965\n",
      "Training iter #7710000:   Batch Loss = 8.999617, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.508177757263184, Accuracy = 0.656640887260437\n",
      "Training iter #7713000:   Batch Loss = 9.136124, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.507246971130371, Accuracy = 0.6572364568710327\n",
      "Training iter #7716000:   Batch Loss = 9.218155, Accuracy = 0.734666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.506522178649902, Accuracy = 0.6569386720657349\n",
      "Training iter #7719000:   Batch Loss = 9.634920, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.50562572479248, Accuracy = 0.6572364568710327\n",
      "Training iter #7722000:   Batch Loss = 9.643539, Accuracy = 0.6106666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.504607200622559, Accuracy = 0.6572364568710327\n",
      "Training iter #7725000:   Batch Loss = 8.968844, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.503622055053711, Accuracy = 0.6575342416763306\n",
      "Training iter #7728000:   Batch Loss = 9.257262, Accuracy = 0.7406666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.502667427062988, Accuracy = 0.6575342416763306\n",
      "Training iter #7731000:   Batch Loss = 9.551933, Accuracy = 0.6380000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.501812934875488, Accuracy = 0.6578320264816284\n",
      "Training iter #7734000:   Batch Loss = 9.697493, Accuracy = 0.5733333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.500955581665039, Accuracy = 0.6584276556968689\n",
      "Training iter #7737000:   Batch Loss = 8.977023, Accuracy = 0.809333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.50010871887207, Accuracy = 0.6584276556968689\n",
      "Training iter #7740000:   Batch Loss = 9.123164, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.499242782592773, Accuracy = 0.6593210101127625\n",
      "Training iter #7743000:   Batch Loss = 9.207598, Accuracy = 0.7459999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.498391151428223, Accuracy = 0.6593210101127625\n",
      "Training iter #7746000:   Batch Loss = 9.578834, Accuracy = 0.6259999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.497514724731445, Accuracy = 0.6596187949180603\n",
      "Training iter #7749000:   Batch Loss = 9.515828, Accuracy = 0.6493333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.49649715423584, Accuracy = 0.6602144241333008\n",
      "Training iter #7752000:   Batch Loss = 9.044415, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.495532989501953, Accuracy = 0.6599166393280029\n",
      "Training iter #7755000:   Batch Loss = 9.232195, Accuracy = 0.7433333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.494722366333008, Accuracy = 0.6602144241333008\n",
      "Training iter #7758000:   Batch Loss = 9.585258, Accuracy = 0.6233333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.49388599395752, Accuracy = 0.6602144241333008\n",
      "Training iter #7761000:   Batch Loss = 9.710741, Accuracy = 0.5653333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.492934226989746, Accuracy = 0.6605122089385986\n",
      "Training iter #7764000:   Batch Loss = 9.020452, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.492012023925781, Accuracy = 0.6605122089385986\n",
      "Training iter #7767000:   Batch Loss = 9.153181, Accuracy = 0.7620000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.49111557006836, Accuracy = 0.6605122089385986\n",
      "Training iter #7770000:   Batch Loss = 9.144737, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.490215301513672, Accuracy = 0.6605122089385986\n",
      "Training iter #7773000:   Batch Loss = 9.577955, Accuracy = 0.621999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.489334106445312, Accuracy = 0.6599166393280029\n",
      "Training iter #7776000:   Batch Loss = 9.440589, Accuracy = 0.6713333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.488351821899414, Accuracy = 0.6605122089385986\n",
      "Training iter #7779000:   Batch Loss = 8.987539, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.48737907409668, Accuracy = 0.6602144241333008\n",
      "Training iter #7782000:   Batch Loss = 9.330566, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.486382484436035, Accuracy = 0.6602144241333008\n",
      "Training iter #7785000:   Batch Loss = 9.607770, Accuracy = 0.6159999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.48548698425293, Accuracy = 0.6599166393280029\n",
      "Training iter #7788000:   Batch Loss = 9.668604, Accuracy = 0.5686666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.484604835510254, Accuracy = 0.6602144241333008\n",
      "Training iter #7791000:   Batch Loss = 8.988290, Accuracy = 0.8033333420753479\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.483705520629883, Accuracy = 0.6608099937438965\n",
      "Training iter #7794000:   Batch Loss = 9.169088, Accuracy = 0.7559999823570251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.482732772827148, Accuracy = 0.6608099937438965\n",
      "Training iter #7797000:   Batch Loss = 9.118008, Accuracy = 0.7580000162124634\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.481793403625488, Accuracy = 0.661405622959137\n",
      "Training iter #7800000:   Batch Loss = 9.585662, Accuracy = 0.6186666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.481000900268555, Accuracy = 0.6608099937438965\n",
      "Training iter #7803000:   Batch Loss = 9.406738, Accuracy = 0.6880000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.480116844177246, Accuracy = 0.6608099937438965\n",
      "Training iter #7806000:   Batch Loss = 8.974329, Accuracy = 0.8193333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.479204177856445, Accuracy = 0.6611077785491943\n",
      "Training iter #7809000:   Batch Loss = 9.276035, Accuracy = 0.7260000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.478293418884277, Accuracy = 0.661405622959137\n",
      "Training iter #7812000:   Batch Loss = 9.612643, Accuracy = 0.612666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.477396011352539, Accuracy = 0.6608099937438965\n",
      "Training iter #7815000:   Batch Loss = 9.624729, Accuracy = 0.5826666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.476533889770508, Accuracy = 0.6611077785491943\n",
      "Training iter #7818000:   Batch Loss = 9.030351, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.475570678710938, Accuracy = 0.661405622959137\n",
      "Training iter #7821000:   Batch Loss = 9.145158, Accuracy = 0.765999972820282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.474642753601074, Accuracy = 0.661405622959137\n",
      "Training iter #7824000:   Batch Loss = 9.072972, Accuracy = 0.7753333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.473737716674805, Accuracy = 0.661405622959137\n",
      "Training iter #7827000:   Batch Loss = 9.658819, Accuracy = 0.5993333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.472846031188965, Accuracy = 0.6611077785491943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7830000:   Batch Loss = 9.319755, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.47194766998291, Accuracy = 0.6611077785491943\n",
      "Training iter #7833000:   Batch Loss = 8.991697, Accuracy = 0.8073333501815796\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.471012115478516, Accuracy = 0.6611077785491943\n",
      "Training iter #7836000:   Batch Loss = 9.289021, Accuracy = 0.7260000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.470061302185059, Accuracy = 0.661405622959137\n",
      "Training iter #7839000:   Batch Loss = 9.665761, Accuracy = 0.5953333377838135\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.469124794006348, Accuracy = 0.661405622959137\n",
      "Training iter #7842000:   Batch Loss = 9.608663, Accuracy = 0.5899999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.468245506286621, Accuracy = 0.661405622959137\n",
      "Training iter #7845000:   Batch Loss = 8.941152, Accuracy = 0.8199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.467376708984375, Accuracy = 0.6620011925697327\n",
      "Training iter #7848000:   Batch Loss = 9.165350, Accuracy = 0.7573333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.466476440429688, Accuracy = 0.6620011925697327\n",
      "Training iter #7851000:   Batch Loss = 9.144279, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.465630531311035, Accuracy = 0.6620011925697327\n",
      "Training iter #7854000:   Batch Loss = 9.677439, Accuracy = 0.5960000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.46473503112793, Accuracy = 0.6620011925697327\n",
      "Training iter #7857000:   Batch Loss = 9.326159, Accuracy = 0.7113333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.463810920715332, Accuracy = 0.6622989773750305\n",
      "Training iter #7860000:   Batch Loss = 8.949233, Accuracy = 0.8173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.462889671325684, Accuracy = 0.662894606590271\n",
      "Training iter #7863000:   Batch Loss = 9.242000, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.461952209472656, Accuracy = 0.662894606590271\n",
      "Training iter #7866000:   Batch Loss = 9.648890, Accuracy = 0.5960000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.461076736450195, Accuracy = 0.662894606590271\n",
      "Training iter #7869000:   Batch Loss = 9.509213, Accuracy = 0.6140000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.460227966308594, Accuracy = 0.662894606590271\n",
      "Training iter #7872000:   Batch Loss = 8.976205, Accuracy = 0.8059999942779541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.459309577941895, Accuracy = 0.6634901762008667\n",
      "Training iter #7875000:   Batch Loss = 9.186713, Accuracy = 0.7480000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.45828914642334, Accuracy = 0.6634901762008667\n",
      "Training iter #7878000:   Batch Loss = 9.201103, Accuracy = 0.7273333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.457426071166992, Accuracy = 0.6640857458114624\n",
      "Training iter #7881000:   Batch Loss = 9.725978, Accuracy = 0.5759999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.456582069396973, Accuracy = 0.6640857458114624\n",
      "Training iter #7884000:   Batch Loss = 9.233858, Accuracy = 0.734666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.455743789672852, Accuracy = 0.6640857458114624\n",
      "Training iter #7887000:   Batch Loss = 8.975073, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.45480728149414, Accuracy = 0.6649791598320007\n",
      "Training iter #7890000:   Batch Loss = 9.245471, Accuracy = 0.734666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.453886032104492, Accuracy = 0.6655747294425964\n",
      "Training iter #7893000:   Batch Loss = 9.639330, Accuracy = 0.596666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.453004837036133, Accuracy = 0.6661703586578369\n",
      "Training iter #7896000:   Batch Loss = 9.478553, Accuracy = 0.6259999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.452109336853027, Accuracy = 0.6655747294425964\n",
      "Training iter #7899000:   Batch Loss = 8.910996, Accuracy = 0.8233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.451200485229492, Accuracy = 0.6652769446372986\n",
      "Training iter #7902000:   Batch Loss = 9.113398, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.450218200683594, Accuracy = 0.6658725142478943\n",
      "Training iter #7905000:   Batch Loss = 9.184164, Accuracy = 0.7286666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.449305534362793, Accuracy = 0.6661703586578369\n",
      "Training iter #7908000:   Batch Loss = 9.819817, Accuracy = 0.5406666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.448359489440918, Accuracy = 0.6664681434631348\n",
      "Training iter #7911000:   Batch Loss = 9.164103, Accuracy = 0.7620000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.447412490844727, Accuracy = 0.6664681434631348\n",
      "Training iter #7914000:   Batch Loss = 8.956398, Accuracy = 0.8086666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.446499824523926, Accuracy = 0.6667659282684326\n",
      "Training iter #7917000:   Batch Loss = 9.318999, Accuracy = 0.731333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.445581436157227, Accuracy = 0.6670637130737305\n",
      "Training iter #7920000:   Batch Loss = 9.570689, Accuracy = 0.6193333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.444684982299805, Accuracy = 0.667659342288971\n",
      "Training iter #7923000:   Batch Loss = 9.416304, Accuracy = 0.6473333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.443737983703613, Accuracy = 0.6682549118995667\n",
      "Training iter #7926000:   Batch Loss = 8.938900, Accuracy = 0.8159999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.442822456359863, Accuracy = 0.6685526967048645\n",
      "Training iter #7929000:   Batch Loss = 9.127461, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.441926002502441, Accuracy = 0.6685526967048645\n",
      "Training iter #7932000:   Batch Loss = 9.236741, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.441086769104004, Accuracy = 0.6688504815101624\n",
      "Training iter #7935000:   Batch Loss = 9.760095, Accuracy = 0.5513333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.440162658691406, Accuracy = 0.6688504815101624\n",
      "Training iter #7938000:   Batch Loss = 9.089947, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.439236640930176, Accuracy = 0.6682549118995667\n",
      "Training iter #7941000:   Batch Loss = 8.966399, Accuracy = 0.8066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.438365936279297, Accuracy = 0.6685526967048645\n",
      "Training iter #7944000:   Batch Loss = 9.282146, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.437490463256836, Accuracy = 0.6688504815101624\n",
      "Training iter #7947000:   Batch Loss = 9.528421, Accuracy = 0.6306666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.436620712280273, Accuracy = 0.669148325920105\n",
      "Training iter #7950000:   Batch Loss = 9.474358, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.435686111450195, Accuracy = 0.6688504815101624\n",
      "Training iter #7953000:   Batch Loss = 8.974782, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.434782028198242, Accuracy = 0.6697438955307007\n",
      "Training iter #7956000:   Batch Loss = 9.131286, Accuracy = 0.7639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.433881759643555, Accuracy = 0.6700416803359985\n",
      "Training iter #7959000:   Batch Loss = 9.295777, Accuracy = 0.6980000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.433032035827637, Accuracy = 0.6703394651412964\n",
      "Training iter #7962000:   Batch Loss = 9.780192, Accuracy = 0.5433333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.432098388671875, Accuracy = 0.6700416803359985\n",
      "Training iter #7965000:   Batch Loss = 9.017993, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.431161880493164, Accuracy = 0.6697438955307007\n",
      "Training iter #7968000:   Batch Loss = 8.926210, Accuracy = 0.8173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.430246353149414, Accuracy = 0.6709350943565369\n",
      "Training iter #7971000:   Batch Loss = 9.276723, Accuracy = 0.7160000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.429302215576172, Accuracy = 0.6712328791618347\n",
      "Training iter #7974000:   Batch Loss = 9.495135, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.428457260131836, Accuracy = 0.6709350943565369\n",
      "Training iter #7977000:   Batch Loss = 9.463711, Accuracy = 0.6353333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.42760944366455, Accuracy = 0.6709350943565369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7980000:   Batch Loss = 8.979717, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.426767349243164, Accuracy = 0.6715306639671326\n",
      "Training iter #7983000:   Batch Loss = 9.143915, Accuracy = 0.7573333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.425870895385742, Accuracy = 0.672126293182373\n",
      "Training iter #7986000:   Batch Loss = 9.310543, Accuracy = 0.6986666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.425004005432129, Accuracy = 0.672126293182373\n",
      "Training iter #7989000:   Batch Loss = 9.739430, Accuracy = 0.5586666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.424079895019531, Accuracy = 0.6718284487724304\n",
      "Training iter #7992000:   Batch Loss = 8.907590, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.423232078552246, Accuracy = 0.6715306639671326\n",
      "Training iter #7995000:   Batch Loss = 8.914425, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.422379493713379, Accuracy = 0.6715306639671326\n",
      "Training iter #7998000:   Batch Loss = 9.275887, Accuracy = 0.7179999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.421462059020996, Accuracy = 0.6715306639671326\n",
      "Training iter #8001000:   Batch Loss = 9.466195, Accuracy = 0.6393333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.420622825622559, Accuracy = 0.672126293182373\n",
      "Training iter #8004000:   Batch Loss = 9.443880, Accuracy = 0.6466666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.4197998046875, Accuracy = 0.6718284487724304\n",
      "Training iter #8007000:   Batch Loss = 8.985620, Accuracy = 0.8080000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.41889762878418, Accuracy = 0.6730196475982666\n",
      "Training iter #8010000:   Batch Loss = 9.156009, Accuracy = 0.7473333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.418050765991211, Accuracy = 0.6733174324035645\n",
      "Training iter #8013000:   Batch Loss = 9.385828, Accuracy = 0.6753333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.417259216308594, Accuracy = 0.6733174324035645\n",
      "Training iter #8016000:   Batch Loss = 9.697843, Accuracy = 0.5826666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.416372299194336, Accuracy = 0.6730196475982666\n",
      "Training iter #8019000:   Batch Loss = 8.904238, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.41550064086914, Accuracy = 0.6736152768135071\n",
      "Training iter #8022000:   Batch Loss = 8.996149, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.414626121520996, Accuracy = 0.6736152768135071\n",
      "Training iter #8025000:   Batch Loss = 9.203669, Accuracy = 0.7379999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.413723945617676, Accuracy = 0.6748064160346985\n",
      "Training iter #8028000:   Batch Loss = 9.503559, Accuracy = 0.6206666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.412863731384277, Accuracy = 0.6745086312294006\n",
      "Training iter #8031000:   Batch Loss = 9.509136, Accuracy = 0.6286666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.411998748779297, Accuracy = 0.6742108464241028\n",
      "Training iter #8034000:   Batch Loss = 8.899770, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.411097526550293, Accuracy = 0.6742108464241028\n",
      "Training iter #8037000:   Batch Loss = 9.139696, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.410235404968262, Accuracy = 0.6748064160346985\n",
      "Training iter #8040000:   Batch Loss = 9.412021, Accuracy = 0.6673333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.40939998626709, Accuracy = 0.6751042008399963\n",
      "Training iter #8043000:   Batch Loss = 9.613956, Accuracy = 0.5946666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.408510208129883, Accuracy = 0.6745086312294006\n",
      "Training iter #8046000:   Batch Loss = 8.847016, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.407574653625488, Accuracy = 0.6745086312294006\n",
      "Training iter #8049000:   Batch Loss = 9.065959, Accuracy = 0.7639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.406560897827148, Accuracy = 0.6748064160346985\n",
      "Training iter #8052000:   Batch Loss = 9.087245, Accuracy = 0.765999972820282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.405631065368652, Accuracy = 0.675402045249939\n",
      "Training iter #8055000:   Batch Loss = 9.506800, Accuracy = 0.6293333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.404753684997559, Accuracy = 0.6751042008399963\n",
      "Training iter #8058000:   Batch Loss = 9.502512, Accuracy = 0.6413333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.403908729553223, Accuracy = 0.6748064160346985\n",
      "Training iter #8061000:   Batch Loss = 8.938353, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.403075218200684, Accuracy = 0.6751042008399963\n",
      "Training iter #8064000:   Batch Loss = 9.130936, Accuracy = 0.7573333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.402268409729004, Accuracy = 0.6756998300552368\n",
      "Training iter #8067000:   Batch Loss = 9.435136, Accuracy = 0.6480000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.401558876037598, Accuracy = 0.6756998300552368\n",
      "Training iter #8070000:   Batch Loss = 9.634735, Accuracy = 0.5773333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.400700569152832, Accuracy = 0.675402045249939\n",
      "Training iter #8073000:   Batch Loss = 8.863253, Accuracy = 0.8306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.399866104125977, Accuracy = 0.6751042008399963\n",
      "Training iter #8076000:   Batch Loss = 9.051682, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.39901351928711, Accuracy = 0.675402045249939\n",
      "Training iter #8079000:   Batch Loss = 9.086068, Accuracy = 0.7680000066757202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.398080825805664, Accuracy = 0.6756998300552368\n",
      "Training iter #8082000:   Batch Loss = 9.473120, Accuracy = 0.640666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.397233963012695, Accuracy = 0.6756998300552368\n",
      "Training iter #8085000:   Batch Loss = 9.359421, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.396368026733398, Accuracy = 0.6751042008399963\n",
      "Training iter #8088000:   Batch Loss = 8.912345, Accuracy = 0.8273333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.395302772521973, Accuracy = 0.675402045249939\n",
      "Training iter #8091000:   Batch Loss = 9.201524, Accuracy = 0.7386666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.394123077392578, Accuracy = 0.6762953996658325\n",
      "Training iter #8094000:   Batch Loss = 9.505248, Accuracy = 0.624666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.393182754516602, Accuracy = 0.676891028881073\n",
      "Training iter #8097000:   Batch Loss = 9.556783, Accuracy = 0.5933333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.392281532287598, Accuracy = 0.6762953996658325\n",
      "Training iter #8100000:   Batch Loss = 8.868330, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.39146614074707, Accuracy = 0.6765931844711304\n",
      "Training iter #8103000:   Batch Loss = 9.057770, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.390643119812012, Accuracy = 0.676891028881073\n",
      "Training iter #8106000:   Batch Loss = 9.021511, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.389713287353516, Accuracy = 0.6774865984916687\n",
      "Training iter #8109000:   Batch Loss = 9.483193, Accuracy = 0.6359999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.388705253601074, Accuracy = 0.6774865984916687\n",
      "Training iter #8112000:   Batch Loss = 9.306567, Accuracy = 0.7066666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.387773513793945, Accuracy = 0.6774865984916687\n",
      "Training iter #8115000:   Batch Loss = 8.895242, Accuracy = 0.8306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.386837005615234, Accuracy = 0.6771888136863708\n",
      "Training iter #8118000:   Batch Loss = 9.209019, Accuracy = 0.737333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.385982513427734, Accuracy = 0.6777843832969666\n",
      "Training iter #8121000:   Batch Loss = 9.534329, Accuracy = 0.6159999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.38509464263916, Accuracy = 0.6774865984916687\n",
      "Training iter #8124000:   Batch Loss = 9.531147, Accuracy = 0.5960000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.38426685333252, Accuracy = 0.6774865984916687\n",
      "Training iter #8127000:   Batch Loss = 8.905083, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.383574485778809, Accuracy = 0.6774865984916687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8130000:   Batch Loss = 9.064135, Accuracy = 0.7726666927337646\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.38292121887207, Accuracy = 0.6777843832969666\n",
      "Training iter #8133000:   Batch Loss = 9.042207, Accuracy = 0.7673333287239075\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.382283210754395, Accuracy = 0.6777843832969666\n",
      "Training iter #8136000:   Batch Loss = 9.509173, Accuracy = 0.6286666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.381612777709961, Accuracy = 0.6771888136863708\n",
      "Training iter #8139000:   Batch Loss = 9.281928, Accuracy = 0.7126666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.380792617797852, Accuracy = 0.6774865984916687\n",
      "Training iter #8142000:   Batch Loss = 8.879648, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.379926681518555, Accuracy = 0.6777843832969666\n",
      "Training iter #8145000:   Batch Loss = 9.198050, Accuracy = 0.737333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.379085540771484, Accuracy = 0.6780821681022644\n",
      "Training iter #8148000:   Batch Loss = 9.518917, Accuracy = 0.6200000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.378268241882324, Accuracy = 0.6780821681022644\n",
      "Training iter #8151000:   Batch Loss = 9.553724, Accuracy = 0.5953333377838135\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.37748908996582, Accuracy = 0.678380012512207\n",
      "Training iter #8154000:   Batch Loss = 8.876882, Accuracy = 0.8186666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.376577377319336, Accuracy = 0.678380012512207\n",
      "Training iter #8157000:   Batch Loss = 9.042391, Accuracy = 0.7746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.37564754486084, Accuracy = 0.678380012512207\n",
      "Training iter #8160000:   Batch Loss = 8.990211, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.374785423278809, Accuracy = 0.6786777973175049\n",
      "Training iter #8163000:   Batch Loss = 9.559225, Accuracy = 0.6153333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.373950004577637, Accuracy = 0.6786777973175049\n",
      "Training iter #8166000:   Batch Loss = 9.221232, Accuracy = 0.734000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.373038291931152, Accuracy = 0.6786777973175049\n",
      "Training iter #8169000:   Batch Loss = 8.877578, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.372057914733887, Accuracy = 0.6792733669281006\n",
      "Training iter #8172000:   Batch Loss = 9.155229, Accuracy = 0.7493333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.3711519241333, Accuracy = 0.6792733669281006\n",
      "Training iter #8175000:   Batch Loss = 9.530992, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.370291709899902, Accuracy = 0.6795711517333984\n",
      "Training iter #8178000:   Batch Loss = 9.466374, Accuracy = 0.6060000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.369405746459961, Accuracy = 0.6795711517333984\n",
      "Training iter #8181000:   Batch Loss = 8.837962, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.368505477905273, Accuracy = 0.6798689961433411\n",
      "Training iter #8184000:   Batch Loss = 9.063842, Accuracy = 0.7680000066757202\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.367592811584473, Accuracy = 0.6798689961433411\n",
      "Training iter #8187000:   Batch Loss = 9.074211, Accuracy = 0.7573333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.366740226745605, Accuracy = 0.6801667809486389\n",
      "Training iter #8190000:   Batch Loss = 9.618826, Accuracy = 0.5926666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.365912437438965, Accuracy = 0.6795711517333984\n",
      "Training iter #8193000:   Batch Loss = 9.180302, Accuracy = 0.7446666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.365079879760742, Accuracy = 0.6801667809486389\n",
      "Training iter #8196000:   Batch Loss = 8.818812, Accuracy = 0.8446666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.364245414733887, Accuracy = 0.6801667809486389\n",
      "Training iter #8199000:   Batch Loss = 9.145463, Accuracy = 0.7546666860580444\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.36340045928955, Accuracy = 0.6804645657539368\n",
      "Training iter #8202000:   Batch Loss = 9.559172, Accuracy = 0.6113333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.362539291381836, Accuracy = 0.6807623505592346\n",
      "Training iter #8205000:   Batch Loss = 9.354633, Accuracy = 0.6493333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.361712455749512, Accuracy = 0.6813579797744751\n",
      "Training iter #8208000:   Batch Loss = 8.857092, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.360843658447266, Accuracy = 0.6810601353645325\n",
      "Training iter #8211000:   Batch Loss = 9.035308, Accuracy = 0.7720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.359969139099121, Accuracy = 0.6819535493850708\n",
      "Training iter #8214000:   Batch Loss = 9.094332, Accuracy = 0.7513333559036255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.359193801879883, Accuracy = 0.6819535493850708\n",
      "Training iter #8217000:   Batch Loss = 9.629531, Accuracy = 0.5826666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.358440399169922, Accuracy = 0.6822513341903687\n",
      "Training iter #8220000:   Batch Loss = 9.099466, Accuracy = 0.768666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.357638359069824, Accuracy = 0.6819535493850708\n",
      "Training iter #8223000:   Batch Loss = 8.888085, Accuracy = 0.8199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.356830596923828, Accuracy = 0.6825491189956665\n",
      "Training iter #8226000:   Batch Loss = 9.171350, Accuracy = 0.7486666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.356025695800781, Accuracy = 0.6828469038009644\n",
      "Training iter #8229000:   Batch Loss = 9.516551, Accuracy = 0.621999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.355203628540039, Accuracy = 0.683144748210907\n",
      "Training iter #8232000:   Batch Loss = 9.338759, Accuracy = 0.656000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.35434341430664, Accuracy = 0.6834425330162048\n",
      "Training iter #8235000:   Batch Loss = 8.815318, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.353492736816406, Accuracy = 0.6834425330162048\n",
      "Training iter #8238000:   Batch Loss = 9.017475, Accuracy = 0.7739999890327454\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.352654457092285, Accuracy = 0.6834425330162048\n",
      "Training iter #8241000:   Batch Loss = 9.096157, Accuracy = 0.7360000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.351896286010742, Accuracy = 0.6834425330162048\n",
      "Training iter #8244000:   Batch Loss = 9.708398, Accuracy = 0.5519999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.351037979125977, Accuracy = 0.6837403178215027\n",
      "Training iter #8247000:   Batch Loss = 9.045395, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.350164413452148, Accuracy = 0.6837403178215027\n",
      "Training iter #8250000:   Batch Loss = 8.860805, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.349324226379395, Accuracy = 0.6834425330162048\n",
      "Training iter #8253000:   Batch Loss = 9.178731, Accuracy = 0.7553333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.348499298095703, Accuracy = 0.6834425330162048\n",
      "Training iter #8256000:   Batch Loss = 9.453304, Accuracy = 0.6453333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.347634315490723, Accuracy = 0.6834425330162048\n",
      "Training iter #8259000:   Batch Loss = 9.363794, Accuracy = 0.6579999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.346720695495605, Accuracy = 0.6834425330162048\n",
      "Training iter #8262000:   Batch Loss = 8.836813, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.345869064331055, Accuracy = 0.6837403178215027\n",
      "Training iter #8265000:   Batch Loss = 9.032259, Accuracy = 0.7746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.345026969909668, Accuracy = 0.6840381026268005\n",
      "Training iter #8268000:   Batch Loss = 9.188364, Accuracy = 0.7113333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.344234466552734, Accuracy = 0.6837403178215027\n",
      "Training iter #8271000:   Batch Loss = 9.607472, Accuracy = 0.5773333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.343381881713867, Accuracy = 0.6837403178215027\n",
      "Training iter #8274000:   Batch Loss = 8.964554, Accuracy = 0.7993333339691162\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.34259033203125, Accuracy = 0.6837403178215027\n",
      "Training iter #8277000:   Batch Loss = 8.850608, Accuracy = 0.8213333487510681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.341814041137695, Accuracy = 0.6837403178215027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8280000:   Batch Loss = 9.167376, Accuracy = 0.7540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.340970039367676, Accuracy = 0.6837403178215027\n",
      "Training iter #8283000:   Batch Loss = 9.407948, Accuracy = 0.6486666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.34012222290039, Accuracy = 0.6837403178215027\n",
      "Training iter #8286000:   Batch Loss = 9.349785, Accuracy = 0.6573333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.339254379272461, Accuracy = 0.6837403178215027\n",
      "Training iter #8289000:   Batch Loss = 8.869481, Accuracy = 0.8220000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.338403701782227, Accuracy = 0.6834425330162048\n",
      "Training iter #8292000:   Batch Loss = 9.071353, Accuracy = 0.7639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.337593078613281, Accuracy = 0.6840381026268005\n",
      "Training iter #8295000:   Batch Loss = 9.182416, Accuracy = 0.7213333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.33681583404541, Accuracy = 0.6837403178215027\n",
      "Training iter #8298000:   Batch Loss = 9.660949, Accuracy = 0.5706666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.336008071899414, Accuracy = 0.6840381026268005\n",
      "Training iter #8301000:   Batch Loss = 8.873548, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.335244178771973, Accuracy = 0.6840381026268005\n",
      "Training iter #8304000:   Batch Loss = 8.853594, Accuracy = 0.8213333487510681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.334507942199707, Accuracy = 0.6840381026268005\n",
      "Training iter #8307000:   Batch Loss = 9.200808, Accuracy = 0.7293333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.333707809448242, Accuracy = 0.6843358874320984\n",
      "Training iter #8310000:   Batch Loss = 9.376539, Accuracy = 0.653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.332889556884766, Accuracy = 0.6843358874320984\n",
      "Training iter #8313000:   Batch Loss = 9.363220, Accuracy = 0.6633333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.33203125, Accuracy = 0.6849315166473389\n",
      "Training iter #8316000:   Batch Loss = 8.890913, Accuracy = 0.8193333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.331198692321777, Accuracy = 0.6852293014526367\n",
      "Training iter #8319000:   Batch Loss = 9.067164, Accuracy = 0.762666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.330486297607422, Accuracy = 0.6849315166473389\n",
      "Training iter #8322000:   Batch Loss = 9.246437, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.329730987548828, Accuracy = 0.6849315166473389\n",
      "Training iter #8325000:   Batch Loss = 9.623956, Accuracy = 0.5806666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.328817367553711, Accuracy = 0.6849315166473389\n",
      "Training iter #8328000:   Batch Loss = 8.794440, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.327951431274414, Accuracy = 0.6849315166473389\n",
      "Training iter #8331000:   Batch Loss = 8.865096, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.327142715454102, Accuracy = 0.6855270862579346\n",
      "Training iter #8334000:   Batch Loss = 9.147420, Accuracy = 0.7473333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.326495170593262, Accuracy = 0.6858248710632324\n",
      "Training iter #8337000:   Batch Loss = 9.385068, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.325603485107422, Accuracy = 0.6858248710632324\n",
      "Training iter #8340000:   Batch Loss = 9.384399, Accuracy = 0.6553333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.324494361877441, Accuracy = 0.6858248710632324\n",
      "Training iter #8343000:   Batch Loss = 8.825236, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.323593139648438, Accuracy = 0.6864205002784729\n",
      "Training iter #8346000:   Batch Loss = 9.048933, Accuracy = 0.7646666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.32300853729248, Accuracy = 0.6864205002784729\n",
      "Training iter #8349000:   Batch Loss = 9.291820, Accuracy = 0.6913333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.322333335876465, Accuracy = 0.6864205002784729\n",
      "Training iter #8352000:   Batch Loss = 9.544022, Accuracy = 0.6053333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.321189880371094, Accuracy = 0.6864205002784729\n",
      "Training iter #8355000:   Batch Loss = 8.788864, Accuracy = 0.8399999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.32033920288086, Accuracy = 0.6867182850837708\n",
      "Training iter #8358000:   Batch Loss = 8.931078, Accuracy = 0.7846666574478149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.319531440734863, Accuracy = 0.6867182850837708\n",
      "Training iter #8361000:   Batch Loss = 9.053038, Accuracy = 0.7646666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.31895637512207, Accuracy = 0.6867182850837708\n",
      "Training iter #8364000:   Batch Loss = 9.403131, Accuracy = 0.6433333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.31795597076416, Accuracy = 0.6867182850837708\n",
      "Training iter #8367000:   Batch Loss = 9.414122, Accuracy = 0.653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.317055702209473, Accuracy = 0.6867182850837708\n",
      "Training iter #8370000:   Batch Loss = 8.812594, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.316218376159668, Accuracy = 0.6873138546943665\n",
      "Training iter #8373000:   Batch Loss = 9.051653, Accuracy = 0.7713333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.315485000610352, Accuracy = 0.6876116991043091\n",
      "Training iter #8376000:   Batch Loss = 9.343359, Accuracy = 0.6713333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.314762115478516, Accuracy = 0.6876116991043091\n",
      "Training iter #8379000:   Batch Loss = 9.466921, Accuracy = 0.6146666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.3139066696167, Accuracy = 0.6876116991043091\n",
      "Training iter #8382000:   Batch Loss = 8.764894, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.313194274902344, Accuracy = 0.6879094839096069\n",
      "Training iter #8385000:   Batch Loss = 8.939768, Accuracy = 0.7839999794960022\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.31246566772461, Accuracy = 0.6882072687149048\n",
      "Training iter #8388000:   Batch Loss = 9.030282, Accuracy = 0.7753333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.31180477142334, Accuracy = 0.6879094839096069\n",
      "Training iter #8391000:   Batch Loss = 9.365644, Accuracy = 0.6586666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.310985565185547, Accuracy = 0.6879094839096069\n",
      "Training iter #8394000:   Batch Loss = 9.361855, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.310175895690918, Accuracy = 0.6885050535202026\n",
      "Training iter #8397000:   Batch Loss = 8.850470, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.309366226196289, Accuracy = 0.6882072687149048\n",
      "Training iter #8400000:   Batch Loss = 9.022609, Accuracy = 0.7760000228881836\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.308587074279785, Accuracy = 0.6879094839096069\n",
      "Training iter #8403000:   Batch Loss = 9.378878, Accuracy = 0.6579999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.307764053344727, Accuracy = 0.6882072687149048\n",
      "Training iter #8406000:   Batch Loss = 9.510827, Accuracy = 0.5979999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.306930541992188, Accuracy = 0.6882072687149048\n",
      "Training iter #8409000:   Batch Loss = 8.793856, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.306218147277832, Accuracy = 0.6885050535202026\n",
      "Training iter #8412000:   Batch Loss = 8.959349, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.305508613586426, Accuracy = 0.6885050535202026\n",
      "Training iter #8415000:   Batch Loss = 8.950593, Accuracy = 0.7919999957084656\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.30469799041748, Accuracy = 0.6885050535202026\n",
      "Training iter #8418000:   Batch Loss = 9.371676, Accuracy = 0.6573333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.303435325622559, Accuracy = 0.6885050535202026\n",
      "Training iter #8421000:   Batch Loss = 9.250903, Accuracy = 0.7080000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.302196502685547, Accuracy = 0.6888028383255005\n",
      "Training iter #8424000:   Batch Loss = 8.815673, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.301090240478516, Accuracy = 0.689398467540741\n",
      "Training iter #8427000:   Batch Loss = 9.144589, Accuracy = 0.7440000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.300149917602539, Accuracy = 0.6896962523460388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8430000:   Batch Loss = 9.398243, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.299281120300293, Accuracy = 0.6899940371513367\n",
      "Training iter #8433000:   Batch Loss = 9.475277, Accuracy = 0.6060000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.298469543457031, Accuracy = 0.6899940371513367\n",
      "Training iter #8436000:   Batch Loss = 8.768166, Accuracy = 0.8413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.297735214233398, Accuracy = 0.6899940371513367\n",
      "Training iter #8439000:   Batch Loss = 8.954042, Accuracy = 0.7893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.29697036743164, Accuracy = 0.690887451171875\n",
      "Training iter #8442000:   Batch Loss = 8.936615, Accuracy = 0.7906666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.296154022216797, Accuracy = 0.690887451171875\n",
      "Training iter #8445000:   Batch Loss = 9.373218, Accuracy = 0.6539999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.295356750488281, Accuracy = 0.6911852359771729\n",
      "Training iter #8448000:   Batch Loss = 9.201279, Accuracy = 0.7279999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.294650077819824, Accuracy = 0.6914830207824707\n",
      "Training iter #8451000:   Batch Loss = 8.771152, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.293907165527344, Accuracy = 0.6911852359771729\n",
      "Training iter #8454000:   Batch Loss = 9.086741, Accuracy = 0.762666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.293164253234863, Accuracy = 0.6914830207824707\n",
      "Training iter #8457000:   Batch Loss = 9.407141, Accuracy = 0.6480000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.29239559173584, Accuracy = 0.6917808055877686\n",
      "Training iter #8460000:   Batch Loss = 9.412333, Accuracy = 0.6240000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.29167652130127, Accuracy = 0.692376434803009\n",
      "Training iter #8463000:   Batch Loss = 8.805797, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.290986061096191, Accuracy = 0.692376434803009\n",
      "Training iter #8466000:   Batch Loss = 8.958182, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.290251731872559, Accuracy = 0.6926742196083069\n",
      "Training iter #8469000:   Batch Loss = 8.900797, Accuracy = 0.8026666641235352\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.289525985717773, Accuracy = 0.6929720044136047\n",
      "Training iter #8472000:   Batch Loss = 9.424342, Accuracy = 0.6359999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.288792610168457, Accuracy = 0.6926742196083069\n",
      "Training iter #8475000:   Batch Loss = 9.139635, Accuracy = 0.7406666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.288121223449707, Accuracy = 0.6929720044136047\n",
      "Training iter #8478000:   Batch Loss = 8.795902, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.2874116897583, Accuracy = 0.6932697892189026\n",
      "Training iter #8481000:   Batch Loss = 9.097903, Accuracy = 0.7613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.28674602508545, Accuracy = 0.6932697892189026\n",
      "Training iter #8484000:   Batch Loss = 9.436795, Accuracy = 0.640666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.286031723022461, Accuracy = 0.6932697892189026\n",
      "Training iter #8487000:   Batch Loss = 9.408583, Accuracy = 0.6286666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.28536605834961, Accuracy = 0.6935675740242004\n",
      "Training iter #8490000:   Batch Loss = 8.766562, Accuracy = 0.8399999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.284708976745605, Accuracy = 0.6938654184341431\n",
      "Training iter #8493000:   Batch Loss = 8.969126, Accuracy = 0.7879999876022339\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.284002304077148, Accuracy = 0.6941632032394409\n",
      "Training iter #8496000:   Batch Loss = 8.953304, Accuracy = 0.7866666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.283329010009766, Accuracy = 0.6938654184341431\n",
      "Training iter #8499000:   Batch Loss = 9.437546, Accuracy = 0.6340000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.282637596130371, Accuracy = 0.6944609880447388\n",
      "Training iter #8502000:   Batch Loss = 9.124462, Accuracy = 0.7473333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.281966209411621, Accuracy = 0.6950565576553345\n",
      "Training iter #8505000:   Batch Loss = 8.791501, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.281250953674316, Accuracy = 0.6950565576553345\n",
      "Training iter #8508000:   Batch Loss = 9.057419, Accuracy = 0.768666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.28055477142334, Accuracy = 0.6950565576553345\n",
      "Training iter #8511000:   Batch Loss = 9.440784, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.279800415039062, Accuracy = 0.6947587728500366\n",
      "Training iter #8514000:   Batch Loss = 9.304603, Accuracy = 0.6526666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.279011726379395, Accuracy = 0.6950565576553345\n",
      "Training iter #8517000:   Batch Loss = 8.781818, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.278234481811523, Accuracy = 0.6947587728500366\n",
      "Training iter #8520000:   Batch Loss = 8.979675, Accuracy = 0.7826666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.277406692504883, Accuracy = 0.6953544020652771\n",
      "Training iter #8523000:   Batch Loss = 8.988774, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.276622772216797, Accuracy = 0.695652186870575\n",
      "Training iter #8526000:   Batch Loss = 9.516333, Accuracy = 0.6026666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.275866508483887, Accuracy = 0.695652186870575\n",
      "Training iter #8529000:   Batch Loss = 9.055840, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.275147438049316, Accuracy = 0.6953544020652771\n",
      "Training iter #8532000:   Batch Loss = 8.776958, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.274375915527344, Accuracy = 0.6959499716758728\n",
      "Training iter #8535000:   Batch Loss = 9.052277, Accuracy = 0.7706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.273569107055664, Accuracy = 0.6959499716758728\n",
      "Training iter #8538000:   Batch Loss = 9.439343, Accuracy = 0.6386666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.272730827331543, Accuracy = 0.6962477564811707\n",
      "Training iter #8541000:   Batch Loss = 9.279112, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.27192497253418, Accuracy = 0.6968433856964111\n",
      "Training iter #8544000:   Batch Loss = 8.732471, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.271132469177246, Accuracy = 0.6968433856964111\n",
      "Training iter #8547000:   Batch Loss = 8.931283, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.270271301269531, Accuracy = 0.6968433856964111\n",
      "Training iter #8550000:   Batch Loss = 9.009977, Accuracy = 0.7639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.269453048706055, Accuracy = 0.6965455412864685\n",
      "Training iter #8553000:   Batch Loss = 9.563238, Accuracy = 0.5846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.26858901977539, Accuracy = 0.6962477564811707\n",
      "Training iter #8556000:   Batch Loss = 8.975134, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.267772674560547, Accuracy = 0.6965455412864685\n",
      "Training iter #8559000:   Batch Loss = 8.788218, Accuracy = 0.8366666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.266951560974121, Accuracy = 0.697141170501709\n",
      "Training iter #8562000:   Batch Loss = 9.128208, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.26607608795166, Accuracy = 0.6968433856964111\n",
      "Training iter #8565000:   Batch Loss = 9.385780, Accuracy = 0.6493333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.265283584594727, Accuracy = 0.697141170501709\n",
      "Training iter #8568000:   Batch Loss = 9.221643, Accuracy = 0.6859999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.264567375183105, Accuracy = 0.6974389553070068\n",
      "Training iter #8571000:   Batch Loss = 8.747378, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.263861656188965, Accuracy = 0.6974389553070068\n",
      "Training iter #8574000:   Batch Loss = 8.939674, Accuracy = 0.7893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.26308822631836, Accuracy = 0.6980345249176025\n",
      "Training iter #8577000:   Batch Loss = 9.070551, Accuracy = 0.7393333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.262354850769043, Accuracy = 0.698630154132843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8580000:   Batch Loss = 9.553851, Accuracy = 0.5860000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.261527061462402, Accuracy = 0.6992257237434387\n",
      "Training iter #8583000:   Batch Loss = 8.907955, Accuracy = 0.8080000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.260786056518555, Accuracy = 0.698630154132843\n",
      "Training iter #8586000:   Batch Loss = 8.777188, Accuracy = 0.8366666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.26002311706543, Accuracy = 0.6992257237434387\n",
      "Training iter #8589000:   Batch Loss = 9.090574, Accuracy = 0.765999972820282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.259227752685547, Accuracy = 0.6992257237434387\n",
      "Training iter #8592000:   Batch Loss = 9.336113, Accuracy = 0.6633333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.25847053527832, Accuracy = 0.7007147073745728\n",
      "Training iter #8595000:   Batch Loss = 9.273888, Accuracy = 0.6806666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.257781982421875, Accuracy = 0.7010124921798706\n",
      "Training iter #8598000:   Batch Loss = 8.777012, Accuracy = 0.8379999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.257094383239746, Accuracy = 0.7007147073745728\n",
      "Training iter #8601000:   Batch Loss = 8.929405, Accuracy = 0.7940000295639038\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.256322860717773, Accuracy = 0.7013102769851685\n",
      "Training iter #8604000:   Batch Loss = 9.115812, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.25552749633789, Accuracy = 0.7013102769851685\n",
      "Training iter #8607000:   Batch Loss = 9.567324, Accuracy = 0.5820000171661377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.254720687866211, Accuracy = 0.7016081213951111\n",
      "Training iter #8610000:   Batch Loss = 8.871227, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.25401496887207, Accuracy = 0.7013102769851685\n",
      "Training iter #8613000:   Batch Loss = 8.749159, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.253266334533691, Accuracy = 0.7016081213951111\n",
      "Training iter #8616000:   Batch Loss = 9.079453, Accuracy = 0.7559999823570251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.252501487731934, Accuracy = 0.7019059062004089\n",
      "Training iter #8619000:   Batch Loss = 9.298384, Accuracy = 0.6740000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.25194263458252, Accuracy = 0.7019059062004089\n",
      "Training iter #8622000:   Batch Loss = 9.262156, Accuracy = 0.6786666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.251405715942383, Accuracy = 0.7027992606163025\n",
      "Training iter #8625000:   Batch Loss = 8.772949, Accuracy = 0.8433333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.250818252563477, Accuracy = 0.7027992606163025\n",
      "Training iter #8628000:   Batch Loss = 8.960483, Accuracy = 0.7806666493415833\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.250146865844727, Accuracy = 0.7036926746368408\n",
      "Training iter #8631000:   Batch Loss = 9.116774, Accuracy = 0.734666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.24944019317627, Accuracy = 0.7030971050262451\n",
      "Training iter #8634000:   Batch Loss = 9.542937, Accuracy = 0.5886666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.248644828796387, Accuracy = 0.703394889831543\n",
      "Training iter #8637000:   Batch Loss = 8.748196, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.247991561889648, Accuracy = 0.7039904594421387\n",
      "Training iter #8640000:   Batch Loss = 8.756406, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.247267723083496, Accuracy = 0.7042882442474365\n",
      "Training iter #8643000:   Batch Loss = 9.095445, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.246496200561523, Accuracy = 0.703394889831543\n",
      "Training iter #8646000:   Batch Loss = 9.264031, Accuracy = 0.6800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.245770454406738, Accuracy = 0.704883873462677\n",
      "Training iter #8649000:   Batch Loss = 9.267468, Accuracy = 0.6753333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.245077133178711, Accuracy = 0.7057772278785706\n",
      "Training iter #8652000:   Batch Loss = 8.804117, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.24435806274414, Accuracy = 0.7057772278785706\n",
      "Training iter #8655000:   Batch Loss = 8.962622, Accuracy = 0.7720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.243576049804688, Accuracy = 0.7060750722885132\n",
      "Training iter #8658000:   Batch Loss = 9.185987, Accuracy = 0.7146666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.242830276489258, Accuracy = 0.706372857093811\n",
      "Training iter #8661000:   Batch Loss = 9.492390, Accuracy = 0.6140000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.242024421691895, Accuracy = 0.7066706418991089\n",
      "Training iter #8664000:   Batch Loss = 8.705274, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.24135971069336, Accuracy = 0.7066706418991089\n",
      "Training iter #8667000:   Batch Loss = 8.791994, Accuracy = 0.8199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.240607261657715, Accuracy = 0.7066706418991089\n",
      "Training iter #8670000:   Batch Loss = 9.063103, Accuracy = 0.7553333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.239870071411133, Accuracy = 0.7066706418991089\n",
      "Training iter #8673000:   Batch Loss = 9.292842, Accuracy = 0.659333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.238985061645508, Accuracy = 0.7069684267044067\n",
      "Training iter #8676000:   Batch Loss = 9.319297, Accuracy = 0.6646666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.238227844238281, Accuracy = 0.7072662115097046\n",
      "Training iter #8679000:   Batch Loss = 8.751105, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.237457275390625, Accuracy = 0.7075640559196472\n",
      "Training iter #8682000:   Batch Loss = 8.954480, Accuracy = 0.7806666493415833\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.23665714263916, Accuracy = 0.7075640559196472\n",
      "Training iter #8685000:   Batch Loss = 9.223444, Accuracy = 0.7013333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.235925674438477, Accuracy = 0.7078618407249451\n",
      "Training iter #8688000:   Batch Loss = 9.412621, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.234435081481934, Accuracy = 0.7084574103355408\n",
      "Training iter #8691000:   Batch Loss = 8.671116, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.233036041259766, Accuracy = 0.7090529799461365\n",
      "Training iter #8694000:   Batch Loss = 8.875120, Accuracy = 0.7900000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.232231140136719, Accuracy = 0.7090529799461365\n",
      "Training iter #8697000:   Batch Loss = 8.944071, Accuracy = 0.781333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.231892585754395, Accuracy = 0.7093508243560791\n",
      "Training iter #8700000:   Batch Loss = 9.310407, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.23107624053955, Accuracy = 0.7099463939666748\n",
      "Training iter #8703000:   Batch Loss = 9.340876, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.230325698852539, Accuracy = 0.7105419635772705\n",
      "Training iter #8706000:   Batch Loss = 8.742090, Accuracy = 0.8579999804496765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.229736328125, Accuracy = 0.7108398079872131\n",
      "Training iter #8709000:   Batch Loss = 8.963566, Accuracy = 0.7839999794960022\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.229571342468262, Accuracy = 0.7108398079872131\n",
      "Training iter #8712000:   Batch Loss = 9.264175, Accuracy = 0.6853333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.229104995727539, Accuracy = 0.7105419635772705\n",
      "Training iter #8715000:   Batch Loss = 9.393217, Accuracy = 0.6266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.227951049804688, Accuracy = 0.7108398079872131\n",
      "Training iter #8718000:   Batch Loss = 8.662018, Accuracy = 0.8659999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.22708797454834, Accuracy = 0.7105419635772705\n",
      "Training iter #8721000:   Batch Loss = 8.880915, Accuracy = 0.7940000295639038\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.226428985595703, Accuracy = 0.7105419635772705\n",
      "Training iter #8724000:   Batch Loss = 8.924645, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.226346969604492, Accuracy = 0.7102441787719727\n",
      "Training iter #8727000:   Batch Loss = 9.276709, Accuracy = 0.6819999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.225752830505371, Accuracy = 0.7102441787719727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8730000:   Batch Loss = 9.215929, Accuracy = 0.7073333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.224905967712402, Accuracy = 0.7105419635772705\n",
      "Training iter #8733000:   Batch Loss = 8.769347, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.224189758300781, Accuracy = 0.7102441787719727\n",
      "Training iter #8736000:   Batch Loss = 9.009433, Accuracy = 0.7726666927337646\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.223689079284668, Accuracy = 0.7105419635772705\n",
      "Training iter #8739000:   Batch Loss = 9.322362, Accuracy = 0.6620000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.223024368286133, Accuracy = 0.7102441787719727\n",
      "Training iter #8742000:   Batch Loss = 9.382881, Accuracy = 0.6266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.222005844116211, Accuracy = 0.7108398079872131\n",
      "Training iter #8745000:   Batch Loss = 8.693629, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.221126556396484, Accuracy = 0.711137592792511\n",
      "Training iter #8748000:   Batch Loss = 8.877237, Accuracy = 0.8066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.220237731933594, Accuracy = 0.711137592792511\n",
      "Training iter #8751000:   Batch Loss = 8.884913, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.219429969787598, Accuracy = 0.711137592792511\n",
      "Training iter #8754000:   Batch Loss = 9.260344, Accuracy = 0.6866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.218320846557617, Accuracy = 0.711137592792511\n",
      "Training iter #8757000:   Batch Loss = 9.138677, Accuracy = 0.7360000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.217711448669434, Accuracy = 0.7120309472084045\n",
      "Training iter #8760000:   Batch Loss = 8.737309, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.217021942138672, Accuracy = 0.7114353775978088\n",
      "Training iter #8763000:   Batch Loss = 9.062576, Accuracy = 0.7606666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.216497421264648, Accuracy = 0.7117331624031067\n",
      "Training iter #8766000:   Batch Loss = 9.350024, Accuracy = 0.6546666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.2157621383667, Accuracy = 0.7120309472084045\n",
      "Training iter #8769000:   Batch Loss = 9.340034, Accuracy = 0.6346666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.214982986450195, Accuracy = 0.7123287916183472\n",
      "Training iter #8772000:   Batch Loss = 8.719197, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.214348793029785, Accuracy = 0.7120309472084045\n",
      "Training iter #8775000:   Batch Loss = 8.895064, Accuracy = 0.7973333597183228\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.2136869430542, Accuracy = 0.7117331624031067\n",
      "Training iter #8778000:   Batch Loss = 8.865781, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.212894439697266, Accuracy = 0.7129243612289429\n",
      "Training iter #8781000:   Batch Loss = 9.276416, Accuracy = 0.6813333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.212137222290039, Accuracy = 0.7129243612289429\n",
      "Training iter #8784000:   Batch Loss = 9.126419, Accuracy = 0.7386666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.211433410644531, Accuracy = 0.712626576423645\n",
      "Training iter #8787000:   Batch Loss = 8.714738, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.21078109741211, Accuracy = 0.7129243612289429\n",
      "Training iter #8790000:   Batch Loss = 9.016886, Accuracy = 0.7673333287239075\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.21007251739502, Accuracy = 0.7132221460342407\n",
      "Training iter #8793000:   Batch Loss = 9.327038, Accuracy = 0.6633333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.20926570892334, Accuracy = 0.7135199308395386\n",
      "Training iter #8796000:   Batch Loss = 9.306580, Accuracy = 0.6446666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.208608627319336, Accuracy = 0.7135199308395386\n",
      "Training iter #8799000:   Batch Loss = 8.705544, Accuracy = 0.859333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.20793342590332, Accuracy = 0.7132221460342407\n",
      "Training iter #8802000:   Batch Loss = 8.882739, Accuracy = 0.7986666560173035\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.20709228515625, Accuracy = 0.714115560054779\n",
      "Training iter #8805000:   Batch Loss = 8.820168, Accuracy = 0.812666654586792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.206311225891113, Accuracy = 0.714115560054779\n",
      "Training iter #8808000:   Batch Loss = 9.362196, Accuracy = 0.6553333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.205594062805176, Accuracy = 0.7144133448600769\n",
      "Training iter #8811000:   Batch Loss = 9.040842, Accuracy = 0.762666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.204977035522461, Accuracy = 0.7138177752494812\n",
      "Training iter #8814000:   Batch Loss = 8.704582, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.204280853271484, Accuracy = 0.714115560054779\n",
      "Training iter #8817000:   Batch Loss = 9.001598, Accuracy = 0.7726666927337646\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.203615188598633, Accuracy = 0.7150089144706726\n",
      "Training iter #8820000:   Batch Loss = 9.369358, Accuracy = 0.6513333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.20278263092041, Accuracy = 0.7150089144706726\n",
      "Training iter #8823000:   Batch Loss = 9.271608, Accuracy = 0.6579999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.202104568481445, Accuracy = 0.7150089144706726\n",
      "Training iter #8826000:   Batch Loss = 8.666121, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.20140266418457, Accuracy = 0.7147111296653748\n",
      "Training iter #8829000:   Batch Loss = 8.901045, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.200621604919434, Accuracy = 0.7153067588806152\n",
      "Training iter #8832000:   Batch Loss = 8.893132, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.199966430664062, Accuracy = 0.7153067588806152\n",
      "Training iter #8835000:   Batch Loss = 9.383152, Accuracy = 0.6439999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.199345588684082, Accuracy = 0.7153067588806152\n",
      "Training iter #8838000:   Batch Loss = 9.029252, Accuracy = 0.7746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.198747634887695, Accuracy = 0.7150089144706726\n",
      "Training iter #8841000:   Batch Loss = 8.662881, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.19803524017334, Accuracy = 0.7159023284912109\n",
      "Training iter #8844000:   Batch Loss = 8.990744, Accuracy = 0.7720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.197389602661133, Accuracy = 0.7159023284912109\n",
      "Training iter #8847000:   Batch Loss = 9.375060, Accuracy = 0.6480000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.196779251098633, Accuracy = 0.7162001132965088\n",
      "Training iter #8850000:   Batch Loss = 9.167253, Accuracy = 0.6953333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.19607162475586, Accuracy = 0.7162001132965088\n",
      "Training iter #8853000:   Batch Loss = 8.704159, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.195399284362793, Accuracy = 0.7162001132965088\n",
      "Training iter #8856000:   Batch Loss = 8.889265, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.194672584533691, Accuracy = 0.7162001132965088\n",
      "Training iter #8859000:   Batch Loss = 8.942125, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.194075584411621, Accuracy = 0.7167956829071045\n",
      "Training iter #8862000:   Batch Loss = 9.423366, Accuracy = 0.624666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.193401336669922, Accuracy = 0.7167956829071045\n",
      "Training iter #8865000:   Batch Loss = 8.938128, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.192726135253906, Accuracy = 0.7167956829071045\n",
      "Training iter #8868000:   Batch Loss = 8.706398, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.191984176635742, Accuracy = 0.717391312122345\n",
      "Training iter #8871000:   Batch Loss = 9.006666, Accuracy = 0.768666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.191240310668945, Accuracy = 0.717391312122345\n",
      "Training iter #8874000:   Batch Loss = 9.355156, Accuracy = 0.6473333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.19056224822998, Accuracy = 0.7182846665382385\n",
      "Training iter #8877000:   Batch Loss = 9.167646, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.189916610717773, Accuracy = 0.7185825109481812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8880000:   Batch Loss = 8.650958, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.189332008361816, Accuracy = 0.7185825109481812\n",
      "Training iter #8883000:   Batch Loss = 8.859636, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.188615798950195, Accuracy = 0.718880295753479\n",
      "Training iter #8886000:   Batch Loss = 8.929171, Accuracy = 0.7760000228881836\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.187868118286133, Accuracy = 0.7200714945793152\n",
      "Training iter #8889000:   Batch Loss = 9.494048, Accuracy = 0.5986666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.18707275390625, Accuracy = 0.7194758653640747\n",
      "Training iter #8892000:   Batch Loss = 8.880373, Accuracy = 0.8153333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.186296463012695, Accuracy = 0.7194758653640747\n",
      "Training iter #8895000:   Batch Loss = 8.675071, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.185412406921387, Accuracy = 0.7209648489952087\n",
      "Training iter #8898000:   Batch Loss = 9.039237, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.18464469909668, Accuracy = 0.7206670641899109\n",
      "Training iter #8901000:   Batch Loss = 9.273290, Accuracy = 0.6746666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.183950424194336, Accuracy = 0.720369279384613\n",
      "Training iter #8904000:   Batch Loss = 9.144593, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.183341979980469, Accuracy = 0.7206670641899109\n",
      "Training iter #8907000:   Batch Loss = 8.663227, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.182882308959961, Accuracy = 0.7206670641899109\n",
      "Training iter #8910000:   Batch Loss = 8.864751, Accuracy = 0.7986666560173035\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.182278633117676, Accuracy = 0.7209648489952087\n",
      "Training iter #8913000:   Batch Loss = 8.994596, Accuracy = 0.7573333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.181584358215332, Accuracy = 0.7212626338005066\n",
      "Training iter #8916000:   Batch Loss = 9.421071, Accuracy = 0.6153333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.180795669555664, Accuracy = 0.7209648489952087\n",
      "Training iter #8919000:   Batch Loss = 8.816233, Accuracy = 0.8273333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.180152893066406, Accuracy = 0.7212626338005066\n",
      "Training iter #8922000:   Batch Loss = 8.691958, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.179445266723633, Accuracy = 0.7215604782104492\n",
      "Training iter #8925000:   Batch Loss = 9.037947, Accuracy = 0.7620000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.17872142791748, Accuracy = 0.7230494618415833\n",
      "Training iter #8928000:   Batch Loss = 9.203408, Accuracy = 0.6959999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.177982330322266, Accuracy = 0.7233472466468811\n",
      "Training iter #8931000:   Batch Loss = 9.177951, Accuracy = 0.7026666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.17748737335205, Accuracy = 0.7227516174316406\n",
      "Training iter #8934000:   Batch Loss = 8.687900, Accuracy = 0.8573333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.176875114440918, Accuracy = 0.7230494618415833\n",
      "Training iter #8937000:   Batch Loss = 8.890985, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.176082611083984, Accuracy = 0.7230494618415833\n",
      "Training iter #8940000:   Batch Loss = 9.024543, Accuracy = 0.7519999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.175394058227539, Accuracy = 0.723645031452179\n",
      "Training iter #8943000:   Batch Loss = 9.461103, Accuracy = 0.6100000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.174633026123047, Accuracy = 0.7230494618415833\n",
      "Training iter #8946000:   Batch Loss = 8.736290, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.173810958862305, Accuracy = 0.7233472466468811\n",
      "Training iter #8949000:   Batch Loss = 8.685841, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.173090934753418, Accuracy = 0.723645031452179\n",
      "Training iter #8952000:   Batch Loss = 9.031463, Accuracy = 0.7526666522026062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.172192573547363, Accuracy = 0.7239428162574768\n",
      "Training iter #8955000:   Batch Loss = 9.195855, Accuracy = 0.6913333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.171228408813477, Accuracy = 0.7233472466468811\n",
      "Training iter #8958000:   Batch Loss = 9.161648, Accuracy = 0.7059999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.170507431030273, Accuracy = 0.723645031452179\n",
      "Training iter #8961000:   Batch Loss = 8.703306, Accuracy = 0.8586666584014893\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.170372009277344, Accuracy = 0.7239428162574768\n",
      "Training iter #8964000:   Batch Loss = 8.889343, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.169719696044922, Accuracy = 0.7242406010627747\n",
      "Training iter #8967000:   Batch Loss = 9.080509, Accuracy = 0.7366666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.169052124023438, Accuracy = 0.723645031452179\n",
      "Training iter #8970000:   Batch Loss = 9.413649, Accuracy = 0.6320000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.168235778808594, Accuracy = 0.7239428162574768\n",
      "Training iter #8973000:   Batch Loss = 8.652481, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.167479515075684, Accuracy = 0.7239428162574768\n",
      "Training iter #8976000:   Batch Loss = 8.684875, Accuracy = 0.8553333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.166808128356934, Accuracy = 0.725134015083313\n",
      "Training iter #8979000:   Batch Loss = 9.008254, Accuracy = 0.7639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.166091918945312, Accuracy = 0.725134015083313\n",
      "Training iter #8982000:   Batch Loss = 9.188478, Accuracy = 0.690666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.165353775024414, Accuracy = 0.7248362302780151\n",
      "Training iter #8985000:   Batch Loss = 9.174560, Accuracy = 0.7053333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.164755821228027, Accuracy = 0.7254317998886108\n",
      "Training iter #8988000:   Batch Loss = 8.690252, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.164199829101562, Accuracy = 0.7260273694992065\n",
      "Training iter #8991000:   Batch Loss = 8.887528, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.163467407226562, Accuracy = 0.7263252139091492\n",
      "Training iter #8994000:   Batch Loss = 9.124128, Accuracy = 0.7246666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.162752151489258, Accuracy = 0.7263252139091492\n",
      "Training iter #8997000:   Batch Loss = 9.353639, Accuracy = 0.6566666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.161983489990234, Accuracy = 0.7260273694992065\n",
      "Training iter #9000000:   Batch Loss = 8.613727, Accuracy = 0.875333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.161371231079102, Accuracy = 0.7263252139091492\n",
      "Training iter #9003000:   Batch Loss = 8.736077, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.160550117492676, Accuracy = 0.7272185683250427\n",
      "Training iter #9006000:   Batch Loss = 8.938049, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.159805297851562, Accuracy = 0.7278141975402832\n",
      "Training iter #9009000:   Batch Loss = 9.214989, Accuracy = 0.6866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.159079551696777, Accuracy = 0.7278141975402832\n",
      "Training iter #9012000:   Batch Loss = 9.213594, Accuracy = 0.6926666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.158541679382324, Accuracy = 0.728111982345581\n",
      "Training iter #9015000:   Batch Loss = 8.678769, Accuracy = 0.8740000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.157794952392578, Accuracy = 0.7287075519561768\n",
      "Training iter #9018000:   Batch Loss = 8.873902, Accuracy = 0.8046666383743286\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.156888961791992, Accuracy = 0.7293031811714172\n",
      "Training iter #9021000:   Batch Loss = 9.156497, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.15634822845459, Accuracy = 0.7293031811714172\n",
      "Training iter #9024000:   Batch Loss = 9.278320, Accuracy = 0.6679999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.155702590942383, Accuracy = 0.7296009659767151\n",
      "Training iter #9027000:   Batch Loss = 8.585142, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.155163764953613, Accuracy = 0.7298987507820129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9030000:   Batch Loss = 8.784784, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.154472351074219, Accuracy = 0.7304943203926086\n",
      "Training iter #9033000:   Batch Loss = 8.847052, Accuracy = 0.8046666383743286\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.1537446975708, Accuracy = 0.7304943203926086\n",
      "Training iter #9036000:   Batch Loss = 9.181443, Accuracy = 0.7006666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.153019905090332, Accuracy = 0.731387734413147\n",
      "Training iter #9039000:   Batch Loss = 9.203609, Accuracy = 0.7073333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.152382850646973, Accuracy = 0.7319833040237427\n",
      "Training iter #9042000:   Batch Loss = 8.701855, Accuracy = 0.8733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.151750564575195, Accuracy = 0.7319833040237427\n",
      "Training iter #9045000:   Batch Loss = 8.879954, Accuracy = 0.7973333597183228\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.150999069213867, Accuracy = 0.7322811484336853\n",
      "Training iter #9048000:   Batch Loss = 9.180603, Accuracy = 0.6980000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.150327682495117, Accuracy = 0.7322811484336853\n",
      "Training iter #9051000:   Batch Loss = 9.305156, Accuracy = 0.6466666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.149659156799316, Accuracy = 0.7322811484336853\n",
      "Training iter #9054000:   Batch Loss = 8.590755, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.149114608764648, Accuracy = 0.732876718044281\n",
      "Training iter #9057000:   Batch Loss = 8.789496, Accuracy = 0.8173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.148466110229492, Accuracy = 0.7325789332389832\n",
      "Training iter #9060000:   Batch Loss = 8.810790, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.14775562286377, Accuracy = 0.7322811484336853\n",
      "Training iter #9063000:   Batch Loss = 9.172749, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.147010803222656, Accuracy = 0.7331745028495789\n",
      "Training iter #9066000:   Batch Loss = 9.060265, Accuracy = 0.7493333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.146434783935547, Accuracy = 0.7325789332389832\n",
      "Training iter #9069000:   Batch Loss = 8.666133, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.145835876464844, Accuracy = 0.7331745028495789\n",
      "Training iter #9072000:   Batch Loss = 8.950808, Accuracy = 0.781333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.14505672454834, Accuracy = 0.732876718044281\n",
      "Training iter #9075000:   Batch Loss = 9.217048, Accuracy = 0.6840000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.144309997558594, Accuracy = 0.7334722876548767\n",
      "Training iter #9078000:   Batch Loss = 9.269889, Accuracy = 0.6520000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.143598556518555, Accuracy = 0.7331745028495789\n",
      "Training iter #9081000:   Batch Loss = 8.607433, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.143086433410645, Accuracy = 0.7331745028495789\n",
      "Training iter #9084000:   Batch Loss = 8.789264, Accuracy = 0.8193333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.142340660095215, Accuracy = 0.732876718044281\n",
      "Training iter #9087000:   Batch Loss = 8.780718, Accuracy = 0.8180000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.141613006591797, Accuracy = 0.7331745028495789\n",
      "Training iter #9090000:   Batch Loss = 9.167990, Accuracy = 0.7139999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.140949249267578, Accuracy = 0.7331745028495789\n",
      "Training iter #9093000:   Batch Loss = 9.032834, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.140396118164062, Accuracy = 0.732876718044281\n",
      "Training iter #9096000:   Batch Loss = 8.627460, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.139820098876953, Accuracy = 0.7325789332389832\n",
      "Training iter #9099000:   Batch Loss = 8.956805, Accuracy = 0.7820000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.139116287231445, Accuracy = 0.7325789332389832\n",
      "Training iter #9102000:   Batch Loss = 9.242789, Accuracy = 0.6759999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.138337135314941, Accuracy = 0.732876718044281\n",
      "Training iter #9105000:   Batch Loss = 9.230958, Accuracy = 0.6626666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.137574195861816, Accuracy = 0.732876718044281\n",
      "Training iter #9108000:   Batch Loss = 8.626766, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.136993408203125, Accuracy = 0.732876718044281\n",
      "Training iter #9111000:   Batch Loss = 8.785034, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.1362943649292, Accuracy = 0.7325789332389832\n",
      "Training iter #9114000:   Batch Loss = 8.786437, Accuracy = 0.8119999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.135641098022461, Accuracy = 0.7325789332389832\n",
      "Training iter #9117000:   Batch Loss = 9.218474, Accuracy = 0.6973333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.135017395019531, Accuracy = 0.7331745028495789\n",
      "Training iter #9120000:   Batch Loss = 8.996031, Accuracy = 0.7733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.134450912475586, Accuracy = 0.7331745028495789\n",
      "Training iter #9123000:   Batch Loss = 8.636283, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.133898735046387, Accuracy = 0.7331745028495789\n",
      "Training iter #9126000:   Batch Loss = 8.957107, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.133302688598633, Accuracy = 0.732876718044281\n",
      "Training iter #9129000:   Batch Loss = 9.248137, Accuracy = 0.6800000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.132608413696289, Accuracy = 0.7331745028495789\n",
      "Training iter #9132000:   Batch Loss = 9.208725, Accuracy = 0.6753333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.131850242614746, Accuracy = 0.7334722876548767\n",
      "Training iter #9135000:   Batch Loss = 8.624238, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.131272315979004, Accuracy = 0.7337700724601746\n",
      "Training iter #9138000:   Batch Loss = 8.789310, Accuracy = 0.8180000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.130643844604492, Accuracy = 0.7337700724601746\n",
      "Training iter #9141000:   Batch Loss = 8.760270, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.130125999450684, Accuracy = 0.7337700724601746\n",
      "Training iter #9144000:   Batch Loss = 9.241333, Accuracy = 0.6773333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.129470825195312, Accuracy = 0.734365701675415\n",
      "Training iter #9147000:   Batch Loss = 8.953583, Accuracy = 0.7906666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.128866195678711, Accuracy = 0.7346634864807129\n",
      "Training iter #9150000:   Batch Loss = 8.633642, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.12828254699707, Accuracy = 0.7346634864807129\n",
      "Training iter #9153000:   Batch Loss = 8.906237, Accuracy = 0.7873333096504211\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.12763500213623, Accuracy = 0.734365701675415\n",
      "Training iter #9156000:   Batch Loss = 9.265896, Accuracy = 0.6740000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.126877784729004, Accuracy = 0.734365701675415\n",
      "Training iter #9159000:   Batch Loss = 9.129186, Accuracy = 0.6986666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.126028060913086, Accuracy = 0.734365701675415\n",
      "Training iter #9162000:   Batch Loss = 8.634960, Accuracy = 0.8859999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.12533950805664, Accuracy = 0.7346634864807129\n",
      "Training iter #9165000:   Batch Loss = 8.800681, Accuracy = 0.8153333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.124568939208984, Accuracy = 0.7349612712860107\n",
      "Training iter #9168000:   Batch Loss = 8.828370, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.123888969421387, Accuracy = 0.7349612712860107\n",
      "Training iter #9171000:   Batch Loss = 9.322545, Accuracy = 0.6486666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.123244285583496, Accuracy = 0.7346634864807129\n",
      "Training iter #9174000:   Batch Loss = 8.884022, Accuracy = 0.812666654586792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.122757911682129, Accuracy = 0.734365701675415\n",
      "Training iter #9177000:   Batch Loss = 8.587777, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.122132301330566, Accuracy = 0.7346634864807129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9180000:   Batch Loss = 8.906310, Accuracy = 0.7893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.121357917785645, Accuracy = 0.7346634864807129\n",
      "Training iter #9183000:   Batch Loss = 9.256149, Accuracy = 0.6740000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.120584487915039, Accuracy = 0.7346634864807129\n",
      "Training iter #9186000:   Batch Loss = 9.098078, Accuracy = 0.7173333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.119893074035645, Accuracy = 0.7346634864807129\n",
      "Training iter #9189000:   Batch Loss = 8.594472, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.119260787963867, Accuracy = 0.734365701675415\n",
      "Training iter #9192000:   Batch Loss = 8.785863, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.118572235107422, Accuracy = 0.7349612712860107\n",
      "Training iter #9195000:   Batch Loss = 8.843006, Accuracy = 0.8026666641235352\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.117914199829102, Accuracy = 0.7352590560913086\n",
      "Training iter #9198000:   Batch Loss = 9.341131, Accuracy = 0.6359999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.117321014404297, Accuracy = 0.7352590560913086\n",
      "Training iter #9201000:   Batch Loss = 8.814314, Accuracy = 0.831333339214325\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.116853713989258, Accuracy = 0.7349612712860107\n",
      "Training iter #9204000:   Batch Loss = 8.638330, Accuracy = 0.875333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.116317749023438, Accuracy = 0.7352590560913086\n",
      "Training iter #9207000:   Batch Loss = 8.974157, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.115546226501465, Accuracy = 0.7352590560913086\n",
      "Training iter #9210000:   Batch Loss = 9.206781, Accuracy = 0.6846666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.114678382873535, Accuracy = 0.7352590560913086\n",
      "Training iter #9213000:   Batch Loss = 9.044067, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.113926887512207, Accuracy = 0.7352590560913086\n",
      "Training iter #9216000:   Batch Loss = 8.576538, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.11316967010498, Accuracy = 0.7352590560913086\n",
      "Training iter #9219000:   Batch Loss = 8.776806, Accuracy = 0.8173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.112330436706543, Accuracy = 0.7361524701118469\n",
      "Training iter #9222000:   Batch Loss = 8.865004, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.111684799194336, Accuracy = 0.7358546853065491\n",
      "Training iter #9225000:   Batch Loss = 9.380402, Accuracy = 0.624666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.11099910736084, Accuracy = 0.7358546853065491\n",
      "Training iter #9228000:   Batch Loss = 8.787768, Accuracy = 0.8379999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.110528945922852, Accuracy = 0.7358546853065491\n",
      "Training iter #9231000:   Batch Loss = 8.618345, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.11001205444336, Accuracy = 0.7361524701118469\n",
      "Training iter #9234000:   Batch Loss = 8.945117, Accuracy = 0.7853333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.109769821166992, Accuracy = 0.7361524701118469\n",
      "Training iter #9237000:   Batch Loss = 9.180899, Accuracy = 0.6946666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.109453201293945, Accuracy = 0.7358546853065491\n",
      "Training iter #9240000:   Batch Loss = 9.094988, Accuracy = 0.7226666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.108830451965332, Accuracy = 0.7358546853065491\n",
      "Training iter #9243000:   Batch Loss = 8.608606, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.10814380645752, Accuracy = 0.7358546853065491\n",
      "Training iter #9246000:   Batch Loss = 8.763947, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.10724925994873, Accuracy = 0.7355569005012512\n",
      "Training iter #9249000:   Batch Loss = 8.940519, Accuracy = 0.768666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.106389045715332, Accuracy = 0.7361524701118469\n",
      "Training iter #9252000:   Batch Loss = 9.318455, Accuracy = 0.6366666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.105485916137695, Accuracy = 0.7364502549171448\n",
      "Training iter #9255000:   Batch Loss = 8.713369, Accuracy = 0.8579999804496765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.10480785369873, Accuracy = 0.7364502549171448\n",
      "Training iter #9258000:   Batch Loss = 8.612340, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.10411548614502, Accuracy = 0.7361524701118469\n",
      "Training iter #9261000:   Batch Loss = 8.946603, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.10336685180664, Accuracy = 0.7364502549171448\n",
      "Training iter #9264000:   Batch Loss = 9.111149, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.102595329284668, Accuracy = 0.7367480397224426\n",
      "Training iter #9267000:   Batch Loss = 9.078008, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.102250099182129, Accuracy = 0.7361524701118469\n",
      "Training iter #9270000:   Batch Loss = 8.605261, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.10183048248291, Accuracy = 0.7364502549171448\n",
      "Training iter #9273000:   Batch Loss = 8.814418, Accuracy = 0.8040000200271606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.101266860961914, Accuracy = 0.7367480397224426\n",
      "Training iter #9276000:   Batch Loss = 8.923296, Accuracy = 0.7826666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.100804328918457, Accuracy = 0.7364502549171448\n",
      "Training iter #9279000:   Batch Loss = 9.330277, Accuracy = 0.6453333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.100336074829102, Accuracy = 0.7370458841323853\n",
      "Training iter #9282000:   Batch Loss = 8.643283, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.099852561950684, Accuracy = 0.7370458841323853\n",
      "Training iter #9285000:   Batch Loss = 8.603976, Accuracy = 0.878000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.099337577819824, Accuracy = 0.7373436689376831\n",
      "Training iter #9288000:   Batch Loss = 8.955941, Accuracy = 0.7673333287239075\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.098482131958008, Accuracy = 0.7379392385482788\n",
      "Training iter #9291000:   Batch Loss = 9.103097, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.097686767578125, Accuracy = 0.7382370233535767\n",
      "Training iter #9294000:   Batch Loss = 9.089561, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.097052574157715, Accuracy = 0.737641453742981\n",
      "Training iter #9297000:   Batch Loss = 8.648464, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.09628963470459, Accuracy = 0.737641453742981\n",
      "Training iter #9300000:   Batch Loss = 8.808121, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.095355033874512, Accuracy = 0.7382370233535767\n",
      "Training iter #9303000:   Batch Loss = 9.003489, Accuracy = 0.7519999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.094735145568848, Accuracy = 0.7382370233535767\n",
      "Training iter #9306000:   Batch Loss = 9.304065, Accuracy = 0.6553333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.094093322753906, Accuracy = 0.7385348677635193\n",
      "Training iter #9309000:   Batch Loss = 8.563471, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.093501091003418, Accuracy = 0.7379392385482788\n",
      "Training iter #9312000:   Batch Loss = 8.627563, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.092856407165527, Accuracy = 0.7382370233535767\n",
      "Training iter #9315000:   Batch Loss = 8.908726, Accuracy = 0.781333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.092033386230469, Accuracy = 0.7385348677635193\n",
      "Training iter #9318000:   Batch Loss = 9.101942, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.091266632080078, Accuracy = 0.7385348677635193\n",
      "Training iter #9321000:   Batch Loss = 9.131700, Accuracy = 0.7073333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.090607643127441, Accuracy = 0.739130437374115\n",
      "Training iter #9324000:   Batch Loss = 8.618696, Accuracy = 0.8840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.089947700500488, Accuracy = 0.739130437374115\n",
      "Training iter #9327000:   Batch Loss = 8.763535, Accuracy = 0.8180000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.089359283447266, Accuracy = 0.739130437374115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9330000:   Batch Loss = 9.040756, Accuracy = 0.7413333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.088822364807129, Accuracy = 0.739130437374115\n",
      "Training iter #9333000:   Batch Loss = 9.232191, Accuracy = 0.6766666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.088293075561523, Accuracy = 0.7388326525688171\n",
      "Training iter #9336000:   Batch Loss = 8.542578, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.087791442871094, Accuracy = 0.7385348677635193\n",
      "Training iter #9339000:   Batch Loss = 8.669127, Accuracy = 0.8446666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.087203979492188, Accuracy = 0.7385348677635193\n",
      "Training iter #9342000:   Batch Loss = 8.824848, Accuracy = 0.7986666560173035\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.086448669433594, Accuracy = 0.7388326525688171\n",
      "Training iter #9345000:   Batch Loss = 9.130058, Accuracy = 0.7020000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.08578872680664, Accuracy = 0.739130437374115\n",
      "Training iter #9348000:   Batch Loss = 9.160700, Accuracy = 0.7053333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.085082054138184, Accuracy = 0.7388326525688171\n",
      "Training iter #9351000:   Batch Loss = 8.603186, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.08441162109375, Accuracy = 0.7388326525688171\n",
      "Training iter #9354000:   Batch Loss = 8.818566, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.083863258361816, Accuracy = 0.739130437374115\n",
      "Training iter #9357000:   Batch Loss = 9.088881, Accuracy = 0.7213333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.08349323272705, Accuracy = 0.7388326525688171\n",
      "Training iter #9360000:   Batch Loss = 9.188443, Accuracy = 0.6779999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.082903861999512, Accuracy = 0.7388326525688171\n",
      "Training iter #9363000:   Batch Loss = 8.512072, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.082270622253418, Accuracy = 0.7388326525688171\n",
      "Training iter #9366000:   Batch Loss = 8.697096, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.08178997039795, Accuracy = 0.7388326525688171\n",
      "Training iter #9369000:   Batch Loss = 8.804101, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.081147193908691, Accuracy = 0.7394282221794128\n",
      "Training iter #9372000:   Batch Loss = 9.087493, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.080561637878418, Accuracy = 0.7397260069847107\n",
      "Training iter #9375000:   Batch Loss = 9.056561, Accuracy = 0.7426666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.079854965209961, Accuracy = 0.7397260069847107\n",
      "Training iter #9378000:   Batch Loss = 8.637037, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.079044342041016, Accuracy = 0.7394282221794128\n",
      "Training iter #9381000:   Batch Loss = 8.804640, Accuracy = 0.8086666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.078391075134277, Accuracy = 0.7400238513946533\n",
      "Training iter #9384000:   Batch Loss = 9.113693, Accuracy = 0.7039999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.077736854553223, Accuracy = 0.7400238513946533\n",
      "Training iter #9387000:   Batch Loss = 9.194057, Accuracy = 0.6693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.077069282531738, Accuracy = 0.7397260069847107\n",
      "Training iter #9390000:   Batch Loss = 8.553699, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.076560974121094, Accuracy = 0.7397260069847107\n",
      "Training iter #9393000:   Batch Loss = 8.719698, Accuracy = 0.8286666870117188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.07606029510498, Accuracy = 0.7403216361999512\n",
      "Training iter #9396000:   Batch Loss = 8.747606, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.07548713684082, Accuracy = 0.7400238513946533\n",
      "Training iter #9399000:   Batch Loss = 9.080263, Accuracy = 0.7273333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.074864387512207, Accuracy = 0.7397260069847107\n",
      "Training iter #9402000:   Batch Loss = 8.988400, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.074335098266602, Accuracy = 0.7403216361999512\n",
      "Training iter #9405000:   Batch Loss = 8.586904, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.0736722946167, Accuracy = 0.740619421005249\n",
      "Training iter #9408000:   Batch Loss = 8.908665, Accuracy = 0.7866666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.07292366027832, Accuracy = 0.740619421005249\n",
      "Training iter #9411000:   Batch Loss = 9.134192, Accuracy = 0.6913333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.07211971282959, Accuracy = 0.740619421005249\n",
      "Training iter #9414000:   Batch Loss = 9.164576, Accuracy = 0.6740000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.071688652038574, Accuracy = 0.7403216361999512\n",
      "Training iter #9417000:   Batch Loss = 8.533531, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.071223258972168, Accuracy = 0.740619421005249\n",
      "Training iter #9420000:   Batch Loss = 8.718973, Accuracy = 0.8299999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.070588111877441, Accuracy = 0.740619421005249\n",
      "Training iter #9423000:   Batch Loss = 8.715062, Accuracy = 0.8253333568572998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.069899559020996, Accuracy = 0.740619421005249\n",
      "Training iter #9426000:   Batch Loss = 9.095046, Accuracy = 0.7213333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.069272994995117, Accuracy = 0.740619421005249\n",
      "Training iter #9429000:   Batch Loss = 8.953703, Accuracy = 0.7746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.068705558776855, Accuracy = 0.7409172058105469\n",
      "Training iter #9432000:   Batch Loss = 8.582453, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.067961692810059, Accuracy = 0.740619421005249\n",
      "Training iter #9435000:   Batch Loss = 8.866921, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.067159652709961, Accuracy = 0.740619421005249\n",
      "Training iter #9438000:   Batch Loss = 9.143827, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.066556930541992, Accuracy = 0.7403216361999512\n",
      "Training iter #9441000:   Batch Loss = 9.121776, Accuracy = 0.6859999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.066009521484375, Accuracy = 0.740619421005249\n",
      "Training iter #9444000:   Batch Loss = 8.566229, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.065423965454102, Accuracy = 0.7409172058105469\n",
      "Training iter #9447000:   Batch Loss = 8.716701, Accuracy = 0.831333339214325\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.064770698547363, Accuracy = 0.7412149906158447\n",
      "Training iter #9450000:   Batch Loss = 8.675976, Accuracy = 0.8379999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.064148902893066, Accuracy = 0.740619421005249\n",
      "Training iter #9453000:   Batch Loss = 9.161357, Accuracy = 0.6893333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.063568115234375, Accuracy = 0.740619421005249\n",
      "Training iter #9456000:   Batch Loss = 8.879064, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.06302547454834, Accuracy = 0.7412149906158447\n",
      "Training iter #9459000:   Batch Loss = 8.572504, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.062383651733398, Accuracy = 0.7418106198310852\n",
      "Training iter #9462000:   Batch Loss = 8.876306, Accuracy = 0.7873333096504211\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.061667442321777, Accuracy = 0.7415128350257874\n",
      "Training iter #9465000:   Batch Loss = 9.188426, Accuracy = 0.6859999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.061038970947266, Accuracy = 0.7421084046363831\n",
      "Training iter #9468000:   Batch Loss = 9.109641, Accuracy = 0.6926666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.060526847839355, Accuracy = 0.7424061894416809\n",
      "Training iter #9471000:   Batch Loss = 8.534857, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.060050010681152, Accuracy = 0.7424061894416809\n",
      "Training iter #9474000:   Batch Loss = 8.731050, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.059444427490234, Accuracy = 0.7424061894416809\n",
      "Training iter #9477000:   Batch Loss = 8.736034, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.05879020690918, Accuracy = 0.7430017590522766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9480000:   Batch Loss = 9.167872, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.058263778686523, Accuracy = 0.7430017590522766\n",
      "Training iter #9483000:   Batch Loss = 8.858064, Accuracy = 0.8080000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.057827949523926, Accuracy = 0.7432996034622192\n",
      "Training iter #9486000:   Batch Loss = 8.559480, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.057291030883789, Accuracy = 0.7432996034622192\n",
      "Training iter #9489000:   Batch Loss = 8.840921, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.056680679321289, Accuracy = 0.7430017590522766\n",
      "Training iter #9492000:   Batch Loss = 9.169221, Accuracy = 0.6893333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.05614948272705, Accuracy = 0.7430017590522766\n",
      "Training iter #9495000:   Batch Loss = 9.023634, Accuracy = 0.7233333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.055663108825684, Accuracy = 0.7432996034622192\n",
      "Training iter #9498000:   Batch Loss = 8.563694, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.055093765258789, Accuracy = 0.7432996034622192\n",
      "Training iter #9501000:   Batch Loss = 8.744403, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.054381370544434, Accuracy = 0.7432996034622192\n",
      "Training iter #9504000:   Batch Loss = 8.782873, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.053714752197266, Accuracy = 0.7430017590522766\n",
      "Training iter #9507000:   Batch Loss = 9.224518, Accuracy = 0.6620000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.053089141845703, Accuracy = 0.7430017590522766\n",
      "Training iter #9510000:   Batch Loss = 8.786244, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.052582740783691, Accuracy = 0.7430017590522766\n",
      "Training iter #9513000:   Batch Loss = 8.555819, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.051992416381836, Accuracy = 0.7430017590522766\n",
      "Training iter #9516000:   Batch Loss = 8.847615, Accuracy = 0.7913333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.051309585571289, Accuracy = 0.7432996034622192\n",
      "Training iter #9519000:   Batch Loss = 9.163815, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.050737380981445, Accuracy = 0.7430017590522766\n",
      "Training iter #9522000:   Batch Loss = 9.008895, Accuracy = 0.7306666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.050251960754395, Accuracy = 0.7430017590522766\n",
      "Training iter #9525000:   Batch Loss = 8.512326, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.049736976623535, Accuracy = 0.7430017590522766\n",
      "Training iter #9528000:   Batch Loss = 8.708931, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.049091339111328, Accuracy = 0.7438951730728149\n",
      "Training iter #9531000:   Batch Loss = 8.762540, Accuracy = 0.8086666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.04831314086914, Accuracy = 0.7441929578781128\n",
      "Training iter #9534000:   Batch Loss = 9.285806, Accuracy = 0.640666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.04764461517334, Accuracy = 0.7435973882675171\n",
      "Training iter #9537000:   Batch Loss = 8.733418, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.047029495239258, Accuracy = 0.7432996034622192\n",
      "Training iter #9540000:   Batch Loss = 8.546868, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.046260833740234, Accuracy = 0.7435973882675171\n",
      "Training iter #9543000:   Batch Loss = 8.901529, Accuracy = 0.7860000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.045265197753906, Accuracy = 0.7444907426834106\n",
      "Training iter #9546000:   Batch Loss = 9.100821, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.044635772705078, Accuracy = 0.7444907426834106\n",
      "Training iter #9549000:   Batch Loss = 8.962139, Accuracy = 0.7426666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.044146537780762, Accuracy = 0.7441929578781128\n",
      "Training iter #9552000:   Batch Loss = 8.522084, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.043630599975586, Accuracy = 0.7444907426834106\n",
      "Training iter #9555000:   Batch Loss = 8.718848, Accuracy = 0.8286666870117188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.0430269241333, Accuracy = 0.7450863718986511\n",
      "Training iter #9558000:   Batch Loss = 8.811236, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.042428016662598, Accuracy = 0.7450863718986511\n",
      "Training iter #9561000:   Batch Loss = 9.238590, Accuracy = 0.6539999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.04182243347168, Accuracy = 0.7444907426834106\n",
      "Training iter #9564000:   Batch Loss = 8.676896, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.041299819946289, Accuracy = 0.7450863718986511\n",
      "Training iter #9567000:   Batch Loss = 8.562048, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.040670394897461, Accuracy = 0.745384156703949\n",
      "Training iter #9570000:   Batch Loss = 8.876921, Accuracy = 0.7873333096504211\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.039981842041016, Accuracy = 0.7450863718986511\n",
      "Training iter #9573000:   Batch Loss = 9.058634, Accuracy = 0.7146666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.0393705368042, Accuracy = 0.7459797263145447\n",
      "Training iter #9576000:   Batch Loss = 9.005543, Accuracy = 0.7326666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.038870811462402, Accuracy = 0.7462775707244873\n",
      "Training iter #9579000:   Batch Loss = 8.541992, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.03831672668457, Accuracy = 0.7462775707244873\n",
      "Training iter #9582000:   Batch Loss = 8.718760, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.037654876708984, Accuracy = 0.7462775707244873\n",
      "Training iter #9585000:   Batch Loss = 8.864660, Accuracy = 0.7826666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.036986351013184, Accuracy = 0.7459797263145447\n",
      "Training iter #9588000:   Batch Loss = 9.274625, Accuracy = 0.6480000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.036354064941406, Accuracy = 0.7465753555297852\n",
      "Training iter #9591000:   Batch Loss = 8.622076, Accuracy = 0.878000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.035840034484863, Accuracy = 0.7462775707244873\n",
      "Training iter #9594000:   Batch Loss = 8.537027, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.035279273986816, Accuracy = 0.7459797263145447\n",
      "Training iter #9597000:   Batch Loss = 8.879086, Accuracy = 0.7746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.034811019897461, Accuracy = 0.7462775707244873\n",
      "Training iter #9600000:   Batch Loss = 9.022093, Accuracy = 0.7226666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.03418254852295, Accuracy = 0.7462775707244873\n",
      "Training iter #9603000:   Batch Loss = 8.996497, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.033699035644531, Accuracy = 0.7459797263145447\n",
      "Training iter #9606000:   Batch Loss = 8.548731, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.03316593170166, Accuracy = 0.7456819415092468\n",
      "Training iter #9609000:   Batch Loss = 8.727894, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.032538414001465, Accuracy = 0.7456819415092468\n",
      "Training iter #9612000:   Batch Loss = 8.884665, Accuracy = 0.7786666750907898\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.031913757324219, Accuracy = 0.7456819415092468\n",
      "Training iter #9615000:   Batch Loss = 9.224747, Accuracy = 0.6706666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.03130054473877, Accuracy = 0.7456819415092468\n",
      "Training iter #9618000:   Batch Loss = 8.541462, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.030762672424316, Accuracy = 0.7456819415092468\n",
      "Training iter #9621000:   Batch Loss = 8.524490, Accuracy = 0.887333333492279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.030086517333984, Accuracy = 0.745384156703949\n",
      "Training iter #9624000:   Batch Loss = 8.876682, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.029325485229492, Accuracy = 0.7459797263145447\n",
      "Training iter #9627000:   Batch Loss = 8.999776, Accuracy = 0.7273333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.028779983520508, Accuracy = 0.7459797263145447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9630000:   Batch Loss = 8.984771, Accuracy = 0.7386666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.028108596801758, Accuracy = 0.7459797263145447\n",
      "Training iter #9633000:   Batch Loss = 8.568202, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.027595520019531, Accuracy = 0.7459797263145447\n",
      "Training iter #9636000:   Batch Loss = 8.728045, Accuracy = 0.8186666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.02698802947998, Accuracy = 0.7459797263145447\n",
      "Training iter #9639000:   Batch Loss = 8.956025, Accuracy = 0.7553333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.026778221130371, Accuracy = 0.7459797263145447\n",
      "Training iter #9642000:   Batch Loss = 9.183055, Accuracy = 0.6926666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.026421546936035, Accuracy = 0.7462775707244873\n",
      "Training iter #9645000:   Batch Loss = 8.493588, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.026092529296875, Accuracy = 0.7465753555297852\n",
      "Training iter #9648000:   Batch Loss = 8.566420, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.025611877441406, Accuracy = 0.7456819415092468\n",
      "Training iter #9651000:   Batch Loss = 8.821205, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.024937629699707, Accuracy = 0.7456819415092468\n",
      "Training iter #9654000:   Batch Loss = 9.033155, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.024131774902344, Accuracy = 0.7462775707244873\n",
      "Training iter #9657000:   Batch Loss = 9.040214, Accuracy = 0.7173333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.023358345031738, Accuracy = 0.7462775707244873\n",
      "Training iter #9660000:   Batch Loss = 8.563560, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.022557258605957, Accuracy = 0.746873140335083\n",
      "Training iter #9663000:   Batch Loss = 8.731041, Accuracy = 0.8206666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.021859169006348, Accuracy = 0.7462775707244873\n",
      "Training iter #9666000:   Batch Loss = 8.970373, Accuracy = 0.7526666522026062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.02112865447998, Accuracy = 0.7465753555297852\n",
      "Training iter #9669000:   Batch Loss = 9.115115, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.020511627197266, Accuracy = 0.7471709251403809\n",
      "Training iter #9672000:   Batch Loss = 8.433198, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.01987075805664, Accuracy = 0.7471709251403809\n",
      "Training iter #9675000:   Batch Loss = 8.632160, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.019186019897461, Accuracy = 0.7474687099456787\n",
      "Training iter #9678000:   Batch Loss = 8.720560, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.01833724975586, Accuracy = 0.7471709251403809\n",
      "Training iter #9681000:   Batch Loss = 9.028669, Accuracy = 0.7273333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.017507553100586, Accuracy = 0.746873140335083\n",
      "Training iter #9684000:   Batch Loss = 9.046591, Accuracy = 0.7246666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.016886711120605, Accuracy = 0.7474687099456787\n",
      "Training iter #9687000:   Batch Loss = 8.566343, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.016403198242188, Accuracy = 0.7474687099456787\n",
      "Training iter #9690000:   Batch Loss = 8.724835, Accuracy = 0.8180000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.015961647033691, Accuracy = 0.7477665543556213\n",
      "Training iter #9693000:   Batch Loss = 8.988953, Accuracy = 0.734000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.01556396484375, Accuracy = 0.7474687099456787\n",
      "Training iter #9696000:   Batch Loss = 9.140289, Accuracy = 0.687333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.015209197998047, Accuracy = 0.7480643391609192\n",
      "Training iter #9699000:   Batch Loss = 8.447900, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.014954566955566, Accuracy = 0.7477665543556213\n",
      "Training iter #9702000:   Batch Loss = 8.640878, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.014348030090332, Accuracy = 0.7474687099456787\n",
      "Training iter #9705000:   Batch Loss = 8.717422, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.013778686523438, Accuracy = 0.7477665543556213\n",
      "Training iter #9708000:   Batch Loss = 9.006126, Accuracy = 0.7366666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.013197898864746, Accuracy = 0.748362123966217\n",
      "Training iter #9711000:   Batch Loss = 8.919949, Accuracy = 0.7620000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.012779235839844, Accuracy = 0.7480643391609192\n",
      "Training iter #9714000:   Batch Loss = 8.538839, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.012228965759277, Accuracy = 0.7477665543556213\n",
      "Training iter #9717000:   Batch Loss = 8.809709, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.011630058288574, Accuracy = 0.7480643391609192\n",
      "Training iter #9720000:   Batch Loss = 9.053746, Accuracy = 0.7053333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.01130485534668, Accuracy = 0.7489576935768127\n",
      "Training iter #9723000:   Batch Loss = 9.078112, Accuracy = 0.6993333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.010576248168945, Accuracy = 0.7480643391609192\n",
      "Training iter #9726000:   Batch Loss = 8.455979, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.00994873046875, Accuracy = 0.7480643391609192\n",
      "Training iter #9729000:   Batch Loss = 8.643941, Accuracy = 0.8553333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.009315490722656, Accuracy = 0.748362123966217\n",
      "Training iter #9732000:   Batch Loss = 8.653049, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.008708953857422, Accuracy = 0.748362123966217\n",
      "Training iter #9735000:   Batch Loss = 9.012127, Accuracy = 0.7386666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.008119583129883, Accuracy = 0.7480643391609192\n",
      "Training iter #9738000:   Batch Loss = 8.878886, Accuracy = 0.7826666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.007370948791504, Accuracy = 0.7480643391609192\n",
      "Training iter #9741000:   Batch Loss = 8.533028, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.006607055664062, Accuracy = 0.748362123966217\n",
      "Training iter #9744000:   Batch Loss = 8.819197, Accuracy = 0.7986666560173035\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.005924224853516, Accuracy = 0.7489576935768127\n",
      "Training iter #9747000:   Batch Loss = 9.079593, Accuracy = 0.6953333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.005301475524902, Accuracy = 0.7486599087715149\n",
      "Training iter #9750000:   Batch Loss = 9.061069, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.004720687866211, Accuracy = 0.7486599087715149\n",
      "Training iter #9753000:   Batch Loss = 8.486956, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.00417709350586, Accuracy = 0.7486599087715149\n",
      "Training iter #9756000:   Batch Loss = 8.631552, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.003548622131348, Accuracy = 0.7486599087715149\n",
      "Training iter #9759000:   Batch Loss = 8.662580, Accuracy = 0.8273333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.002872467041016, Accuracy = 0.7489576935768127\n",
      "Training iter #9762000:   Batch Loss = 9.039793, Accuracy = 0.7286666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.002446174621582, Accuracy = 0.7486599087715149\n",
      "Training iter #9765000:   Batch Loss = 8.855567, Accuracy = 0.7846666574478149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.002124786376953, Accuracy = 0.7486599087715149\n",
      "Training iter #9768000:   Batch Loss = 8.508534, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.00156307220459, Accuracy = 0.7489576935768127\n",
      "Training iter #9771000:   Batch Loss = 8.815593, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.000887870788574, Accuracy = 0.7492555379867554\n",
      "Training iter #9774000:   Batch Loss = 9.066883, Accuracy = 0.7046666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 9.000282287597656, Accuracy = 0.7498511075973511\n",
      "Training iter #9777000:   Batch Loss = 9.064992, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99979019165039, Accuracy = 0.7498511075973511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9780000:   Batch Loss = 8.475252, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99921989440918, Accuracy = 0.7498511075973511\n",
      "Training iter #9783000:   Batch Loss = 8.636374, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.998566627502441, Accuracy = 0.7498511075973511\n",
      "Training iter #9786000:   Batch Loss = 8.621142, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.997933387756348, Accuracy = 0.7498511075973511\n",
      "Training iter #9789000:   Batch Loss = 9.080873, Accuracy = 0.7053333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.997308731079102, Accuracy = 0.7492555379867554\n",
      "Training iter #9792000:   Batch Loss = 8.794919, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.996864318847656, Accuracy = 0.7489576935768127\n",
      "Training iter #9795000:   Batch Loss = 8.502687, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.996248245239258, Accuracy = 0.7501488924026489\n",
      "Training iter #9798000:   Batch Loss = 8.776272, Accuracy = 0.7993333339691162\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.995599746704102, Accuracy = 0.7501488924026489\n",
      "Training iter #9801000:   Batch Loss = 9.077120, Accuracy = 0.7059999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.995016098022461, Accuracy = 0.7495533227920532\n",
      "Training iter #9804000:   Batch Loss = 8.995069, Accuracy = 0.7213333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99453353881836, Accuracy = 0.7495533227920532\n",
      "Training iter #9807000:   Batch Loss = 8.468273, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.993999481201172, Accuracy = 0.7495533227920532\n",
      "Training iter #9810000:   Batch Loss = 8.651482, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99336051940918, Accuracy = 0.7492555379867554\n",
      "Training iter #9813000:   Batch Loss = 8.689142, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99259090423584, Accuracy = 0.7498511075973511\n",
      "Training iter #9816000:   Batch Loss = 9.133713, Accuracy = 0.6886666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99210262298584, Accuracy = 0.7501488924026489\n",
      "Training iter #9819000:   Batch Loss = 8.751167, Accuracy = 0.8286666870117188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99170970916748, Accuracy = 0.7501488924026489\n",
      "Training iter #9822000:   Batch Loss = 8.466098, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.991066932678223, Accuracy = 0.7507444620132446\n",
      "Training iter #9825000:   Batch Loss = 8.775248, Accuracy = 0.8033333420753479\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.99061107635498, Accuracy = 0.7510423064231873\n",
      "Training iter #9828000:   Batch Loss = 9.090701, Accuracy = 0.6993333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.989990234375, Accuracy = 0.7513400912284851\n",
      "Training iter #9831000:   Batch Loss = 8.898390, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.989498138427734, Accuracy = 0.751637876033783\n",
      "Training iter #9834000:   Batch Loss = 8.475129, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.988958358764648, Accuracy = 0.751637876033783\n",
      "Training iter #9837000:   Batch Loss = 8.651791, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.988335609436035, Accuracy = 0.7510423064231873\n",
      "Training iter #9840000:   Batch Loss = 8.705688, Accuracy = 0.8206666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.98773193359375, Accuracy = 0.7519356608390808\n",
      "Training iter #9843000:   Batch Loss = 9.140715, Accuracy = 0.6919999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.987165451049805, Accuracy = 0.7519356608390808\n",
      "Training iter #9846000:   Batch Loss = 8.687969, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.986725807189941, Accuracy = 0.7519356608390808\n",
      "Training iter #9849000:   Batch Loss = 8.500271, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.986135482788086, Accuracy = 0.7522334456443787\n",
      "Training iter #9852000:   Batch Loss = 8.803485, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.985486030578613, Accuracy = 0.7519356608390808\n",
      "Training iter #9855000:   Batch Loss = 9.055483, Accuracy = 0.7066666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.984922409057617, Accuracy = 0.7519356608390808\n",
      "Training iter #9858000:   Batch Loss = 8.897655, Accuracy = 0.7553333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.98444938659668, Accuracy = 0.7519356608390808\n",
      "Training iter #9861000:   Batch Loss = 8.444117, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.983924865722656, Accuracy = 0.7525312900543213\n",
      "Training iter #9864000:   Batch Loss = 8.639669, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.983328819274902, Accuracy = 0.7522334456443787\n",
      "Training iter #9867000:   Batch Loss = 8.707565, Accuracy = 0.8119999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.982758522033691, Accuracy = 0.7525312900543213\n",
      "Training iter #9870000:   Batch Loss = 9.196394, Accuracy = 0.6759999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.982190132141113, Accuracy = 0.7522334456443787\n",
      "Training iter #9873000:   Batch Loss = 8.658227, Accuracy = 0.8560000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.981742858886719, Accuracy = 0.7525312900543213\n",
      "Training iter #9876000:   Batch Loss = 8.485183, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.98114013671875, Accuracy = 0.7528290748596191\n",
      "Training iter #9879000:   Batch Loss = 8.801729, Accuracy = 0.8026666641235352\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.980446815490723, Accuracy = 0.753126859664917\n",
      "Training iter #9882000:   Batch Loss = 9.009669, Accuracy = 0.7239999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.979793548583984, Accuracy = 0.7528290748596191\n",
      "Training iter #9885000:   Batch Loss = 8.920653, Accuracy = 0.7506666779518127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.979247093200684, Accuracy = 0.7525312900543213\n",
      "Training iter #9888000:   Batch Loss = 8.459938, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.978670120239258, Accuracy = 0.7528290748596191\n",
      "Training iter #9891000:   Batch Loss = 8.649457, Accuracy = 0.8399999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.978021621704102, Accuracy = 0.7525312900543213\n",
      "Training iter #9894000:   Batch Loss = 8.781820, Accuracy = 0.7940000295639038\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.97743034362793, Accuracy = 0.7525312900543213\n",
      "Training iter #9897000:   Batch Loss = 9.128088, Accuracy = 0.6913333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.976897239685059, Accuracy = 0.7522334456443787\n",
      "Training iter #9900000:   Batch Loss = 8.595494, Accuracy = 0.8700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.976494789123535, Accuracy = 0.7525312900543213\n",
      "Training iter #9903000:   Batch Loss = 8.485822, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.975902557373047, Accuracy = 0.753126859664917\n",
      "Training iter #9906000:   Batch Loss = 8.792434, Accuracy = 0.8019999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.975075721740723, Accuracy = 0.7528290748596191\n",
      "Training iter #9909000:   Batch Loss = 8.966284, Accuracy = 0.7326666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.974630355834961, Accuracy = 0.7528290748596191\n",
      "Training iter #9912000:   Batch Loss = 8.901424, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.974194526672363, Accuracy = 0.7528290748596191\n",
      "Training iter #9915000:   Batch Loss = 8.469809, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.973638534545898, Accuracy = 0.7528290748596191\n",
      "Training iter #9918000:   Batch Loss = 8.679858, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.972982406616211, Accuracy = 0.7528290748596191\n",
      "Training iter #9921000:   Batch Loss = 8.782648, Accuracy = 0.7986666560173035\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.972305297851562, Accuracy = 0.753126859664917\n",
      "Training iter #9924000:   Batch Loss = 9.176136, Accuracy = 0.6840000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.971689224243164, Accuracy = 0.753126859664917\n",
      "Training iter #9927000:   Batch Loss = 8.527012, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.971209526062012, Accuracy = 0.753126859664917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9930000:   Batch Loss = 8.484776, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.970545768737793, Accuracy = 0.7534246444702148\n",
      "Training iter #9933000:   Batch Loss = 8.823708, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.970006942749023, Accuracy = 0.7534246444702148\n",
      "Training iter #9936000:   Batch Loss = 8.930721, Accuracy = 0.7480000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.96940803527832, Accuracy = 0.7537224292755127\n",
      "Training iter #9939000:   Batch Loss = 8.910773, Accuracy = 0.7526666522026062\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.968962669372559, Accuracy = 0.7534246444702148\n",
      "Training iter #9942000:   Batch Loss = 8.495839, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.968429565429688, Accuracy = 0.7534246444702148\n",
      "Training iter #9945000:   Batch Loss = 8.673281, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.967771530151367, Accuracy = 0.7534246444702148\n",
      "Training iter #9948000:   Batch Loss = 8.845372, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.967132568359375, Accuracy = 0.7540202736854553\n",
      "Training iter #9951000:   Batch Loss = 9.133482, Accuracy = 0.6966666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.966682434082031, Accuracy = 0.7540202736854553\n",
      "Training iter #9954000:   Batch Loss = 8.469751, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.966285705566406, Accuracy = 0.753126859664917\n",
      "Training iter #9957000:   Batch Loss = 8.495555, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.965682983398438, Accuracy = 0.7540202736854553\n",
      "Training iter #9960000:   Batch Loss = 8.784142, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.965048789978027, Accuracy = 0.7543180584907532\n",
      "Training iter #9963000:   Batch Loss = 8.932448, Accuracy = 0.7459999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.964483261108398, Accuracy = 0.754615843296051\n",
      "Training iter #9966000:   Batch Loss = 8.939589, Accuracy = 0.7453333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.964049339294434, Accuracy = 0.7549136281013489\n",
      "Training iter #9969000:   Batch Loss = 8.495666, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.963460922241211, Accuracy = 0.7549136281013489\n",
      "Training iter #9972000:   Batch Loss = 8.670835, Accuracy = 0.8306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.962581634521484, Accuracy = 0.7549136281013489\n",
      "Training iter #9975000:   Batch Loss = 8.876684, Accuracy = 0.7713333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.96189022064209, Accuracy = 0.7552114129066467\n",
      "Training iter #9978000:   Batch Loss = 9.064131, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.961494445800781, Accuracy = 0.7552114129066467\n",
      "Training iter #9981000:   Batch Loss = 8.417235, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.961118698120117, Accuracy = 0.754615843296051\n",
      "Training iter #9984000:   Batch Loss = 8.524101, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.960561752319336, Accuracy = 0.7549136281013489\n",
      "Training iter #9987000:   Batch Loss = 8.707542, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.959943771362305, Accuracy = 0.7549136281013489\n",
      "Training iter #9990000:   Batch Loss = 8.954709, Accuracy = 0.7419999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.959392547607422, Accuracy = 0.7555092573165894\n",
      "Training iter #9993000:   Batch Loss = 8.967594, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.95897388458252, Accuracy = 0.7552114129066467\n",
      "Training iter #9996000:   Batch Loss = 8.501286, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.958369255065918, Accuracy = 0.7555092573165894\n",
      "Training iter #9999000:   Batch Loss = 8.665553, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.957722663879395, Accuracy = 0.7552114129066467\n",
      "Training iter #10002000:   Batch Loss = 8.915497, Accuracy = 0.753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.957144737243652, Accuracy = 0.7552114129066467\n",
      "Training iter #10005000:   Batch Loss = 9.006408, Accuracy = 0.7319999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.956613540649414, Accuracy = 0.7552114129066467\n",
      "Training iter #10008000:   Batch Loss = 8.385131, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.956119537353516, Accuracy = 0.7552114129066467\n",
      "Training iter #10011000:   Batch Loss = 8.566362, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.955521583557129, Accuracy = 0.7549136281013489\n",
      "Training iter #10014000:   Batch Loss = 8.685878, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.954859733581543, Accuracy = 0.7555092573165894\n",
      "Training iter #10017000:   Batch Loss = 8.921391, Accuracy = 0.7559999823570251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.954269409179688, Accuracy = 0.7564026117324829\n",
      "Training iter #10020000:   Batch Loss = 8.925517, Accuracy = 0.7573333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.953807830810547, Accuracy = 0.7561048269271851\n",
      "Training iter #10023000:   Batch Loss = 8.502873, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.953176498413086, Accuracy = 0.7564026117324829\n",
      "Training iter #10026000:   Batch Loss = 8.660243, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.952566146850586, Accuracy = 0.7567003965377808\n",
      "Training iter #10029000:   Batch Loss = 8.945354, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.951869010925293, Accuracy = 0.7561048269271851\n",
      "Training iter #10032000:   Batch Loss = 9.044455, Accuracy = 0.7173333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.9512357711792, Accuracy = 0.7564026117324829\n",
      "Training iter #10035000:   Batch Loss = 8.412717, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.950749397277832, Accuracy = 0.7567003965377808\n",
      "Training iter #10038000:   Batch Loss = 8.577109, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.950010299682617, Accuracy = 0.7572960257530212\n",
      "Training iter #10041000:   Batch Loss = 8.604513, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.949384689331055, Accuracy = 0.7572960257530212\n",
      "Training iter #10044000:   Batch Loss = 8.930875, Accuracy = 0.7639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.948871612548828, Accuracy = 0.7581893801689148\n",
      "Training iter #10047000:   Batch Loss = 8.835837, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.948482513427734, Accuracy = 0.7584871649742126\n",
      "Training iter #10050000:   Batch Loss = 8.476267, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.94796085357666, Accuracy = 0.7578915953636169\n",
      "Training iter #10053000:   Batch Loss = 8.770532, Accuracy = 0.7973333597183228\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.94737434387207, Accuracy = 0.7581893801689148\n",
      "Training iter #10056000:   Batch Loss = 8.965864, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.946845054626465, Accuracy = 0.7584871649742126\n",
      "Training iter #10059000:   Batch Loss = 9.011522, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.94635009765625, Accuracy = 0.7581893801689148\n",
      "Training iter #10062000:   Batch Loss = 8.395803, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.94583511352539, Accuracy = 0.7578915953636169\n",
      "Training iter #10065000:   Batch Loss = 8.561664, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.945171356201172, Accuracy = 0.7581893801689148\n",
      "Training iter #10068000:   Batch Loss = 8.593142, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.944567680358887, Accuracy = 0.7575938105583191\n",
      "Training iter #10071000:   Batch Loss = 8.932992, Accuracy = 0.7586666941642761\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.943951606750488, Accuracy = 0.7575938105583191\n",
      "Training iter #10074000:   Batch Loss = 8.784950, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.943577766418457, Accuracy = 0.7572960257530212\n",
      "Training iter #10077000:   Batch Loss = 8.448892, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.942955017089844, Accuracy = 0.7575938105583191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10080000:   Batch Loss = 8.728224, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.942292213439941, Accuracy = 0.7581893801689148\n",
      "Training iter #10083000:   Batch Loss = 8.979352, Accuracy = 0.7266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.941712379455566, Accuracy = 0.7575938105583191\n",
      "Training iter #10086000:   Batch Loss = 8.962479, Accuracy = 0.734000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.941166877746582, Accuracy = 0.7572960257530212\n",
      "Training iter #10089000:   Batch Loss = 8.427094, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.940567016601562, Accuracy = 0.7575938105583191\n",
      "Training iter #10092000:   Batch Loss = 8.576457, Accuracy = 0.859333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.939835548400879, Accuracy = 0.7575938105583191\n",
      "Training iter #10095000:   Batch Loss = 8.559701, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.939229965209961, Accuracy = 0.7575938105583191\n",
      "Training iter #10098000:   Batch Loss = 8.979517, Accuracy = 0.737333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.93899917602539, Accuracy = 0.7569982409477234\n",
      "Training iter #10101000:   Batch Loss = 8.735090, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.938705444335938, Accuracy = 0.7572960257530212\n",
      "Training iter #10104000:   Batch Loss = 8.460281, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.938056945800781, Accuracy = 0.7569982409477234\n",
      "Training iter #10107000:   Batch Loss = 8.739918, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.937291145324707, Accuracy = 0.7569982409477234\n",
      "Training iter #10110000:   Batch Loss = 8.994199, Accuracy = 0.7253333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.936681747436523, Accuracy = 0.7575938105583191\n",
      "Training iter #10113000:   Batch Loss = 8.944761, Accuracy = 0.7453333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.936144828796387, Accuracy = 0.7572960257530212\n",
      "Training iter #10116000:   Batch Loss = 8.418517, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.935514450073242, Accuracy = 0.7575938105583191\n",
      "Training iter #10119000:   Batch Loss = 8.589607, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.934785842895508, Accuracy = 0.7575938105583191\n",
      "Training iter #10122000:   Batch Loss = 8.599979, Accuracy = 0.8413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.934318542480469, Accuracy = 0.7569982409477234\n",
      "Training iter #10125000:   Batch Loss = 8.994230, Accuracy = 0.7306666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.933852195739746, Accuracy = 0.7575938105583191\n",
      "Training iter #10128000:   Batch Loss = 8.707958, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.933456420898438, Accuracy = 0.7575938105583191\n",
      "Training iter #10131000:   Batch Loss = 8.456229, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.932867050170898, Accuracy = 0.7572960257530212\n",
      "Training iter #10134000:   Batch Loss = 8.705261, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.932326316833496, Accuracy = 0.7572960257530212\n",
      "Training iter #10137000:   Batch Loss = 8.997600, Accuracy = 0.7239999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.931785583496094, Accuracy = 0.7575938105583191\n",
      "Training iter #10140000:   Batch Loss = 8.872693, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.93137264251709, Accuracy = 0.7567003965377808\n",
      "Training iter #10143000:   Batch Loss = 8.434934, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.9308443069458, Accuracy = 0.7569982409477234\n",
      "Training iter #10146000:   Batch Loss = 8.594075, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.93005084991455, Accuracy = 0.7572960257530212\n",
      "Training iter #10149000:   Batch Loss = 8.627542, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.929289817810059, Accuracy = 0.7575938105583191\n",
      "Training iter #10152000:   Batch Loss = 9.057890, Accuracy = 0.7120000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.92874526977539, Accuracy = 0.7572960257530212\n",
      "Training iter #10155000:   Batch Loss = 8.669057, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.928258895874023, Accuracy = 0.7578915953636169\n",
      "Training iter #10158000:   Batch Loss = 8.431024, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.927440643310547, Accuracy = 0.7584871649742126\n",
      "Training iter #10161000:   Batch Loss = 8.709056, Accuracy = 0.8153333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.926663398742676, Accuracy = 0.7590827941894531\n",
      "Training iter #10164000:   Batch Loss = 8.994670, Accuracy = 0.7213333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.92615795135498, Accuracy = 0.7590827941894531\n",
      "Training iter #10167000:   Batch Loss = 8.853126, Accuracy = 0.768666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.925848007202148, Accuracy = 0.7590827941894531\n",
      "Training iter #10170000:   Batch Loss = 8.390631, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.925397872924805, Accuracy = 0.759380578994751\n",
      "Training iter #10173000:   Batch Loss = 8.578092, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.924896240234375, Accuracy = 0.7599761486053467\n",
      "Training iter #10176000:   Batch Loss = 8.651377, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.924493789672852, Accuracy = 0.7599761486053467\n",
      "Training iter #10179000:   Batch Loss = 9.093200, Accuracy = 0.7039999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.924174308776855, Accuracy = 0.7599761486053467\n",
      "Training iter #10182000:   Batch Loss = 8.602178, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.923776626586914, Accuracy = 0.7596783638000488\n",
      "Training iter #10185000:   Batch Loss = 8.435375, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.923065185546875, Accuracy = 0.7605717778205872\n",
      "Training iter #10188000:   Batch Loss = 8.766439, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.922388076782227, Accuracy = 0.7605717778205872\n",
      "Training iter #10191000:   Batch Loss = 8.950844, Accuracy = 0.7279999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.921707153320312, Accuracy = 0.7605717778205872\n",
      "Training iter #10194000:   Batch Loss = 8.806284, Accuracy = 0.7826666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.921249389648438, Accuracy = 0.7599761486053467\n",
      "Training iter #10197000:   Batch Loss = 8.397635, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.920612335205078, Accuracy = 0.7599761486053467\n",
      "Training iter #10200000:   Batch Loss = 8.594615, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.91979694366455, Accuracy = 0.7605717778205872\n",
      "Training iter #10203000:   Batch Loss = 8.688164, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.9191255569458, Accuracy = 0.7602739930152893\n",
      "Training iter #10206000:   Batch Loss = 9.077275, Accuracy = 0.7073333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.918697357177734, Accuracy = 0.7605717778205872\n",
      "Training iter #10209000:   Batch Loss = 8.553020, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.918292999267578, Accuracy = 0.7605717778205872\n",
      "Training iter #10212000:   Batch Loss = 8.432318, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.917688369750977, Accuracy = 0.7605717778205872\n",
      "Training iter #10215000:   Batch Loss = 8.734873, Accuracy = 0.8146666884422302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.917049407958984, Accuracy = 0.7611673474311829\n",
      "Training iter #10218000:   Batch Loss = 8.901065, Accuracy = 0.7453333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.9165620803833, Accuracy = 0.7614651322364807\n",
      "Training iter #10221000:   Batch Loss = 8.848546, Accuracy = 0.7739999890327454\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.91623306274414, Accuracy = 0.7605717778205872\n",
      "Training iter #10224000:   Batch Loss = 8.416732, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.915745735168457, Accuracy = 0.7614651322364807\n",
      "Training iter #10227000:   Batch Loss = 8.583344, Accuracy = 0.8519999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.915078163146973, Accuracy = 0.7626563310623169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10230000:   Batch Loss = 8.725268, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.914655685424805, Accuracy = 0.762358546257019\n",
      "Training iter #10233000:   Batch Loss = 9.104275, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.913993835449219, Accuracy = 0.7611673474311829\n",
      "Training iter #10236000:   Batch Loss = 8.533534, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.913361549377441, Accuracy = 0.7620607614517212\n",
      "Training iter #10239000:   Batch Loss = 8.417703, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.912561416625977, Accuracy = 0.7626563310623169\n",
      "Training iter #10242000:   Batch Loss = 8.728446, Accuracy = 0.809333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.91203498840332, Accuracy = 0.7626563310623169\n",
      "Training iter #10245000:   Batch Loss = 8.874989, Accuracy = 0.7586666941642761\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.911534309387207, Accuracy = 0.7626563310623169\n",
      "Training iter #10248000:   Batch Loss = 8.832777, Accuracy = 0.7733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.91124439239502, Accuracy = 0.7626563310623169\n",
      "Training iter #10251000:   Batch Loss = 8.410565, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.9107666015625, Accuracy = 0.7626563310623169\n",
      "Training iter #10254000:   Batch Loss = 8.601715, Accuracy = 0.8413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.91015625, Accuracy = 0.7629541158676147\n",
      "Training iter #10257000:   Batch Loss = 8.725563, Accuracy = 0.8026666641235352\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.909553527832031, Accuracy = 0.7632519602775574\n",
      "Training iter #10260000:   Batch Loss = 9.076252, Accuracy = 0.7073333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.909036636352539, Accuracy = 0.7629541158676147\n",
      "Training iter #10263000:   Batch Loss = 8.438884, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.908642768859863, Accuracy = 0.7629541158676147\n",
      "Training iter #10266000:   Batch Loss = 8.424616, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.90805721282959, Accuracy = 0.7632519602775574\n",
      "Training iter #10269000:   Batch Loss = 8.745115, Accuracy = 0.7979999780654907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.907464981079102, Accuracy = 0.7632519602775574\n",
      "Training iter #10272000:   Batch Loss = 8.842473, Accuracy = 0.7713333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.906858444213867, Accuracy = 0.7638475298881531\n",
      "Training iter #10275000:   Batch Loss = 8.829854, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.906410217285156, Accuracy = 0.7641453146934509\n",
      "Training iter #10278000:   Batch Loss = 8.442567, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.905835151672363, Accuracy = 0.7626563310623169\n",
      "Training iter #10281000:   Batch Loss = 8.597599, Accuracy = 0.8433333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.905210494995117, Accuracy = 0.7632519602775574\n",
      "Training iter #10284000:   Batch Loss = 8.794580, Accuracy = 0.7826666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.904808044433594, Accuracy = 0.7635497450828552\n",
      "Training iter #10287000:   Batch Loss = 9.026802, Accuracy = 0.7286666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.904473304748535, Accuracy = 0.7638475298881531\n",
      "Training iter #10290000:   Batch Loss = 8.381089, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.90417766571045, Accuracy = 0.7635497450828552\n",
      "Training iter #10293000:   Batch Loss = 8.444342, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.903661727905273, Accuracy = 0.7644430994987488\n",
      "Training iter #10296000:   Batch Loss = 8.724324, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.90296745300293, Accuracy = 0.7650387287139893\n",
      "Training iter #10299000:   Batch Loss = 8.862636, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.902423858642578, Accuracy = 0.7650387287139893\n",
      "Training iter #10302000:   Batch Loss = 8.878416, Accuracy = 0.7620000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.902006149291992, Accuracy = 0.7647409439086914\n",
      "Training iter #10305000:   Batch Loss = 8.453571, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.901314735412598, Accuracy = 0.7647409439086914\n",
      "Training iter #10308000:   Batch Loss = 8.605451, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.900931358337402, Accuracy = 0.7644430994987488\n",
      "Training iter #10311000:   Batch Loss = 8.817969, Accuracy = 0.781333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.900132179260254, Accuracy = 0.7647409439086914\n",
      "Training iter #10314000:   Batch Loss = 8.962626, Accuracy = 0.7473333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.899553298950195, Accuracy = 0.7653365135192871\n",
      "Training iter #10317000:   Batch Loss = 8.334515, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.899002075195312, Accuracy = 0.765634298324585\n",
      "Training iter #10320000:   Batch Loss = 8.508317, Accuracy = 0.874666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.898421287536621, Accuracy = 0.7653365135192871\n",
      "Training iter #10323000:   Batch Loss = 8.630486, Accuracy = 0.8320000171661377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.897756576538086, Accuracy = 0.7653365135192871\n",
      "Training iter #10326000:   Batch Loss = 8.882112, Accuracy = 0.7586666941642761\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.897192001342773, Accuracy = 0.7662299275398254\n",
      "Training iter #10329000:   Batch Loss = 8.907493, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.896771430969238, Accuracy = 0.765634298324585\n",
      "Training iter #10332000:   Batch Loss = 8.441635, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.896136283874512, Accuracy = 0.7650387287139893\n",
      "Training iter #10335000:   Batch Loss = 8.604777, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.89555835723877, Accuracy = 0.7647409439086914\n",
      "Training iter #10338000:   Batch Loss = 8.854561, Accuracy = 0.7620000243186951\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.8949556350708, Accuracy = 0.765634298324585\n",
      "Training iter #10341000:   Batch Loss = 8.946488, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.894550323486328, Accuracy = 0.7662299275398254\n",
      "Training iter #10344000:   Batch Loss = 8.324142, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.894248008728027, Accuracy = 0.765634298324585\n",
      "Training iter #10347000:   Batch Loss = 8.526519, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.8939208984375, Accuracy = 0.7650387287139893\n",
      "Training iter #10350000:   Batch Loss = 8.609062, Accuracy = 0.8433333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.893562316894531, Accuracy = 0.7662299275398254\n",
      "Training iter #10353000:   Batch Loss = 8.850891, Accuracy = 0.7706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.89315128326416, Accuracy = 0.7668254971504211\n",
      "Training iter #10356000:   Batch Loss = 8.803006, Accuracy = 0.7860000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.892849922180176, Accuracy = 0.7674210667610168\n",
      "Training iter #10359000:   Batch Loss = 8.451802, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.892184257507324, Accuracy = 0.7668254971504211\n",
      "Training iter #10362000:   Batch Loss = 8.659721, Accuracy = 0.8273333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.891457557678223, Accuracy = 0.767123281955719\n",
      "Training iter #10365000:   Batch Loss = 8.901751, Accuracy = 0.7419999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.890802383422852, Accuracy = 0.7677188515663147\n",
      "Training iter #10368000:   Batch Loss = 8.938261, Accuracy = 0.7473333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.890190124511719, Accuracy = 0.7680166959762573\n",
      "Training iter #10371000:   Batch Loss = 8.348919, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.889659881591797, Accuracy = 0.7680166959762573\n",
      "Training iter #10374000:   Batch Loss = 8.520608, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.889060020446777, Accuracy = 0.7668254971504211\n",
      "Training iter #10377000:   Batch Loss = 8.562228, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.888440132141113, Accuracy = 0.7674210667610168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10380000:   Batch Loss = 8.845859, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.887795448303223, Accuracy = 0.7677188515663147\n",
      "Training iter #10383000:   Batch Loss = 8.742825, Accuracy = 0.8153333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.887412071228027, Accuracy = 0.7683144807815552\n",
      "Training iter #10386000:   Batch Loss = 8.428927, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.886860847473145, Accuracy = 0.767123281955719\n",
      "Training iter #10389000:   Batch Loss = 8.712792, Accuracy = 0.8146666884422302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.886404037475586, Accuracy = 0.7677188515663147\n",
      "Training iter #10392000:   Batch Loss = 8.922887, Accuracy = 0.734666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.886061668395996, Accuracy = 0.7683144807815552\n",
      "Training iter #10395000:   Batch Loss = 8.906897, Accuracy = 0.7540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.88577651977539, Accuracy = 0.7680166959762573\n",
      "Training iter #10398000:   Batch Loss = 8.366817, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.885347366333008, Accuracy = 0.7683144807815552\n",
      "Training iter #10401000:   Batch Loss = 8.529505, Accuracy = 0.8659999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.884856224060059, Accuracy = 0.768612265586853\n",
      "Training iter #10404000:   Batch Loss = 8.544147, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.884449005126953, Accuracy = 0.7692078351974487\n",
      "Training iter #10407000:   Batch Loss = 8.865296, Accuracy = 0.765999972820282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.884001731872559, Accuracy = 0.7692078351974487\n",
      "Training iter #10410000:   Batch Loss = 8.721247, Accuracy = 0.8206666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.883678436279297, Accuracy = 0.7692078351974487\n",
      "Training iter #10413000:   Batch Loss = 8.411117, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.883100509643555, Accuracy = 0.7689100503921509\n",
      "Training iter #10416000:   Batch Loss = 8.684158, Accuracy = 0.8180000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.88247299194336, Accuracy = 0.7689100503921509\n",
      "Training iter #10419000:   Batch Loss = 8.906119, Accuracy = 0.7406666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.881784439086914, Accuracy = 0.7689100503921509\n",
      "Training iter #10422000:   Batch Loss = 8.874635, Accuracy = 0.762666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.881329536437988, Accuracy = 0.7692078351974487\n",
      "Training iter #10425000:   Batch Loss = 8.364987, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.88083553314209, Accuracy = 0.7695056796073914\n",
      "Training iter #10428000:   Batch Loss = 8.524686, Accuracy = 0.8653333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.880175590515137, Accuracy = 0.7695056796073914\n",
      "Training iter #10431000:   Batch Loss = 8.503264, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.879582405090332, Accuracy = 0.7692078351974487\n",
      "Training iter #10434000:   Batch Loss = 8.937742, Accuracy = 0.7379999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.87907886505127, Accuracy = 0.7692078351974487\n",
      "Training iter #10437000:   Batch Loss = 8.654829, Accuracy = 0.8453333377838135\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.878642082214355, Accuracy = 0.7692078351974487\n",
      "Training iter #10440000:   Batch Loss = 8.396045, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.878016471862793, Accuracy = 0.7689100503921509\n",
      "Training iter #10443000:   Batch Loss = 8.662463, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.877413749694824, Accuracy = 0.7689100503921509\n",
      "Training iter #10446000:   Batch Loss = 8.929908, Accuracy = 0.734000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.87682819366455, Accuracy = 0.7689100503921509\n",
      "Training iter #10449000:   Batch Loss = 8.849138, Accuracy = 0.762666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.876328468322754, Accuracy = 0.7695056796073914\n",
      "Training iter #10452000:   Batch Loss = 8.356203, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.875839233398438, Accuracy = 0.7692078351974487\n",
      "Training iter #10455000:   Batch Loss = 8.543066, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.87516975402832, Accuracy = 0.7689100503921509\n",
      "Training iter #10458000:   Batch Loss = 8.562910, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.87440299987793, Accuracy = 0.7689100503921509\n",
      "Training iter #10461000:   Batch Loss = 8.944678, Accuracy = 0.7379999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.87392520904541, Accuracy = 0.7692078351974487\n",
      "Training iter #10464000:   Batch Loss = 8.637092, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.87354564666748, Accuracy = 0.7695056796073914\n",
      "Training iter #10467000:   Batch Loss = 8.370619, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.872953414916992, Accuracy = 0.7695056796073914\n",
      "Training iter #10470000:   Batch Loss = 8.659530, Accuracy = 0.8253333568572998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.872235298156738, Accuracy = 0.7692078351974487\n",
      "Training iter #10473000:   Batch Loss = 8.942877, Accuracy = 0.7286666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.871675491333008, Accuracy = 0.7692078351974487\n",
      "Training iter #10476000:   Batch Loss = 8.753819, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.871329307556152, Accuracy = 0.7692078351974487\n",
      "Training iter #10479000:   Batch Loss = 8.377668, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.87087631225586, Accuracy = 0.7692078351974487\n",
      "Training iter #10482000:   Batch Loss = 8.539823, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.870320320129395, Accuracy = 0.7692078351974487\n",
      "Training iter #10485000:   Batch Loss = 8.600847, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.869596481323242, Accuracy = 0.7695056796073914\n",
      "Training iter #10488000:   Batch Loss = 8.981096, Accuracy = 0.7279999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.869256019592285, Accuracy = 0.7695056796073914\n",
      "Training iter #10491000:   Batch Loss = 8.573771, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.868916511535645, Accuracy = 0.7695056796073914\n",
      "Training iter #10494000:   Batch Loss = 8.386760, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.868339538574219, Accuracy = 0.7698034644126892\n",
      "Training iter #10497000:   Batch Loss = 8.669714, Accuracy = 0.8220000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.867666244506836, Accuracy = 0.7698034644126892\n",
      "Training iter #10500000:   Batch Loss = 8.922742, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.867081642150879, Accuracy = 0.7698034644126892\n",
      "Training iter #10503000:   Batch Loss = 8.762007, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.866735458374023, Accuracy = 0.7695056796073914\n",
      "Training iter #10506000:   Batch Loss = 8.334751, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.866265296936035, Accuracy = 0.7698034644126892\n",
      "Training iter #10509000:   Batch Loss = 8.534229, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.865671157836914, Accuracy = 0.7701012492179871\n",
      "Training iter #10512000:   Batch Loss = 8.584516, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.864953994750977, Accuracy = 0.7701012492179871\n",
      "Training iter #10515000:   Batch Loss = 9.024480, Accuracy = 0.7213333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.864503860473633, Accuracy = 0.7701012492179871\n",
      "Training iter #10518000:   Batch Loss = 8.539497, Accuracy = 0.874666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.864087104797363, Accuracy = 0.7703990340232849\n",
      "Training iter #10521000:   Batch Loss = 8.365774, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.863492965698242, Accuracy = 0.7706968188285828\n",
      "Training iter #10524000:   Batch Loss = 8.693225, Accuracy = 0.8199999928474426\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.862794876098633, Accuracy = 0.7706968188285828\n",
      "Training iter #10527000:   Batch Loss = 8.859797, Accuracy = 0.7440000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.862235069274902, Accuracy = 0.7706968188285828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10530000:   Batch Loss = 8.745400, Accuracy = 0.7940000295639038\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.861939430236816, Accuracy = 0.7706968188285828\n",
      "Training iter #10533000:   Batch Loss = 8.342537, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.861488342285156, Accuracy = 0.7703990340232849\n",
      "Training iter #10536000:   Batch Loss = 8.538235, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.86091423034668, Accuracy = 0.7703990340232849\n",
      "Training iter #10539000:   Batch Loss = 8.644586, Accuracy = 0.8173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.86021614074707, Accuracy = 0.7706968188285828\n",
      "Training iter #10542000:   Batch Loss = 8.980021, Accuracy = 0.7279999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.859729766845703, Accuracy = 0.7706968188285828\n",
      "Training iter #10545000:   Batch Loss = 8.486291, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.859275817871094, Accuracy = 0.7706968188285828\n",
      "Training iter #10548000:   Batch Loss = 8.380313, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.858665466308594, Accuracy = 0.7706968188285828\n",
      "Training iter #10551000:   Batch Loss = 8.697042, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.858031272888184, Accuracy = 0.7706968188285828\n",
      "Training iter #10554000:   Batch Loss = 8.802016, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.857430458068848, Accuracy = 0.7706968188285828\n",
      "Training iter #10557000:   Batch Loss = 8.762169, Accuracy = 0.7906666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.856950759887695, Accuracy = 0.7709946632385254\n",
      "Training iter #10560000:   Batch Loss = 8.353609, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.85632038116455, Accuracy = 0.7709946632385254\n",
      "Training iter #10563000:   Batch Loss = 8.558447, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.855655670166016, Accuracy = 0.7709946632385254\n",
      "Training iter #10566000:   Batch Loss = 8.652820, Accuracy = 0.8146666884422302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.854904174804688, Accuracy = 0.7709946632385254\n",
      "Training iter #10569000:   Batch Loss = 9.017879, Accuracy = 0.7173333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.854391098022461, Accuracy = 0.7709946632385254\n",
      "Training iter #10572000:   Batch Loss = 8.429378, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.853934288024902, Accuracy = 0.7709946632385254\n",
      "Training iter #10575000:   Batch Loss = 8.376441, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.853294372558594, Accuracy = 0.7709946632385254\n",
      "Training iter #10578000:   Batch Loss = 8.696848, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.852578163146973, Accuracy = 0.7709946632385254\n",
      "Training iter #10581000:   Batch Loss = 8.792059, Accuracy = 0.7693333625793457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.851842880249023, Accuracy = 0.7709946632385254\n",
      "Training iter #10584000:   Batch Loss = 8.745766, Accuracy = 0.7940000295639038\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.85142993927002, Accuracy = 0.7706968188285828\n",
      "Training iter #10587000:   Batch Loss = 8.371211, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.850852966308594, Accuracy = 0.7709946632385254\n",
      "Training iter #10590000:   Batch Loss = 8.546137, Accuracy = 0.8546666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.850165367126465, Accuracy = 0.7712924480438232\n",
      "Training iter #10593000:   Batch Loss = 8.705414, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.849488258361816, Accuracy = 0.7712924480438232\n",
      "Training iter #10596000:   Batch Loss = 8.968582, Accuracy = 0.7366666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.848897933959961, Accuracy = 0.7712924480438232\n",
      "Training iter #10599000:   Batch Loss = 8.369824, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.84843921661377, Accuracy = 0.7712924480438232\n",
      "Training iter #10602000:   Batch Loss = 8.376078, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.847774505615234, Accuracy = 0.7715902328491211\n",
      "Training iter #10605000:   Batch Loss = 8.677535, Accuracy = 0.8119999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.847153663635254, Accuracy = 0.771888017654419\n",
      "Training iter #10608000:   Batch Loss = 8.778367, Accuracy = 0.7753333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.846623420715332, Accuracy = 0.7715902328491211\n",
      "Training iter #10611000:   Batch Loss = 8.768858, Accuracy = 0.7873333096504211\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.846356391906738, Accuracy = 0.7712924480438232\n",
      "Training iter #10614000:   Batch Loss = 8.378262, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.845832824707031, Accuracy = 0.7721858024597168\n",
      "Training iter #10617000:   Batch Loss = 8.536358, Accuracy = 0.8560000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.845151901245117, Accuracy = 0.771888017654419\n",
      "Training iter #10620000:   Batch Loss = 8.744254, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.844392776489258, Accuracy = 0.7721858024597168\n",
      "Training iter #10623000:   Batch Loss = 8.910521, Accuracy = 0.7613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.843835830688477, Accuracy = 0.7727814316749573\n",
      "Training iter #10626000:   Batch Loss = 8.311598, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.843304634094238, Accuracy = 0.7727814316749573\n",
      "Training iter #10629000:   Batch Loss = 8.397722, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.842620849609375, Accuracy = 0.773377001285553\n",
      "Training iter #10632000:   Batch Loss = 8.623241, Accuracy = 0.8220000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.842425346374512, Accuracy = 0.773377001285553\n",
      "Training iter #10635000:   Batch Loss = 8.808204, Accuracy = 0.7613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.842103958129883, Accuracy = 0.7742704153060913\n",
      "Training iter #10638000:   Batch Loss = 8.796589, Accuracy = 0.7739999890327454\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.842121124267578, Accuracy = 0.7745682001113892\n",
      "Training iter #10641000:   Batch Loss = 8.401555, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.84179973602295, Accuracy = 0.7742704153060913\n",
      "Training iter #10644000:   Batch Loss = 8.533411, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.841373443603516, Accuracy = 0.7739726305007935\n",
      "Training iter #10647000:   Batch Loss = 8.766130, Accuracy = 0.7860000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.840590476989746, Accuracy = 0.7739726305007935\n",
      "Training iter #10650000:   Batch Loss = 8.851635, Accuracy = 0.7706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.839923858642578, Accuracy = 0.7742704153060913\n",
      "Training iter #10653000:   Batch Loss = 8.274651, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.839354515075684, Accuracy = 0.7745682001113892\n",
      "Training iter #10656000:   Batch Loss = 8.450476, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.838770866394043, Accuracy = 0.7742704153060913\n",
      "Training iter #10659000:   Batch Loss = 8.555660, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.838284492492676, Accuracy = 0.7739726305007935\n",
      "Training iter #10662000:   Batch Loss = 8.780258, Accuracy = 0.7739999890327454\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.837800025939941, Accuracy = 0.7745682001113892\n",
      "Training iter #10665000:   Batch Loss = 8.792896, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.837453842163086, Accuracy = 0.774865984916687\n",
      "Training iter #10668000:   Batch Loss = 8.409046, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.836845397949219, Accuracy = 0.7742704153060913\n",
      "Training iter #10671000:   Batch Loss = 8.536409, Accuracy = 0.8519999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.836036682128906, Accuracy = 0.7745682001113892\n",
      "Training iter #10674000:   Batch Loss = 8.789579, Accuracy = 0.768666684627533\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.834796905517578, Accuracy = 0.7739726305007935\n",
      "Training iter #10677000:   Batch Loss = 8.875371, Accuracy = 0.7606666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.834083557128906, Accuracy = 0.7742704153060913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10680000:   Batch Loss = 8.278739, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.833514213562012, Accuracy = 0.7745682001113892\n",
      "Training iter #10683000:   Batch Loss = 8.464183, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.832830429077148, Accuracy = 0.7745682001113892\n",
      "Training iter #10686000:   Batch Loss = 8.511427, Accuracy = 0.8560000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.832772254943848, Accuracy = 0.7745682001113892\n",
      "Training iter #10689000:   Batch Loss = 8.785283, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.832100868225098, Accuracy = 0.7751637697219849\n",
      "Training iter #10692000:   Batch Loss = 8.675165, Accuracy = 0.8159999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.831707954406738, Accuracy = 0.774865984916687\n",
      "Training iter #10695000:   Batch Loss = 8.380339, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.831082344055176, Accuracy = 0.7751637697219849\n",
      "Training iter #10698000:   Batch Loss = 8.606609, Accuracy = 0.8346666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.830551147460938, Accuracy = 0.7754615545272827\n",
      "Training iter #10701000:   Batch Loss = 8.815916, Accuracy = 0.7606666684150696\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.830074310302734, Accuracy = 0.7751637697219849\n",
      "Training iter #10704000:   Batch Loss = 8.857254, Accuracy = 0.7580000162124634\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.82956314086914, Accuracy = 0.7751637697219849\n",
      "Training iter #10707000:   Batch Loss = 8.289496, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.829086303710938, Accuracy = 0.7745682001113892\n",
      "Training iter #10710000:   Batch Loss = 8.453403, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.82852554321289, Accuracy = 0.774865984916687\n",
      "Training iter #10713000:   Batch Loss = 8.484259, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.827925682067871, Accuracy = 0.7751637697219849\n",
      "Training iter #10716000:   Batch Loss = 8.771538, Accuracy = 0.781333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.82735824584961, Accuracy = 0.7751637697219849\n",
      "Training iter #10719000:   Batch Loss = 8.649088, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.826950073242188, Accuracy = 0.7751637697219849\n",
      "Training iter #10722000:   Batch Loss = 8.352397, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.82637882232666, Accuracy = 0.774865984916687\n",
      "Training iter #10725000:   Batch Loss = 8.617249, Accuracy = 0.8366666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.825943946838379, Accuracy = 0.774865984916687\n",
      "Training iter #10728000:   Batch Loss = 8.837714, Accuracy = 0.7513333559036255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.825395584106445, Accuracy = 0.774865984916687\n",
      "Training iter #10731000:   Batch Loss = 8.829865, Accuracy = 0.765333354473114\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.82546615600586, Accuracy = 0.7751637697219849\n",
      "Training iter #10734000:   Batch Loss = 8.308985, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.825258255004883, Accuracy = 0.774865984916687\n",
      "Training iter #10737000:   Batch Loss = 8.453498, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.824162483215332, Accuracy = 0.774865984916687\n",
      "Training iter #10740000:   Batch Loss = 8.487160, Accuracy = 0.8586666584014893\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.82349967956543, Accuracy = 0.7754615545272827\n",
      "Training iter #10743000:   Batch Loss = 8.817825, Accuracy = 0.7646666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.823005676269531, Accuracy = 0.7757593989372253\n",
      "Training iter #10746000:   Batch Loss = 8.617067, Accuracy = 0.8393333554267883\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.822226524353027, Accuracy = 0.7757593989372253\n",
      "Training iter #10749000:   Batch Loss = 8.354751, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.820859909057617, Accuracy = 0.7757593989372253\n",
      "Training iter #10752000:   Batch Loss = 8.634549, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.820306777954102, Accuracy = 0.7760571837425232\n",
      "Training iter #10755000:   Batch Loss = 8.838898, Accuracy = 0.7513333559036255\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.819782257080078, Accuracy = 0.776354968547821\n",
      "Training iter #10758000:   Batch Loss = 8.790975, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.819778442382812, Accuracy = 0.7757593989372253\n",
      "Training iter #10761000:   Batch Loss = 8.318796, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.819822311401367, Accuracy = 0.7757593989372253\n",
      "Training iter #10764000:   Batch Loss = 8.472628, Accuracy = 0.875333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.81961441040039, Accuracy = 0.7751637697219849\n",
      "Training iter #10767000:   Batch Loss = 8.463194, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.819417953491211, Accuracy = 0.7754615545272827\n",
      "Training iter #10770000:   Batch Loss = 8.836147, Accuracy = 0.7546666860580444\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.819253921508789, Accuracy = 0.7760571837425232\n",
      "Training iter #10773000:   Batch Loss = 8.572674, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.818984031677246, Accuracy = 0.776354968547821\n",
      "Training iter #10776000:   Batch Loss = 8.353284, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.81820011138916, Accuracy = 0.7766527533531189\n",
      "Training iter #10779000:   Batch Loss = 8.588625, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.817821502685547, Accuracy = 0.776354968547821\n",
      "Training iter #10782000:   Batch Loss = 8.855651, Accuracy = 0.7459999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.817432403564453, Accuracy = 0.7769505381584167\n",
      "Training iter #10785000:   Batch Loss = 8.739021, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.816848754882812, Accuracy = 0.7775461673736572\n",
      "Training iter #10788000:   Batch Loss = 8.334722, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.81607723236084, Accuracy = 0.7784395217895508\n",
      "Training iter #10791000:   Batch Loss = 8.473053, Accuracy = 0.874666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.815374374389648, Accuracy = 0.7781417369842529\n",
      "Training iter #10794000:   Batch Loss = 8.519042, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.815054893493652, Accuracy = 0.7781417369842529\n",
      "Training iter #10797000:   Batch Loss = 8.895130, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.814745903015137, Accuracy = 0.7787373661994934\n",
      "Training iter #10800000:   Batch Loss = 8.541131, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.815098762512207, Accuracy = 0.7781417369842529\n",
      "Training iter #10803000:   Batch Loss = 8.325925, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.814834594726562, Accuracy = 0.7778439521789551\n",
      "Training iter #10806000:   Batch Loss = 8.601724, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.81425952911377, Accuracy = 0.7778439521789551\n",
      "Training iter #10809000:   Batch Loss = 8.845159, Accuracy = 0.7440000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.813681602478027, Accuracy = 0.7784395217895508\n",
      "Training iter #10812000:   Batch Loss = 8.703594, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.813277244567871, Accuracy = 0.7787373661994934\n",
      "Training iter #10815000:   Batch Loss = 8.292489, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.812731742858887, Accuracy = 0.7781417369842529\n",
      "Training iter #10818000:   Batch Loss = 8.478258, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.812061309814453, Accuracy = 0.7790351510047913\n",
      "Training iter #10821000:   Batch Loss = 8.530544, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.81155776977539, Accuracy = 0.7793329358100891\n",
      "Training iter #10824000:   Batch Loss = 8.895388, Accuracy = 0.7426666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.811324119567871, Accuracy = 0.779630720615387\n",
      "Training iter #10827000:   Batch Loss = 8.485811, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.811115264892578, Accuracy = 0.7787373661994934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10830000:   Batch Loss = 8.347572, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.810173034667969, Accuracy = 0.7790351510047913\n",
      "Training iter #10833000:   Batch Loss = 8.647182, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.809502601623535, Accuracy = 0.7790351510047913\n",
      "Training iter #10836000:   Batch Loss = 8.804037, Accuracy = 0.7519999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.809016227722168, Accuracy = 0.7793329358100891\n",
      "Training iter #10839000:   Batch Loss = 8.670272, Accuracy = 0.809333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.808457374572754, Accuracy = 0.779630720615387\n",
      "Training iter #10842000:   Batch Loss = 8.280007, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.807867050170898, Accuracy = 0.779630720615387\n",
      "Training iter #10845000:   Batch Loss = 8.477972, Accuracy = 0.8733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.807168006896973, Accuracy = 0.7793329358100891\n",
      "Training iter #10848000:   Batch Loss = 8.548852, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.806503295898438, Accuracy = 0.7793329358100891\n",
      "Training iter #10851000:   Batch Loss = 8.925091, Accuracy = 0.7379999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.80637264251709, Accuracy = 0.779630720615387\n",
      "Training iter #10854000:   Batch Loss = 8.471333, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.806132316589355, Accuracy = 0.779630720615387\n",
      "Training iter #10857000:   Batch Loss = 8.332932, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.805602073669434, Accuracy = 0.7793329358100891\n",
      "Training iter #10860000:   Batch Loss = 8.623016, Accuracy = 0.8306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.805059432983398, Accuracy = 0.7793329358100891\n",
      "Training iter #10863000:   Batch Loss = 8.779190, Accuracy = 0.7639999985694885\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.804434776306152, Accuracy = 0.779630720615387\n",
      "Training iter #10866000:   Batch Loss = 8.710006, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.803994178771973, Accuracy = 0.7802263498306274\n",
      "Training iter #10869000:   Batch Loss = 8.302643, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.803407669067383, Accuracy = 0.779630720615387\n",
      "Training iter #10872000:   Batch Loss = 8.464689, Accuracy = 0.8846666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.802644729614258, Accuracy = 0.779630720615387\n",
      "Training iter #10875000:   Batch Loss = 8.611078, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.802163124084473, Accuracy = 0.779630720615387\n",
      "Training iter #10878000:   Batch Loss = 8.892421, Accuracy = 0.7473333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.801767349243164, Accuracy = 0.779630720615387\n",
      "Training iter #10881000:   Batch Loss = 8.415439, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.801353454589844, Accuracy = 0.779630720615387\n",
      "Training iter #10884000:   Batch Loss = 8.335209, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.800708770751953, Accuracy = 0.7790351510047913\n",
      "Training iter #10887000:   Batch Loss = 8.625574, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.800108909606934, Accuracy = 0.779630720615387\n",
      "Training iter #10890000:   Batch Loss = 8.726007, Accuracy = 0.7839999794960022\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.79947280883789, Accuracy = 0.779630720615387\n",
      "Training iter #10893000:   Batch Loss = 8.683735, Accuracy = 0.809333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.799094200134277, Accuracy = 0.7808219194412231\n",
      "Training iter #10896000:   Batch Loss = 8.301815, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.798412322998047, Accuracy = 0.7799285054206848\n",
      "Training iter #10899000:   Batch Loss = 8.500308, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.797623634338379, Accuracy = 0.7793329358100891\n",
      "Training iter #10902000:   Batch Loss = 8.582639, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.796847343444824, Accuracy = 0.7790351510047913\n",
      "Training iter #10905000:   Batch Loss = 8.904504, Accuracy = 0.7433333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.796317100524902, Accuracy = 0.7799285054206848\n",
      "Training iter #10908000:   Batch Loss = 8.357092, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.795918464660645, Accuracy = 0.7802263498306274\n",
      "Training iter #10911000:   Batch Loss = 8.323968, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.795272827148438, Accuracy = 0.779630720615387\n",
      "Training iter #10914000:   Batch Loss = 8.638419, Accuracy = 0.8119999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.794681549072266, Accuracy = 0.779630720615387\n",
      "Training iter #10917000:   Batch Loss = 8.720057, Accuracy = 0.7846666574478149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.794469833374023, Accuracy = 0.7799285054206848\n",
      "Training iter #10920000:   Batch Loss = 8.690919, Accuracy = 0.8040000200271606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.794357299804688, Accuracy = 0.7802263498306274\n",
      "Training iter #10923000:   Batch Loss = 8.342323, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.793930053710938, Accuracy = 0.7799285054206848\n",
      "Training iter #10926000:   Batch Loss = 8.485887, Accuracy = 0.875333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.793306350708008, Accuracy = 0.7799285054206848\n",
      "Training iter #10929000:   Batch Loss = 8.655748, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.79269027709961, Accuracy = 0.7802263498306274\n",
      "Training iter #10932000:   Batch Loss = 8.872311, Accuracy = 0.7553333044052124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.792224884033203, Accuracy = 0.7802263498306274\n",
      "Training iter #10935000:   Batch Loss = 8.297120, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.791872024536133, Accuracy = 0.7805241346359253\n",
      "Training iter #10938000:   Batch Loss = 8.339561, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.791255950927734, Accuracy = 0.779630720615387\n",
      "Training iter #10941000:   Batch Loss = 8.608612, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.790863990783691, Accuracy = 0.7805241346359253\n",
      "Training iter #10944000:   Batch Loss = 8.718315, Accuracy = 0.7793333530426025\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.790964126586914, Accuracy = 0.7808219194412231\n",
      "Training iter #10947000:   Batch Loss = 8.732232, Accuracy = 0.7900000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.790548324584961, Accuracy = 0.7808219194412231\n",
      "Training iter #10950000:   Batch Loss = 8.349510, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.78967571258545, Accuracy = 0.781119704246521\n",
      "Training iter #10953000:   Batch Loss = 8.458053, Accuracy = 0.874666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.789055824279785, Accuracy = 0.7808219194412231\n",
      "Training iter #10956000:   Batch Loss = 8.683043, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.788434028625488, Accuracy = 0.781119704246521\n",
      "Training iter #10959000:   Batch Loss = 8.805084, Accuracy = 0.7753333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.78801441192627, Accuracy = 0.781119704246521\n",
      "Training iter #10962000:   Batch Loss = 8.258975, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.787630081176758, Accuracy = 0.7814174890518188\n",
      "Training iter #10965000:   Batch Loss = 8.362879, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.787135124206543, Accuracy = 0.7823109030723572\n",
      "Training iter #10968000:   Batch Loss = 8.542844, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.786633491516113, Accuracy = 0.7823109030723572\n",
      "Training iter #10971000:   Batch Loss = 8.742455, Accuracy = 0.7760000228881836\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.786067962646484, Accuracy = 0.7823109030723572\n",
      "Training iter #10974000:   Batch Loss = 8.761307, Accuracy = 0.7873333096504211\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.785663604736328, Accuracy = 0.7823109030723572\n",
      "Training iter #10977000:   Batch Loss = 8.344354, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.785009384155273, Accuracy = 0.7817153334617615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10980000:   Batch Loss = 8.501850, Accuracy = 0.8546666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.784401893615723, Accuracy = 0.7820131182670593\n",
      "Training iter #10983000:   Batch Loss = 8.720894, Accuracy = 0.7853333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.7838773727417, Accuracy = 0.7820131182670593\n",
      "Training iter #10986000:   Batch Loss = 8.781856, Accuracy = 0.781333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.78331184387207, Accuracy = 0.7820131182670593\n",
      "Training iter #10989000:   Batch Loss = 8.221499, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.782657623291016, Accuracy = 0.7817153334617615\n",
      "Training iter #10992000:   Batch Loss = 8.400448, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.782140731811523, Accuracy = 0.7814174890518188\n",
      "Training iter #10995000:   Batch Loss = 8.520945, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.781672477722168, Accuracy = 0.7817153334617615\n",
      "Training iter #10998000:   Batch Loss = 8.708892, Accuracy = 0.7960000038146973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.781106948852539, Accuracy = 0.7823109030723572\n",
      "Training iter #11001000:   Batch Loss = 8.679666, Accuracy = 0.8086666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.780523300170898, Accuracy = 0.782608687877655\n",
      "Training iter #11004000:   Batch Loss = 8.361461, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.779733657836914, Accuracy = 0.7829064726829529\n",
      "Training iter #11007000:   Batch Loss = 8.490980, Accuracy = 0.859333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.779232025146484, Accuracy = 0.7829064726829529\n",
      "Training iter #11010000:   Batch Loss = 8.737880, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.778841972351074, Accuracy = 0.7829064726829529\n",
      "Training iter #11013000:   Batch Loss = 8.789372, Accuracy = 0.7773333191871643\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.778274536132812, Accuracy = 0.782608687877655\n",
      "Training iter #11016000:   Batch Loss = 8.255384, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.777606964111328, Accuracy = 0.782608687877655\n",
      "Training iter #11019000:   Batch Loss = 8.421342, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.776966094970703, Accuracy = 0.7829064726829529\n",
      "Training iter #11022000:   Batch Loss = 8.473496, Accuracy = 0.8546666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.77669906616211, Accuracy = 0.782608687877655\n",
      "Training iter #11025000:   Batch Loss = 8.703731, Accuracy = 0.7940000295639038\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.776080131530762, Accuracy = 0.7829064726829529\n",
      "Training iter #11028000:   Batch Loss = 8.620914, Accuracy = 0.8299999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.775511741638184, Accuracy = 0.7829064726829529\n",
      "Training iter #11031000:   Batch Loss = 8.323993, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.774763107299805, Accuracy = 0.7829064726829529\n",
      "Training iter #11034000:   Batch Loss = 8.584123, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.774133682250977, Accuracy = 0.7832043170928955\n",
      "Training iter #11037000:   Batch Loss = 8.761394, Accuracy = 0.7713333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.773591041564941, Accuracy = 0.7829064726829529\n",
      "Training iter #11040000:   Batch Loss = 8.776510, Accuracy = 0.7753333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.773070335388184, Accuracy = 0.782608687877655\n",
      "Training iter #11043000:   Batch Loss = 8.243826, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.772610664367676, Accuracy = 0.7829064726829529\n",
      "Training iter #11046000:   Batch Loss = 8.410171, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.77208423614502, Accuracy = 0.7832043170928955\n",
      "Training iter #11049000:   Batch Loss = 8.439137, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.771571159362793, Accuracy = 0.7829064726829529\n",
      "Training iter #11052000:   Batch Loss = 8.716067, Accuracy = 0.7853333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.770998001098633, Accuracy = 0.7829064726829529\n",
      "Training iter #11055000:   Batch Loss = 8.584962, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.77058219909668, Accuracy = 0.7829064726829529\n",
      "Training iter #11058000:   Batch Loss = 8.320954, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.76992130279541, Accuracy = 0.7832043170928955\n",
      "Training iter #11061000:   Batch Loss = 8.557730, Accuracy = 0.8446666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.769338607788086, Accuracy = 0.7832043170928955\n",
      "Training iter #11064000:   Batch Loss = 8.761127, Accuracy = 0.7720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.768858909606934, Accuracy = 0.7835021018981934\n",
      "Training iter #11067000:   Batch Loss = 8.741943, Accuracy = 0.7860000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.768616676330566, Accuracy = 0.7832043170928955\n",
      "Training iter #11070000:   Batch Loss = 8.266229, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.768261909484863, Accuracy = 0.7835021018981934\n",
      "Training iter #11073000:   Batch Loss = 8.416527, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.767765998840332, Accuracy = 0.7843954563140869\n",
      "Training iter #11076000:   Batch Loss = 8.406381, Accuracy = 0.874666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.767218589782715, Accuracy = 0.7837998867034912\n",
      "Training iter #11079000:   Batch Loss = 8.773292, Accuracy = 0.7593333125114441\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.766669273376465, Accuracy = 0.7840976715087891\n",
      "Training iter #11082000:   Batch Loss = 8.530267, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.766250610351562, Accuracy = 0.7840976715087891\n",
      "Training iter #11085000:   Batch Loss = 8.305343, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.765610694885254, Accuracy = 0.7843954563140869\n",
      "Training iter #11088000:   Batch Loss = 8.564144, Accuracy = 0.8393333554267883\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.765082359313965, Accuracy = 0.7840976715087891\n",
      "Training iter #11091000:   Batch Loss = 8.789433, Accuracy = 0.762666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.764684677124023, Accuracy = 0.7840976715087891\n",
      "Training iter #11094000:   Batch Loss = 8.726507, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.764374732971191, Accuracy = 0.7840976715087891\n",
      "Training iter #11097000:   Batch Loss = 8.259116, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.764057159423828, Accuracy = 0.7843954563140869\n",
      "Training iter #11100000:   Batch Loss = 8.430164, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.76356029510498, Accuracy = 0.7843954563140869\n",
      "Training iter #11103000:   Batch Loss = 8.453543, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.763046264648438, Accuracy = 0.7843954563140869\n",
      "Training iter #11106000:   Batch Loss = 8.768404, Accuracy = 0.7673333287239075\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.762539863586426, Accuracy = 0.7843954563140869\n",
      "Training iter #11109000:   Batch Loss = 8.507805, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.762048721313477, Accuracy = 0.7846932411193848\n",
      "Training iter #11112000:   Batch Loss = 8.298290, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.761346817016602, Accuracy = 0.7846932411193848\n",
      "Training iter #11115000:   Batch Loss = 8.536787, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.760683059692383, Accuracy = 0.7846932411193848\n",
      "Training iter #11118000:   Batch Loss = 8.779086, Accuracy = 0.7646666765213013\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.760320663452148, Accuracy = 0.7849910855293274\n",
      "Training iter #11121000:   Batch Loss = 8.645626, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.760076522827148, Accuracy = 0.7846932411193848\n",
      "Training iter #11124000:   Batch Loss = 8.273472, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.759759902954102, Accuracy = 0.7852888703346252\n",
      "Training iter #11127000:   Batch Loss = 8.441489, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.759313583374023, Accuracy = 0.7852888703346252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #11130000:   Batch Loss = 8.490990, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.75881290435791, Accuracy = 0.7849910855293274\n",
      "Training iter #11133000:   Batch Loss = 8.816156, Accuracy = 0.7506666779518127\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.75836181640625, Accuracy = 0.7852888703346252\n",
      "Training iter #11136000:   Batch Loss = 8.465193, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.75798511505127, Accuracy = 0.7855866551399231\n",
      "Training iter #11139000:   Batch Loss = 8.293806, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.757411003112793, Accuracy = 0.785884439945221\n",
      "Training iter #11142000:   Batch Loss = 8.545816, Accuracy = 0.840666651725769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.756842613220215, Accuracy = 0.7864800691604614\n",
      "Training iter #11145000:   Batch Loss = 8.774242, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.756185531616211, Accuracy = 0.7864800691604614\n",
      "Training iter #11148000:   Batch Loss = 8.647885, Accuracy = 0.8159999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.755867004394531, Accuracy = 0.7864800691604614\n",
      "Training iter #11151000:   Batch Loss = 8.230531, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.75543212890625, Accuracy = 0.7867778539657593\n",
      "Training iter #11154000:   Batch Loss = 8.427446, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.754841804504395, Accuracy = 0.7870756387710571\n",
      "Training iter #11157000:   Batch Loss = 8.466246, Accuracy = 0.8546666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.754302978515625, Accuracy = 0.7864800691604614\n",
      "Training iter #11160000:   Batch Loss = 8.853120, Accuracy = 0.7453333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.753908157348633, Accuracy = 0.7861822247505188\n",
      "Training iter #11163000:   Batch Loss = 8.429883, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.753762245178223, Accuracy = 0.7867778539657593\n",
      "Training iter #11166000:   Batch Loss = 8.288122, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.753860473632812, Accuracy = 0.7864800691604614\n",
      "Training iter #11169000:   Batch Loss = 8.583288, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.753286361694336, Accuracy = 0.7861822247505188\n",
      "Training iter #11172000:   Batch Loss = 8.721462, Accuracy = 0.7713333368301392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.752326965332031, Accuracy = 0.785884439945221\n",
      "Training iter #11175000:   Batch Loss = 8.610760, Accuracy = 0.8233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.751555442810059, Accuracy = 0.7864800691604614\n",
      "Training iter #11178000:   Batch Loss = 8.235284, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.75109577178955, Accuracy = 0.7864800691604614\n",
      "Training iter #11181000:   Batch Loss = 8.436115, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.7509183883667, Accuracy = 0.7864800691604614\n",
      "Training iter #11184000:   Batch Loss = 8.511310, Accuracy = 0.8413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.75014877319336, Accuracy = 0.7861822247505188\n",
      "Training iter #11187000:   Batch Loss = 8.819266, Accuracy = 0.7546666860580444\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.74942398071289, Accuracy = 0.7861822247505188\n",
      "Training iter #11190000:   Batch Loss = 8.387335, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.748953819274902, Accuracy = 0.7861822247505188\n",
      "Training iter #11193000:   Batch Loss = 8.294555, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.748424530029297, Accuracy = 0.7861822247505188\n",
      "Training iter #11196000:   Batch Loss = 8.566807, Accuracy = 0.8393333554267883\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.747946739196777, Accuracy = 0.7861822247505188\n",
      "Training iter #11199000:   Batch Loss = 8.679523, Accuracy = 0.7820000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.747438430786133, Accuracy = 0.7861822247505188\n",
      "Training iter #11202000:   Batch Loss = 8.645082, Accuracy = 0.8159999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.747149467468262, Accuracy = 0.7861822247505188\n",
      "Training iter #11205000:   Batch Loss = 8.250188, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.746829986572266, Accuracy = 0.7861822247505188\n",
      "Training iter #11208000:   Batch Loss = 8.431640, Accuracy = 0.887333333492279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.746448516845703, Accuracy = 0.7861822247505188\n",
      "Training iter #11211000:   Batch Loss = 8.542895, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.745930671691895, Accuracy = 0.7864800691604614\n",
      "Training iter #11214000:   Batch Loss = 8.867204, Accuracy = 0.7446666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.74547004699707, Accuracy = 0.7861822247505188\n",
      "Training iter #11217000:   Batch Loss = 8.336926, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.745027542114258, Accuracy = 0.7864800691604614\n",
      "Training iter #11220000:   Batch Loss = 8.278968, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.744363784790039, Accuracy = 0.7864800691604614\n",
      "Training iter #11223000:   Batch Loss = 8.575434, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.74384880065918, Accuracy = 0.7864800691604614\n",
      "Training iter #11226000:   Batch Loss = 8.659719, Accuracy = 0.7940000295639038\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.743317604064941, Accuracy = 0.7867778539657593\n",
      "Training iter #11229000:   Batch Loss = 8.627055, Accuracy = 0.8213333487510681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.742928504943848, Accuracy = 0.7867778539657593\n",
      "Training iter #11232000:   Batch Loss = 8.268667, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.742325782775879, Accuracy = 0.7870756387710571\n",
      "Training iter #11235000:   Batch Loss = 8.433257, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.741693496704102, Accuracy = 0.7867778539657593\n",
      "Training iter #11238000:   Batch Loss = 8.558477, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.741195678710938, Accuracy = 0.7870756387710571\n",
      "Training iter #11241000:   Batch Loss = 8.811129, Accuracy = 0.7613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.740704536437988, Accuracy = 0.7870756387710571\n",
      "Training iter #11244000:   Batch Loss = 8.281841, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.740304946899414, Accuracy = 0.7870756387710571\n",
      "Training iter #11247000:   Batch Loss = 8.265220, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.739852905273438, Accuracy = 0.7870756387710571\n",
      "Training iter #11250000:   Batch Loss = 8.576588, Accuracy = 0.8253333568572998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.73941707611084, Accuracy = 0.7870756387710571\n",
      "Training iter #11253000:   Batch Loss = 8.642237, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.739066123962402, Accuracy = 0.7867778539657593\n",
      "Training iter #11256000:   Batch Loss = 8.625154, Accuracy = 0.8206666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.738908767700195, Accuracy = 0.7870756387710571\n",
      "Training iter #11259000:   Batch Loss = 8.286753, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.738579750061035, Accuracy = 0.7864800691604614\n",
      "Training iter #11262000:   Batch Loss = 8.429088, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.738212585449219, Accuracy = 0.7867778539657593\n",
      "Training iter #11265000:   Batch Loss = 8.619947, Accuracy = 0.8119999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.737762451171875, Accuracy = 0.7864800691604614\n",
      "Training iter #11268000:   Batch Loss = 8.774061, Accuracy = 0.7820000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.737410545349121, Accuracy = 0.7870756387710571\n",
      "Training iter #11271000:   Batch Loss = 8.226869, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.736978530883789, Accuracy = 0.7867778539657593\n",
      "Training iter #11274000:   Batch Loss = 8.293043, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.736385345458984, Accuracy = 0.7870756387710571\n",
      "Training iter #11277000:   Batch Loss = 8.538759, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.735811233520508, Accuracy = 0.7876712083816528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #11280000:   Batch Loss = 8.666732, Accuracy = 0.7886666655540466\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.73529052734375, Accuracy = 0.787373423576355\n",
      "Training iter #11283000:   Batch Loss = 8.665869, Accuracy = 0.8066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.734912872314453, Accuracy = 0.787373423576355\n",
      "Training iter #11286000:   Batch Loss = 8.307564, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.734416961669922, Accuracy = 0.787373423576355\n",
      "Training iter #11289000:   Batch Loss = 8.435348, Accuracy = 0.8706666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.733916282653809, Accuracy = 0.7876712083816528\n",
      "Training iter #11292000:   Batch Loss = 8.625976, Accuracy = 0.8146666884422302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.733404159545898, Accuracy = 0.7876712083816528\n",
      "Training iter #11295000:   Batch Loss = 8.721704, Accuracy = 0.7900000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.732986450195312, Accuracy = 0.7876712083816528\n",
      "Training iter #11298000:   Batch Loss = 8.171611, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.732474327087402, Accuracy = 0.7876712083816528\n",
      "Training iter #11301000:   Batch Loss = 8.347943, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.7318754196167, Accuracy = 0.7876712083816528\n",
      "Training iter #11304000:   Batch Loss = 8.455737, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.731330871582031, Accuracy = 0.7879690527915955\n",
      "Training iter #11307000:   Batch Loss = 8.668562, Accuracy = 0.7906666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.730798721313477, Accuracy = 0.7879690527915955\n",
      "Training iter #11310000:   Batch Loss = 8.674461, Accuracy = 0.8026666641235352\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.730295181274414, Accuracy = 0.7879690527915955\n",
      "Training iter #11313000:   Batch Loss = 8.313003, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.729696273803711, Accuracy = 0.7879690527915955\n",
      "Training iter #11316000:   Batch Loss = 8.422688, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.729169845581055, Accuracy = 0.7879690527915955\n",
      "Training iter #11319000:   Batch Loss = 8.641336, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.728629112243652, Accuracy = 0.7879690527915955\n",
      "Training iter #11322000:   Batch Loss = 8.742134, Accuracy = 0.7839999794960022\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.7282075881958, Accuracy = 0.788862407207489\n",
      "Training iter #11325000:   Batch Loss = 8.178606, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.727752685546875, Accuracy = 0.7891601920127869\n",
      "Training iter #11328000:   Batch Loss = 8.362576, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.727262496948242, Accuracy = 0.788862407207489\n",
      "Training iter #11331000:   Batch Loss = 8.455623, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.726755142211914, Accuracy = 0.7891601920127869\n",
      "Training iter #11334000:   Batch Loss = 8.656949, Accuracy = 0.7986666560173035\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.726293563842773, Accuracy = 0.7891601920127869\n",
      "Training iter #11337000:   Batch Loss = 8.565761, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.725940704345703, Accuracy = 0.7891601920127869\n",
      "Training iter #11340000:   Batch Loss = 8.285889, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.725367546081543, Accuracy = 0.7894580364227295\n",
      "Training iter #11343000:   Batch Loss = 8.500588, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.724810600280762, Accuracy = 0.7900536060333252\n",
      "Training iter #11346000:   Batch Loss = 8.684956, Accuracy = 0.7866666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.724306106567383, Accuracy = 0.7900536060333252\n",
      "Training iter #11349000:   Batch Loss = 8.700387, Accuracy = 0.7979999780654907\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.72382640838623, Accuracy = 0.7900536060333252\n",
      "Training iter #11352000:   Batch Loss = 8.186440, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.723310470581055, Accuracy = 0.7906491756439209\n",
      "Training iter #11355000:   Batch Loss = 8.365075, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.722698211669922, Accuracy = 0.7906491756439209\n",
      "Training iter #11358000:   Batch Loss = 8.393362, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.722146987915039, Accuracy = 0.7906491756439209\n",
      "Training iter #11361000:   Batch Loss = 8.650878, Accuracy = 0.8019999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.721637725830078, Accuracy = 0.790351390838623\n",
      "Training iter #11364000:   Batch Loss = 8.531762, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.72120475769043, Accuracy = 0.7906491756439209\n",
      "Training iter #11367000:   Batch Loss = 8.288367, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.720600128173828, Accuracy = 0.7909470200538635\n",
      "Training iter #11370000:   Batch Loss = 8.517568, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.720046997070312, Accuracy = 0.7909470200538635\n",
      "Training iter #11373000:   Batch Loss = 8.706341, Accuracy = 0.7806666493415833\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.719550132751465, Accuracy = 0.7906491756439209\n",
      "Training iter #11376000:   Batch Loss = 8.695836, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.71911907196045, Accuracy = 0.7909470200538635\n",
      "Training iter #11379000:   Batch Loss = 8.209440, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.718551635742188, Accuracy = 0.7912448048591614\n",
      "Training iter #11382000:   Batch Loss = 8.350140, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.717926025390625, Accuracy = 0.7912448048591614\n",
      "Training iter #11385000:   Batch Loss = 8.400082, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.717365264892578, Accuracy = 0.7912448048591614\n",
      "Training iter #11388000:   Batch Loss = 8.675096, Accuracy = 0.7893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.71681022644043, Accuracy = 0.7912448048591614\n",
      "Training iter #11391000:   Batch Loss = 8.505906, Accuracy = 0.8560000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.716317176818848, Accuracy = 0.7909470200538635\n",
      "Training iter #11394000:   Batch Loss = 8.265962, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.715816497802734, Accuracy = 0.7915425896644592\n",
      "Training iter #11397000:   Batch Loss = 8.516652, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.71526050567627, Accuracy = 0.7912448048591614\n",
      "Training iter #11400000:   Batch Loss = 8.693325, Accuracy = 0.7866666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.714641571044922, Accuracy = 0.7912448048591614\n",
      "Training iter #11403000:   Batch Loss = 8.687062, Accuracy = 0.8019999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.714132308959961, Accuracy = 0.7912448048591614\n",
      "Training iter #11406000:   Batch Loss = 8.211087, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.71363353729248, Accuracy = 0.7918403744697571\n",
      "Training iter #11409000:   Batch Loss = 8.358000, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.713229179382324, Accuracy = 0.7921381592750549\n",
      "Training iter #11412000:   Batch Loss = 8.364635, Accuracy = 0.8840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.712799072265625, Accuracy = 0.7918403744697571\n",
      "Training iter #11415000:   Batch Loss = 8.706572, Accuracy = 0.7706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.712423324584961, Accuracy = 0.7921381592750549\n",
      "Training iter #11418000:   Batch Loss = 8.464783, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.712098121643066, Accuracy = 0.7921381592750549\n",
      "Training iter #11421000:   Batch Loss = 8.260862, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.711527824401855, Accuracy = 0.7918403744697571\n",
      "Training iter #11424000:   Batch Loss = 8.485120, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.711007118225098, Accuracy = 0.7924359440803528\n",
      "Training iter #11427000:   Batch Loss = 8.703687, Accuracy = 0.7860000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.710630416870117, Accuracy = 0.7924359440803528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #11430000:   Batch Loss = 8.635492, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.710236549377441, Accuracy = 0.7927337884902954\n",
      "Training iter #11433000:   Batch Loss = 8.214496, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.709677696228027, Accuracy = 0.7927337884902954\n",
      "Training iter #11436000:   Batch Loss = 8.370892, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.709043502807617, Accuracy = 0.7933293581008911\n",
      "Training iter #11439000:   Batch Loss = 8.420713, Accuracy = 0.8659999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.708494186401367, Accuracy = 0.7933293581008911\n",
      "Training iter #11442000:   Batch Loss = 8.743843, Accuracy = 0.7633333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.707992553710938, Accuracy = 0.793627142906189\n",
      "Training iter #11445000:   Batch Loss = 8.440950, Accuracy = 0.8786666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.707548141479492, Accuracy = 0.793627142906189\n",
      "Training iter #11448000:   Batch Loss = 8.235423, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.706947326660156, Accuracy = 0.793627142906189\n",
      "Training iter #11451000:   Batch Loss = 8.485929, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.706524848937988, Accuracy = 0.7942227721214294\n",
      "Training iter #11454000:   Batch Loss = 8.704067, Accuracy = 0.7826666831970215\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.706294059753418, Accuracy = 0.7939249277114868\n",
      "Training iter #11457000:   Batch Loss = 8.554759, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.705987930297852, Accuracy = 0.7939249277114868\n",
      "Training iter #11460000:   Batch Loss = 8.204728, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.705531120300293, Accuracy = 0.7939249277114868\n",
      "Training iter #11463000:   Batch Loss = 8.375956, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.705005645751953, Accuracy = 0.793627142906189\n",
      "Training iter #11466000:   Batch Loss = 8.428689, Accuracy = 0.8653333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.70450496673584, Accuracy = 0.793627142906189\n",
      "Training iter #11469000:   Batch Loss = 8.746707, Accuracy = 0.7633333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.704051971435547, Accuracy = 0.7939249277114868\n",
      "Training iter #11472000:   Batch Loss = 8.392598, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.70361614227295, Accuracy = 0.7939249277114868\n",
      "Training iter #11475000:   Batch Loss = 8.257062, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.70302677154541, Accuracy = 0.793627142906189\n",
      "Training iter #11478000:   Batch Loss = 8.513444, Accuracy = 0.840666651725769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.702618598937988, Accuracy = 0.7942227721214294\n",
      "Training iter #11481000:   Batch Loss = 8.677907, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.702378273010254, Accuracy = 0.7942227721214294\n",
      "Training iter #11484000:   Batch Loss = 8.556125, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.702115058898926, Accuracy = 0.7942227721214294\n",
      "Training iter #11487000:   Batch Loss = 8.180047, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.701706886291504, Accuracy = 0.7939249277114868\n",
      "Training iter #11490000:   Batch Loss = 8.376205, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.701264381408691, Accuracy = 0.7945205569267273\n",
      "Training iter #11493000:   Batch Loss = 8.430758, Accuracy = 0.859333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.700785636901855, Accuracy = 0.7945205569267273\n",
      "Training iter #11496000:   Batch Loss = 8.782084, Accuracy = 0.7593333125114441\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.700360298156738, Accuracy = 0.7945205569267273\n",
      "Training iter #11499000:   Batch Loss = 8.374303, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.700010299682617, Accuracy = 0.7945205569267273\n",
      "Training iter #11502000:   Batch Loss = 8.246455, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.699501037597656, Accuracy = 0.7948183417320251\n",
      "Training iter #11505000:   Batch Loss = 8.508852, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.699117660522461, Accuracy = 0.7948183417320251\n",
      "Training iter #11508000:   Batch Loss = 8.645164, Accuracy = 0.7900000214576721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.698718070983887, Accuracy = 0.7945205569267273\n",
      "Training iter #11511000:   Batch Loss = 8.574591, Accuracy = 0.8286666870117188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.698358535766602, Accuracy = 0.7948183417320251\n",
      "Training iter #11514000:   Batch Loss = 8.196040, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.697898864746094, Accuracy = 0.795116126537323\n",
      "Training iter #11517000:   Batch Loss = 8.383146, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.697360038757324, Accuracy = 0.795116126537323\n",
      "Training iter #11520000:   Batch Loss = 8.483997, Accuracy = 0.8446666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.696836471557617, Accuracy = 0.795116126537323\n",
      "Training iter #11523000:   Batch Loss = 8.743287, Accuracy = 0.7706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.696353912353516, Accuracy = 0.795116126537323\n",
      "Training iter #11526000:   Batch Loss = 8.325494, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.695985794067383, Accuracy = 0.7954139113426208\n",
      "Training iter #11529000:   Batch Loss = 8.246261, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.695484161376953, Accuracy = 0.7954139113426208\n",
      "Training iter #11532000:   Batch Loss = 8.505482, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.695108413696289, Accuracy = 0.7954139113426208\n",
      "Training iter #11535000:   Batch Loss = 8.614238, Accuracy = 0.7973333597183228\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.694673538208008, Accuracy = 0.7957117557525635\n",
      "Training iter #11538000:   Batch Loss = 8.552701, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.694230079650879, Accuracy = 0.7957117557525635\n",
      "Training iter #11541000:   Batch Loss = 8.209969, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.693689346313477, Accuracy = 0.7960095405578613\n",
      "Training iter #11544000:   Batch Loss = 8.401573, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.693183898925781, Accuracy = 0.7957117557525635\n",
      "Training iter #11547000:   Batch Loss = 8.480968, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.692729949951172, Accuracy = 0.7963073253631592\n",
      "Training iter #11550000:   Batch Loss = 8.782594, Accuracy = 0.7613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.692329406738281, Accuracy = 0.7963073253631592\n",
      "Training iter #11553000:   Batch Loss = 8.265269, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.691959381103516, Accuracy = 0.7960095405578613\n",
      "Training iter #11556000:   Batch Loss = 8.242262, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.691433906555176, Accuracy = 0.7960095405578613\n",
      "Training iter #11559000:   Batch Loss = 8.536690, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.691010475158691, Accuracy = 0.7954139113426208\n",
      "Training iter #11562000:   Batch Loss = 8.584456, Accuracy = 0.8100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.690576553344727, Accuracy = 0.7957117557525635\n",
      "Training iter #11565000:   Batch Loss = 8.557541, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.690223693847656, Accuracy = 0.7957117557525635\n",
      "Training iter #11568000:   Batch Loss = 8.236411, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.689879417419434, Accuracy = 0.796605110168457\n",
      "Training iter #11571000:   Batch Loss = 8.391549, Accuracy = 0.8853333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.689447402954102, Accuracy = 0.7963073253631592\n",
      "Training iter #11574000:   Batch Loss = 8.529937, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.688969612121582, Accuracy = 0.7963073253631592\n",
      "Training iter #11577000:   Batch Loss = 8.736696, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.688481330871582, Accuracy = 0.7963073253631592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #11580000:   Batch Loss = 8.226920, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.687896728515625, Accuracy = 0.796605110168457\n",
      "Training iter #11583000:   Batch Loss = 8.247386, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.687217712402344, Accuracy = 0.796605110168457\n",
      "Training iter #11586000:   Batch Loss = 8.510411, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.686906814575195, Accuracy = 0.7963073253631592\n",
      "Training iter #11589000:   Batch Loss = 8.584972, Accuracy = 0.8066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.686530113220215, Accuracy = 0.7963073253631592\n",
      "Training iter #11592000:   Batch Loss = 8.582804, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.686248779296875, Accuracy = 0.796605110168457\n",
      "Training iter #11595000:   Batch Loss = 8.246970, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.68590259552002, Accuracy = 0.796605110168457\n",
      "Training iter #11598000:   Batch Loss = 8.392349, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.685568809509277, Accuracy = 0.7963073253631592\n",
      "Training iter #11601000:   Batch Loss = 8.553648, Accuracy = 0.8306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.68506908416748, Accuracy = 0.7963073253631592\n",
      "Training iter #11604000:   Batch Loss = 8.683484, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.684623718261719, Accuracy = 0.7963073253631592\n",
      "Training iter #11607000:   Batch Loss = 8.175138, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.684225082397461, Accuracy = 0.796605110168457\n",
      "Training iter #11610000:   Batch Loss = 8.260955, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.683759689331055, Accuracy = 0.7969028949737549\n",
      "Training iter #11613000:   Batch Loss = 8.450441, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.683379173278809, Accuracy = 0.7963073253631592\n",
      "Training iter #11616000:   Batch Loss = 8.608778, Accuracy = 0.8040000200271606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.682920455932617, Accuracy = 0.7963073253631592\n",
      "Training iter #11619000:   Batch Loss = 8.611145, Accuracy = 0.8180000185966492\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.682559967041016, Accuracy = 0.7963073253631592\n",
      "Training iter #11622000:   Batch Loss = 8.258169, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.682028770446777, Accuracy = 0.796605110168457\n",
      "Training iter #11625000:   Batch Loss = 8.383425, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.681658744812012, Accuracy = 0.7963073253631592\n",
      "Training iter #11628000:   Batch Loss = 8.579154, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.681153297424316, Accuracy = 0.7963073253631592\n",
      "Training iter #11631000:   Batch Loss = 8.638309, Accuracy = 0.8073333501815796\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.680668830871582, Accuracy = 0.7963073253631592\n",
      "Training iter #11634000:   Batch Loss = 8.136382, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.68026065826416, Accuracy = 0.7960095405578613\n",
      "Training iter #11637000:   Batch Loss = 8.305230, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.679811477661133, Accuracy = 0.7960095405578613\n",
      "Training iter #11640000:   Batch Loss = 8.426408, Accuracy = 0.8653333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.67939567565918, Accuracy = 0.7960095405578613\n",
      "Training iter #11643000:   Batch Loss = 8.582147, Accuracy = 0.8086666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.67895221710205, Accuracy = 0.7963073253631592\n",
      "Training iter #11646000:   Batch Loss = 8.570907, Accuracy = 0.8253333568572998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.678621292114258, Accuracy = 0.7963073253631592\n",
      "Training iter #11649000:   Batch Loss = 8.261322, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.678204536437988, Accuracy = 0.7960095405578613\n",
      "Training iter #11652000:   Batch Loss = 8.382523, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.677803993225098, Accuracy = 0.7960095405578613\n",
      "Training iter #11655000:   Batch Loss = 8.596706, Accuracy = 0.8140000104904175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.677382469177246, Accuracy = 0.7960095405578613\n",
      "Training iter #11658000:   Batch Loss = 8.668207, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.676886558532715, Accuracy = 0.7963073253631592\n",
      "Training iter #11661000:   Batch Loss = 8.159959, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.676371574401855, Accuracy = 0.7963073253631592\n",
      "Training iter #11664000:   Batch Loss = 8.318162, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.675888061523438, Accuracy = 0.7963073253631592\n",
      "Training iter #11667000:   Batch Loss = 8.359067, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.675564765930176, Accuracy = 0.7963073253631592\n",
      "Training iter #11670000:   Batch Loss = 8.595737, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.675195693969727, Accuracy = 0.796605110168457\n",
      "Training iter #11673000:   Batch Loss = 8.504244, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.674967765808105, Accuracy = 0.796605110168457\n",
      "Training iter #11676000:   Batch Loss = 8.239541, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.674612045288086, Accuracy = 0.796605110168457\n",
      "Training iter #11679000:   Batch Loss = 8.475851, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.67419147491455, Accuracy = 0.7972007393836975\n",
      "Training iter #11682000:   Batch Loss = 8.613726, Accuracy = 0.8100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.673660278320312, Accuracy = 0.7972007393836975\n",
      "Training iter #11685000:   Batch Loss = 8.648998, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.67312240600586, Accuracy = 0.7972007393836975\n",
      "Training iter #11688000:   Batch Loss = 8.139463, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.672541618347168, Accuracy = 0.7972007393836975\n",
      "Training iter #11691000:   Batch Loss = 8.300467, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.671998023986816, Accuracy = 0.7974985241889954\n",
      "Training iter #11694000:   Batch Loss = 8.345002, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.671836853027344, Accuracy = 0.7977963089942932\n",
      "Training iter #11697000:   Batch Loss = 8.589222, Accuracy = 0.8080000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.671463012695312, Accuracy = 0.7980940937995911\n",
      "Training iter #11700000:   Batch Loss = 8.460210, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.671114921569824, Accuracy = 0.7980940937995911\n",
      "Training iter #11703000:   Batch Loss = 8.226257, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.670583724975586, Accuracy = 0.7980940937995911\n",
      "Training iter #11706000:   Batch Loss = 8.447797, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.670198440551758, Accuracy = 0.7980940937995911\n",
      "Training iter #11709000:   Batch Loss = 8.622907, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.66971492767334, Accuracy = 0.7980940937995911\n",
      "Training iter #11712000:   Batch Loss = 8.613821, Accuracy = 0.8140000104904175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.669267654418945, Accuracy = 0.7983918786048889\n",
      "Training iter #11715000:   Batch Loss = 8.171459, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.668743133544922, Accuracy = 0.7980940937995911\n",
      "Training iter #11718000:   Batch Loss = 8.312503, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.668212890625, Accuracy = 0.7980940937995911\n",
      "Training iter #11721000:   Batch Loss = 8.320038, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.667899131774902, Accuracy = 0.7980940937995911\n",
      "Training iter #11724000:   Batch Loss = 8.623474, Accuracy = 0.7926666736602783\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.667535781860352, Accuracy = 0.7980940937995911\n",
      "Training iter #11727000:   Batch Loss = 8.418494, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.667165756225586, Accuracy = 0.7983918786048889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #11730000:   Batch Loss = 8.229162, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.666705131530762, Accuracy = 0.7983918786048889\n",
      "Training iter #11733000:   Batch Loss = 8.459559, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.666237831115723, Accuracy = 0.7983918786048889\n",
      "Training iter #11736000:   Batch Loss = 8.629536, Accuracy = 0.8100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.665756225585938, Accuracy = 0.7986897230148315\n",
      "Training iter #11739000:   Batch Loss = 8.595858, Accuracy = 0.8213333487510681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.665376663208008, Accuracy = 0.7989875078201294\n",
      "Training iter #11742000:   Batch Loss = 8.174240, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.664794921875, Accuracy = 0.799880862236023\n",
      "Training iter #11745000:   Batch Loss = 8.329555, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.664237022399902, Accuracy = 0.7992852926254272\n",
      "Training iter #11748000:   Batch Loss = 8.350574, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.663880348205566, Accuracy = 0.7995830774307251\n",
      "Training iter #11751000:   Batch Loss = 8.640482, Accuracy = 0.7853333353996277\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.663557052612305, Accuracy = 0.8001787066459656\n",
      "Training iter #11754000:   Batch Loss = 8.394715, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.663287162780762, Accuracy = 0.8001787066459656\n",
      "Training iter #11757000:   Batch Loss = 8.228522, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.66290283203125, Accuracy = 0.7995830774307251\n",
      "Training iter #11760000:   Batch Loss = 8.428804, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.662452697753906, Accuracy = 0.799880862236023\n",
      "Training iter #11763000:   Batch Loss = 8.630892, Accuracy = 0.8100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.661943435668945, Accuracy = 0.8010720610618591\n",
      "Training iter #11766000:   Batch Loss = 8.536460, Accuracy = 0.8399999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.661511421203613, Accuracy = 0.801369845867157\n",
      "Training iter #11769000:   Batch Loss = 8.181664, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.661088943481445, Accuracy = 0.8016676306724548\n",
      "Training iter #11772000:   Batch Loss = 8.329019, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.660627365112305, Accuracy = 0.801369845867157\n",
      "Training iter #11775000:   Batch Loss = 8.374476, Accuracy = 0.8740000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.660273551940918, Accuracy = 0.8016676306724548\n",
      "Training iter #11778000:   Batch Loss = 8.686698, Accuracy = 0.7720000147819519\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.659941673278809, Accuracy = 0.8019654750823975\n",
      "Training iter #11781000:   Batch Loss = 8.386249, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.65950870513916, Accuracy = 0.8022632598876953\n",
      "Training iter #11784000:   Batch Loss = 8.208801, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.658926963806152, Accuracy = 0.8022632598876953\n",
      "Training iter #11787000:   Batch Loss = 8.437764, Accuracy = 0.8519999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.658517837524414, Accuracy = 0.8022632598876953\n",
      "Training iter #11790000:   Batch Loss = 8.632130, Accuracy = 0.8019999861717224\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.65798568725586, Accuracy = 0.8025610446929932\n",
      "Training iter #11793000:   Batch Loss = 8.524449, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.657547950744629, Accuracy = 0.8025610446929932\n",
      "Training iter #11796000:   Batch Loss = 8.144523, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.657027244567871, Accuracy = 0.8022632598876953\n",
      "Training iter #11799000:   Batch Loss = 8.320487, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.656530380249023, Accuracy = 0.8022632598876953\n",
      "Training iter #11802000:   Batch Loss = 8.394119, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.656241416931152, Accuracy = 0.802858829498291\n",
      "Training iter #11805000:   Batch Loss = 8.704052, Accuracy = 0.7733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.65588092803955, Accuracy = 0.802858829498291\n",
      "Training iter #11808000:   Batch Loss = 8.332260, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.655549049377441, Accuracy = 0.802858829498291\n",
      "Training iter #11811000:   Batch Loss = 8.207630, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.655177116394043, Accuracy = 0.8025610446929932\n",
      "Training iter #11814000:   Batch Loss = 8.480718, Accuracy = 0.8446666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.654741287231445, Accuracy = 0.8031566143035889\n",
      "Training iter #11817000:   Batch Loss = 8.594254, Accuracy = 0.8073333501815796\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.654186248779297, Accuracy = 0.8031566143035889\n",
      "Training iter #11820000:   Batch Loss = 8.488313, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.653696060180664, Accuracy = 0.802858829498291\n",
      "Training iter #11823000:   Batch Loss = 8.148119, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.653044700622559, Accuracy = 0.8031566143035889\n",
      "Training iter #11826000:   Batch Loss = 8.336110, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.652565956115723, Accuracy = 0.8034544587135315\n",
      "Training iter #11829000:   Batch Loss = 8.413857, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.65245246887207, Accuracy = 0.8034544587135315\n",
      "Training iter #11832000:   Batch Loss = 8.687439, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.652193069458008, Accuracy = 0.8034544587135315\n",
      "Training iter #11835000:   Batch Loss = 8.296213, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.65190601348877, Accuracy = 0.8034544587135315\n",
      "Training iter #11838000:   Batch Loss = 8.204099, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.651533126831055, Accuracy = 0.8034544587135315\n",
      "Training iter #11841000:   Batch Loss = 8.459826, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.651033401489258, Accuracy = 0.8037522435188293\n",
      "Training iter #11844000:   Batch Loss = 8.559946, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.650490760803223, Accuracy = 0.8034544587135315\n",
      "Training iter #11847000:   Batch Loss = 8.518992, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.650068283081055, Accuracy = 0.8034544587135315\n",
      "Training iter #11850000:   Batch Loss = 8.162307, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.64963436126709, Accuracy = 0.8037522435188293\n",
      "Training iter #11853000:   Batch Loss = 8.329241, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.649174690246582, Accuracy = 0.804347813129425\n",
      "Training iter #11856000:   Batch Loss = 8.431331, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.648833274841309, Accuracy = 0.8040500283241272\n",
      "Training iter #11859000:   Batch Loss = 8.724297, Accuracy = 0.7739999890327454\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.64842414855957, Accuracy = 0.8040500283241272\n",
      "Training iter #11862000:   Batch Loss = 8.272892, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.648042678833008, Accuracy = 0.804347813129425\n",
      "Training iter #11865000:   Batch Loss = 8.191556, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.647563934326172, Accuracy = 0.8046455979347229\n",
      "Training iter #11868000:   Batch Loss = 8.458117, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.647238731384277, Accuracy = 0.8046455979347229\n",
      "Training iter #11871000:   Batch Loss = 8.538482, Accuracy = 0.8193333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.646745681762695, Accuracy = 0.8049434423446655\n",
      "Training iter #11874000:   Batch Loss = 8.500065, Accuracy = 0.8446666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.646282196044922, Accuracy = 0.8049434423446655\n",
      "Training iter #11877000:   Batch Loss = 8.167387, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.645834922790527, Accuracy = 0.8049434423446655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #11880000:   Batch Loss = 8.336649, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.645402908325195, Accuracy = 0.8055390119552612\n",
      "Training iter #11883000:   Batch Loss = 8.429915, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.645187377929688, Accuracy = 0.8058367967605591\n",
      "Training iter #11886000:   Batch Loss = 8.697772, Accuracy = 0.781333327293396\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.6448392868042, Accuracy = 0.8058367967605591\n",
      "Training iter #11889000:   Batch Loss = 8.202863, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.64455795288086, Accuracy = 0.8058367967605591\n",
      "Training iter #11892000:   Batch Loss = 8.196340, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.64420223236084, Accuracy = 0.8058367967605591\n",
      "Training iter #11895000:   Batch Loss = 8.475425, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.643660545349121, Accuracy = 0.8058367967605591\n",
      "Training iter #11898000:   Batch Loss = 8.517873, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.64307975769043, Accuracy = 0.8058367967605591\n",
      "Training iter #11901000:   Batch Loss = 8.495291, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.642483711242676, Accuracy = 0.8058367967605591\n",
      "Training iter #11904000:   Batch Loss = 8.197526, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.64189338684082, Accuracy = 0.8055390119552612\n",
      "Training iter #11907000:   Batch Loss = 8.330317, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.641498565673828, Accuracy = 0.8058367967605591\n",
      "Training iter #11910000:   Batch Loss = 8.487369, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.641411781311035, Accuracy = 0.8058367967605591\n",
      "Training iter #11913000:   Batch Loss = 8.657398, Accuracy = 0.79666668176651\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.641228675842285, Accuracy = 0.8061345815658569\n",
      "Training iter #11916000:   Batch Loss = 8.154716, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.640984535217285, Accuracy = 0.8058367967605591\n",
      "Training iter #11919000:   Batch Loss = 8.205717, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.640663146972656, Accuracy = 0.8061345815658569\n",
      "Training iter #11922000:   Batch Loss = 8.458931, Accuracy = 0.8379999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.640129089355469, Accuracy = 0.8061345815658569\n",
      "Training iter #11925000:   Batch Loss = 8.530090, Accuracy = 0.8193333148956299\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.639400482177734, Accuracy = 0.8061345815658569\n",
      "Training iter #11928000:   Batch Loss = 8.532062, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.638833045959473, Accuracy = 0.8064324259757996\n",
      "Training iter #11931000:   Batch Loss = 8.221663, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.638428688049316, Accuracy = 0.8064324259757996\n",
      "Training iter #11934000:   Batch Loss = 8.336135, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.63807201385498, Accuracy = 0.8064324259757996\n",
      "Training iter #11937000:   Batch Loss = 8.506131, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.63804817199707, Accuracy = 0.8064324259757996\n",
      "Training iter #11940000:   Batch Loss = 8.606006, Accuracy = 0.8066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.637760162353516, Accuracy = 0.8067302107810974\n",
      "Training iter #11943000:   Batch Loss = 8.111430, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.637317657470703, Accuracy = 0.8067302107810974\n",
      "Training iter #11946000:   Batch Loss = 8.255831, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.636968612670898, Accuracy = 0.8061345815658569\n",
      "Training iter #11949000:   Batch Loss = 8.386858, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.636626243591309, Accuracy = 0.8064324259757996\n",
      "Training iter #11952000:   Batch Loss = 8.549784, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.636330604553223, Accuracy = 0.8064324259757996\n",
      "Training iter #11955000:   Batch Loss = 8.560686, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.635726928710938, Accuracy = 0.8061345815658569\n",
      "Training iter #11958000:   Batch Loss = 8.217529, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.635120391845703, Accuracy = 0.8064324259757996\n",
      "Training iter #11961000:   Batch Loss = 8.331497, Accuracy = 0.878000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.634637832641602, Accuracy = 0.8070279955863953\n",
      "Training iter #11964000:   Batch Loss = 8.534538, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.634500503540039, Accuracy = 0.8067302107810974\n",
      "Training iter #11967000:   Batch Loss = 8.594365, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.634336471557617, Accuracy = 0.8067302107810974\n",
      "Training iter #11970000:   Batch Loss = 8.095711, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.634071350097656, Accuracy = 0.8067302107810974\n",
      "Training iter #11973000:   Batch Loss = 8.274976, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.63375473022461, Accuracy = 0.8067302107810974\n",
      "Training iter #11976000:   Batch Loss = 8.370365, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.633272171020508, Accuracy = 0.8070279955863953\n",
      "Training iter #11979000:   Batch Loss = 8.521667, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.632652282714844, Accuracy = 0.8070279955863953\n",
      "Training iter #11982000:   Batch Loss = 8.472442, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.632081031799316, Accuracy = 0.8070279955863953\n",
      "Training iter #11985000:   Batch Loss = 8.218399, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.631525993347168, Accuracy = 0.8073257803916931\n",
      "Training iter #11988000:   Batch Loss = 8.386540, Accuracy = 0.8659999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.631292343139648, Accuracy = 0.8070279955863953\n",
      "Training iter #11991000:   Batch Loss = 8.559011, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.631084442138672, Accuracy = 0.8067302107810974\n",
      "Training iter #11994000:   Batch Loss = 8.588030, Accuracy = 0.8100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.630934715270996, Accuracy = 0.8067302107810974\n",
      "Training iter #11997000:   Batch Loss = 8.114102, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.630694389343262, Accuracy = 0.8067302107810974\n",
      "Training iter #12000000:   Batch Loss = 8.270282, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.630342483520508, Accuracy = 0.8067302107810974\n",
      "Training iter #12003000:   Batch Loss = 8.322847, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.629861831665039, Accuracy = 0.8067302107810974\n",
      "Training iter #12006000:   Batch Loss = 8.526473, Accuracy = 0.8273333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.629311561584473, Accuracy = 0.8067302107810974\n",
      "Training iter #12009000:   Batch Loss = 8.428211, Accuracy = 0.8659999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.628908157348633, Accuracy = 0.8070279955863953\n",
      "Training iter #12012000:   Batch Loss = 8.209172, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.628467559814453, Accuracy = 0.8067302107810974\n",
      "Training iter #12015000:   Batch Loss = 8.433969, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.6279878616333, Accuracy = 0.8070279955863953\n",
      "Training iter #12018000:   Batch Loss = 8.576225, Accuracy = 0.8220000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.627501487731934, Accuracy = 0.8067302107810974\n",
      "Training iter #12021000:   Batch Loss = 8.573974, Accuracy = 0.812666654586792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.62700080871582, Accuracy = 0.8070279955863953\n",
      "Training iter #12024000:   Batch Loss = 8.121365, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.626545906066895, Accuracy = 0.8070279955863953\n",
      "Training iter #12027000:   Batch Loss = 8.273314, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.626164436340332, Accuracy = 0.8073257803916931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12030000:   Batch Loss = 8.311671, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.625865936279297, Accuracy = 0.8070279955863953\n",
      "Training iter #12033000:   Batch Loss = 8.537452, Accuracy = 0.8186666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.625307083129883, Accuracy = 0.8073257803916931\n",
      "Training iter #12036000:   Batch Loss = 8.405066, Accuracy = 0.8706666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.625, Accuracy = 0.8070279955863953\n",
      "Training iter #12039000:   Batch Loss = 8.191124, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.624720573425293, Accuracy = 0.8070279955863953\n",
      "Training iter #12042000:   Batch Loss = 8.418673, Accuracy = 0.8513333201408386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.624435424804688, Accuracy = 0.8070279955863953\n",
      "Training iter #12045000:   Batch Loss = 8.560933, Accuracy = 0.8273333311080933\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.624100685119629, Accuracy = 0.8070279955863953\n",
      "Training iter #12048000:   Batch Loss = 8.558140, Accuracy = 0.8206666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.623842239379883, Accuracy = 0.8070279955863953\n",
      "Training iter #12051000:   Batch Loss = 8.132395, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.623468399047852, Accuracy = 0.8070279955863953\n",
      "Training iter #12054000:   Batch Loss = 8.272801, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.623098373413086, Accuracy = 0.8070279955863953\n",
      "Training iter #12057000:   Batch Loss = 8.276389, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.62264633178711, Accuracy = 0.8067302107810974\n",
      "Training iter #12060000:   Batch Loss = 8.594851, Accuracy = 0.7913333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.622016906738281, Accuracy = 0.8067302107810974\n",
      "Training iter #12063000:   Batch Loss = 8.357482, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.621519088745117, Accuracy = 0.8067302107810974\n",
      "Training iter #12066000:   Batch Loss = 8.177773, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.621036529541016, Accuracy = 0.8070279955863953\n",
      "Training iter #12069000:   Batch Loss = 8.393341, Accuracy = 0.8573333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.620706558227539, Accuracy = 0.8070279955863953\n",
      "Training iter #12072000:   Batch Loss = 8.573136, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.620354652404785, Accuracy = 0.8067302107810974\n",
      "Training iter #12075000:   Batch Loss = 8.525790, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.620138168334961, Accuracy = 0.8070279955863953\n",
      "Training iter #12078000:   Batch Loss = 8.130678, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.619867324829102, Accuracy = 0.8070279955863953\n",
      "Training iter #12081000:   Batch Loss = 8.286703, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.619592666625977, Accuracy = 0.8070279955863953\n",
      "Training iter #12084000:   Batch Loss = 8.323476, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.61915397644043, Accuracy = 0.8070279955863953\n",
      "Training iter #12087000:   Batch Loss = 8.594789, Accuracy = 0.7893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.618560791015625, Accuracy = 0.8070279955863953\n",
      "Training iter #12090000:   Batch Loss = 8.346660, Accuracy = 0.8859999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.618108749389648, Accuracy = 0.8070279955863953\n",
      "Training iter #12093000:   Batch Loss = 8.157481, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.617690086364746, Accuracy = 0.8070279955863953\n",
      "Training iter #12096000:   Batch Loss = 8.397661, Accuracy = 0.8560000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.61725902557373, Accuracy = 0.8070279955863953\n",
      "Training iter #12099000:   Batch Loss = 8.582556, Accuracy = 0.8146666884422302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.616881370544434, Accuracy = 0.8073257803916931\n",
      "Training iter #12102000:   Batch Loss = 8.442898, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.616521835327148, Accuracy = 0.8073257803916931\n",
      "Training iter #12105000:   Batch Loss = 8.140292, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.616127014160156, Accuracy = 0.807623565196991\n",
      "Training iter #12108000:   Batch Loss = 8.284661, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.615791320800781, Accuracy = 0.8079214096069336\n",
      "Training iter #12111000:   Batch Loss = 8.345838, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.615347862243652, Accuracy = 0.807623565196991\n",
      "Training iter #12114000:   Batch Loss = 8.623588, Accuracy = 0.7839999794960022\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.614860534667969, Accuracy = 0.8073257803916931\n",
      "Training iter #12117000:   Batch Loss = 8.310755, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.614500045776367, Accuracy = 0.807623565196991\n",
      "Training iter #12120000:   Batch Loss = 8.168253, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.61418342590332, Accuracy = 0.8079214096069336\n",
      "Training iter #12123000:   Batch Loss = 8.404561, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.613886833190918, Accuracy = 0.8082191944122314\n",
      "Training iter #12126000:   Batch Loss = 8.569208, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.613555908203125, Accuracy = 0.8073257803916931\n",
      "Training iter #12129000:   Batch Loss = 8.451880, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.613370895385742, Accuracy = 0.8079214096069336\n",
      "Training iter #12132000:   Batch Loss = 8.104769, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.613134384155273, Accuracy = 0.8085169792175293\n",
      "Training iter #12135000:   Batch Loss = 8.288382, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.612860679626465, Accuracy = 0.8085169792175293\n",
      "Training iter #12138000:   Batch Loss = 8.332458, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.612346649169922, Accuracy = 0.8088147640228271\n",
      "Training iter #12141000:   Batch Loss = 8.650480, Accuracy = 0.7820000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.611319541931152, Accuracy = 0.8085169792175293\n",
      "Training iter #12144000:   Batch Loss = 8.289945, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.610679626464844, Accuracy = 0.8088147640228271\n",
      "Training iter #12147000:   Batch Loss = 8.148855, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.610260963439941, Accuracy = 0.8088147640228271\n",
      "Training iter #12150000:   Batch Loss = 8.424193, Accuracy = 0.8519999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.610082626342773, Accuracy = 0.809112548828125\n",
      "Training iter #12153000:   Batch Loss = 8.521885, Accuracy = 0.8226666450500488\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.610089302062988, Accuracy = 0.809112548828125\n",
      "Training iter #12156000:   Batch Loss = 8.444816, Accuracy = 0.8546666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.609848976135254, Accuracy = 0.8088147640228271\n",
      "Training iter #12159000:   Batch Loss = 8.111529, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.609651565551758, Accuracy = 0.809112548828125\n",
      "Training iter #12162000:   Batch Loss = 8.294054, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.609445571899414, Accuracy = 0.809112548828125\n",
      "Training iter #12165000:   Batch Loss = 8.378749, Accuracy = 0.8673333525657654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.608942031860352, Accuracy = 0.8094103336334229\n",
      "Training iter #12168000:   Batch Loss = 8.621063, Accuracy = 0.7873333096504211\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.608291625976562, Accuracy = 0.8094103336334229\n",
      "Training iter #12171000:   Batch Loss = 8.245891, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.60749626159668, Accuracy = 0.8094103336334229\n",
      "Training iter #12174000:   Batch Loss = 8.159880, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.607190132141113, Accuracy = 0.8094103336334229\n",
      "Training iter #12177000:   Batch Loss = 8.430528, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.606832504272461, Accuracy = 0.8097081780433655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12180000:   Batch Loss = 8.483216, Accuracy = 0.8299999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.60694694519043, Accuracy = 0.8094103336334229\n",
      "Training iter #12183000:   Batch Loss = 8.447038, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.606569290161133, Accuracy = 0.809112548828125\n",
      "Training iter #12186000:   Batch Loss = 8.120475, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.606315612792969, Accuracy = 0.809112548828125\n",
      "Training iter #12189000:   Batch Loss = 8.306004, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.606053352355957, Accuracy = 0.8094103336334229\n",
      "Training iter #12192000:   Batch Loss = 8.375332, Accuracy = 0.8700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.605624198913574, Accuracy = 0.809112548828125\n",
      "Training iter #12195000:   Batch Loss = 8.663414, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.60522174835205, Accuracy = 0.8094103336334229\n",
      "Training iter #12198000:   Batch Loss = 8.194586, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.60472583770752, Accuracy = 0.8094103336334229\n",
      "Training iter #12201000:   Batch Loss = 8.156029, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.604010581970215, Accuracy = 0.8097081780433655\n",
      "Training iter #12204000:   Batch Loss = 8.434773, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.603377342224121, Accuracy = 0.8097081780433655\n",
      "Training iter #12207000:   Batch Loss = 8.479218, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.602777481079102, Accuracy = 0.8097081780433655\n",
      "Training iter #12210000:   Batch Loss = 8.436869, Accuracy = 0.8573333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.602428436279297, Accuracy = 0.8094103336334229\n",
      "Training iter #12213000:   Batch Loss = 8.139708, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.602416038513184, Accuracy = 0.809112548828125\n",
      "Training iter #12216000:   Batch Loss = 8.291694, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.602213859558105, Accuracy = 0.809112548828125\n",
      "Training iter #12219000:   Batch Loss = 8.414030, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.601716995239258, Accuracy = 0.8088147640228271\n",
      "Training iter #12222000:   Batch Loss = 8.622373, Accuracy = 0.7913333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.601081848144531, Accuracy = 0.809112548828125\n",
      "Training iter #12225000:   Batch Loss = 8.153542, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.600659370422363, Accuracy = 0.8088147640228271\n",
      "Training iter #12228000:   Batch Loss = 8.151375, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.600203514099121, Accuracy = 0.8088147640228271\n",
      "Training iter #12231000:   Batch Loss = 8.419781, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.599642753601074, Accuracy = 0.8088147640228271\n",
      "Training iter #12234000:   Batch Loss = 8.464033, Accuracy = 0.8353333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.59916877746582, Accuracy = 0.809112548828125\n",
      "Training iter #12237000:   Batch Loss = 8.448010, Accuracy = 0.8519999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.598970413208008, Accuracy = 0.8094103336334229\n",
      "Training iter #12240000:   Batch Loss = 8.151360, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.598878860473633, Accuracy = 0.8100059628486633\n",
      "Training iter #12243000:   Batch Loss = 8.283503, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.598586082458496, Accuracy = 0.8100059628486633\n",
      "Training iter #12246000:   Batch Loss = 8.447131, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.598051071166992, Accuracy = 0.8100059628486633\n",
      "Training iter #12249000:   Batch Loss = 8.576110, Accuracy = 0.8066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.597563743591309, Accuracy = 0.8103037476539612\n",
      "Training iter #12252000:   Batch Loss = 8.106894, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.597284317016602, Accuracy = 0.8103037476539612\n",
      "Training iter #12255000:   Batch Loss = 8.161956, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.59705638885498, Accuracy = 0.8103037476539612\n",
      "Training iter #12258000:   Batch Loss = 8.371822, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.596362113952637, Accuracy = 0.8103037476539612\n",
      "Training iter #12261000:   Batch Loss = 8.494010, Accuracy = 0.8233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.595742225646973, Accuracy = 0.8100059628486633\n",
      "Training iter #12264000:   Batch Loss = 8.477410, Accuracy = 0.8433333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.595401763916016, Accuracy = 0.8103037476539612\n",
      "Training iter #12267000:   Batch Loss = 8.180627, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.595117568969727, Accuracy = 0.8100059628486633\n",
      "Training iter #12270000:   Batch Loss = 8.282961, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.594783782958984, Accuracy = 0.8100059628486633\n",
      "Training iter #12273000:   Batch Loss = 8.458014, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.594264030456543, Accuracy = 0.8103037476539612\n",
      "Training iter #12276000:   Batch Loss = 8.529906, Accuracy = 0.8159999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.593728065490723, Accuracy = 0.8103037476539612\n",
      "Training iter #12279000:   Batch Loss = 8.063230, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.593452453613281, Accuracy = 0.810601532459259\n",
      "Training iter #12282000:   Batch Loss = 8.207494, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.59330940246582, Accuracy = 0.8103037476539612\n",
      "Training iter #12285000:   Batch Loss = 8.324012, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.592874526977539, Accuracy = 0.810601532459259\n",
      "Training iter #12288000:   Batch Loss = 8.473180, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.592612266540527, Accuracy = 0.8103037476539612\n",
      "Training iter #12291000:   Batch Loss = 8.473775, Accuracy = 0.8433333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.59260368347168, Accuracy = 0.8103037476539612\n",
      "Training iter #12294000:   Batch Loss = 8.187146, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.592455863952637, Accuracy = 0.8100059628486633\n",
      "Training iter #12297000:   Batch Loss = 8.277446, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.591889381408691, Accuracy = 0.8103037476539612\n",
      "Training iter #12300000:   Batch Loss = 8.478139, Accuracy = 0.8379999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.591107368469238, Accuracy = 0.8103037476539612\n",
      "Training iter #12303000:   Batch Loss = 8.549413, Accuracy = 0.8106666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.590272903442383, Accuracy = 0.810601532459259\n",
      "Training iter #12306000:   Batch Loss = 8.066337, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.589822769165039, Accuracy = 0.810601532459259\n",
      "Training iter #12309000:   Batch Loss = 8.224231, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.589570045471191, Accuracy = 0.810601532459259\n",
      "Training iter #12312000:   Batch Loss = 8.281349, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.589340209960938, Accuracy = 0.8103037476539612\n",
      "Training iter #12315000:   Batch Loss = 8.485584, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.589488983154297, Accuracy = 0.8103037476539612\n",
      "Training iter #12318000:   Batch Loss = 8.379995, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.589454650878906, Accuracy = 0.8100059628486633\n",
      "Training iter #12321000:   Batch Loss = 8.163667, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.589305877685547, Accuracy = 0.8097081780433655\n",
      "Training iter #12324000:   Batch Loss = 8.352142, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.588937759399414, Accuracy = 0.8103037476539612\n",
      "Training iter #12327000:   Batch Loss = 8.490400, Accuracy = 0.8379999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.58837604522705, Accuracy = 0.810601532459259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12330000:   Batch Loss = 8.533949, Accuracy = 0.8173333406448364\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.587815284729004, Accuracy = 0.8103037476539612\n",
      "Training iter #12333000:   Batch Loss = 8.067456, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.587149620056152, Accuracy = 0.8103037476539612\n",
      "Training iter #12336000:   Batch Loss = 8.205379, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.586636543273926, Accuracy = 0.8108993172645569\n",
      "Training iter #12339000:   Batch Loss = 8.259336, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.586094856262207, Accuracy = 0.8108993172645569\n",
      "Training iter #12342000:   Batch Loss = 8.475044, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.586024284362793, Accuracy = 0.8108993172645569\n",
      "Training iter #12345000:   Batch Loss = 8.357288, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.585759162902832, Accuracy = 0.8108993172645569\n",
      "Training iter #12348000:   Batch Loss = 8.149719, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.585460662841797, Accuracy = 0.8103037476539612\n",
      "Training iter #12351000:   Batch Loss = 8.361250, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.585173606872559, Accuracy = 0.810601532459259\n",
      "Training iter #12354000:   Batch Loss = 8.505650, Accuracy = 0.8299999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.584718704223633, Accuracy = 0.8108993172645569\n",
      "Training iter #12357000:   Batch Loss = 8.516210, Accuracy = 0.8220000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.584375381469727, Accuracy = 0.8108993172645569\n",
      "Training iter #12360000:   Batch Loss = 8.090203, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.58376693725586, Accuracy = 0.8111971616744995\n",
      "Training iter #12363000:   Batch Loss = 8.207916, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.583487510681152, Accuracy = 0.8123883008956909\n",
      "Training iter #12366000:   Batch Loss = 8.262352, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.583468437194824, Accuracy = 0.8117927312850952\n",
      "Training iter #12369000:   Batch Loss = 8.506905, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.583460807800293, Accuracy = 0.8111971616744995\n",
      "Training iter #12372000:   Batch Loss = 8.329629, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.583358764648438, Accuracy = 0.8108993172645569\n",
      "Training iter #12375000:   Batch Loss = 8.141356, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.583123207092285, Accuracy = 0.8114949464797974\n",
      "Training iter #12378000:   Batch Loss = 8.371180, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.58265209197998, Accuracy = 0.8114949464797974\n",
      "Training iter #12381000:   Batch Loss = 8.503898, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.581891059875488, Accuracy = 0.8114949464797974\n",
      "Training iter #12384000:   Batch Loss = 8.476516, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.581401824951172, Accuracy = 0.8114949464797974\n",
      "Training iter #12387000:   Batch Loss = 8.103658, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.580782890319824, Accuracy = 0.8123883008956909\n",
      "Training iter #12390000:   Batch Loss = 8.233299, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.580175399780273, Accuracy = 0.8123883008956909\n",
      "Training iter #12393000:   Batch Loss = 8.242800, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.580042839050293, Accuracy = 0.8120905160903931\n",
      "Training iter #12396000:   Batch Loss = 8.524050, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.579998970031738, Accuracy = 0.8114949464797974\n",
      "Training iter #12399000:   Batch Loss = 8.298255, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.579952239990234, Accuracy = 0.8114949464797974\n",
      "Training iter #12402000:   Batch Loss = 8.142285, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.57979965209961, Accuracy = 0.8114949464797974\n",
      "Training iter #12405000:   Batch Loss = 8.336225, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.579695701599121, Accuracy = 0.8114949464797974\n",
      "Training iter #12408000:   Batch Loss = 8.520420, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.57938003540039, Accuracy = 0.8108993172645569\n",
      "Training iter #12411000:   Batch Loss = 8.442009, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.579059600830078, Accuracy = 0.8111971616744995\n",
      "Training iter #12414000:   Batch Loss = 8.117505, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.578428268432617, Accuracy = 0.8111971616744995\n",
      "Training iter #12417000:   Batch Loss = 8.230127, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.577803611755371, Accuracy = 0.8120905160903931\n",
      "Training iter #12420000:   Batch Loss = 8.288960, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.577291488647461, Accuracy = 0.8114949464797974\n",
      "Training iter #12423000:   Batch Loss = 8.565076, Accuracy = 0.7913333177566528\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.576964378356934, Accuracy = 0.8111971616744995\n",
      "Training iter #12426000:   Batch Loss = 8.284382, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.57672119140625, Accuracy = 0.8114949464797974\n",
      "Training iter #12429000:   Batch Loss = 8.120832, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.576482772827148, Accuracy = 0.8117927312850952\n",
      "Training iter #12432000:   Batch Loss = 8.348260, Accuracy = 0.8673333525657654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.576274871826172, Accuracy = 0.8120905160903931\n",
      "Training iter #12435000:   Batch Loss = 8.508006, Accuracy = 0.8206666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.575860977172852, Accuracy = 0.8120905160903931\n",
      "Training iter #12438000:   Batch Loss = 8.408772, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.575543403625488, Accuracy = 0.8120905160903931\n",
      "Training iter #12441000:   Batch Loss = 8.076283, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.575296401977539, Accuracy = 0.8123883008956909\n",
      "Training iter #12444000:   Batch Loss = 8.235173, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.575021743774414, Accuracy = 0.8126861453056335\n",
      "Training iter #12447000:   Batch Loss = 8.296269, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.575249671936035, Accuracy = 0.8129839301109314\n",
      "Training iter #12450000:   Batch Loss = 8.561965, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.57508373260498, Accuracy = 0.8126861453056335\n",
      "Training iter #12453000:   Batch Loss = 8.244347, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.574626922607422, Accuracy = 0.8126861453056335\n",
      "Training iter #12456000:   Batch Loss = 8.129330, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.5741605758667, Accuracy = 0.8126861453056335\n",
      "Training iter #12459000:   Batch Loss = 8.383625, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.573838233947754, Accuracy = 0.8123883008956909\n",
      "Training iter #12462000:   Batch Loss = 8.479077, Accuracy = 0.8286666870117188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.573371887207031, Accuracy = 0.8123883008956909\n",
      "Training iter #12465000:   Batch Loss = 8.386057, Accuracy = 0.862666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.572966575622559, Accuracy = 0.8120905160903931\n",
      "Training iter #12468000:   Batch Loss = 8.069105, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.572367668151855, Accuracy = 0.8120905160903931\n",
      "Training iter #12471000:   Batch Loss = 8.241446, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.571792602539062, Accuracy = 0.8126861453056335\n",
      "Training iter #12474000:   Batch Loss = 8.305733, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.571783065795898, Accuracy = 0.8117927312850952\n",
      "Training iter #12477000:   Batch Loss = 8.586004, Accuracy = 0.7906666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.571338653564453, Accuracy = 0.8120905160903931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12480000:   Batch Loss = 8.233453, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.57100772857666, Accuracy = 0.8123883008956909\n",
      "Training iter #12483000:   Batch Loss = 8.116261, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.570684432983398, Accuracy = 0.8129839301109314\n",
      "Training iter #12486000:   Batch Loss = 8.370941, Accuracy = 0.8679999709129333\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.570540428161621, Accuracy = 0.8123883008956909\n",
      "Training iter #12489000:   Batch Loss = 8.456449, Accuracy = 0.8326666951179504\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.570284843444824, Accuracy = 0.8126861453056335\n",
      "Training iter #12492000:   Batch Loss = 8.416240, Accuracy = 0.8579999804496765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.570175170898438, Accuracy = 0.8123883008956909\n",
      "Training iter #12495000:   Batch Loss = 8.084481, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.570084571838379, Accuracy = 0.8120905160903931\n",
      "Training iter #12498000:   Batch Loss = 8.231919, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.569726943969727, Accuracy = 0.8123883008956909\n",
      "Training iter #12501000:   Batch Loss = 8.342465, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.569232940673828, Accuracy = 0.8120905160903931\n",
      "Training iter #12504000:   Batch Loss = 8.563558, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.568719863891602, Accuracy = 0.8120905160903931\n",
      "Training iter #12507000:   Batch Loss = 8.185261, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.56830883026123, Accuracy = 0.8120905160903931\n",
      "Training iter #12510000:   Batch Loss = 8.120554, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.567743301391602, Accuracy = 0.8126861453056335\n",
      "Training iter #12513000:   Batch Loss = 8.378307, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.567249298095703, Accuracy = 0.8129839301109314\n",
      "Training iter #12516000:   Batch Loss = 8.431198, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.566734313964844, Accuracy = 0.8129839301109314\n",
      "Training iter #12519000:   Batch Loss = 8.388117, Accuracy = 0.8673333525657654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.566452026367188, Accuracy = 0.8129839301109314\n",
      "Training iter #12522000:   Batch Loss = 8.089592, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.56624698638916, Accuracy = 0.8135794997215271\n",
      "Training iter #12525000:   Batch Loss = 8.254195, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.566030502319336, Accuracy = 0.8129839301109314\n",
      "Training iter #12528000:   Batch Loss = 8.312549, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.565969467163086, Accuracy = 0.8126861453056335\n",
      "Training iter #12531000:   Batch Loss = 8.578377, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.565657615661621, Accuracy = 0.8120905160903931\n",
      "Training iter #12534000:   Batch Loss = 8.134797, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.565383911132812, Accuracy = 0.8132817149162292\n",
      "Training iter #12537000:   Batch Loss = 8.112505, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.565088272094727, Accuracy = 0.8132817149162292\n",
      "Training iter #12540000:   Batch Loss = 8.392530, Accuracy = 0.8586666584014893\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.5645112991333, Accuracy = 0.8129839301109314\n",
      "Training iter #12543000:   Batch Loss = 8.425684, Accuracy = 0.840666651725769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.563773155212402, Accuracy = 0.8129839301109314\n",
      "Training iter #12546000:   Batch Loss = 8.387832, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.563372611999512, Accuracy = 0.813877284526825\n",
      "Training iter #12549000:   Batch Loss = 8.125124, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.563114166259766, Accuracy = 0.813877284526825\n",
      "Training iter #12552000:   Batch Loss = 8.236195, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.562856674194336, Accuracy = 0.8132817149162292\n",
      "Training iter #12555000:   Batch Loss = 8.377856, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.562541007995605, Accuracy = 0.8132817149162292\n",
      "Training iter #12558000:   Batch Loss = 8.547880, Accuracy = 0.8046666383743286\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.56224536895752, Accuracy = 0.8141751289367676\n",
      "Training iter #12561000:   Batch Loss = 8.100860, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.561944007873535, Accuracy = 0.8144729137420654\n",
      "Training iter #12564000:   Batch Loss = 8.117110, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.561617851257324, Accuracy = 0.8135794997215271\n",
      "Training iter #12567000:   Batch Loss = 8.368436, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.561330795288086, Accuracy = 0.8135794997215271\n",
      "Training iter #12570000:   Batch Loss = 8.421818, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.560959815979004, Accuracy = 0.8141751289367676\n",
      "Training iter #12573000:   Batch Loss = 8.413940, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.560821533203125, Accuracy = 0.8147706985473633\n",
      "Training iter #12576000:   Batch Loss = 8.135908, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.560624122619629, Accuracy = 0.8147706985473633\n",
      "Training iter #12579000:   Batch Loss = 8.222319, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.560074806213379, Accuracy = 0.8147706985473633\n",
      "Training iter #12582000:   Batch Loss = 8.400916, Accuracy = 0.8566666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.55949878692627, Accuracy = 0.8150684833526611\n",
      "Training iter #12585000:   Batch Loss = 8.492700, Accuracy = 0.8220000267028809\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.559098243713379, Accuracy = 0.815366268157959\n",
      "Training iter #12588000:   Batch Loss = 8.067200, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.558847427368164, Accuracy = 0.815366268157959\n",
      "Training iter #12591000:   Batch Loss = 8.131247, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.55862045288086, Accuracy = 0.815366268157959\n",
      "Training iter #12594000:   Batch Loss = 8.315892, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.558281898498535, Accuracy = 0.815366268157959\n",
      "Training iter #12597000:   Batch Loss = 8.444309, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.557877540588379, Accuracy = 0.815366268157959\n",
      "Training iter #12600000:   Batch Loss = 8.442771, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.557683944702148, Accuracy = 0.815366268157959\n",
      "Training iter #12603000:   Batch Loss = 8.139772, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.55750846862793, Accuracy = 0.815366268157959\n",
      "Training iter #12606000:   Batch Loss = 8.253643, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.557321548461914, Accuracy = 0.815366268157959\n",
      "Training iter #12609000:   Batch Loss = 8.424314, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.557065963745117, Accuracy = 0.815366268157959\n",
      "Training iter #12612000:   Batch Loss = 8.477587, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.556806564331055, Accuracy = 0.8156641125679016\n",
      "Training iter #12615000:   Batch Loss = 8.026617, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.55659008026123, Accuracy = 0.8156641125679016\n",
      "Training iter #12618000:   Batch Loss = 8.175135, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.556294441223145, Accuracy = 0.8156641125679016\n",
      "Training iter #12621000:   Batch Loss = 8.291421, Accuracy = 0.8859999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.555530548095703, Accuracy = 0.8156641125679016\n",
      "Training iter #12624000:   Batch Loss = 8.422856, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.554832458496094, Accuracy = 0.8156641125679016\n",
      "Training iter #12627000:   Batch Loss = 8.377917, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.554439544677734, Accuracy = 0.8156641125679016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12630000:   Batch Loss = 8.146790, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.554122924804688, Accuracy = 0.8159618973731995\n",
      "Training iter #12633000:   Batch Loss = 8.249283, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.55393123626709, Accuracy = 0.8162596821784973\n",
      "Training iter #12636000:   Batch Loss = 8.433110, Accuracy = 0.8433333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.553630828857422, Accuracy = 0.8159618973731995\n",
      "Training iter #12639000:   Batch Loss = 8.477530, Accuracy = 0.8233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.553441047668457, Accuracy = 0.8156641125679016\n",
      "Training iter #12642000:   Batch Loss = 8.053772, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.5532808303833, Accuracy = 0.8162596821784973\n",
      "Training iter #12645000:   Batch Loss = 8.188523, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.553081512451172, Accuracy = 0.8162596821784973\n",
      "Training iter #12648000:   Batch Loss = 8.256663, Accuracy = 0.8840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.552777290344238, Accuracy = 0.8159618973731995\n",
      "Training iter #12651000:   Batch Loss = 8.428918, Accuracy = 0.8366666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.552380561828613, Accuracy = 0.8162596821784973\n",
      "Training iter #12654000:   Batch Loss = 8.335178, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.552242279052734, Accuracy = 0.8162596821784973\n",
      "Training iter #12657000:   Batch Loss = 8.123535, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.55208969116211, Accuracy = 0.8162596821784973\n",
      "Training iter #12660000:   Batch Loss = 8.336258, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.551639556884766, Accuracy = 0.8162596821784973\n",
      "Training iter #12663000:   Batch Loss = 8.448770, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.551156044006348, Accuracy = 0.8162596821784973\n",
      "Training iter #12666000:   Batch Loss = 8.472560, Accuracy = 0.8233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.55089282989502, Accuracy = 0.8165574669837952\n",
      "Training iter #12669000:   Batch Loss = 8.046663, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.550580024719238, Accuracy = 0.8162596821784973\n",
      "Training iter #12672000:   Batch Loss = 8.172387, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.550191879272461, Accuracy = 0.8162596821784973\n",
      "Training iter #12675000:   Batch Loss = 8.227465, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54974365234375, Accuracy = 0.8162596821784973\n",
      "Training iter #12678000:   Batch Loss = 8.432514, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.549354553222656, Accuracy = 0.8171530961990356\n",
      "Training iter #12681000:   Batch Loss = 8.301446, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.549127578735352, Accuracy = 0.816855251789093\n",
      "Training iter #12684000:   Batch Loss = 8.116708, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.548860549926758, Accuracy = 0.8174508810043335\n",
      "Training iter #12687000:   Batch Loss = 8.319791, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.548158645629883, Accuracy = 0.8171530961990356\n",
      "Training iter #12690000:   Batch Loss = 8.443733, Accuracy = 0.8433333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.547500610351562, Accuracy = 0.8171530961990356\n",
      "Training iter #12693000:   Batch Loss = 8.448972, Accuracy = 0.8320000171661377\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54704761505127, Accuracy = 0.8171530961990356\n",
      "Training iter #12696000:   Batch Loss = 8.063824, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.546700477600098, Accuracy = 0.816855251789093\n",
      "Training iter #12699000:   Batch Loss = 8.183150, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.546513557434082, Accuracy = 0.816855251789093\n",
      "Training iter #12702000:   Batch Loss = 8.199866, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.546243667602539, Accuracy = 0.816855251789093\n",
      "Training iter #12705000:   Batch Loss = 8.478627, Accuracy = 0.8153333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.546006202697754, Accuracy = 0.816855251789093\n",
      "Training iter #12708000:   Batch Loss = 8.265029, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54598617553711, Accuracy = 0.816855251789093\n",
      "Training iter #12711000:   Batch Loss = 8.102298, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.545909881591797, Accuracy = 0.8171530961990356\n",
      "Training iter #12714000:   Batch Loss = 8.322776, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.545608520507812, Accuracy = 0.8174508810043335\n",
      "Training iter #12717000:   Batch Loss = 8.463969, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.544764518737793, Accuracy = 0.8177486658096313\n",
      "Training iter #12720000:   Batch Loss = 8.429835, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54448127746582, Accuracy = 0.818344235420227\n",
      "Training iter #12723000:   Batch Loss = 8.064146, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.544198036193848, Accuracy = 0.818344235420227\n",
      "Training iter #12726000:   Batch Loss = 8.197011, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54378604888916, Accuracy = 0.818344235420227\n",
      "Training iter #12729000:   Batch Loss = 8.235641, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.543352127075195, Accuracy = 0.8180464506149292\n",
      "Training iter #12732000:   Batch Loss = 8.464406, Accuracy = 0.8206666707992554\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54334545135498, Accuracy = 0.818344235420227\n",
      "Training iter #12735000:   Batch Loss = 8.246684, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.543277740478516, Accuracy = 0.8171530961990356\n",
      "Training iter #12738000:   Batch Loss = 8.095679, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.543012619018555, Accuracy = 0.8177486658096313\n",
      "Training iter #12741000:   Batch Loss = 8.300440, Accuracy = 0.878000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.542810440063477, Accuracy = 0.8177486658096313\n",
      "Training iter #12744000:   Batch Loss = 8.454612, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54230785369873, Accuracy = 0.8186420202255249\n",
      "Training iter #12747000:   Batch Loss = 8.359579, Accuracy = 0.8700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.5419921875, Accuracy = 0.8177486658096313\n",
      "Training iter #12750000:   Batch Loss = 8.068992, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.541546821594238, Accuracy = 0.8180464506149292\n",
      "Training iter #12753000:   Batch Loss = 8.201397, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.54111385345459, Accuracy = 0.8180464506149292\n",
      "Training iter #12756000:   Batch Loss = 8.264855, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.540489196777344, Accuracy = 0.8189398646354675\n",
      "Training iter #12759000:   Batch Loss = 8.502478, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.540077209472656, Accuracy = 0.8189398646354675\n",
      "Training iter #12762000:   Batch Loss = 8.220946, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.539989471435547, Accuracy = 0.8189398646354675\n",
      "Training iter #12765000:   Batch Loss = 8.090357, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.539850234985352, Accuracy = 0.8189398646354675\n",
      "Training iter #12768000:   Batch Loss = 8.308853, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.539202690124512, Accuracy = 0.8192376494407654\n",
      "Training iter #12771000:   Batch Loss = 8.452191, Accuracy = 0.8306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.538466453552246, Accuracy = 0.8186420202255249\n",
      "Training iter #12774000:   Batch Loss = 8.365874, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.538114547729492, Accuracy = 0.8189398646354675\n",
      "Training iter #12777000:   Batch Loss = 8.034377, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.537769317626953, Accuracy = 0.8186420202255249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12780000:   Batch Loss = 8.204617, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.537477493286133, Accuracy = 0.8189398646354675\n",
      "Training iter #12783000:   Batch Loss = 8.240849, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.537101745605469, Accuracy = 0.8189398646354675\n",
      "Training iter #12786000:   Batch Loss = 8.525745, Accuracy = 0.8026666641235352\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.536826133728027, Accuracy = 0.8186420202255249\n",
      "Training iter #12789000:   Batch Loss = 8.202657, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.53683853149414, Accuracy = 0.8189398646354675\n",
      "Training iter #12792000:   Batch Loss = 8.082778, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.536914825439453, Accuracy = 0.8192376494407654\n",
      "Training iter #12795000:   Batch Loss = 8.340028, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.536752700805664, Accuracy = 0.8189398646354675\n",
      "Training iter #12798000:   Batch Loss = 8.415316, Accuracy = 0.840666651725769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.535655975341797, Accuracy = 0.8189398646354675\n",
      "Training iter #12801000:   Batch Loss = 8.338242, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.535138130187988, Accuracy = 0.8189398646354675\n",
      "Training iter #12804000:   Batch Loss = 8.038013, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.534884452819824, Accuracy = 0.8186420202255249\n",
      "Training iter #12807000:   Batch Loss = 8.211067, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.534567832946777, Accuracy = 0.8189398646354675\n",
      "Training iter #12810000:   Batch Loss = 8.271214, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.534383773803711, Accuracy = 0.8192376494407654\n",
      "Training iter #12813000:   Batch Loss = 8.499304, Accuracy = 0.8080000281333923\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.534392356872559, Accuracy = 0.8189398646354675\n",
      "Training iter #12816000:   Batch Loss = 8.165522, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.534796714782715, Accuracy = 0.8189398646354675\n",
      "Training iter #12819000:   Batch Loss = 8.086122, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.534404754638672, Accuracy = 0.8189398646354675\n",
      "Training iter #12822000:   Batch Loss = 8.329467, Accuracy = 0.8740000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.534085273742676, Accuracy = 0.8198332190513611\n",
      "Training iter #12825000:   Batch Loss = 8.387347, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.533711433410645, Accuracy = 0.8198332190513611\n",
      "Training iter #12828000:   Batch Loss = 8.365613, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.531623840332031, Accuracy = 0.8204288482666016\n",
      "Training iter #12831000:   Batch Loss = 8.049712, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.530149459838867, Accuracy = 0.8204288482666016\n",
      "Training iter #12834000:   Batch Loss = 8.204628, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.529627799987793, Accuracy = 0.8210244178771973\n",
      "Training iter #12837000:   Batch Loss = 8.284899, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.529170036315918, Accuracy = 0.8207266330718994\n",
      "Training iter #12840000:   Batch Loss = 8.545671, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.529047966003418, Accuracy = 0.8210244178771973\n",
      "Training iter #12843000:   Batch Loss = 8.117615, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.529027938842773, Accuracy = 0.8210244178771973\n",
      "Training iter #12846000:   Batch Loss = 8.075171, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.528742790222168, Accuracy = 0.821619987487793\n",
      "Training iter #12849000:   Batch Loss = 8.345447, Accuracy = 0.8706666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.527997970581055, Accuracy = 0.8207266330718994\n",
      "Training iter #12852000:   Batch Loss = 8.384010, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.527186393737793, Accuracy = 0.8207266330718994\n",
      "Training iter #12855000:   Batch Loss = 8.346647, Accuracy = 0.8700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.52680492401123, Accuracy = 0.8204288482666016\n",
      "Training iter #12858000:   Batch Loss = 8.066716, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.527461051940918, Accuracy = 0.8201310038566589\n",
      "Training iter #12861000:   Batch Loss = 8.202742, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.527585983276367, Accuracy = 0.8198332190513611\n",
      "Training iter #12864000:   Batch Loss = 8.298069, Accuracy = 0.8733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.528048515319824, Accuracy = 0.8195354342460632\n",
      "Training iter #12867000:   Batch Loss = 8.494706, Accuracy = 0.8119999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.528153419494629, Accuracy = 0.8204288482666016\n",
      "Training iter #12870000:   Batch Loss = 8.079202, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.528059959411621, Accuracy = 0.8204288482666016\n",
      "Training iter #12873000:   Batch Loss = 8.058784, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.527885437011719, Accuracy = 0.8207266330718994\n",
      "Training iter #12876000:   Batch Loss = 8.341314, Accuracy = 0.8673333525657654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.527523040771484, Accuracy = 0.8207266330718994\n",
      "Training iter #12879000:   Batch Loss = 8.367261, Accuracy = 0.8553333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.526954650878906, Accuracy = 0.8204288482666016\n",
      "Training iter #12882000:   Batch Loss = 8.338729, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.526531219482422, Accuracy = 0.8201310038566589\n",
      "Training iter #12885000:   Batch Loss = 8.084778, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.52612590789795, Accuracy = 0.8201310038566589\n",
      "Training iter #12888000:   Batch Loss = 8.198983, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.525579452514648, Accuracy = 0.8207266330718994\n",
      "Training iter #12891000:   Batch Loss = 8.348782, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.524603843688965, Accuracy = 0.8207266330718994\n",
      "Training iter #12894000:   Batch Loss = 8.472338, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.523828506469727, Accuracy = 0.8204288482666016\n",
      "Training iter #12897000:   Batch Loss = 8.045804, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.523405075073242, Accuracy = 0.8204288482666016\n",
      "Training iter #12900000:   Batch Loss = 8.081970, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.52318286895752, Accuracy = 0.8207266330718994\n",
      "Training iter #12903000:   Batch Loss = 8.312141, Accuracy = 0.8726666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.52299690246582, Accuracy = 0.8213222026824951\n",
      "Training iter #12906000:   Batch Loss = 8.381952, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.52270221710205, Accuracy = 0.8210244178771973\n",
      "Training iter #12909000:   Batch Loss = 8.363892, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.522512435913086, Accuracy = 0.8207266330718994\n",
      "Training iter #12912000:   Batch Loss = 8.108982, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.522294044494629, Accuracy = 0.8207266330718994\n",
      "Training iter #12915000:   Batch Loss = 8.209028, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.521666526794434, Accuracy = 0.8204288482666016\n",
      "Training iter #12918000:   Batch Loss = 8.349455, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.520943641662598, Accuracy = 0.8210244178771973\n",
      "Training iter #12921000:   Batch Loss = 8.424421, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.520390510559082, Accuracy = 0.8207266330718994\n",
      "Training iter #12924000:   Batch Loss = 7.996679, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.51996898651123, Accuracy = 0.821619987487793\n",
      "Training iter #12927000:   Batch Loss = 8.125179, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.51966667175293, Accuracy = 0.821619987487793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12930000:   Batch Loss = 8.243066, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.519715309143066, Accuracy = 0.8219178318977356\n",
      "Training iter #12933000:   Batch Loss = 8.389037, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.52033805847168, Accuracy = 0.8219178318977356\n",
      "Training iter #12936000:   Batch Loss = 8.379736, Accuracy = 0.8579999804496765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.520317077636719, Accuracy = 0.8219178318977356\n",
      "Training iter #12939000:   Batch Loss = 8.115393, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.519906997680664, Accuracy = 0.8219178318977356\n",
      "Training iter #12942000:   Batch Loss = 8.199457, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.519436836242676, Accuracy = 0.821619987487793\n",
      "Training iter #12945000:   Batch Loss = 8.361420, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.518950462341309, Accuracy = 0.8219178318977356\n",
      "Training iter #12948000:   Batch Loss = 8.441510, Accuracy = 0.8246666789054871\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.518633842468262, Accuracy = 0.8222156167030334\n",
      "Training iter #12951000:   Batch Loss = 7.998883, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.518843650817871, Accuracy = 0.8225134015083313\n",
      "Training iter #12954000:   Batch Loss = 8.141170, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.518527030944824, Accuracy = 0.8219178318977356\n",
      "Training iter #12957000:   Batch Loss = 8.243134, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.517593383789062, Accuracy = 0.8222156167030334\n",
      "Training iter #12960000:   Batch Loss = 8.386570, Accuracy = 0.8446666598320007\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.516944885253906, Accuracy = 0.8219178318977356\n",
      "Training iter #12963000:   Batch Loss = 8.296603, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.517085075378418, Accuracy = 0.8222156167030334\n",
      "Training iter #12966000:   Batch Loss = 8.087614, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.517189025878906, Accuracy = 0.8225134015083313\n",
      "Training iter #12969000:   Batch Loss = 8.265639, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.517006874084473, Accuracy = 0.8225134015083313\n",
      "Training iter #12972000:   Batch Loss = 8.386118, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.516663551330566, Accuracy = 0.8228111863136292\n",
      "Training iter #12975000:   Batch Loss = 8.402520, Accuracy = 0.8413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.516462326049805, Accuracy = 0.8234068155288696\n",
      "Training iter #12978000:   Batch Loss = 8.006452, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.516228675842285, Accuracy = 0.823108971118927\n",
      "Training iter #12981000:   Batch Loss = 8.141273, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.515952110290527, Accuracy = 0.8228111863136292\n",
      "Training iter #12984000:   Batch Loss = 8.190457, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.515504837036133, Accuracy = 0.8228111863136292\n",
      "Training iter #12987000:   Batch Loss = 8.381302, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.514945030212402, Accuracy = 0.8237046003341675\n",
      "Training iter #12990000:   Batch Loss = 8.271904, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.514544486999512, Accuracy = 0.823108971118927\n",
      "Training iter #12993000:   Batch Loss = 8.096382, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.514235496520996, Accuracy = 0.8234068155288696\n",
      "Training iter #12996000:   Batch Loss = 8.286777, Accuracy = 0.8853333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.513806343078613, Accuracy = 0.823108971118927\n",
      "Training iter #12999000:   Batch Loss = 8.401525, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.513191223144531, Accuracy = 0.8234068155288696\n",
      "Training iter #13002000:   Batch Loss = 8.408561, Accuracy = 0.8386666774749756\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.513019561767578, Accuracy = 0.8237046003341675\n",
      "Training iter #13005000:   Batch Loss = 8.018291, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.512855529785156, Accuracy = 0.8243001699447632\n",
      "Training iter #13008000:   Batch Loss = 8.129099, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.512588500976562, Accuracy = 0.8240023851394653\n",
      "Training iter #13011000:   Batch Loss = 8.194819, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.512190818786621, Accuracy = 0.8240023851394653\n",
      "Training iter #13014000:   Batch Loss = 8.396441, Accuracy = 0.8413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.51175594329834, Accuracy = 0.824597954750061\n",
      "Training iter #13017000:   Batch Loss = 8.244032, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.511412620544434, Accuracy = 0.8243001699447632\n",
      "Training iter #13020000:   Batch Loss = 8.070111, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.511065483093262, Accuracy = 0.8243001699447632\n",
      "Training iter #13023000:   Batch Loss = 8.287715, Accuracy = 0.8786666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.510577201843262, Accuracy = 0.8240023851394653\n",
      "Training iter #13026000:   Batch Loss = 8.390512, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.509994506835938, Accuracy = 0.8240023851394653\n",
      "Training iter #13029000:   Batch Loss = 8.391516, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.510172843933105, Accuracy = 0.8240023851394653\n",
      "Training iter #13032000:   Batch Loss = 8.024949, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.510187149047852, Accuracy = 0.8243001699447632\n",
      "Training iter #13035000:   Batch Loss = 8.138040, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.510053634643555, Accuracy = 0.8243001699447632\n",
      "Training iter #13038000:   Batch Loss = 8.165535, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.50970458984375, Accuracy = 0.8243001699447632\n",
      "Training iter #13041000:   Batch Loss = 8.422916, Accuracy = 0.8253333568572998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.509269714355469, Accuracy = 0.824597954750061\n",
      "Training iter #13044000:   Batch Loss = 8.216187, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.508962631225586, Accuracy = 0.824597954750061\n",
      "Training iter #13047000:   Batch Loss = 8.068727, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.50865650177002, Accuracy = 0.8243001699447632\n",
      "Training iter #13050000:   Batch Loss = 8.257685, Accuracy = 0.8853333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.508270263671875, Accuracy = 0.8243001699447632\n",
      "Training iter #13053000:   Batch Loss = 8.397386, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.507761001586914, Accuracy = 0.8243001699447632\n",
      "Training iter #13056000:   Batch Loss = 8.353430, Accuracy = 0.859333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.507651329040527, Accuracy = 0.824597954750061\n",
      "Training iter #13059000:   Batch Loss = 8.031780, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.507513046264648, Accuracy = 0.824597954750061\n",
      "Training iter #13062000:   Batch Loss = 8.154668, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.507187843322754, Accuracy = 0.8243001699447632\n",
      "Training iter #13065000:   Batch Loss = 8.217340, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.506616592407227, Accuracy = 0.8243001699447632\n",
      "Training iter #13068000:   Batch Loss = 8.447941, Accuracy = 0.8159999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.505992889404297, Accuracy = 0.8248957991600037\n",
      "Training iter #13071000:   Batch Loss = 8.201581, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.505646705627441, Accuracy = 0.8251935839653015\n",
      "Training iter #13074000:   Batch Loss = 8.046708, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.50534439086914, Accuracy = 0.8251935839653015\n",
      "Training iter #13077000:   Batch Loss = 8.260021, Accuracy = 0.8853333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.504969596862793, Accuracy = 0.8251935839653015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13080000:   Batch Loss = 8.395576, Accuracy = 0.8479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.504280090332031, Accuracy = 0.8251935839653015\n",
      "Training iter #13083000:   Batch Loss = 8.298503, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.503744125366211, Accuracy = 0.8248957991600037\n",
      "Training iter #13086000:   Batch Loss = 8.016081, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.503382682800293, Accuracy = 0.8248957991600037\n",
      "Training iter #13089000:   Batch Loss = 8.161614, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.502964973449707, Accuracy = 0.8254913687705994\n",
      "Training iter #13092000:   Batch Loss = 8.221180, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.502578735351562, Accuracy = 0.8254913687705994\n",
      "Training iter #13095000:   Batch Loss = 8.454699, Accuracy = 0.8153333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.50253677368164, Accuracy = 0.8257891535758972\n",
      "Training iter #13098000:   Batch Loss = 8.170750, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.502225875854492, Accuracy = 0.8266825675964355\n",
      "Training iter #13101000:   Batch Loss = 8.058461, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.501697540283203, Accuracy = 0.8266825675964355\n",
      "Training iter #13104000:   Batch Loss = 8.279754, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.501466751098633, Accuracy = 0.8269803524017334\n",
      "Training iter #13107000:   Batch Loss = 8.383917, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.50095272064209, Accuracy = 0.8269803524017334\n",
      "Training iter #13110000:   Batch Loss = 8.304277, Accuracy = 0.8726666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.500696182250977, Accuracy = 0.8263847231864929\n",
      "Training iter #13113000:   Batch Loss = 7.997192, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.500479698181152, Accuracy = 0.8263847231864929\n",
      "Training iter #13116000:   Batch Loss = 8.165705, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.50023078918457, Accuracy = 0.8266825675964355\n",
      "Training iter #13119000:   Batch Loss = 8.212635, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.49996566772461, Accuracy = 0.8266825675964355\n",
      "Training iter #13122000:   Batch Loss = 8.481954, Accuracy = 0.8119999766349792\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.499580383300781, Accuracy = 0.8269803524017334\n",
      "Training iter #13125000:   Batch Loss = 8.158387, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.49919319152832, Accuracy = 0.8269803524017334\n",
      "Training iter #13128000:   Batch Loss = 8.046765, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.498637199401855, Accuracy = 0.8272781372070312\n",
      "Training iter #13131000:   Batch Loss = 8.282035, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.497915267944336, Accuracy = 0.8269803524017334\n",
      "Training iter #13134000:   Batch Loss = 8.360488, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.497685432434082, Accuracy = 0.8281715512275696\n",
      "Training iter #13137000:   Batch Loss = 8.318273, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.497662544250488, Accuracy = 0.8275759220123291\n",
      "Training iter #13140000:   Batch Loss = 8.012635, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.49742603302002, Accuracy = 0.8272781372070312\n",
      "Training iter #13143000:   Batch Loss = 8.171294, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.497051239013672, Accuracy = 0.8272781372070312\n",
      "Training iter #13146000:   Batch Loss = 8.246613, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.49660873413086, Accuracy = 0.8272781372070312\n",
      "Training iter #13149000:   Batch Loss = 8.454650, Accuracy = 0.8153333067893982\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.496617317199707, Accuracy = 0.8266825675964355\n",
      "Training iter #13152000:   Batch Loss = 8.114985, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.496957778930664, Accuracy = 0.8269803524017334\n",
      "Training iter #13155000:   Batch Loss = 8.047083, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.496891021728516, Accuracy = 0.8269803524017334\n",
      "Training iter #13158000:   Batch Loss = 8.284194, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.496309280395508, Accuracy = 0.8272781372070312\n",
      "Training iter #13161000:   Batch Loss = 8.346980, Accuracy = 0.8553333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.495894432067871, Accuracy = 0.8275759220123291\n",
      "Training iter #13164000:   Batch Loss = 8.299521, Accuracy = 0.875333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.495591163635254, Accuracy = 0.8266825675964355\n",
      "Training iter #13167000:   Batch Loss = 8.022826, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.495327949523926, Accuracy = 0.8266825675964355\n",
      "Training iter #13170000:   Batch Loss = 8.180610, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.49498462677002, Accuracy = 0.8269803524017334\n",
      "Training iter #13173000:   Batch Loss = 8.239804, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.49444580078125, Accuracy = 0.8260869383811951\n",
      "Training iter #13176000:   Batch Loss = 8.485598, Accuracy = 0.8086666464805603\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.494611740112305, Accuracy = 0.8257891535758972\n",
      "Training iter #13179000:   Batch Loss = 8.062885, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.494584083557129, Accuracy = 0.8260869383811951\n",
      "Training iter #13182000:   Batch Loss = 8.044050, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.493998527526855, Accuracy = 0.8263847231864929\n",
      "Training iter #13185000:   Batch Loss = 8.310724, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.493359565734863, Accuracy = 0.8266825675964355\n",
      "Training iter #13188000:   Batch Loss = 8.323535, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.492742538452148, Accuracy = 0.8266825675964355\n",
      "Training iter #13191000:   Batch Loss = 8.300758, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.492332458496094, Accuracy = 0.8266825675964355\n",
      "Training iter #13194000:   Batch Loss = 8.048497, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.491907119750977, Accuracy = 0.8266825675964355\n",
      "Training iter #13197000:   Batch Loss = 8.172948, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.490994453430176, Accuracy = 0.8275759220123291\n",
      "Training iter #13200000:   Batch Loss = 8.280860, Accuracy = 0.8733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.490713119506836, Accuracy = 0.8275759220123291\n",
      "Training iter #13203000:   Batch Loss = 8.445657, Accuracy = 0.8213333487510681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.490650177001953, Accuracy = 0.8275759220123291\n",
      "Training iter #13206000:   Batch Loss = 8.035653, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.490580558776855, Accuracy = 0.8275759220123291\n",
      "Training iter #13209000:   Batch Loss = 8.044850, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.49081039428711, Accuracy = 0.8275759220123291\n",
      "Training iter #13212000:   Batch Loss = 8.288377, Accuracy = 0.8740000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.490463256835938, Accuracy = 0.8275759220123291\n",
      "Training iter #13215000:   Batch Loss = 8.319280, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.489806175231934, Accuracy = 0.8275759220123291\n",
      "Training iter #13218000:   Batch Loss = 8.307684, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.489269256591797, Accuracy = 0.8272781372070312\n",
      "Training iter #13221000:   Batch Loss = 8.060072, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.488957405090332, Accuracy = 0.8272781372070312\n",
      "Training iter #13224000:   Batch Loss = 8.182454, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.488752365112305, Accuracy = 0.8269803524017334\n",
      "Training iter #13227000:   Batch Loss = 8.295875, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.488801956176758, Accuracy = 0.8266825675964355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13230000:   Batch Loss = 8.409844, Accuracy = 0.8346666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.48893928527832, Accuracy = 0.8266825675964355\n",
      "Training iter #13233000:   Batch Loss = 8.005727, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.488792419433594, Accuracy = 0.8266825675964355\n",
      "Training iter #13236000:   Batch Loss = 8.050895, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.488420486450195, Accuracy = 0.8266825675964355\n",
      "Training iter #13239000:   Batch Loss = 8.239602, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.48773193359375, Accuracy = 0.8275759220123291\n",
      "Training iter #13242000:   Batch Loss = 8.349215, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.487067222595215, Accuracy = 0.8275759220123291\n",
      "Training iter #13245000:   Batch Loss = 8.339628, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.486645698547363, Accuracy = 0.8281715512275696\n",
      "Training iter #13248000:   Batch Loss = 8.071392, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.486122131347656, Accuracy = 0.8281715512275696\n",
      "Training iter #13251000:   Batch Loss = 8.169026, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.485801696777344, Accuracy = 0.8284693360328674\n",
      "Training iter #13254000:   Batch Loss = 8.309290, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.485421180725098, Accuracy = 0.8281715512275696\n",
      "Training iter #13257000:   Batch Loss = 8.368211, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.48517894744873, Accuracy = 0.8284693360328674\n",
      "Training iter #13260000:   Batch Loss = 7.964874, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.484935760498047, Accuracy = 0.8284693360328674\n",
      "Training iter #13263000:   Batch Loss = 8.096503, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.484619140625, Accuracy = 0.8281715512275696\n",
      "Training iter #13266000:   Batch Loss = 8.216750, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.48422908782959, Accuracy = 0.8281715512275696\n",
      "Training iter #13269000:   Batch Loss = 8.327138, Accuracy = 0.8586666584014893\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.483770370483398, Accuracy = 0.8284693360328674\n",
      "Training iter #13272000:   Batch Loss = 8.304337, Accuracy = 0.8733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.483627319335938, Accuracy = 0.8284693360328674\n",
      "Training iter #13275000:   Batch Loss = 8.074719, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.483397483825684, Accuracy = 0.8284693360328674\n",
      "Training iter #13278000:   Batch Loss = 8.171075, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.483134269714355, Accuracy = 0.8281715512275696\n",
      "Training iter #13281000:   Batch Loss = 8.322048, Accuracy = 0.862666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.48275375366211, Accuracy = 0.8281715512275696\n",
      "Training iter #13284000:   Batch Loss = 8.390355, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.482455253601074, Accuracy = 0.827873706817627\n",
      "Training iter #13287000:   Batch Loss = 7.986503, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.482195854187012, Accuracy = 0.827873706817627\n",
      "Training iter #13290000:   Batch Loss = 8.109448, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.481952667236328, Accuracy = 0.8284693360328674\n",
      "Training iter #13293000:   Batch Loss = 8.166482, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.481677055358887, Accuracy = 0.8284693360328674\n",
      "Training iter #13296000:   Batch Loss = 8.348122, Accuracy = 0.8560000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.481231689453125, Accuracy = 0.8275759220123291\n",
      "Training iter #13299000:   Batch Loss = 8.250707, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.480953216552734, Accuracy = 0.8272781372070312\n",
      "Training iter #13302000:   Batch Loss = 8.055152, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.480674743652344, Accuracy = 0.8275759220123291\n",
      "Training iter #13305000:   Batch Loss = 8.250448, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.480430603027344, Accuracy = 0.827873706817627\n",
      "Training iter #13308000:   Batch Loss = 8.331603, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.4802827835083, Accuracy = 0.8284693360328674\n",
      "Training iter #13311000:   Batch Loss = 8.371645, Accuracy = 0.8413333296775818\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.480274200439453, Accuracy = 0.8284693360328674\n",
      "Training iter #13314000:   Batch Loss = 7.967776, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.479973793029785, Accuracy = 0.8284693360328674\n",
      "Training iter #13317000:   Batch Loss = 8.092086, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.479459762573242, Accuracy = 0.8281715512275696\n",
      "Training iter #13320000:   Batch Loss = 8.150288, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.478882789611816, Accuracy = 0.8281715512275696\n",
      "Training iter #13323000:   Batch Loss = 8.340365, Accuracy = 0.8553333282470703\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.47835922241211, Accuracy = 0.8284693360328674\n",
      "Training iter #13326000:   Batch Loss = 8.215112, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.478015899658203, Accuracy = 0.8281715512275696\n",
      "Training iter #13329000:   Batch Loss = 8.044621, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.477787017822266, Accuracy = 0.8284693360328674\n",
      "Training iter #13332000:   Batch Loss = 8.231923, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.477555274963379, Accuracy = 0.8287671208381653\n",
      "Training iter #13335000:   Batch Loss = 8.335091, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.47723388671875, Accuracy = 0.8287671208381653\n",
      "Training iter #13338000:   Batch Loss = 8.350400, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.476980209350586, Accuracy = 0.8290649056434631\n",
      "Training iter #13341000:   Batch Loss = 7.995374, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.476665496826172, Accuracy = 0.8287671208381653\n",
      "Training iter #13344000:   Batch Loss = 8.103959, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.476299285888672, Accuracy = 0.8287671208381653\n",
      "Training iter #13347000:   Batch Loss = 8.131078, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.475935935974121, Accuracy = 0.8284693360328674\n",
      "Training iter #13350000:   Batch Loss = 8.368299, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.47558307647705, Accuracy = 0.8290649056434631\n",
      "Training iter #13353000:   Batch Loss = 8.184989, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.475419044494629, Accuracy = 0.8287671208381653\n",
      "Training iter #13356000:   Batch Loss = 8.042699, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.47517204284668, Accuracy = 0.8287671208381653\n",
      "Training iter #13359000:   Batch Loss = 8.242579, Accuracy = 0.887333333492279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.474821090698242, Accuracy = 0.8287671208381653\n",
      "Training iter #13362000:   Batch Loss = 8.338929, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.474470138549805, Accuracy = 0.8290649056434631\n",
      "Training iter #13365000:   Batch Loss = 8.326971, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.47426700592041, Accuracy = 0.8290649056434631\n",
      "Training iter #13368000:   Batch Loss = 7.997516, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.474076271057129, Accuracy = 0.8287671208381653\n",
      "Training iter #13371000:   Batch Loss = 8.126132, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.473837852478027, Accuracy = 0.8287671208381653\n",
      "Training iter #13374000:   Batch Loss = 8.158416, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.473551750183105, Accuracy = 0.8287671208381653\n",
      "Training iter #13377000:   Batch Loss = 8.388896, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.473133087158203, Accuracy = 0.8287671208381653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13380000:   Batch Loss = 8.168680, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.472729682922363, Accuracy = 0.8287671208381653\n",
      "Training iter #13383000:   Batch Loss = 8.043233, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.472376823425293, Accuracy = 0.8287671208381653\n",
      "Training iter #13386000:   Batch Loss = 8.211546, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.471927642822266, Accuracy = 0.8287671208381653\n",
      "Training iter #13389000:   Batch Loss = 8.337284, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.471776008605957, Accuracy = 0.8287671208381653\n",
      "Training iter #13392000:   Batch Loss = 8.280870, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.47148609161377, Accuracy = 0.8290649056434631\n",
      "Training iter #13395000:   Batch Loss = 8.001325, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.47113037109375, Accuracy = 0.8290649056434631\n",
      "Training iter #13398000:   Batch Loss = 8.124940, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.470744132995605, Accuracy = 0.8290649056434631\n",
      "Training iter #13401000:   Batch Loss = 8.173882, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.470340728759766, Accuracy = 0.8290649056434631\n",
      "Training iter #13404000:   Batch Loss = 8.415346, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.470067024230957, Accuracy = 0.8290649056434631\n",
      "Training iter #13407000:   Batch Loss = 8.167366, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.469806671142578, Accuracy = 0.8290649056434631\n",
      "Training iter #13410000:   Batch Loss = 8.023217, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.469536781311035, Accuracy = 0.8290649056434631\n",
      "Training iter #13413000:   Batch Loss = 8.224198, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.469037055969238, Accuracy = 0.8290649056434631\n",
      "Training iter #13416000:   Batch Loss = 8.342683, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.468493461608887, Accuracy = 0.8290649056434631\n",
      "Training iter #13419000:   Batch Loss = 8.275003, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.468161582946777, Accuracy = 0.829362690448761\n",
      "Training iter #13422000:   Batch Loss = 7.971281, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.467897415161133, Accuracy = 0.8290649056434631\n",
      "Training iter #13425000:   Batch Loss = 8.123418, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.46770191192627, Accuracy = 0.8290649056434631\n",
      "Training iter #13428000:   Batch Loss = 8.189981, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.467544555664062, Accuracy = 0.829362690448761\n",
      "Training iter #13431000:   Batch Loss = 8.422688, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.467367172241211, Accuracy = 0.8287671208381653\n",
      "Training iter #13434000:   Batch Loss = 8.127770, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.467153549194336, Accuracy = 0.8287671208381653\n",
      "Training iter #13437000:   Batch Loss = 8.018496, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.466928482055664, Accuracy = 0.8284693360328674\n",
      "Training iter #13440000:   Batch Loss = 8.255246, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.466443061828613, Accuracy = 0.8287671208381653\n",
      "Training iter #13443000:   Batch Loss = 8.319655, Accuracy = 0.8579999804496765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.46589469909668, Accuracy = 0.8290649056434631\n",
      "Training iter #13446000:   Batch Loss = 8.248775, Accuracy = 0.8846666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.465492248535156, Accuracy = 0.829362690448761\n",
      "Training iter #13449000:   Batch Loss = 7.975883, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.46515941619873, Accuracy = 0.8296605348587036\n",
      "Training iter #13452000:   Batch Loss = 8.135714, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.464810371398926, Accuracy = 0.8296605348587036\n",
      "Training iter #13455000:   Batch Loss = 8.194135, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.464498519897461, Accuracy = 0.829362690448761\n",
      "Training iter #13458000:   Batch Loss = 8.408195, Accuracy = 0.8306666612625122\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.4641752243042, Accuracy = 0.8299583196640015\n",
      "Training iter #13461000:   Batch Loss = 8.096363, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.464011192321777, Accuracy = 0.8290649056434631\n",
      "Training iter #13464000:   Batch Loss = 8.014557, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.46387767791748, Accuracy = 0.8290649056434631\n",
      "Training iter #13467000:   Batch Loss = 8.242218, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.463566780090332, Accuracy = 0.829362690448761\n",
      "Training iter #13470000:   Batch Loss = 8.300602, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.463135719299316, Accuracy = 0.829362690448761\n",
      "Training iter #13473000:   Batch Loss = 8.278540, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.462821960449219, Accuracy = 0.8296605348587036\n",
      "Training iter #13476000:   Batch Loss = 7.985642, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.462514877319336, Accuracy = 0.8290649056434631\n",
      "Training iter #13479000:   Batch Loss = 8.132475, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.46217155456543, Accuracy = 0.8296605348587036\n",
      "Training iter #13482000:   Batch Loss = 8.196774, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.461930274963379, Accuracy = 0.8302561044692993\n",
      "Training iter #13485000:   Batch Loss = 8.441009, Accuracy = 0.8240000009536743\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.461441993713379, Accuracy = 0.8302561044692993\n",
      "Training iter #13488000:   Batch Loss = 8.069947, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.461139678955078, Accuracy = 0.8302561044692993\n",
      "Training iter #13491000:   Batch Loss = 8.002931, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.460892677307129, Accuracy = 0.8302561044692993\n",
      "Training iter #13494000:   Batch Loss = 8.245594, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.460434913635254, Accuracy = 0.8305538892745972\n",
      "Training iter #13497000:   Batch Loss = 8.285737, Accuracy = 0.8706666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.459872245788574, Accuracy = 0.8305538892745972\n",
      "Training iter #13500000:   Batch Loss = 8.259645, Accuracy = 0.8853333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.459426879882812, Accuracy = 0.830851674079895\n",
      "Training iter #13503000:   Batch Loss = 7.993836, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.459111213684082, Accuracy = 0.8305538892745972\n",
      "Training iter #13506000:   Batch Loss = 8.132116, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.458843231201172, Accuracy = 0.8305538892745972\n",
      "Training iter #13509000:   Batch Loss = 8.199604, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.458812713623047, Accuracy = 0.830851674079895\n",
      "Training iter #13512000:   Batch Loss = 8.420822, Accuracy = 0.8266666531562805\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.458782196044922, Accuracy = 0.830851674079895\n",
      "Training iter #13515000:   Batch Loss = 8.017374, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.458542823791504, Accuracy = 0.8305538892745972\n",
      "Training iter #13518000:   Batch Loss = 8.008204, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.458312034606934, Accuracy = 0.8299583196640015\n",
      "Training iter #13521000:   Batch Loss = 8.260510, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.457783699035645, Accuracy = 0.830851674079895\n",
      "Training iter #13524000:   Batch Loss = 8.264456, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.457321166992188, Accuracy = 0.8305538892745972\n",
      "Training iter #13527000:   Batch Loss = 8.254292, Accuracy = 0.8859999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.457025527954102, Accuracy = 0.8311495184898376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13530000:   Batch Loss = 8.018558, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456872940063477, Accuracy = 0.8311495184898376\n",
      "Training iter #13533000:   Batch Loss = 8.127314, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456857681274414, Accuracy = 0.8314473032951355\n",
      "Training iter #13536000:   Batch Loss = 8.245985, Accuracy = 0.878000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456717491149902, Accuracy = 0.8311495184898376\n",
      "Training iter #13539000:   Batch Loss = 8.388183, Accuracy = 0.8393333554267883\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456384658813477, Accuracy = 0.830851674079895\n",
      "Training iter #13542000:   Batch Loss = 7.989836, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456149101257324, Accuracy = 0.8311495184898376\n",
      "Training iter #13545000:   Batch Loss = 8.002771, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456103324890137, Accuracy = 0.830851674079895\n",
      "Training iter #13548000:   Batch Loss = 8.242174, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456082344055176, Accuracy = 0.8302561044692993\n",
      "Training iter #13551000:   Batch Loss = 8.276543, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456230163574219, Accuracy = 0.8302561044692993\n",
      "Training iter #13554000:   Batch Loss = 8.274793, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.456024169921875, Accuracy = 0.8305538892745972\n",
      "Training iter #13557000:   Batch Loss = 8.041121, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.455279350280762, Accuracy = 0.8305538892745972\n",
      "Training iter #13560000:   Batch Loss = 8.134996, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.454557418823242, Accuracy = 0.830851674079895\n",
      "Training iter #13563000:   Batch Loss = 8.257400, Accuracy = 0.874666690826416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.453786849975586, Accuracy = 0.8311495184898376\n",
      "Training iter #13566000:   Batch Loss = 8.339116, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.4532470703125, Accuracy = 0.8314473032951355\n",
      "Training iter #13569000:   Batch Loss = 7.952409, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.452587127685547, Accuracy = 0.8314473032951355\n",
      "Training iter #13572000:   Batch Loss = 8.055962, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.452199935913086, Accuracy = 0.8314473032951355\n",
      "Training iter #13575000:   Batch Loss = 8.187442, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.45193862915039, Accuracy = 0.8320428729057312\n",
      "Training iter #13578000:   Batch Loss = 8.300298, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.45174789428711, Accuracy = 0.832340657711029\n",
      "Training iter #13581000:   Batch Loss = 8.301533, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.451685905456543, Accuracy = 0.8320428729057312\n",
      "Training iter #13584000:   Batch Loss = 8.040659, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.451638221740723, Accuracy = 0.8317450881004333\n",
      "Training iter #13587000:   Batch Loss = 8.127587, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.451579093933105, Accuracy = 0.832340657711029\n",
      "Training iter #13590000:   Batch Loss = 8.270969, Accuracy = 0.8733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.451050758361816, Accuracy = 0.8326385021209717\n",
      "Training iter #13593000:   Batch Loss = 8.330709, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.450295448303223, Accuracy = 0.8326385021209717\n",
      "Training iter #13596000:   Batch Loss = 7.935609, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.449933052062988, Accuracy = 0.8317450881004333\n",
      "Training iter #13599000:   Batch Loss = 8.072628, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.448890686035156, Accuracy = 0.8320428729057312\n",
      "Training iter #13602000:   Batch Loss = 8.173965, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.448623657226562, Accuracy = 0.8317450881004333\n",
      "Training iter #13605000:   Batch Loss = 8.278482, Accuracy = 0.8740000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.448393821716309, Accuracy = 0.8320428729057312\n",
      "Training iter #13608000:   Batch Loss = 8.229155, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.447846412658691, Accuracy = 0.8317450881004333\n",
      "Training iter #13611000:   Batch Loss = 8.036372, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.447600364685059, Accuracy = 0.8317450881004333\n",
      "Training iter #13614000:   Batch Loss = 8.177279, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44714069366455, Accuracy = 0.8317450881004333\n",
      "Training iter #13617000:   Batch Loss = 8.291098, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.4468994140625, Accuracy = 0.832340657711029\n",
      "Training iter #13620000:   Batch Loss = 8.318460, Accuracy = 0.8473333120346069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.447007179260254, Accuracy = 0.8326385021209717\n",
      "Training iter #13623000:   Batch Loss = 7.949909, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.446928977966309, Accuracy = 0.8329362869262695\n",
      "Training iter #13626000:   Batch Loss = 8.067411, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44664478302002, Accuracy = 0.832340657711029\n",
      "Training iter #13629000:   Batch Loss = 8.131569, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44637680053711, Accuracy = 0.832340657711029\n",
      "Training iter #13632000:   Batch Loss = 8.293741, Accuracy = 0.8679999709129333\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44629955291748, Accuracy = 0.8329362869262695\n",
      "Training iter #13635000:   Batch Loss = 8.195697, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44609260559082, Accuracy = 0.8329362869262695\n",
      "Training iter #13638000:   Batch Loss = 8.031854, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.445840835571289, Accuracy = 0.8338296413421631\n",
      "Training iter #13641000:   Batch Loss = 8.218874, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44539737701416, Accuracy = 0.8335318565368652\n",
      "Training iter #13644000:   Batch Loss = 8.305712, Accuracy = 0.8679999709129333\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.445263862609863, Accuracy = 0.8338296413421631\n",
      "Training iter #13647000:   Batch Loss = 8.318391, Accuracy = 0.8526666760444641\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.445042610168457, Accuracy = 0.8329362869262695\n",
      "Training iter #13650000:   Batch Loss = 7.953064, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.444292068481445, Accuracy = 0.8329362869262695\n",
      "Training iter #13653000:   Batch Loss = 8.071081, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44350528717041, Accuracy = 0.8329362869262695\n",
      "Training iter #13656000:   Batch Loss = 8.123010, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.442991256713867, Accuracy = 0.8332340717315674\n",
      "Training iter #13659000:   Batch Loss = 8.299861, Accuracy = 0.862666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.442468643188477, Accuracy = 0.8335318565368652\n",
      "Training iter #13662000:   Batch Loss = 8.175171, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.442133903503418, Accuracy = 0.8335318565368652\n",
      "Training iter #13665000:   Batch Loss = 8.010677, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.442054748535156, Accuracy = 0.8335318565368652\n",
      "Training iter #13668000:   Batch Loss = 8.209808, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.441770553588867, Accuracy = 0.8329362869262695\n",
      "Training iter #13671000:   Batch Loss = 8.291497, Accuracy = 0.875333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44151496887207, Accuracy = 0.8329362869262695\n",
      "Training iter #13674000:   Batch Loss = 8.306752, Accuracy = 0.8560000061988831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.441368103027344, Accuracy = 0.8332340717315674\n",
      "Training iter #13677000:   Batch Loss = 7.966462, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.441317558288574, Accuracy = 0.8338296413421631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13680000:   Batch Loss = 8.075459, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44128704071045, Accuracy = 0.8335318565368652\n",
      "Training iter #13683000:   Batch Loss = 8.096195, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.441241264343262, Accuracy = 0.8329362869262695\n",
      "Training iter #13686000:   Batch Loss = 8.348837, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.441137313842773, Accuracy = 0.8332340717315674\n",
      "Training iter #13689000:   Batch Loss = 8.142194, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.441004753112793, Accuracy = 0.8341274857521057\n",
      "Training iter #13692000:   Batch Loss = 8.003532, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.44092845916748, Accuracy = 0.8338296413421631\n",
      "Training iter #13695000:   Batch Loss = 8.183251, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.440563201904297, Accuracy = 0.8338296413421631\n",
      "Training iter #13698000:   Batch Loss = 8.296838, Accuracy = 0.8733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.440125465393066, Accuracy = 0.8338296413421631\n",
      "Training iter #13701000:   Batch Loss = 8.273001, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.439613342285156, Accuracy = 0.8338296413421631\n",
      "Training iter #13704000:   Batch Loss = 7.963459, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.439297676086426, Accuracy = 0.8335318565368652\n",
      "Training iter #13707000:   Batch Loss = 8.090696, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.438987731933594, Accuracy = 0.8332340717315674\n",
      "Training iter #13710000:   Batch Loss = 8.142756, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.438760757446289, Accuracy = 0.8332340717315674\n",
      "Training iter #13713000:   Batch Loss = 8.344102, Accuracy = 0.8379999995231628\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.43844985961914, Accuracy = 0.8332340717315674\n",
      "Training iter #13716000:   Batch Loss = 8.130640, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.438165664672852, Accuracy = 0.8332340717315674\n",
      "Training iter #13719000:   Batch Loss = 7.983385, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.437990188598633, Accuracy = 0.8338296413421631\n",
      "Training iter #13722000:   Batch Loss = 8.189491, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.437724113464355, Accuracy = 0.8338296413421631\n",
      "Training iter #13725000:   Batch Loss = 8.302481, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.4376220703125, Accuracy = 0.8338296413421631\n",
      "Training iter #13728000:   Batch Loss = 8.209299, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.437312126159668, Accuracy = 0.8335318565368652\n",
      "Training iter #13731000:   Batch Loss = 7.969780, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.437091827392578, Accuracy = 0.8335318565368652\n",
      "Training iter #13734000:   Batch Loss = 8.092796, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.436789512634277, Accuracy = 0.8341274857521057\n",
      "Training iter #13737000:   Batch Loss = 8.150169, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.436454772949219, Accuracy = 0.8341274857521057\n",
      "Training iter #13740000:   Batch Loss = 8.361340, Accuracy = 0.831333339214325\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.436104774475098, Accuracy = 0.8344252705574036\n",
      "Training iter #13743000:   Batch Loss = 8.107127, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.435823440551758, Accuracy = 0.8344252705574036\n",
      "Training iter #13746000:   Batch Loss = 7.989965, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.435979843139648, Accuracy = 0.8344252705574036\n",
      "Training iter #13749000:   Batch Loss = 8.194747, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.435831069946289, Accuracy = 0.8344252705574036\n",
      "Training iter #13752000:   Batch Loss = 8.291032, Accuracy = 0.8726666569709778\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.435813903808594, Accuracy = 0.8341274857521057\n",
      "Training iter #13755000:   Batch Loss = 8.220107, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.43548583984375, Accuracy = 0.8338296413421631\n",
      "Training iter #13758000:   Batch Loss = 7.940566, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.434968948364258, Accuracy = 0.8338296413421631\n",
      "Training iter #13761000:   Batch Loss = 8.099896, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.434395790100098, Accuracy = 0.8344252705574036\n",
      "Training iter #13764000:   Batch Loss = 8.128875, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.43409538269043, Accuracy = 0.8344252705574036\n",
      "Training iter #13767000:   Batch Loss = 8.380809, Accuracy = 0.831333339214325\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.433883666992188, Accuracy = 0.8338296413421631\n",
      "Training iter #13770000:   Batch Loss = 8.091512, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.433751106262207, Accuracy = 0.8335318565368652\n",
      "Training iter #13773000:   Batch Loss = 7.975403, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.433698654174805, Accuracy = 0.8335318565368652\n",
      "Training iter #13776000:   Batch Loss = 8.210604, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.433345794677734, Accuracy = 0.8347230553627014\n",
      "Training iter #13779000:   Batch Loss = 8.263483, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.432855606079102, Accuracy = 0.8344252705574036\n",
      "Training iter #13782000:   Batch Loss = 8.228296, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.432095527648926, Accuracy = 0.8347230553627014\n",
      "Training iter #13785000:   Batch Loss = 7.946374, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.431506156921387, Accuracy = 0.8347230553627014\n",
      "Training iter #13788000:   Batch Loss = 8.103639, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.43116569519043, Accuracy = 0.8353186249732971\n",
      "Training iter #13791000:   Batch Loss = 8.162504, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.431215286254883, Accuracy = 0.835616409778595\n",
      "Training iter #13794000:   Batch Loss = 8.359580, Accuracy = 0.8366666436195374\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.431230545043945, Accuracy = 0.8350208401679993\n",
      "Training iter #13797000:   Batch Loss = 8.052503, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.430830955505371, Accuracy = 0.8350208401679993\n",
      "Training iter #13800000:   Batch Loss = 7.982791, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.43032169342041, Accuracy = 0.8350208401679993\n",
      "Training iter #13803000:   Batch Loss = 8.217405, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.429505348205566, Accuracy = 0.8353186249732971\n",
      "Training iter #13806000:   Batch Loss = 8.241542, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.428771018981934, Accuracy = 0.8353186249732971\n",
      "Training iter #13809000:   Batch Loss = 8.219507, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.428276062011719, Accuracy = 0.8350208401679993\n",
      "Training iter #13812000:   Batch Loss = 7.954807, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.428079605102539, Accuracy = 0.8353186249732971\n",
      "Training iter #13815000:   Batch Loss = 8.116415, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.427994728088379, Accuracy = 0.8359142541885376\n",
      "Training iter #13818000:   Batch Loss = 8.155648, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.428083419799805, Accuracy = 0.8353186249732971\n",
      "Training iter #13821000:   Batch Loss = 8.393567, Accuracy = 0.8259999752044678\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.428211212158203, Accuracy = 0.8350208401679993\n",
      "Training iter #13824000:   Batch Loss = 8.008952, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.428053855895996, Accuracy = 0.8353186249732971\n",
      "Training iter #13827000:   Batch Loss = 7.978459, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.42790699005127, Accuracy = 0.835616409778595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13830000:   Batch Loss = 8.227071, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.42768383026123, Accuracy = 0.835616409778595\n",
      "Training iter #13833000:   Batch Loss = 8.237054, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.427389144897461, Accuracy = 0.8353186249732971\n",
      "Training iter #13836000:   Batch Loss = 8.217734, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.42697811126709, Accuracy = 0.835616409778595\n",
      "Training iter #13839000:   Batch Loss = 7.971370, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.426727294921875, Accuracy = 0.8359142541885376\n",
      "Training iter #13842000:   Batch Loss = 8.099421, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.426360130310059, Accuracy = 0.8362120389938354\n",
      "Training iter #13845000:   Batch Loss = 8.187403, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.425926208496094, Accuracy = 0.8365098237991333\n",
      "Training iter #13848000:   Batch Loss = 8.360057, Accuracy = 0.8373333215713501\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.425619125366211, Accuracy = 0.8353186249732971\n",
      "Training iter #13851000:   Batch Loss = 7.977660, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.425284385681152, Accuracy = 0.835616409778595\n",
      "Training iter #13854000:   Batch Loss = 7.974045, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.425045013427734, Accuracy = 0.835616409778595\n",
      "Training iter #13857000:   Batch Loss = 8.212632, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.424729347229004, Accuracy = 0.8353186249732971\n",
      "Training iter #13860000:   Batch Loss = 8.224439, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.424427032470703, Accuracy = 0.835616409778595\n",
      "Training iter #13863000:   Batch Loss = 8.214438, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.423898696899414, Accuracy = 0.8353186249732971\n",
      "Training iter #13866000:   Batch Loss = 7.980894, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.423482894897461, Accuracy = 0.835616409778595\n",
      "Training iter #13869000:   Batch Loss = 8.096868, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.423032760620117, Accuracy = 0.8362120389938354\n",
      "Training iter #13872000:   Batch Loss = 8.214617, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.422881126403809, Accuracy = 0.8362120389938354\n",
      "Training iter #13875000:   Batch Loss = 8.325257, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.422666549682617, Accuracy = 0.8362120389938354\n",
      "Training iter #13878000:   Batch Loss = 7.950610, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.42240047454834, Accuracy = 0.8359142541885376\n",
      "Training iter #13881000:   Batch Loss = 7.982835, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.422171592712402, Accuracy = 0.8359142541885376\n",
      "Training iter #13884000:   Batch Loss = 8.167809, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.421856880187988, Accuracy = 0.8359142541885376\n",
      "Training iter #13887000:   Batch Loss = 8.258924, Accuracy = 0.8786666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.421560287475586, Accuracy = 0.8359142541885376\n",
      "Training iter #13890000:   Batch Loss = 8.242126, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.421120643615723, Accuracy = 0.835616409778595\n",
      "Training iter #13893000:   Batch Loss = 8.005902, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.420745849609375, Accuracy = 0.835616409778595\n",
      "Training iter #13896000:   Batch Loss = 8.093106, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.420526504516602, Accuracy = 0.835616409778595\n",
      "Training iter #13899000:   Batch Loss = 8.216353, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.420245170593262, Accuracy = 0.8362120389938354\n",
      "Training iter #13902000:   Batch Loss = 8.279292, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.419829368591309, Accuracy = 0.8362120389938354\n",
      "Training iter #13905000:   Batch Loss = 7.909191, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.419451713562012, Accuracy = 0.8359142541885376\n",
      "Training iter #13908000:   Batch Loss = 8.026570, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.41913890838623, Accuracy = 0.8362120389938354\n",
      "Training iter #13911000:   Batch Loss = 8.136731, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.418877601623535, Accuracy = 0.8359142541885376\n",
      "Training iter #13914000:   Batch Loss = 8.242566, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.418601989746094, Accuracy = 0.8353186249732971\n",
      "Training iter #13917000:   Batch Loss = 8.238369, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.418306350708008, Accuracy = 0.8347230553627014\n",
      "Training iter #13920000:   Batch Loss = 8.012775, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.418061256408691, Accuracy = 0.8347230553627014\n",
      "Training iter #13923000:   Batch Loss = 8.086114, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.41797924041748, Accuracy = 0.8362120389938354\n",
      "Training iter #13926000:   Batch Loss = 8.232773, Accuracy = 0.8786666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.417891502380371, Accuracy = 0.8365098237991333\n",
      "Training iter #13929000:   Batch Loss = 8.295449, Accuracy = 0.8533333539962769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.41765022277832, Accuracy = 0.8365098237991333\n",
      "Training iter #13932000:   Batch Loss = 7.912144, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.41742992401123, Accuracy = 0.8365098237991333\n",
      "Training iter #13935000:   Batch Loss = 8.040110, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.417202949523926, Accuracy = 0.8365098237991333\n",
      "Training iter #13938000:   Batch Loss = 8.097692, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.416933059692383, Accuracy = 0.8362120389938354\n",
      "Training iter #13941000:   Batch Loss = 8.263069, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.416500091552734, Accuracy = 0.8365098237991333\n",
      "Training iter #13944000:   Batch Loss = 8.164248, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.416069030761719, Accuracy = 0.8374032378196716\n",
      "Training iter #13947000:   Batch Loss = 7.994345, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.415773391723633, Accuracy = 0.8379988074302673\n",
      "Training iter #13950000:   Batch Loss = 8.155171, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.415550231933594, Accuracy = 0.8379988074302673\n",
      "Training iter #13953000:   Batch Loss = 8.242212, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.41534423828125, Accuracy = 0.8379988074302673\n",
      "Training iter #13956000:   Batch Loss = 8.280227, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.415040969848633, Accuracy = 0.8382965922355652\n",
      "Training iter #13959000:   Batch Loss = 7.910466, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.414772033691406, Accuracy = 0.8388922214508057\n",
      "Training iter #13962000:   Batch Loss = 8.022513, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.414461135864258, Accuracy = 0.838594377040863\n",
      "Training iter #13965000:   Batch Loss = 8.078206, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.41418170928955, Accuracy = 0.8388922214508057\n",
      "Training iter #13968000:   Batch Loss = 8.252497, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.413749694824219, Accuracy = 0.8397855758666992\n",
      "Training iter #13971000:   Batch Loss = 8.143270, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.413297653198242, Accuracy = 0.8397855758666992\n",
      "Training iter #13974000:   Batch Loss = 7.983168, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.412968635559082, Accuracy = 0.8394877910614014\n",
      "Training iter #13977000:   Batch Loss = 8.161063, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.412583351135254, Accuracy = 0.8394877910614014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13980000:   Batch Loss = 8.252764, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.412249565124512, Accuracy = 0.8397855758666992\n",
      "Training iter #13983000:   Batch Loss = 8.270590, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.411895751953125, Accuracy = 0.8397855758666992\n",
      "Training iter #13986000:   Batch Loss = 7.930827, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.411638259887695, Accuracy = 0.8400833606719971\n",
      "Training iter #13989000:   Batch Loss = 8.028676, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.411373138427734, Accuracy = 0.8394877910614014\n",
      "Training iter #13992000:   Batch Loss = 8.078415, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.411327362060547, Accuracy = 0.8403812050819397\n",
      "Training iter #13995000:   Batch Loss = 8.275480, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.41104507446289, Accuracy = 0.8400833606719971\n",
      "Training iter #13998000:   Batch Loss = 8.120070, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.410621643066406, Accuracy = 0.8400833606719971\n",
      "Training iter #14001000:   Batch Loss = 7.971630, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.410330772399902, Accuracy = 0.8397855758666992\n",
      "Training iter #14004000:   Batch Loss = 8.169827, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.410066604614258, Accuracy = 0.8403812050819397\n",
      "Training iter #14007000:   Batch Loss = 8.248473, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.409807205200195, Accuracy = 0.8406789898872375\n",
      "Training iter #14010000:   Batch Loss = 8.230834, Accuracy = 0.8786666393280029\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.409446716308594, Accuracy = 0.8406789898872375\n",
      "Training iter #14013000:   Batch Loss = 7.938906, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.409096717834473, Accuracy = 0.8403812050819397\n",
      "Training iter #14016000:   Batch Loss = 8.054504, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.408682823181152, Accuracy = 0.8400833606719971\n",
      "Training iter #14019000:   Batch Loss = 8.064724, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.408303260803223, Accuracy = 0.8397855758666992\n",
      "Training iter #14022000:   Batch Loss = 8.293113, Accuracy = 0.8519999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.407853126525879, Accuracy = 0.8397855758666992\n",
      "Training iter #14025000:   Batch Loss = 8.098927, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.407455444335938, Accuracy = 0.8400833606719971\n",
      "Training iter #14028000:   Batch Loss = 7.976017, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.40725040435791, Accuracy = 0.8400833606719971\n",
      "Training iter #14031000:   Batch Loss = 8.137090, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.407135009765625, Accuracy = 0.8397855758666992\n",
      "Training iter #14034000:   Batch Loss = 8.259543, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.406964302062988, Accuracy = 0.8406789898872375\n",
      "Training iter #14037000:   Batch Loss = 8.208660, Accuracy = 0.8840000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.406682014465332, Accuracy = 0.8400833606719971\n",
      "Training iter #14040000:   Batch Loss = 7.954211, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.406461715698242, Accuracy = 0.8403812050819397\n",
      "Training iter #14043000:   Batch Loss = 8.054781, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.40604305267334, Accuracy = 0.8406789898872375\n",
      "Training iter #14046000:   Batch Loss = 8.105277, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.405488014221191, Accuracy = 0.8406789898872375\n",
      "Training iter #14049000:   Batch Loss = 8.319280, Accuracy = 0.840666651725769\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.404937744140625, Accuracy = 0.8403812050819397\n",
      "Training iter #14052000:   Batch Loss = 8.088189, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.404497146606445, Accuracy = 0.8406789898872375\n",
      "Training iter #14055000:   Batch Loss = 7.957524, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.404343605041504, Accuracy = 0.8406789898872375\n",
      "Training iter #14058000:   Batch Loss = 8.150805, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.404234886169434, Accuracy = 0.8406789898872375\n",
      "Training iter #14061000:   Batch Loss = 8.245804, Accuracy = 0.8859999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.403964042663574, Accuracy = 0.8400833606719971\n",
      "Training iter #14064000:   Batch Loss = 8.188698, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.403648376464844, Accuracy = 0.8409767746925354\n",
      "Training iter #14067000:   Batch Loss = 7.917112, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.403160095214844, Accuracy = 0.8406789898872375\n",
      "Training iter #14070000:   Batch Loss = 8.059005, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.402609825134277, Accuracy = 0.8409767746925354\n",
      "Training iter #14073000:   Batch Loss = 8.110518, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.402654647827148, Accuracy = 0.8403812050819397\n",
      "Training iter #14076000:   Batch Loss = 8.321535, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.402527809143066, Accuracy = 0.8406789898872375\n",
      "Training iter #14079000:   Batch Loss = 8.060394, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.402361869812012, Accuracy = 0.8400833606719971\n",
      "Training iter #14082000:   Batch Loss = 7.960613, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.402030944824219, Accuracy = 0.8412745594978333\n",
      "Training iter #14085000:   Batch Loss = 8.178339, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.40168571472168, Accuracy = 0.8409767746925354\n",
      "Training iter #14088000:   Batch Loss = 8.231189, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.401529312133789, Accuracy = 0.8409767746925354\n",
      "Training iter #14091000:   Batch Loss = 8.172129, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.401298522949219, Accuracy = 0.8418701887130737\n",
      "Training iter #14094000:   Batch Loss = 7.913919, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.400839805603027, Accuracy = 0.8421679735183716\n",
      "Training iter #14097000:   Batch Loss = 8.066467, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.40022087097168, Accuracy = 0.8418701887130737\n",
      "Training iter #14100000:   Batch Loss = 8.108485, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.400435447692871, Accuracy = 0.8415723443031311\n",
      "Training iter #14103000:   Batch Loss = 8.331188, Accuracy = 0.8426666855812073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.400230407714844, Accuracy = 0.8412745594978333\n",
      "Training iter #14106000:   Batch Loss = 8.049205, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.399606704711914, Accuracy = 0.8412745594978333\n",
      "Training iter #14109000:   Batch Loss = 7.949692, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.398558616638184, Accuracy = 0.8412745594978333\n",
      "Training iter #14112000:   Batch Loss = 8.169326, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.398136138916016, Accuracy = 0.8406789898872375\n",
      "Training iter #14115000:   Batch Loss = 8.213257, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.398104667663574, Accuracy = 0.8409767746925354\n",
      "Training iter #14118000:   Batch Loss = 8.200314, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.398150444030762, Accuracy = 0.8412745594978333\n",
      "Training iter #14121000:   Batch Loss = 7.922794, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.397953987121582, Accuracy = 0.8409767746925354\n",
      "Training iter #14124000:   Batch Loss = 8.061680, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.396624565124512, Accuracy = 0.8409767746925354\n",
      "Training iter #14127000:   Batch Loss = 8.129023, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.396476745605469, Accuracy = 0.8409767746925354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #14130000:   Batch Loss = 8.314997, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.39707088470459, Accuracy = 0.8409767746925354\n",
      "Training iter #14133000:   Batch Loss = 8.006892, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.397234916687012, Accuracy = 0.8412745594978333\n",
      "Training iter #14136000:   Batch Loss = 7.953341, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.397148132324219, Accuracy = 0.8412745594978333\n",
      "Training iter #14139000:   Batch Loss = 8.174177, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.396895408630371, Accuracy = 0.8412745594978333\n",
      "Training iter #14142000:   Batch Loss = 8.199361, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.396538734436035, Accuracy = 0.8412745594978333\n",
      "Training iter #14145000:   Batch Loss = 8.177093, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.395980834960938, Accuracy = 0.8415723443031311\n",
      "Training iter #14148000:   Batch Loss = 7.930148, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.395577430725098, Accuracy = 0.8415723443031311\n",
      "Training iter #14151000:   Batch Loss = 8.074960, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.39503288269043, Accuracy = 0.8412745594978333\n",
      "Training iter #14154000:   Batch Loss = 8.108397, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.39449405670166, Accuracy = 0.8409767746925354\n",
      "Training iter #14157000:   Batch Loss = 8.327487, Accuracy = 0.8420000076293945\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.394346237182617, Accuracy = 0.8409767746925354\n",
      "Training iter #14160000:   Batch Loss = 7.963742, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.394243240356445, Accuracy = 0.8412745594978333\n",
      "Training iter #14163000:   Batch Loss = 7.945909, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.394246101379395, Accuracy = 0.8412745594978333\n",
      "Training iter #14166000:   Batch Loss = 8.189176, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.394064903259277, Accuracy = 0.8412745594978333\n",
      "Training iter #14169000:   Batch Loss = 8.194545, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.393798828125, Accuracy = 0.8418701887130737\n",
      "Training iter #14172000:   Batch Loss = 8.172773, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.39344310760498, Accuracy = 0.8415723443031311\n",
      "Training iter #14175000:   Batch Loss = 7.958914, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.393339157104492, Accuracy = 0.8418701887130737\n",
      "Training iter #14178000:   Batch Loss = 8.061570, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.393111228942871, Accuracy = 0.8418701887130737\n",
      "Training iter #14181000:   Batch Loss = 8.160464, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.39285945892334, Accuracy = 0.8418701887130737\n",
      "Training iter #14184000:   Batch Loss = 8.303006, Accuracy = 0.8493333458900452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.39254093170166, Accuracy = 0.8421679735183716\n",
      "Training iter #14187000:   Batch Loss = 7.942787, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.392326354980469, Accuracy = 0.8421679735183716\n",
      "Training iter #14190000:   Batch Loss = 7.946661, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.392158508300781, Accuracy = 0.8421679735183716\n",
      "Training iter #14193000:   Batch Loss = 8.168982, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.391755104064941, Accuracy = 0.8421679735183716\n",
      "Training iter #14196000:   Batch Loss = 8.193872, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.391403198242188, Accuracy = 0.8418701887130737\n",
      "Training iter #14199000:   Batch Loss = 8.192932, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.391158103942871, Accuracy = 0.8415723443031311\n",
      "Training iter #14202000:   Batch Loss = 7.972730, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.391075134277344, Accuracy = 0.8418701887130737\n",
      "Training iter #14205000:   Batch Loss = 8.050696, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.390718460083008, Accuracy = 0.8421679735183716\n",
      "Training iter #14208000:   Batch Loss = 8.174536, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.390313148498535, Accuracy = 0.8424657583236694\n",
      "Training iter #14211000:   Batch Loss = 8.249414, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.389913558959961, Accuracy = 0.8424657583236694\n",
      "Training iter #14214000:   Batch Loss = 7.914448, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.389493942260742, Accuracy = 0.8424657583236694\n",
      "Training iter #14217000:   Batch Loss = 7.964096, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.389189720153809, Accuracy = 0.8427635431289673\n",
      "Training iter #14220000:   Batch Loss = 8.125980, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.388853073120117, Accuracy = 0.8427635431289673\n",
      "Training iter #14223000:   Batch Loss = 8.221069, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.388435363769531, Accuracy = 0.8430613279342651\n",
      "Training iter #14226000:   Batch Loss = 8.221728, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.387750625610352, Accuracy = 0.8430613279342651\n",
      "Training iter #14229000:   Batch Loss = 7.974569, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.387105941772461, Accuracy = 0.8430613279342651\n",
      "Training iter #14232000:   Batch Loss = 8.071322, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.386281967163086, Accuracy = 0.8430613279342651\n",
      "Training iter #14235000:   Batch Loss = 8.189589, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.385680198669434, Accuracy = 0.8430613279342651\n",
      "Training iter #14238000:   Batch Loss = 8.239497, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.385014533996582, Accuracy = 0.8427635431289673\n",
      "Training iter #14241000:   Batch Loss = 7.876797, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.384491920471191, Accuracy = 0.8427635431289673\n",
      "Training iter #14244000:   Batch Loss = 8.005551, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.384113311767578, Accuracy = 0.8430613279342651\n",
      "Training iter #14247000:   Batch Loss = 8.103442, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.383735656738281, Accuracy = 0.8430613279342651\n",
      "Training iter #14250000:   Batch Loss = 8.200699, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.383251190185547, Accuracy = 0.8430613279342651\n",
      "Training iter #14253000:   Batch Loss = 8.167722, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.38278865814209, Accuracy = 0.8430613279342651\n",
      "Training iter #14256000:   Batch Loss = 7.978222, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.382558822631836, Accuracy = 0.8430613279342651\n",
      "Training iter #14259000:   Batch Loss = 8.072178, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.382134437561035, Accuracy = 0.8430613279342651\n",
      "Training iter #14262000:   Batch Loss = 8.192202, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.38185977935791, Accuracy = 0.8427635431289673\n",
      "Training iter #14265000:   Batch Loss = 8.232801, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.381619453430176, Accuracy = 0.8427635431289673\n",
      "Training iter #14268000:   Batch Loss = 7.896394, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.381468772888184, Accuracy = 0.8424657583236694\n",
      "Training iter #14271000:   Batch Loss = 8.009093, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.38131332397461, Accuracy = 0.8421679735183716\n",
      "Training iter #14274000:   Batch Loss = 8.074951, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.381052017211914, Accuracy = 0.8424657583236694\n",
      "Training iter #14277000:   Batch Loss = 8.214685, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.380586624145508, Accuracy = 0.8421679735183716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #14280000:   Batch Loss = 8.126166, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.380135536193848, Accuracy = 0.8418701887130737\n",
      "Training iter #14283000:   Batch Loss = 7.965562, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.379974365234375, Accuracy = 0.8418701887130737\n",
      "Training iter #14286000:   Batch Loss = 8.140489, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.37962818145752, Accuracy = 0.8421679735183716\n",
      "Training iter #14289000:   Batch Loss = 8.212727, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.379692077636719, Accuracy = 0.8418701887130737\n",
      "Training iter #14292000:   Batch Loss = 8.233843, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.379467964172363, Accuracy = 0.8421679735183716\n",
      "Training iter #14295000:   Batch Loss = 7.892148, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.379292488098145, Accuracy = 0.8424657583236694\n",
      "Training iter #14298000:   Batch Loss = 7.999236, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.379105567932129, Accuracy = 0.8421679735183716\n",
      "Training iter #14301000:   Batch Loss = 8.047618, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.378811836242676, Accuracy = 0.8421679735183716\n",
      "Training iter #14304000:   Batch Loss = 8.216046, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.378437995910645, Accuracy = 0.8421679735183716\n",
      "Training iter #14307000:   Batch Loss = 8.103953, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.377950668334961, Accuracy = 0.8427635431289673\n",
      "Training iter #14310000:   Batch Loss = 7.950982, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.37773609161377, Accuracy = 0.8427635431289673\n",
      "Training iter #14313000:   Batch Loss = 8.128428, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.37745475769043, Accuracy = 0.8424657583236694\n",
      "Training iter #14316000:   Batch Loss = 8.202991, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.377222061157227, Accuracy = 0.8421679735183716\n",
      "Training iter #14319000:   Batch Loss = 8.217001, Accuracy = 0.8700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.377391815185547, Accuracy = 0.8424657583236694\n",
      "Training iter #14322000:   Batch Loss = 7.906126, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.37697982788086, Accuracy = 0.8427635431289673\n",
      "Training iter #14325000:   Batch Loss = 8.011243, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.376084327697754, Accuracy = 0.8427635431289673\n",
      "Training iter #14328000:   Batch Loss = 8.025600, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.374844551086426, Accuracy = 0.8424657583236694\n",
      "Training iter #14331000:   Batch Loss = 8.255059, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.37435245513916, Accuracy = 0.8421679735183716\n",
      "Training iter #14334000:   Batch Loss = 8.079125, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.37429141998291, Accuracy = 0.8421679735183716\n",
      "Training iter #14337000:   Batch Loss = 7.943553, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.374954223632812, Accuracy = 0.8415723443031311\n",
      "Training iter #14340000:   Batch Loss = 8.128686, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.375049591064453, Accuracy = 0.8415723443031311\n",
      "Training iter #14343000:   Batch Loss = 8.217010, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.375040054321289, Accuracy = 0.8415723443031311\n",
      "Training iter #14346000:   Batch Loss = 8.194138, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.374649047851562, Accuracy = 0.8415723443031311\n",
      "Training iter #14349000:   Batch Loss = 7.904490, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.374034881591797, Accuracy = 0.8415723443031311\n",
      "Training iter #14352000:   Batch Loss = 8.025775, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.373339653015137, Accuracy = 0.8418701887130737\n",
      "Training iter #14355000:   Batch Loss = 8.055819, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.372709274291992, Accuracy = 0.8418701887130737\n",
      "Training iter #14358000:   Batch Loss = 8.240993, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.372254371643066, Accuracy = 0.8421679735183716\n",
      "Training iter #14361000:   Batch Loss = 8.060756, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.371933937072754, Accuracy = 0.8421679735183716\n",
      "Training iter #14364000:   Batch Loss = 7.931414, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.371527671813965, Accuracy = 0.8424657583236694\n",
      "Training iter #14367000:   Batch Loss = 8.106964, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.371281623840332, Accuracy = 0.8427635431289673\n",
      "Training iter #14370000:   Batch Loss = 8.206745, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.371158599853516, Accuracy = 0.8427635431289673\n",
      "Training iter #14373000:   Batch Loss = 8.144522, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.371337890625, Accuracy = 0.8427635431289673\n",
      "Training iter #14376000:   Batch Loss = 7.912266, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.371159553527832, Accuracy = 0.8427635431289673\n",
      "Training iter #14379000:   Batch Loss = 8.029707, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.37066650390625, Accuracy = 0.8430613279342651\n",
      "Training iter #14382000:   Batch Loss = 8.076997, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.370000839233398, Accuracy = 0.8430613279342651\n",
      "Training iter #14385000:   Batch Loss = 8.270632, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.369441032409668, Accuracy = 0.843359112739563\n",
      "Training iter #14388000:   Batch Loss = 8.037189, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.369040489196777, Accuracy = 0.843359112739563\n",
      "Training iter #14391000:   Batch Loss = 7.930335, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.369061470031738, Accuracy = 0.8430613279342651\n",
      "Training iter #14394000:   Batch Loss = 8.115051, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.368766784667969, Accuracy = 0.843359112739563\n",
      "Training iter #14397000:   Batch Loss = 8.204917, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.368484497070312, Accuracy = 0.8436569571495056\n",
      "Training iter #14400000:   Batch Loss = 8.154970, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.368289947509766, Accuracy = 0.8439547419548035\n",
      "Training iter #14403000:   Batch Loss = 7.883093, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.368165969848633, Accuracy = 0.8439547419548035\n",
      "Training iter #14406000:   Batch Loss = 8.035431, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.36783504486084, Accuracy = 0.8442525267601013\n",
      "Training iter #14409000:   Batch Loss = 8.052800, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.367631912231445, Accuracy = 0.8439547419548035\n",
      "Training iter #14412000:   Batch Loss = 8.287800, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.367259979248047, Accuracy = 0.8442525267601013\n",
      "Training iter #14415000:   Batch Loss = 8.023821, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.36677074432373, Accuracy = 0.8442525267601013\n",
      "Training iter #14418000:   Batch Loss = 7.923810, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.366853713989258, Accuracy = 0.8442525267601013\n",
      "Training iter #14421000:   Batch Loss = 8.141647, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.367019653320312, Accuracy = 0.8439547419548035\n",
      "Training iter #14424000:   Batch Loss = 8.180420, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.36746597290039, Accuracy = 0.8442525267601013\n",
      "Training iter #14427000:   Batch Loss = 8.132923, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.367351531982422, Accuracy = 0.8439547419548035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #14430000:   Batch Loss = 7.889328, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.36660385131836, Accuracy = 0.8442525267601013\n",
      "Training iter #14433000:   Batch Loss = 8.043097, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.365659713745117, Accuracy = 0.844848096370697\n",
      "Training iter #14436000:   Batch Loss = 8.076199, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.365524291992188, Accuracy = 0.8451459407806396\n",
      "Training iter #14439000:   Batch Loss = 8.265976, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.365191459655762, Accuracy = 0.844848096370697\n",
      "Training iter #14442000:   Batch Loss = 7.993540, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.364474296569824, Accuracy = 0.844848096370697\n",
      "Training iter #14445000:   Batch Loss = 7.924478, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.36406135559082, Accuracy = 0.844848096370697\n",
      "Training iter #14448000:   Batch Loss = 8.134539, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.363872528076172, Accuracy = 0.8451459407806396\n",
      "Training iter #14451000:   Batch Loss = 8.161859, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.364038467407227, Accuracy = 0.8451459407806396\n",
      "Training iter #14454000:   Batch Loss = 8.161222, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.364185333251953, Accuracy = 0.8454437255859375\n",
      "Training iter #14457000:   Batch Loss = 7.894271, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.364189147949219, Accuracy = 0.844848096370697\n",
      "Training iter #14460000:   Batch Loss = 8.039755, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.363242149353027, Accuracy = 0.844848096370697\n",
      "Training iter #14463000:   Batch Loss = 8.084561, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.362116813659668, Accuracy = 0.8457415103912354\n",
      "Training iter #14466000:   Batch Loss = 8.304376, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.361528396606445, Accuracy = 0.8460392951965332\n",
      "Training iter #14469000:   Batch Loss = 7.951505, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.361014366149902, Accuracy = 0.8460392951965332\n",
      "Training iter #14472000:   Batch Loss = 7.914752, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.360661506652832, Accuracy = 0.8460392951965332\n",
      "Training iter #14475000:   Batch Loss = 8.147129, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.360560417175293, Accuracy = 0.8460392951965332\n",
      "Training iter #14478000:   Batch Loss = 8.159770, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.361071586608887, Accuracy = 0.8460392951965332\n",
      "Training iter #14481000:   Batch Loss = 8.144108, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.361145973205566, Accuracy = 0.8466349244117737\n",
      "Training iter #14484000:   Batch Loss = 7.912035, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.361136436462402, Accuracy = 0.8454437255859375\n",
      "Training iter #14487000:   Batch Loss = 8.033692, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.360355377197266, Accuracy = 0.8454437255859375\n",
      "Training iter #14490000:   Batch Loss = 8.094208, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.359830856323242, Accuracy = 0.8451459407806396\n",
      "Training iter #14493000:   Batch Loss = 8.256055, Accuracy = 0.859333336353302\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.359292984008789, Accuracy = 0.8454437255859375\n",
      "Training iter #14496000:   Batch Loss = 7.922020, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.358977317810059, Accuracy = 0.8454437255859375\n",
      "Training iter #14499000:   Batch Loss = 7.898855, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.35880184173584, Accuracy = 0.8454437255859375\n",
      "Training iter #14502000:   Batch Loss = 8.142395, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.358588218688965, Accuracy = 0.8454437255859375\n",
      "Training iter #14505000:   Batch Loss = 8.146641, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.358590126037598, Accuracy = 0.8454437255859375\n",
      "Training iter #14508000:   Batch Loss = 8.133769, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.358619689941406, Accuracy = 0.8457415103912354\n",
      "Training iter #14511000:   Batch Loss = 7.925782, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.359192848205566, Accuracy = 0.8451459407806396\n",
      "Training iter #14514000:   Batch Loss = 8.030927, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.359061241149902, Accuracy = 0.8454437255859375\n",
      "Training iter #14517000:   Batch Loss = 8.135766, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.358695983886719, Accuracy = 0.8457415103912354\n",
      "Training iter #14520000:   Batch Loss = 8.244411, Accuracy = 0.8673333525657654\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.35819149017334, Accuracy = 0.8466349244117737\n",
      "Training iter #14523000:   Batch Loss = 7.895387, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.357828140258789, Accuracy = 0.8457415103912354\n",
      "Training iter #14526000:   Batch Loss = 7.925663, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.357488632202148, Accuracy = 0.8454437255859375\n",
      "Training iter #14529000:   Batch Loss = 8.119190, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.356561660766602, Accuracy = 0.8457415103912354\n",
      "Training iter #14532000:   Batch Loss = 8.164843, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.356223106384277, Accuracy = 0.8466349244117737\n",
      "Training iter #14535000:   Batch Loss = 8.154390, Accuracy = 0.8933333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.356061935424805, Accuracy = 0.846337080001831\n",
      "Training iter #14538000:   Batch Loss = 7.945584, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.35602855682373, Accuracy = 0.8457415103912354\n",
      "Training iter #14541000:   Batch Loss = 8.036325, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.355780601501465, Accuracy = 0.8454437255859375\n",
      "Training iter #14544000:   Batch Loss = 8.133424, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.355581283569336, Accuracy = 0.8460392951965332\n",
      "Training iter #14547000:   Batch Loss = 8.199291, Accuracy = 0.875333309173584\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.355237007141113, Accuracy = 0.8466349244117737\n",
      "Training iter #14550000:   Batch Loss = 7.853907, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.354673385620117, Accuracy = 0.8466349244117737\n",
      "Training iter #14553000:   Batch Loss = 7.959363, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.354118347167969, Accuracy = 0.8466349244117737\n",
      "Training iter #14556000:   Batch Loss = 8.062933, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.353611946105957, Accuracy = 0.8466349244117737\n",
      "Training iter #14559000:   Batch Loss = 8.173667, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.353326797485352, Accuracy = 0.8466349244117737\n",
      "Training iter #14562000:   Batch Loss = 8.169498, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.353189468383789, Accuracy = 0.8466349244117737\n",
      "Training iter #14565000:   Batch Loss = 7.953160, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.353266716003418, Accuracy = 0.846337080001831\n",
      "Training iter #14568000:   Batch Loss = 8.024465, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.353180885314941, Accuracy = 0.8454437255859375\n",
      "Training iter #14571000:   Batch Loss = 8.141212, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.352898597717285, Accuracy = 0.846337080001831\n",
      "Training iter #14574000:   Batch Loss = 8.213243, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.352431297302246, Accuracy = 0.8469327092170715\n",
      "Training iter #14577000:   Batch Loss = 7.854319, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.352161407470703, Accuracy = 0.8469327092170715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #14580000:   Batch Loss = 7.973311, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.351444244384766, Accuracy = 0.8466349244117737\n",
      "Training iter #14583000:   Batch Loss = 8.065052, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.350998878479004, Accuracy = 0.8466349244117737\n",
      "Training iter #14586000:   Batch Loss = 8.182468, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.350961685180664, Accuracy = 0.8469327092170715\n",
      "Training iter #14589000:   Batch Loss = 8.103975, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.351067543029785, Accuracy = 0.8472304940223694\n",
      "Training iter #14592000:   Batch Loss = 7.931937, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.350687980651855, Accuracy = 0.846337080001831\n",
      "Training iter #14595000:   Batch Loss = 8.076821, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.350489616394043, Accuracy = 0.846337080001831\n",
      "Training iter #14598000:   Batch Loss = 8.161332, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.35024356842041, Accuracy = 0.8460392951965332\n",
      "Training iter #14601000:   Batch Loss = 8.177496, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.3499174118042, Accuracy = 0.8454437255859375\n",
      "Training iter #14604000:   Batch Loss = 7.859040, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.350064277648926, Accuracy = 0.8454437255859375\n",
      "Training iter #14607000:   Batch Loss = 7.971389, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.34959602355957, Accuracy = 0.8451459407806396\n",
      "Training iter #14610000:   Batch Loss = 8.016036, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.349224090576172, Accuracy = 0.8454437255859375\n",
      "Training iter #14613000:   Batch Loss = 8.177997, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.349210739135742, Accuracy = 0.8457415103912354\n",
      "Training iter #14616000:   Batch Loss = 8.085186, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.349221229553223, Accuracy = 0.8457415103912354\n",
      "Training iter #14619000:   Batch Loss = 7.938965, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.349212646484375, Accuracy = 0.8457415103912354\n",
      "Training iter #14622000:   Batch Loss = 8.096310, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.349093437194824, Accuracy = 0.8457415103912354\n",
      "Training iter #14625000:   Batch Loss = 8.173976, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.348462104797363, Accuracy = 0.8457415103912354\n",
      "Training iter #14628000:   Batch Loss = 8.185987, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.348105430603027, Accuracy = 0.8454437255859375\n",
      "Training iter #14631000:   Batch Loss = 7.869768, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.347415924072266, Accuracy = 0.8454437255859375\n",
      "Training iter #14634000:   Batch Loss = 7.966538, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.346659660339355, Accuracy = 0.8454437255859375\n",
      "Training iter #14637000:   Batch Loss = 8.018430, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.345895767211914, Accuracy = 0.8454437255859375\n",
      "Training iter #14640000:   Batch Loss = 8.189259, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.345250129699707, Accuracy = 0.8454437255859375\n",
      "Training iter #14643000:   Batch Loss = 8.060551, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.344891548156738, Accuracy = 0.846337080001831\n",
      "Training iter #14646000:   Batch Loss = 7.913155, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.344922065734863, Accuracy = 0.846337080001831\n",
      "Training iter #14649000:   Batch Loss = 8.099143, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.344813346862793, Accuracy = 0.8466349244117737\n",
      "Training iter #14652000:   Batch Loss = 8.164158, Accuracy = 0.8966666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.344900131225586, Accuracy = 0.846337080001831\n",
      "Training iter #14655000:   Batch Loss = 8.163692, Accuracy = 0.8853333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.344813346862793, Accuracy = 0.8469327092170715\n",
      "Training iter #14658000:   Batch Loss = 7.877729, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.345293045043945, Accuracy = 0.8469327092170715\n",
      "Training iter #14661000:   Batch Loss = 7.974452, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.344898223876953, Accuracy = 0.8469327092170715\n",
      "Training iter #14664000:   Batch Loss = 7.994152, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.344149589538574, Accuracy = 0.8472304940223694\n",
      "Training iter #14667000:   Batch Loss = 8.210755, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.343578338623047, Accuracy = 0.8472304940223694\n",
      "Training iter #14670000:   Batch Loss = 8.038549, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.343120574951172, Accuracy = 0.8472304940223694\n",
      "Training iter #14673000:   Batch Loss = 7.916116, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.343070030212402, Accuracy = 0.8469327092170715\n",
      "Training iter #14676000:   Batch Loss = 8.071801, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.343172073364258, Accuracy = 0.8469327092170715\n",
      "Training iter #14679000:   Batch Loss = 8.170192, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.343766212463379, Accuracy = 0.846337080001831\n",
      "Training iter #14682000:   Batch Loss = 8.138727, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.343362808227539, Accuracy = 0.8460392951965332\n",
      "Training iter #14685000:   Batch Loss = 7.881323, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.343332290649414, Accuracy = 0.8454437255859375\n",
      "Training iter #14688000:   Batch Loss = 7.996829, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.342708587646484, Accuracy = 0.8454437255859375\n",
      "Training iter #14691000:   Batch Loss = 8.038415, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.341706275939941, Accuracy = 0.8451459407806396\n",
      "Training iter #14694000:   Batch Loss = 8.225638, Accuracy = 0.8613333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.340753555297852, Accuracy = 0.8454437255859375\n",
      "Training iter #14697000:   Batch Loss = 8.023535, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.339812278747559, Accuracy = 0.8460392951965332\n",
      "Training iter #14700000:   Batch Loss = 7.896120, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.339162826538086, Accuracy = 0.8460392951965332\n",
      "Training iter #14703000:   Batch Loss = 8.073008, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.33889102935791, Accuracy = 0.8466349244117737\n",
      "Training iter #14706000:   Batch Loss = 8.163731, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.33896255493164, Accuracy = 0.8472304940223694\n",
      "Training iter #14709000:   Batch Loss = 8.099543, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.339160919189453, Accuracy = 0.8475282788276672\n",
      "Training iter #14712000:   Batch Loss = 7.868407, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.339111328125, Accuracy = 0.8469327092170715\n",
      "Training iter #14715000:   Batch Loss = 7.998027, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.338375091552734, Accuracy = 0.8469327092170715\n",
      "Training iter #14718000:   Batch Loss = 8.038806, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.337928771972656, Accuracy = 0.8469327092170715\n",
      "Training iter #14721000:   Batch Loss = 8.227256, Accuracy = 0.8640000224113464\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.337617874145508, Accuracy = 0.8472304940223694\n",
      "Training iter #14724000:   Batch Loss = 7.998041, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.337499618530273, Accuracy = 0.8478260636329651\n",
      "Training iter #14727000:   Batch Loss = 7.903146, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.3377046585083, Accuracy = 0.8472304940223694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #14730000:   Batch Loss = 8.085606, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.337666511535645, Accuracy = 0.8469327092170715\n",
      "Training iter #14733000:   Batch Loss = 8.155372, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.337443351745605, Accuracy = 0.8469327092170715\n",
      "Training iter #14736000:   Batch Loss = 8.104456, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.336954116821289, Accuracy = 0.8466349244117737\n",
      "Training iter #14739000:   Batch Loss = 7.854199, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.3365478515625, Accuracy = 0.8466349244117737\n",
      "Training iter #14742000:   Batch Loss = 8.002048, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.335765838623047, Accuracy = 0.8466349244117737\n",
      "Training iter #14745000:   Batch Loss = 8.026597, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.33549976348877, Accuracy = 0.8469327092170715\n",
      "Training iter #14748000:   Batch Loss = 8.244939, Accuracy = 0.862666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.33518123626709, Accuracy = 0.8472304940223694\n",
      "Training iter #14751000:   Batch Loss = 7.988521, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.334981918334961, Accuracy = 0.8469327092170715\n",
      "Training iter #14754000:   Batch Loss = 7.891722, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.33519458770752, Accuracy = 0.8466349244117737\n",
      "Training iter #14757000:   Batch Loss = 8.090208, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.335312843322754, Accuracy = 0.846337080001831\n",
      "Training iter #14760000:   Batch Loss = 8.138309, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.33527660369873, Accuracy = 0.846337080001831\n",
      "Training iter #14763000:   Batch Loss = 8.121063, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.335102081298828, Accuracy = 0.8466349244117737\n",
      "Training iter #14766000:   Batch Loss = 7.862822, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.334989547729492, Accuracy = 0.8466349244117737\n",
      "Training iter #14769000:   Batch Loss = 8.007569, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.334251403808594, Accuracy = 0.8469327092170715\n",
      "Training iter #14772000:   Batch Loss = 8.051104, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.333745002746582, Accuracy = 0.8469327092170715\n",
      "Training iter #14775000:   Batch Loss = 8.228003, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.333157539367676, Accuracy = 0.8469327092170715\n",
      "Training iter #14778000:   Batch Loss = 7.949588, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.332425117492676, Accuracy = 0.8475282788276672\n",
      "Training iter #14781000:   Batch Loss = 7.892788, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.331972122192383, Accuracy = 0.8481239080429077\n",
      "Training iter #14784000:   Batch Loss = 8.092957, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.331785202026367, Accuracy = 0.8487194776535034\n",
      "Training iter #14787000:   Batch Loss = 8.130964, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.331727027893066, Accuracy = 0.8493150472640991\n",
      "Training iter #14790000:   Batch Loss = 8.102414, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.331796646118164, Accuracy = 0.8490172624588013\n",
      "Training iter #14793000:   Batch Loss = 7.876636, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.331666946411133, Accuracy = 0.8490172624588013\n",
      "Training iter #14796000:   Batch Loss = 8.015466, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.33089828491211, Accuracy = 0.8493150472640991\n",
      "Training iter #14799000:   Batch Loss = 8.048016, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.330349922180176, Accuracy = 0.8493150472640991\n",
      "Training iter #14802000:   Batch Loss = 8.250539, Accuracy = 0.8573333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.329867362976074, Accuracy = 0.8493150472640991\n",
      "Training iter #14805000:   Batch Loss = 7.906907, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.329776763916016, Accuracy = 0.8493150472640991\n",
      "Training iter #14808000:   Batch Loss = 7.890999, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32983112335205, Accuracy = 0.8493150472640991\n",
      "Training iter #14811000:   Batch Loss = 8.116417, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.329803466796875, Accuracy = 0.8487194776535034\n",
      "Training iter #14814000:   Batch Loss = 8.114665, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.329748153686523, Accuracy = 0.8484216928482056\n",
      "Training iter #14817000:   Batch Loss = 8.106049, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.329690933227539, Accuracy = 0.8481239080429077\n",
      "Training iter #14820000:   Batch Loss = 7.897736, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.329330444335938, Accuracy = 0.8484216928482056\n",
      "Training iter #14823000:   Batch Loss = 8.012400, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.328603744506836, Accuracy = 0.8478260636329651\n",
      "Training iter #14826000:   Batch Loss = 8.080506, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.327803611755371, Accuracy = 0.8484216928482056\n",
      "Training iter #14829000:   Batch Loss = 8.221369, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32680892944336, Accuracy = 0.8484216928482056\n",
      "Training iter #14832000:   Batch Loss = 7.886562, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.326272964477539, Accuracy = 0.8481239080429077\n",
      "Training iter #14835000:   Batch Loss = 7.889138, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325996398925781, Accuracy = 0.8487194776535034\n",
      "Training iter #14838000:   Batch Loss = 8.096602, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32565689086914, Accuracy = 0.8490172624588013\n",
      "Training iter #14841000:   Batch Loss = 8.110844, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325594902038574, Accuracy = 0.8484216928482056\n",
      "Training iter #14844000:   Batch Loss = 8.110775, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32577133178711, Accuracy = 0.8484216928482056\n",
      "Training iter #14847000:   Batch Loss = 7.905858, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325996398925781, Accuracy = 0.8481239080429077\n",
      "Training iter #14850000:   Batch Loss = 8.013029, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325559616088867, Accuracy = 0.8490172624588013\n",
      "Training iter #14853000:   Batch Loss = 8.092031, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325292587280273, Accuracy = 0.8499106764793396\n",
      "Training iter #14856000:   Batch Loss = 8.187748, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32517147064209, Accuracy = 0.8505062460899353\n",
      "Training iter #14859000:   Batch Loss = 7.863134, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325540542602539, Accuracy = 0.8496128916740417\n",
      "Training iter #14862000:   Batch Loss = 7.896579, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.326057434082031, Accuracy = 0.8493150472640991\n",
      "Training iter #14865000:   Batch Loss = 8.058349, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32602310180664, Accuracy = 0.8487194776535034\n",
      "Training iter #14868000:   Batch Loss = 8.144920, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325613021850586, Accuracy = 0.8487194776535034\n",
      "Training iter #14871000:   Batch Loss = 8.143766, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.325215339660645, Accuracy = 0.8484216928482056\n",
      "Training iter #14874000:   Batch Loss = 7.920841, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.324204444885254, Accuracy = 0.8484216928482056\n",
      "Training iter #14877000:   Batch Loss = 8.002200, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.323187828063965, Accuracy = 0.8487194776535034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #14880000:   Batch Loss = 8.100828, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.322720527648926, Accuracy = 0.8484216928482056\n",
      "Training iter #14883000:   Batch Loss = 8.151864, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.322334289550781, Accuracy = 0.8484216928482056\n",
      "Training iter #14886000:   Batch Loss = 7.829648, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.322022438049316, Accuracy = 0.8481239080429077\n",
      "Training iter #14889000:   Batch Loss = 7.937984, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.321817398071289, Accuracy = 0.8484216928482056\n",
      "Training iter #14892000:   Batch Loss = 8.038323, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32129192352295, Accuracy = 0.8484216928482056\n",
      "Training iter #14895000:   Batch Loss = 8.125168, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32055950164795, Accuracy = 0.8484216928482056\n",
      "Training iter #14898000:   Batch Loss = 8.109453, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.319997787475586, Accuracy = 0.8490172624588013\n",
      "Training iter #14901000:   Batch Loss = 7.921147, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.319744110107422, Accuracy = 0.8490172624588013\n",
      "Training iter #14904000:   Batch Loss = 7.998405, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.319069862365723, Accuracy = 0.8490172624588013\n",
      "Training iter #14907000:   Batch Loss = 8.108099, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.318918228149414, Accuracy = 0.8490172624588013\n",
      "Training iter #14910000:   Batch Loss = 8.163534, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.319409370422363, Accuracy = 0.8493150472640991\n",
      "Training iter #14913000:   Batch Loss = 7.843821, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.31993293762207, Accuracy = 0.8496128916740417\n",
      "Training iter #14916000:   Batch Loss = 7.948107, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.320457458496094, Accuracy = 0.8490172624588013\n",
      "Training iter #14919000:   Batch Loss = 7.996150, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32026195526123, Accuracy = 0.8490172624588013\n",
      "Training iter #14922000:   Batch Loss = 8.148860, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32029914855957, Accuracy = 0.8487194776535034\n",
      "Training iter #14925000:   Batch Loss = 8.068713, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.320353507995605, Accuracy = 0.8487194776535034\n",
      "Training iter #14928000:   Batch Loss = 7.907092, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.320429801940918, Accuracy = 0.8484216928482056\n",
      "Training iter #14931000:   Batch Loss = 8.065998, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.32022762298584, Accuracy = 0.8481239080429077\n",
      "Training iter #14934000:   Batch Loss = 8.118961, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.319844245910645, Accuracy = 0.8484216928482056\n",
      "Training iter #14937000:   Batch Loss = 8.149908, Accuracy = 0.8846666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.319242477416992, Accuracy = 0.8481239080429077\n",
      "Training iter #14940000:   Batch Loss = 7.826807, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.31793212890625, Accuracy = 0.8490172624588013\n",
      "Training iter #14943000:   Batch Loss = 7.935985, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.317588806152344, Accuracy = 0.8490172624588013\n",
      "Training iter #14946000:   Batch Loss = 7.979960, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.317273139953613, Accuracy = 0.8490172624588013\n",
      "Training iter #14949000:   Batch Loss = 8.142296, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.317001342773438, Accuracy = 0.8487194776535034\n",
      "Training iter #14952000:   Batch Loss = 8.040261, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.316903114318848, Accuracy = 0.8487194776535034\n",
      "Training iter #14955000:   Batch Loss = 7.894316, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.316856384277344, Accuracy = 0.8487194776535034\n",
      "Training iter #14958000:   Batch Loss = 8.051608, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.316486358642578, Accuracy = 0.8490172624588013\n",
      "Training iter #14961000:   Batch Loss = 8.124515, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.316147804260254, Accuracy = 0.8490172624588013\n",
      "Training iter #14964000:   Batch Loss = 8.136673, Accuracy = 0.8853333592414856\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.315751075744629, Accuracy = 0.8487194776535034\n",
      "Training iter #14967000:   Batch Loss = 7.852033, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.315354347229004, Accuracy = 0.8490172624588013\n",
      "Training iter #14970000:   Batch Loss = 7.945392, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.314924240112305, Accuracy = 0.8493150472640991\n",
      "Training iter #14973000:   Batch Loss = 7.964339, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.314444541931152, Accuracy = 0.8499106764793396\n",
      "Training iter #14976000:   Batch Loss = 8.160364, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.313935279846191, Accuracy = 0.8499106764793396\n",
      "Training iter #14979000:   Batch Loss = 8.015079, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.313667297363281, Accuracy = 0.8499106764793396\n",
      "Training iter #14982000:   Batch Loss = 7.891877, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.31332778930664, Accuracy = 0.8502084612846375\n",
      "Training iter #14985000:   Batch Loss = 8.059436, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.312700271606445, Accuracy = 0.8499106764793396\n",
      "Training iter #14988000:   Batch Loss = 8.127093, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.312050819396973, Accuracy = 0.8502084612846375\n",
      "Training iter #14991000:   Batch Loss = 8.118242, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.31140422821045, Accuracy = 0.8502084612846375\n",
      "Training iter #14994000:   Batch Loss = 7.850703, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.311105728149414, Accuracy = 0.8502084612846375\n",
      "Training iter #14997000:   Batch Loss = 7.968585, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.31081485748291, Accuracy = 0.8505062460899353\n",
      "Training iter #15000000:   Batch Loss = 7.989572, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.31059455871582, Accuracy = 0.8505062460899353\n",
      "Training iter #15003000:   Batch Loss = 8.177448, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.31062126159668, Accuracy = 0.8505062460899353\n",
      "Training iter #15006000:   Batch Loss = 8.000151, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.3104829788208, Accuracy = 0.8505062460899353\n",
      "Training iter #15009000:   Batch Loss = 7.891670, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.310070037841797, Accuracy = 0.8505062460899353\n",
      "Training iter #15012000:   Batch Loss = 8.030494, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.309596061706543, Accuracy = 0.8508040308952332\n",
      "Training iter #15015000:   Batch Loss = 8.126114, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.309098243713379, Accuracy = 0.8508040308952332\n",
      "Training iter #15018000:   Batch Loss = 8.082387, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.308598518371582, Accuracy = 0.8505062460899353\n",
      "Training iter #15021000:   Batch Loss = 7.858653, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.307639122009277, Accuracy = 0.8505062460899353\n",
      "Training iter #15024000:   Batch Loss = 7.968934, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.30682373046875, Accuracy = 0.8508040308952332\n",
      "Training iter #15027000:   Batch Loss = 7.999701, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.307511329650879, Accuracy = 0.8508040308952332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15030000:   Batch Loss = 8.197562, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.308589935302734, Accuracy = 0.8511018753051758\n",
      "Training iter #15033000:   Batch Loss = 7.997461, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.308547019958496, Accuracy = 0.8505062460899353\n",
      "Training iter #15036000:   Batch Loss = 7.877423, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.306513786315918, Accuracy = 0.8502084612846375\n",
      "Training iter #15039000:   Batch Loss = 8.054645, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.3060941696167, Accuracy = 0.8508040308952332\n",
      "Training iter #15042000:   Batch Loss = 8.125029, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.306072235107422, Accuracy = 0.8499106764793396\n",
      "Training iter #15045000:   Batch Loss = 8.085554, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.30604076385498, Accuracy = 0.8496128916740417\n",
      "Training iter #15048000:   Batch Loss = 7.832342, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.30625057220459, Accuracy = 0.8490172624588013\n",
      "Training iter #15051000:   Batch Loss = 7.959182, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.306526184082031, Accuracy = 0.8493150472640991\n",
      "Training iter #15054000:   Batch Loss = 8.013286, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.307991027832031, Accuracy = 0.8493150472640991\n",
      "Training iter #15057000:   Batch Loss = 8.203337, Accuracy = 0.8706666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.308043479919434, Accuracy = 0.8496128916740417\n",
      "Training iter #15060000:   Batch Loss = 7.965670, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.307878494262695, Accuracy = 0.8496128916740417\n",
      "Training iter #15063000:   Batch Loss = 7.872294, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.307638168334961, Accuracy = 0.8496128916740417\n",
      "Training iter #15066000:   Batch Loss = 8.079941, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.30721378326416, Accuracy = 0.8493150472640991\n",
      "Training iter #15069000:   Batch Loss = 8.109258, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.307080268859863, Accuracy = 0.8490172624588013\n",
      "Training iter #15072000:   Batch Loss = 8.063858, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.306327819824219, Accuracy = 0.8493150472640991\n",
      "Training iter #15075000:   Batch Loss = 7.836955, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.304607391357422, Accuracy = 0.8496128916740417\n",
      "Training iter #15078000:   Batch Loss = 7.967870, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.304336547851562, Accuracy = 0.8502084612846375\n",
      "Training iter #15081000:   Batch Loss = 8.009642, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.304081916809082, Accuracy = 0.8502084612846375\n",
      "Training iter #15084000:   Batch Loss = 8.190521, Accuracy = 0.8740000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303872108459473, Accuracy = 0.8505062460899353\n",
      "Training iter #15087000:   Batch Loss = 7.938810, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303302764892578, Accuracy = 0.8499106764793396\n",
      "Training iter #15090000:   Batch Loss = 7.867707, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303396224975586, Accuracy = 0.8505062460899353\n",
      "Training iter #15093000:   Batch Loss = 8.071164, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303574562072754, Accuracy = 0.8505062460899353\n",
      "Training iter #15096000:   Batch Loss = 8.096950, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303634643554688, Accuracy = 0.8505062460899353\n",
      "Training iter #15099000:   Batch Loss = 8.093262, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303325653076172, Accuracy = 0.8505062460899353\n",
      "Training iter #15102000:   Batch Loss = 7.843863, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303224563598633, Accuracy = 0.8505062460899353\n",
      "Training iter #15105000:   Batch Loss = 7.970602, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303043365478516, Accuracy = 0.8508040308952332\n",
      "Training iter #15108000:   Batch Loss = 8.009746, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.303576469421387, Accuracy = 0.8502084612846375\n",
      "Training iter #15111000:   Batch Loss = 8.220301, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.30269718170166, Accuracy = 0.8502084612846375\n",
      "Training iter #15114000:   Batch Loss = 7.908503, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.300518035888672, Accuracy = 0.8508040308952332\n",
      "Training iter #15117000:   Batch Loss = 7.856944, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.300455093383789, Accuracy = 0.8502084612846375\n",
      "Training iter #15120000:   Batch Loss = 8.070563, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.29985523223877, Accuracy = 0.8505062460899353\n",
      "Training iter #15123000:   Batch Loss = 8.087898, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.299050331115723, Accuracy = 0.8505062460899353\n",
      "Training iter #15126000:   Batch Loss = 8.075824, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298284530639648, Accuracy = 0.8516974449157715\n",
      "Training iter #15129000:   Batch Loss = 7.852587, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2982759475708, Accuracy = 0.8513996601104736\n",
      "Training iter #15132000:   Batch Loss = 7.968091, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.299348831176758, Accuracy = 0.8511018753051758\n",
      "Training iter #15135000:   Batch Loss = 8.015212, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.299992561340332, Accuracy = 0.8508040308952332\n",
      "Training iter #15138000:   Batch Loss = 8.200315, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.299894332885742, Accuracy = 0.8508040308952332\n",
      "Training iter #15141000:   Batch Loss = 7.869923, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.299657821655273, Accuracy = 0.8505062460899353\n",
      "Training iter #15144000:   Batch Loss = 7.857370, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.299297332763672, Accuracy = 0.8505062460899353\n",
      "Training iter #15147000:   Batch Loss = 8.078639, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298772811889648, Accuracy = 0.8505062460899353\n",
      "Training iter #15150000:   Batch Loss = 8.072057, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298832893371582, Accuracy = 0.8508040308952332\n",
      "Training iter #15153000:   Batch Loss = 8.068083, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.29881763458252, Accuracy = 0.8511018753051758\n",
      "Training iter #15156000:   Batch Loss = 7.873701, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298775672912598, Accuracy = 0.8508040308952332\n",
      "Training iter #15159000:   Batch Loss = 7.963753, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298593521118164, Accuracy = 0.8505062460899353\n",
      "Training iter #15162000:   Batch Loss = 8.053468, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298843383789062, Accuracy = 0.8508040308952332\n",
      "Training iter #15165000:   Batch Loss = 8.174722, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.29879093170166, Accuracy = 0.8511018753051758\n",
      "Training iter #15168000:   Batch Loss = 7.848408, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298616409301758, Accuracy = 0.8511018753051758\n",
      "Training iter #15171000:   Batch Loss = 7.853805, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.298179626464844, Accuracy = 0.8511018753051758\n",
      "Training iter #15174000:   Batch Loss = 8.060564, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.297319412231445, Accuracy = 0.8508040308952332\n",
      "Training iter #15177000:   Batch Loss = 8.084823, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.296448707580566, Accuracy = 0.8513996601104736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15180000:   Batch Loss = 8.084296, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.29581356048584, Accuracy = 0.8516974449157715\n",
      "Training iter #15183000:   Batch Loss = 7.893567, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.295280456542969, Accuracy = 0.8513996601104736\n",
      "Training iter #15186000:   Batch Loss = 7.969528, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.294892311096191, Accuracy = 0.8508040308952332\n",
      "Training iter #15189000:   Batch Loss = 8.060548, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.29466724395752, Accuracy = 0.8508040308952332\n",
      "Training iter #15192000:   Batch Loss = 8.128691, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.294784545898438, Accuracy = 0.8508040308952332\n",
      "Training iter #15195000:   Batch Loss = 7.816371, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.295053482055664, Accuracy = 0.8508040308952332\n",
      "Training iter #15198000:   Batch Loss = 7.891747, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.295010566711426, Accuracy = 0.8508040308952332\n",
      "Training iter #15201000:   Batch Loss = 8.018056, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.294801712036133, Accuracy = 0.8511018753051758\n",
      "Training iter #15204000:   Batch Loss = 8.105832, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.294540405273438, Accuracy = 0.8519952297210693\n",
      "Training iter #15207000:   Batch Loss = 8.105759, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.29440689086914, Accuracy = 0.8522930145263672\n",
      "Training iter #15210000:   Batch Loss = 7.895824, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.294248580932617, Accuracy = 0.8516974449157715\n",
      "Training iter #15213000:   Batch Loss = 7.970858, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.293353080749512, Accuracy = 0.8516974449157715\n",
      "Training iter #15216000:   Batch Loss = 8.068933, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.292791366577148, Accuracy = 0.8519952297210693\n",
      "Training iter #15219000:   Batch Loss = 8.122680, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.292378425598145, Accuracy = 0.852590799331665\n",
      "Training iter #15222000:   Batch Loss = 7.804436, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.292282104492188, Accuracy = 0.8516974449157715\n",
      "Training iter #15225000:   Batch Loss = 7.906249, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.291986465454102, Accuracy = 0.8516974449157715\n",
      "Training iter #15228000:   Batch Loss = 8.006782, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.291325569152832, Accuracy = 0.8519952297210693\n",
      "Training iter #15231000:   Batch Loss = 8.097693, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.290836334228516, Accuracy = 0.8528886437416077\n",
      "Training iter #15234000:   Batch Loss = 8.046557, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.290660858154297, Accuracy = 0.852590799331665\n",
      "Training iter #15237000:   Batch Loss = 7.889349, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.290732383728027, Accuracy = 0.852590799331665\n",
      "Training iter #15240000:   Batch Loss = 8.011465, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.29051685333252, Accuracy = 0.852590799331665\n",
      "Training iter #15243000:   Batch Loss = 8.081517, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.290318489074707, Accuracy = 0.8528886437416077\n",
      "Training iter #15246000:   Batch Loss = 8.105027, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.290037155151367, Accuracy = 0.8528886437416077\n",
      "Training iter #15249000:   Batch Loss = 7.813868, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.289946556091309, Accuracy = 0.8528886437416077\n",
      "Training iter #15252000:   Batch Loss = 7.902004, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.28961181640625, Accuracy = 0.8528886437416077\n",
      "Training iter #15255000:   Batch Loss = 7.964037, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.289097785949707, Accuracy = 0.8531864285469055\n",
      "Training iter #15258000:   Batch Loss = 8.112978, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.288717269897461, Accuracy = 0.8543776273727417\n",
      "Training iter #15261000:   Batch Loss = 8.023704, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.28858470916748, Accuracy = 0.8537819981575012\n",
      "Training iter #15264000:   Batch Loss = 7.889027, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.28860855102539, Accuracy = 0.8537819981575012\n",
      "Training iter #15267000:   Batch Loss = 8.045671, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.288366317749023, Accuracy = 0.8537819981575012\n",
      "Training iter #15270000:   Batch Loss = 8.097177, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.288055419921875, Accuracy = 0.8528886437416077\n",
      "Training iter #15273000:   Batch Loss = 8.103873, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.287750244140625, Accuracy = 0.8528886437416077\n",
      "Training iter #15276000:   Batch Loss = 7.817123, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.287802696228027, Accuracy = 0.8534842133522034\n",
      "Training iter #15279000:   Batch Loss = 7.906884, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.287614822387695, Accuracy = 0.8537819981575012\n",
      "Training iter #15282000:   Batch Loss = 7.965462, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.286832809448242, Accuracy = 0.8540797829627991\n",
      "Training iter #15285000:   Batch Loss = 8.112145, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.286205291748047, Accuracy = 0.8540797829627991\n",
      "Training iter #15288000:   Batch Loss = 8.005265, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.28597640991211, Accuracy = 0.8543776273727417\n",
      "Training iter #15291000:   Batch Loss = 7.867849, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.285989761352539, Accuracy = 0.8543776273727417\n",
      "Training iter #15294000:   Batch Loss = 8.039631, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.285836219787598, Accuracy = 0.8540797829627991\n",
      "Training iter #15297000:   Batch Loss = 8.087077, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.285717964172363, Accuracy = 0.8540797829627991\n",
      "Training iter #15300000:   Batch Loss = 8.098677, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.285575866699219, Accuracy = 0.8537819981575012\n",
      "Training iter #15303000:   Batch Loss = 7.826588, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.285578727722168, Accuracy = 0.8534842133522034\n",
      "Training iter #15306000:   Batch Loss = 7.912537, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.285118103027344, Accuracy = 0.8534842133522034\n",
      "Training iter #15309000:   Batch Loss = 7.935925, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.284486770629883, Accuracy = 0.8537819981575012\n",
      "Training iter #15312000:   Batch Loss = 8.146677, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.284112930297852, Accuracy = 0.8543776273727417\n",
      "Training iter #15315000:   Batch Loss = 7.981515, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.283987998962402, Accuracy = 0.8546754121780396\n",
      "Training iter #15318000:   Batch Loss = 7.865440, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.283934593200684, Accuracy = 0.8546754121780396\n",
      "Training iter #15321000:   Batch Loss = 8.012615, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.283792495727539, Accuracy = 0.8543776273727417\n",
      "Training iter #15324000:   Batch Loss = 8.091245, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.283636093139648, Accuracy = 0.8540797829627991\n",
      "Training iter #15327000:   Batch Loss = 8.072289, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.283393859863281, Accuracy = 0.8543776273727417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15330000:   Batch Loss = 7.822924, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.28327751159668, Accuracy = 0.8543776273727417\n",
      "Training iter #15333000:   Batch Loss = 7.929596, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.28270435333252, Accuracy = 0.8543776273727417\n",
      "Training iter #15336000:   Batch Loss = 7.973181, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.282135009765625, Accuracy = 0.8546754121780396\n",
      "Training iter #15339000:   Batch Loss = 8.139955, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.282055854797363, Accuracy = 0.8546754121780396\n",
      "Training iter #15342000:   Batch Loss = 7.969630, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.281333923339844, Accuracy = 0.8546754121780396\n",
      "Training iter #15345000:   Batch Loss = 7.846686, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.281256675720215, Accuracy = 0.8546754121780396\n",
      "Training iter #15348000:   Batch Loss = 8.019512, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.281143188476562, Accuracy = 0.8540797829627991\n",
      "Training iter #15351000:   Batch Loss = 8.095641, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.281067848205566, Accuracy = 0.8537819981575012\n",
      "Training iter #15354000:   Batch Loss = 8.023491, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.280900955200195, Accuracy = 0.8540797829627991\n",
      "Training iter #15357000:   Batch Loss = 7.831151, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.280550956726074, Accuracy = 0.8540797829627991\n",
      "Training iter #15360000:   Batch Loss = 7.931970, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.27966594696045, Accuracy = 0.8537819981575012\n",
      "Training iter #15363000:   Batch Loss = 7.978651, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.278660774230957, Accuracy = 0.8546754121780396\n",
      "Training iter #15366000:   Batch Loss = 8.155867, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.278226852416992, Accuracy = 0.8552709817886353\n",
      "Training iter #15369000:   Batch Loss = 7.944625, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.277984619140625, Accuracy = 0.8555687665939331\n",
      "Training iter #15372000:   Batch Loss = 7.849941, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.277735710144043, Accuracy = 0.8555687665939331\n",
      "Training iter #15375000:   Batch Loss = 8.021922, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.277384757995605, Accuracy = 0.8552709817886353\n",
      "Training iter #15378000:   Batch Loss = 8.088489, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.27713680267334, Accuracy = 0.8552709817886353\n",
      "Training iter #15381000:   Batch Loss = 8.036425, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2767915725708, Accuracy = 0.8549731969833374\n",
      "Training iter #15384000:   Batch Loss = 7.808233, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.276576042175293, Accuracy = 0.8549731969833374\n",
      "Training iter #15387000:   Batch Loss = 7.936779, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.276012420654297, Accuracy = 0.8546754121780396\n",
      "Training iter #15390000:   Batch Loss = 7.957245, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275389671325684, Accuracy = 0.8552709817886353\n",
      "Training iter #15393000:   Batch Loss = 8.171895, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275161743164062, Accuracy = 0.8549731969833374\n",
      "Training iter #15396000:   Batch Loss = 7.933495, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275168418884277, Accuracy = 0.8555687665939331\n",
      "Training iter #15399000:   Batch Loss = 7.838839, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275110244750977, Accuracy = 0.8552709817886353\n",
      "Training iter #15402000:   Batch Loss = 8.036194, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275005340576172, Accuracy = 0.8549731969833374\n",
      "Training iter #15405000:   Batch Loss = 8.069414, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275201797485352, Accuracy = 0.8546754121780396\n",
      "Training iter #15408000:   Batch Loss = 8.048105, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275432586669922, Accuracy = 0.8549731969833374\n",
      "Training iter #15411000:   Batch Loss = 7.812403, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.275184631347656, Accuracy = 0.8543776273727417\n",
      "Training iter #15414000:   Batch Loss = 7.941383, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.274524688720703, Accuracy = 0.8540797829627991\n",
      "Training iter #15417000:   Batch Loss = 7.982531, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.273941040039062, Accuracy = 0.8552709817886353\n",
      "Training iter #15420000:   Batch Loss = 8.156426, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.273575782775879, Accuracy = 0.8558666110038757\n",
      "Training iter #15423000:   Batch Loss = 7.896196, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.273262977600098, Accuracy = 0.8555687665939331\n",
      "Training iter #15426000:   Batch Loss = 7.842128, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.273134231567383, Accuracy = 0.8549731969833374\n",
      "Training iter #15429000:   Batch Loss = 8.040044, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.272676467895508, Accuracy = 0.8549731969833374\n",
      "Training iter #15432000:   Batch Loss = 8.052350, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.272400856018066, Accuracy = 0.8549731969833374\n",
      "Training iter #15435000:   Batch Loss = 8.034525, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.272099494934082, Accuracy = 0.8549731969833374\n",
      "Training iter #15438000:   Batch Loss = 7.821685, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.271796226501465, Accuracy = 0.8552709817886353\n",
      "Training iter #15441000:   Batch Loss = 7.951487, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.270896911621094, Accuracy = 0.8555687665939331\n",
      "Training iter #15444000:   Batch Loss = 7.979765, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.270395278930664, Accuracy = 0.8552709817886353\n",
      "Training iter #15447000:   Batch Loss = 8.182030, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.270063400268555, Accuracy = 0.8558666110038757\n",
      "Training iter #15450000:   Batch Loss = 7.861756, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.269729614257812, Accuracy = 0.8561643958091736\n",
      "Training iter #15453000:   Batch Loss = 7.838239, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.268860816955566, Accuracy = 0.8567599654197693\n",
      "Training iter #15456000:   Batch Loss = 8.051807, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.268436431884766, Accuracy = 0.8567599654197693\n",
      "Training iter #15459000:   Batch Loss = 8.050533, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.268569946289062, Accuracy = 0.8570577502250671\n",
      "Training iter #15462000:   Batch Loss = 8.039378, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.268732070922852, Accuracy = 0.8567599654197693\n",
      "Training iter #15465000:   Batch Loss = 7.833784, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.268835067749023, Accuracy = 0.8570577502250671\n",
      "Training iter #15468000:   Batch Loss = 7.940956, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.268414497375488, Accuracy = 0.8570577502250671\n",
      "Training iter #15471000:   Batch Loss = 8.004696, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.268043518066406, Accuracy = 0.8573555946350098\n",
      "Training iter #15474000:   Batch Loss = 8.153258, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.267702102661133, Accuracy = 0.8576533794403076\n",
      "Training iter #15477000:   Batch Loss = 7.835614, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.267492294311523, Accuracy = 0.8576533794403076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15480000:   Batch Loss = 7.832542, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.267324447631836, Accuracy = 0.8576533794403076\n",
      "Training iter #15483000:   Batch Loss = 8.037342, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.266818046569824, Accuracy = 0.8579511642456055\n",
      "Training iter #15486000:   Batch Loss = 8.040041, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.266425132751465, Accuracy = 0.8570577502250671\n",
      "Training iter #15489000:   Batch Loss = 8.032417, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.26601791381836, Accuracy = 0.8564621806144714\n",
      "Training iter #15492000:   Batch Loss = 7.842996, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.265863418579102, Accuracy = 0.8564621806144714\n",
      "Training iter #15495000:   Batch Loss = 7.937521, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.26548957824707, Accuracy = 0.8564621806144714\n",
      "Training iter #15498000:   Batch Loss = 8.026992, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.265081405639648, Accuracy = 0.8573555946350098\n",
      "Training iter #15501000:   Batch Loss = 8.122578, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.264809608459473, Accuracy = 0.8573555946350098\n",
      "Training iter #15504000:   Batch Loss = 7.813212, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.264650344848633, Accuracy = 0.8573555946350098\n",
      "Training iter #15507000:   Batch Loss = 7.841033, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.264381408691406, Accuracy = 0.8567599654197693\n",
      "Training iter #15510000:   Batch Loss = 7.998018, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.264180183410645, Accuracy = 0.8573555946350098\n",
      "Training iter #15513000:   Batch Loss = 8.075691, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.264021873474121, Accuracy = 0.8573555946350098\n",
      "Training iter #15516000:   Batch Loss = 8.056386, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2637939453125, Accuracy = 0.8576533794403076\n",
      "Training iter #15519000:   Batch Loss = 7.865196, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.263609886169434, Accuracy = 0.8570577502250671\n",
      "Training iter #15522000:   Batch Loss = 7.927721, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.263290405273438, Accuracy = 0.8570577502250671\n",
      "Training iter #15525000:   Batch Loss = 8.025661, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.26297378540039, Accuracy = 0.8576533794403076\n",
      "Training iter #15528000:   Batch Loss = 8.078938, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.26278018951416, Accuracy = 0.8570577502250671\n",
      "Training iter #15531000:   Batch Loss = 7.781925, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.262496948242188, Accuracy = 0.8570577502250671\n",
      "Training iter #15534000:   Batch Loss = 7.868657, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2620210647583, Accuracy = 0.8576533794403076\n",
      "Training iter #15537000:   Batch Loss = 7.975098, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.261672019958496, Accuracy = 0.8576533794403076\n",
      "Training iter #15540000:   Batch Loss = 8.062488, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.261434555053711, Accuracy = 0.8579511642456055\n",
      "Training iter #15543000:   Batch Loss = 8.051311, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.261287689208984, Accuracy = 0.8576533794403076\n",
      "Training iter #15546000:   Batch Loss = 7.871213, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.261313438415527, Accuracy = 0.8579511642456055\n",
      "Training iter #15549000:   Batch Loss = 7.932714, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.261090278625488, Accuracy = 0.8576533794403076\n",
      "Training iter #15552000:   Batch Loss = 8.038519, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.260774612426758, Accuracy = 0.8579511642456055\n",
      "Training iter #15555000:   Batch Loss = 8.091761, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.260396003723145, Accuracy = 0.8579511642456055\n",
      "Training iter #15558000:   Batch Loss = 7.784351, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.260260581970215, Accuracy = 0.8582489490509033\n",
      "Training iter #15561000:   Batch Loss = 7.879530, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.259943008422852, Accuracy = 0.8576533794403076\n",
      "Training iter #15564000:   Batch Loss = 7.937888, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.259647369384766, Accuracy = 0.8576533794403076\n",
      "Training iter #15567000:   Batch Loss = 8.089166, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.259299278259277, Accuracy = 0.8573555946350098\n",
      "Training iter #15570000:   Batch Loss = 7.994393, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.258995056152344, Accuracy = 0.8582489490509033\n",
      "Training iter #15573000:   Batch Loss = 7.858071, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.258882522583008, Accuracy = 0.8582489490509033\n",
      "Training iter #15576000:   Batch Loss = 7.993797, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.258645057678223, Accuracy = 0.8582489490509033\n",
      "Training iter #15579000:   Batch Loss = 8.045917, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.258380889892578, Accuracy = 0.8588445782661438\n",
      "Training iter #15582000:   Batch Loss = 8.077821, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.258090019226074, Accuracy = 0.8585467338562012\n",
      "Training iter #15585000:   Batch Loss = 7.778737, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.258049011230469, Accuracy = 0.8585467338562012\n",
      "Training iter #15588000:   Batch Loss = 7.865741, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25784683227539, Accuracy = 0.8582489490509033\n",
      "Training iter #15591000:   Batch Loss = 7.920469, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.257665634155273, Accuracy = 0.8585467338562012\n",
      "Training iter #15594000:   Batch Loss = 8.078259, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25733757019043, Accuracy = 0.8582489490509033\n",
      "Training iter #15597000:   Batch Loss = 7.978343, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.256983757019043, Accuracy = 0.8582489490509033\n",
      "Training iter #15600000:   Batch Loss = 7.846898, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2567777633667, Accuracy = 0.8585467338562012\n",
      "Training iter #15603000:   Batch Loss = 7.994751, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.256442070007324, Accuracy = 0.8588445782661438\n",
      "Training iter #15606000:   Batch Loss = 8.056302, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.256105422973633, Accuracy = 0.8588445782661438\n",
      "Training iter #15609000:   Batch Loss = 8.067909, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.255746841430664, Accuracy = 0.8585467338562012\n",
      "Training iter #15612000:   Batch Loss = 7.796688, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2552490234375, Accuracy = 0.8588445782661438\n",
      "Training iter #15615000:   Batch Loss = 7.872862, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.254748344421387, Accuracy = 0.8588445782661438\n",
      "Training iter #15618000:   Batch Loss = 7.918853, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25448226928711, Accuracy = 0.8588445782661438\n",
      "Training iter #15621000:   Batch Loss = 8.093390, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25429630279541, Accuracy = 0.8588445782661438\n",
      "Training iter #15624000:   Batch Loss = 7.959166, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.253783226013184, Accuracy = 0.8591423630714417\n",
      "Training iter #15627000:   Batch Loss = 7.836497, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.253503799438477, Accuracy = 0.8594401478767395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15630000:   Batch Loss = 8.001109, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.253270149230957, Accuracy = 0.8591423630714417\n",
      "Training iter #15633000:   Batch Loss = 8.054871, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25313949584961, Accuracy = 0.8588445782661438\n",
      "Training iter #15636000:   Batch Loss = 8.039407, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.252737998962402, Accuracy = 0.8591423630714417\n",
      "Training iter #15639000:   Batch Loss = 7.802482, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25248908996582, Accuracy = 0.8582489490509033\n",
      "Training iter #15642000:   Batch Loss = 7.897403, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25168514251709, Accuracy = 0.8588445782661438\n",
      "Training iter #15645000:   Batch Loss = 7.908175, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.251751899719238, Accuracy = 0.8588445782661438\n",
      "Training iter #15648000:   Batch Loss = 8.107167, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.251782417297363, Accuracy = 0.8591423630714417\n",
      "Training iter #15651000:   Batch Loss = 7.942010, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.251591682434082, Accuracy = 0.8591423630714417\n",
      "Training iter #15654000:   Batch Loss = 7.841328, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.251482009887695, Accuracy = 0.8594401478767395\n",
      "Training iter #15657000:   Batch Loss = 7.970628, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25129222869873, Accuracy = 0.8594401478767395\n",
      "Training iter #15660000:   Batch Loss = 8.066710, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25096321105957, Accuracy = 0.8597379326820374\n",
      "Training iter #15663000:   Batch Loss = 8.021196, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.25022029876709, Accuracy = 0.8597379326820374\n",
      "Training iter #15666000:   Batch Loss = 7.812132, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.249574661254883, Accuracy = 0.8597379326820374\n",
      "Training iter #15669000:   Batch Loss = 7.898478, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.249252319335938, Accuracy = 0.8591423630714417\n",
      "Training iter #15672000:   Batch Loss = 7.942980, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.248960494995117, Accuracy = 0.8594401478767395\n",
      "Training iter #15675000:   Batch Loss = 8.124511, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.248485565185547, Accuracy = 0.8597379326820374\n",
      "Training iter #15678000:   Batch Loss = 7.927805, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.248052597045898, Accuracy = 0.8597379326820374\n",
      "Training iter #15681000:   Batch Loss = 7.824450, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.247743606567383, Accuracy = 0.8597379326820374\n",
      "Training iter #15684000:   Batch Loss = 7.981503, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.247710227966309, Accuracy = 0.8597379326820374\n",
      "Training iter #15687000:   Batch Loss = 8.050904, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.248085021972656, Accuracy = 0.8600357174873352\n",
      "Training iter #15690000:   Batch Loss = 8.008101, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.248136520385742, Accuracy = 0.8600357174873352\n",
      "Training iter #15693000:   Batch Loss = 7.788632, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.248044967651367, Accuracy = 0.8600357174873352\n",
      "Training iter #15696000:   Batch Loss = 7.900557, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.24771499633789, Accuracy = 0.8597379326820374\n",
      "Training iter #15699000:   Batch Loss = 7.946267, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.246956825256348, Accuracy = 0.8600357174873352\n",
      "Training iter #15702000:   Batch Loss = 8.125496, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.245771408081055, Accuracy = 0.8597379326820374\n",
      "Training iter #15705000:   Batch Loss = 7.904852, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.245503425598145, Accuracy = 0.8600357174873352\n",
      "Training iter #15708000:   Batch Loss = 7.825602, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.24521255493164, Accuracy = 0.8603335022926331\n",
      "Training iter #15711000:   Batch Loss = 8.005959, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.244898796081543, Accuracy = 0.8606313467025757\n",
      "Training iter #15714000:   Batch Loss = 8.043812, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.244678497314453, Accuracy = 0.8612269163131714\n",
      "Training iter #15717000:   Batch Loss = 7.993765, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.244394302368164, Accuracy = 0.8600357174873352\n",
      "Training iter #15720000:   Batch Loss = 7.788066, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.244343757629395, Accuracy = 0.8603335022926331\n",
      "Training iter #15723000:   Batch Loss = 7.905431, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.244111061096191, Accuracy = 0.8606313467025757\n",
      "Training iter #15726000:   Batch Loss = 7.942787, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.243487358093262, Accuracy = 0.8603335022926331\n",
      "Training iter #15729000:   Batch Loss = 8.130771, Accuracy = 0.8859999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.24324893951416, Accuracy = 0.8603335022926331\n",
      "Training iter #15732000:   Batch Loss = 7.895364, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.243067741394043, Accuracy = 0.8603335022926331\n",
      "Training iter #15735000:   Batch Loss = 7.814965, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.242982864379883, Accuracy = 0.8606313467025757\n",
      "Training iter #15738000:   Batch Loss = 7.998697, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.242774963378906, Accuracy = 0.8603335022926331\n",
      "Training iter #15741000:   Batch Loss = 8.027164, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.242443084716797, Accuracy = 0.8606313467025757\n",
      "Training iter #15744000:   Batch Loss = 8.017101, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.241905212402344, Accuracy = 0.8609291315078735\n",
      "Training iter #15747000:   Batch Loss = 7.793527, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.241494178771973, Accuracy = 0.8612269163131714\n",
      "Training iter #15750000:   Batch Loss = 7.903349, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.241161346435547, Accuracy = 0.8612269163131714\n",
      "Training iter #15753000:   Batch Loss = 7.955206, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.240924835205078, Accuracy = 0.8612269163131714\n",
      "Training iter #15756000:   Batch Loss = 8.121889, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2404146194458, Accuracy = 0.8612269163131714\n",
      "Training iter #15759000:   Batch Loss = 7.857330, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.240106582641602, Accuracy = 0.8612269163131714\n",
      "Training iter #15762000:   Batch Loss = 7.815988, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.239888191223145, Accuracy = 0.8612269163131714\n",
      "Training iter #15765000:   Batch Loss = 8.002405, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.239404678344727, Accuracy = 0.8615247011184692\n",
      "Training iter #15768000:   Batch Loss = 8.018862, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.23897933959961, Accuracy = 0.8612269163131714\n",
      "Training iter #15771000:   Batch Loss = 8.001978, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.238665580749512, Accuracy = 0.8615247011184692\n",
      "Training iter #15774000:   Batch Loss = 7.801052, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.23864459991455, Accuracy = 0.8612269163131714\n",
      "Training iter #15777000:   Batch Loss = 7.914891, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.238492965698242, Accuracy = 0.8612269163131714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15780000:   Batch Loss = 7.942224, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.238344192504883, Accuracy = 0.8615247011184692\n",
      "Training iter #15783000:   Batch Loss = 8.130753, Accuracy = 0.8820000290870667\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.238177299499512, Accuracy = 0.8615247011184692\n",
      "Training iter #15786000:   Batch Loss = 7.822056, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.238131523132324, Accuracy = 0.8612269163131714\n",
      "Training iter #15789000:   Batch Loss = 7.810031, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.238258361816406, Accuracy = 0.8612269163131714\n",
      "Training iter #15792000:   Batch Loss = 8.013509, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.237732887268066, Accuracy = 0.8609291315078735\n",
      "Training iter #15795000:   Batch Loss = 8.014688, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.237090110778809, Accuracy = 0.8612269163131714\n",
      "Training iter #15798000:   Batch Loss = 7.996075, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.236441612243652, Accuracy = 0.8612269163131714\n",
      "Training iter #15801000:   Batch Loss = 7.824679, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.237226486206055, Accuracy = 0.8609291315078735\n",
      "Training iter #15804000:   Batch Loss = 7.903727, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.237395286560059, Accuracy = 0.8609291315078735\n",
      "Training iter #15807000:   Batch Loss = 7.985324, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.236701011657715, Accuracy = 0.8609291315078735\n",
      "Training iter #15810000:   Batch Loss = 8.108644, Accuracy = 0.8866666555404663\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.235547065734863, Accuracy = 0.8612269163131714\n",
      "Training iter #15813000:   Batch Loss = 7.809850, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.23572826385498, Accuracy = 0.8603335022926331\n",
      "Training iter #15816000:   Batch Loss = 7.809205, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.235300064086914, Accuracy = 0.8600357174873352\n",
      "Training iter #15819000:   Batch Loss = 7.999375, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.234784126281738, Accuracy = 0.8603335022926331\n",
      "Training iter #15822000:   Batch Loss = 8.017362, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.234816551208496, Accuracy = 0.8609291315078735\n",
      "Training iter #15825000:   Batch Loss = 8.012967, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.234517097473145, Accuracy = 0.8609291315078735\n",
      "Training iter #15828000:   Batch Loss = 7.840688, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.233922004699707, Accuracy = 0.8606313467025757\n",
      "Training iter #15831000:   Batch Loss = 7.892901, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.234467506408691, Accuracy = 0.8609291315078735\n",
      "Training iter #15834000:   Batch Loss = 7.994918, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.235257148742676, Accuracy = 0.8603335022926331\n",
      "Training iter #15837000:   Batch Loss = 8.056250, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.235085487365723, Accuracy = 0.8609291315078735\n",
      "Training iter #15840000:   Batch Loss = 7.785627, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.234869956970215, Accuracy = 0.8612269163131714\n",
      "Training iter #15843000:   Batch Loss = 7.825167, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.234491348266602, Accuracy = 0.8612269163131714\n",
      "Training iter #15846000:   Batch Loss = 7.961299, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.234067916870117, Accuracy = 0.8612269163131714\n",
      "Training iter #15849000:   Batch Loss = 8.044014, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.233736991882324, Accuracy = 0.8612269163131714\n",
      "Training iter #15852000:   Batch Loss = 8.033650, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.23336410522461, Accuracy = 0.8612269163131714\n",
      "Training iter #15855000:   Batch Loss = 7.839702, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.232958793640137, Accuracy = 0.8618224859237671\n",
      "Training iter #15858000:   Batch Loss = 7.918825, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.232626914978027, Accuracy = 0.8618224859237671\n",
      "Training iter #15861000:   Batch Loss = 8.003965, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.232380867004395, Accuracy = 0.8615247011184692\n",
      "Training iter #15864000:   Batch Loss = 8.048539, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.233535766601562, Accuracy = 0.8612269163131714\n",
      "Training iter #15867000:   Batch Loss = 7.758531, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.233453750610352, Accuracy = 0.8615247011184692\n",
      "Training iter #15870000:   Batch Loss = 7.847808, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.233197212219238, Accuracy = 0.8615247011184692\n",
      "Training iter #15873000:   Batch Loss = 7.944955, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.232937812805176, Accuracy = 0.8609291315078735\n",
      "Training iter #15876000:   Batch Loss = 8.029359, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.232725143432617, Accuracy = 0.8609291315078735\n",
      "Training iter #15879000:   Batch Loss = 7.994562, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.23222827911377, Accuracy = 0.8609291315078735\n",
      "Training iter #15882000:   Batch Loss = 7.841459, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.231952667236328, Accuracy = 0.8609291315078735\n",
      "Training iter #15885000:   Batch Loss = 7.924888, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.23164176940918, Accuracy = 0.8609291315078735\n",
      "Training iter #15888000:   Batch Loss = 8.001482, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.231444358825684, Accuracy = 0.8612269163131714\n",
      "Training iter #15891000:   Batch Loss = 8.040531, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.231159210205078, Accuracy = 0.8612269163131714\n",
      "Training iter #15894000:   Batch Loss = 7.772579, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.230925559997559, Accuracy = 0.8609291315078735\n",
      "Training iter #15897000:   Batch Loss = 7.849756, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.23064136505127, Accuracy = 0.8606313467025757\n",
      "Training iter #15900000:   Batch Loss = 7.922324, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.230400085449219, Accuracy = 0.8603335022926331\n",
      "Training iter #15903000:   Batch Loss = 8.049168, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.230310440063477, Accuracy = 0.8606313467025757\n",
      "Training iter #15906000:   Batch Loss = 7.957371, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.230062484741211, Accuracy = 0.8606313467025757\n",
      "Training iter #15909000:   Batch Loss = 7.834104, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.229833602905273, Accuracy = 0.8609291315078735\n",
      "Training iter #15912000:   Batch Loss = 7.975431, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.229360580444336, Accuracy = 0.8609291315078735\n",
      "Training iter #15915000:   Batch Loss = 8.024488, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.229116439819336, Accuracy = 0.8621203303337097\n",
      "Training iter #15918000:   Batch Loss = 8.040408, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.228906631469727, Accuracy = 0.8618224859237671\n",
      "Training iter #15921000:   Batch Loss = 7.767089, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.228642463684082, Accuracy = 0.8618224859237671\n",
      "Training iter #15924000:   Batch Loss = 7.846320, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.228362083435059, Accuracy = 0.8612269163131714\n",
      "Training iter #15927000:   Batch Loss = 7.896295, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.228203773498535, Accuracy = 0.8615247011184692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15930000:   Batch Loss = 8.047574, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.228055953979492, Accuracy = 0.8612269163131714\n",
      "Training iter #15933000:   Batch Loss = 7.942653, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.22729778289795, Accuracy = 0.8612269163131714\n",
      "Training iter #15936000:   Batch Loss = 7.818399, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.226516723632812, Accuracy = 0.8612269163131714\n",
      "Training iter #15939000:   Batch Loss = 7.966121, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.22594928741455, Accuracy = 0.8609291315078735\n",
      "Training iter #15942000:   Batch Loss = 8.016379, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.225608825683594, Accuracy = 0.8615247011184692\n",
      "Training iter #15945000:   Batch Loss = 8.028343, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.226465225219727, Accuracy = 0.8618224859237671\n",
      "Training iter #15948000:   Batch Loss = 7.777614, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.226140975952148, Accuracy = 0.8615247011184692\n",
      "Training iter #15951000:   Batch Loss = 7.857338, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.225274085998535, Accuracy = 0.8615247011184692\n",
      "Training iter #15954000:   Batch Loss = 7.878488, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.22557258605957, Accuracy = 0.8615247011184692\n",
      "Training iter #15957000:   Batch Loss = 8.075794, Accuracy = 0.8953333497047424\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.225638389587402, Accuracy = 0.8612269163131714\n",
      "Training iter #15960000:   Batch Loss = 7.923065, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.223942756652832, Accuracy = 0.8618224859237671\n",
      "Training iter #15963000:   Batch Loss = 7.814222, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.224361419677734, Accuracy = 0.8612269163131714\n",
      "Training iter #15966000:   Batch Loss = 7.964513, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.224109649658203, Accuracy = 0.8603335022926331\n",
      "Training iter #15969000:   Batch Loss = 8.028362, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.223808288574219, Accuracy = 0.8609291315078735\n",
      "Training iter #15972000:   Batch Loss = 8.009530, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.223491668701172, Accuracy = 0.8606313467025757\n",
      "Training iter #15975000:   Batch Loss = 7.775661, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.223297119140625, Accuracy = 0.8615247011184692\n",
      "Training iter #15978000:   Batch Loss = 7.873087, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.222936630249023, Accuracy = 0.8609291315078735\n",
      "Training iter #15981000:   Batch Loss = 7.900691, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.222455978393555, Accuracy = 0.8615247011184692\n",
      "Training iter #15984000:   Batch Loss = 8.062486, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.22408390045166, Accuracy = 0.8615247011184692\n",
      "Training iter #15987000:   Batch Loss = 7.907636, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.22253131866455, Accuracy = 0.8615247011184692\n",
      "Training iter #15990000:   Batch Loss = 7.796048, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.221869468688965, Accuracy = 0.8615247011184692\n",
      "Training iter #15993000:   Batch Loss = 7.944403, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.220720291137695, Accuracy = 0.8618224859237671\n",
      "Training iter #15996000:   Batch Loss = 8.023749, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.221303939819336, Accuracy = 0.8621203303337097\n",
      "Training iter #15999000:   Batch Loss = 7.973092, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.221508979797363, Accuracy = 0.8618224859237671\n",
      "Training iter #16002000:   Batch Loss = 7.783096, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.221916198730469, Accuracy = 0.8615247011184692\n",
      "Training iter #16005000:   Batch Loss = 7.877529, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.222406387329102, Accuracy = 0.8615247011184692\n",
      "Training iter #16008000:   Batch Loss = 7.919804, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.222200393676758, Accuracy = 0.8618224859237671\n",
      "Training iter #16011000:   Batch Loss = 8.084249, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.221744537353516, Accuracy = 0.8621203303337097\n",
      "Training iter #16014000:   Batch Loss = 7.885894, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.221272468566895, Accuracy = 0.8618224859237671\n",
      "Training iter #16017000:   Batch Loss = 7.800016, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.22092342376709, Accuracy = 0.8618224859237671\n",
      "Training iter #16020000:   Batch Loss = 7.951328, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.220778465270996, Accuracy = 0.8612269163131714\n",
      "Training iter #16023000:   Batch Loss = 8.019136, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.22050952911377, Accuracy = 0.8612269163131714\n",
      "Training iter #16026000:   Batch Loss = 7.983794, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.220173835754395, Accuracy = 0.8624181151390076\n",
      "Training iter #16029000:   Batch Loss = 7.759395, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.219951629638672, Accuracy = 0.8621203303337097\n",
      "Training iter #16032000:   Batch Loss = 7.881093, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.219539642333984, Accuracy = 0.8618224859237671\n",
      "Training iter #16035000:   Batch Loss = 7.896854, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.218952178955078, Accuracy = 0.8618224859237671\n",
      "Training iter #16038000:   Batch Loss = 8.099166, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.218276977539062, Accuracy = 0.8624181151390076\n",
      "Training iter #16041000:   Batch Loss = 7.874433, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.21813678741455, Accuracy = 0.8624181151390076\n",
      "Training iter #16044000:   Batch Loss = 7.794054, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.218070030212402, Accuracy = 0.8624181151390076\n",
      "Training iter #16047000:   Batch Loss = 7.972871, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.217900276184082, Accuracy = 0.8624181151390076\n",
      "Training iter #16050000:   Batch Loss = 8.002243, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.217619895935059, Accuracy = 0.8624181151390076\n",
      "Training iter #16053000:   Batch Loss = 7.965603, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.217510223388672, Accuracy = 0.8621203303337097\n",
      "Training iter #16056000:   Batch Loss = 7.765318, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.217665672302246, Accuracy = 0.8621203303337097\n",
      "Training iter #16059000:   Batch Loss = 7.886487, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.21756649017334, Accuracy = 0.8621203303337097\n",
      "Training iter #16062000:   Batch Loss = 7.916189, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.217361450195312, Accuracy = 0.8621203303337097\n",
      "Training iter #16065000:   Batch Loss = 8.079844, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.217105865478516, Accuracy = 0.8621203303337097\n",
      "Training iter #16068000:   Batch Loss = 7.845845, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.217045783996582, Accuracy = 0.8624181151390076\n",
      "Training iter #16071000:   Batch Loss = 7.791677, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.216955184936523, Accuracy = 0.8621203303337097\n",
      "Training iter #16074000:   Batch Loss = 7.967733, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.216720581054688, Accuracy = 0.8621203303337097\n",
      "Training iter #16077000:   Batch Loss = 7.985207, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.216424942016602, Accuracy = 0.8624181151390076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16080000:   Batch Loss = 7.989694, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.216155052185059, Accuracy = 0.8624181151390076\n",
      "Training iter #16083000:   Batch Loss = 7.772090, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.21526050567627, Accuracy = 0.8630136847496033\n",
      "Training iter #16086000:   Batch Loss = 7.882438, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.214681625366211, Accuracy = 0.8636093139648438\n",
      "Training iter #16089000:   Batch Loss = 7.923895, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.214325904846191, Accuracy = 0.8633114695549011\n",
      "Training iter #16092000:   Batch Loss = 8.106224, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.214017868041992, Accuracy = 0.8636093139648438\n",
      "Training iter #16095000:   Batch Loss = 7.811534, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.213809967041016, Accuracy = 0.8639070987701416\n",
      "Training iter #16098000:   Batch Loss = 7.785457, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.213679313659668, Accuracy = 0.8636093139648438\n",
      "Training iter #16101000:   Batch Loss = 7.978902, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.213449478149414, Accuracy = 0.8636093139648438\n",
      "Training iter #16104000:   Batch Loss = 7.988681, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.213143348693848, Accuracy = 0.8636093139648438\n",
      "Training iter #16107000:   Batch Loss = 7.973960, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.21298885345459, Accuracy = 0.8636093139648438\n",
      "Training iter #16110000:   Batch Loss = 7.785169, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.213096618652344, Accuracy = 0.8642048835754395\n",
      "Training iter #16113000:   Batch Loss = 7.879798, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.212784767150879, Accuracy = 0.8639070987701416\n",
      "Training iter #16116000:   Batch Loss = 7.930617, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.21243953704834, Accuracy = 0.8636093139648438\n",
      "Training iter #16119000:   Batch Loss = 8.068954, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.212127685546875, Accuracy = 0.8633114695549011\n",
      "Training iter #16122000:   Batch Loss = 7.790270, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.21224594116211, Accuracy = 0.8636093139648438\n",
      "Training iter #16125000:   Batch Loss = 7.770800, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.212247848510742, Accuracy = 0.8633114695549011\n",
      "Training iter #16128000:   Batch Loss = 7.973805, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.212095260620117, Accuracy = 0.8633114695549011\n",
      "Training iter #16131000:   Batch Loss = 7.975495, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.211838722229004, Accuracy = 0.8630136847496033\n",
      "Training iter #16134000:   Batch Loss = 7.961702, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.211662292480469, Accuracy = 0.8633114695549011\n",
      "Training iter #16137000:   Batch Loss = 7.796572, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.211606979370117, Accuracy = 0.8633114695549011\n",
      "Training iter #16140000:   Batch Loss = 7.877859, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.211102485656738, Accuracy = 0.8633114695549011\n",
      "Training iter #16143000:   Batch Loss = 7.963844, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.210693359375, Accuracy = 0.8633114695549011\n",
      "Training iter #16146000:   Batch Loss = 8.057746, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.210405349731445, Accuracy = 0.8630136847496033\n",
      "Training iter #16149000:   Batch Loss = 7.769584, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.21019172668457, Accuracy = 0.8633114695549011\n",
      "Training iter #16152000:   Batch Loss = 7.796413, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.209880828857422, Accuracy = 0.8633114695549011\n",
      "Training iter #16155000:   Batch Loss = 7.954258, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.209774017333984, Accuracy = 0.8627158999443054\n",
      "Training iter #16158000:   Batch Loss = 7.995798, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.209653854370117, Accuracy = 0.8627158999443054\n",
      "Training iter #16161000:   Batch Loss = 7.981041, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.209737777709961, Accuracy = 0.8630136847496033\n",
      "Training iter #16164000:   Batch Loss = 7.813498, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.20975112915039, Accuracy = 0.8636093139648438\n",
      "Training iter #16167000:   Batch Loss = 7.880033, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.209565162658691, Accuracy = 0.8630136847496033\n",
      "Training iter #16170000:   Batch Loss = 7.959841, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.20942211151123, Accuracy = 0.8627158999443054\n",
      "Training iter #16173000:   Batch Loss = 8.012401, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.209123611450195, Accuracy = 0.8621203303337097\n",
      "Training iter #16176000:   Batch Loss = 7.740290, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.208925247192383, Accuracy = 0.8624181151390076\n",
      "Training iter #16179000:   Batch Loss = 7.810278, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.208622932434082, Accuracy = 0.8624181151390076\n",
      "Training iter #16182000:   Batch Loss = 7.910479, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.208442687988281, Accuracy = 0.8624181151390076\n",
      "Training iter #16185000:   Batch Loss = 8.006050, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.207990646362305, Accuracy = 0.8618224859237671\n",
      "Training iter #16188000:   Batch Loss = 7.987522, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.207584381103516, Accuracy = 0.8618224859237671\n",
      "Training iter #16191000:   Batch Loss = 7.819546, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.207213401794434, Accuracy = 0.8621203303337097\n",
      "Training iter #16194000:   Batch Loss = 7.880213, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.206670761108398, Accuracy = 0.8621203303337097\n",
      "Training iter #16197000:   Batch Loss = 7.964281, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.206335067749023, Accuracy = 0.8627158999443054\n",
      "Training iter #16200000:   Batch Loss = 8.025197, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.206192970275879, Accuracy = 0.8627158999443054\n",
      "Training iter #16203000:   Batch Loss = 7.738140, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.206062316894531, Accuracy = 0.8630136847496033\n",
      "Training iter #16206000:   Batch Loss = 7.824350, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.20592975616455, Accuracy = 0.8636093139648438\n",
      "Training iter #16209000:   Batch Loss = 7.914461, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.205626487731934, Accuracy = 0.8636093139648438\n",
      "Training iter #16212000:   Batch Loss = 8.020864, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.2052640914917, Accuracy = 0.8633114695549011\n",
      "Training iter #16215000:   Batch Loss = 7.937967, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.204975128173828, Accuracy = 0.8639070987701416\n",
      "Training iter #16218000:   Batch Loss = 7.802469, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.204812049865723, Accuracy = 0.8645026683807373\n",
      "Training iter #16221000:   Batch Loss = 7.917805, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.204654693603516, Accuracy = 0.8648004531860352\n",
      "Training iter #16224000:   Batch Loss = 7.980658, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.204400062561035, Accuracy = 0.8648004531860352\n",
      "Training iter #16227000:   Batch Loss = 7.993348, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.204169273376465, Accuracy = 0.8642048835754395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16230000:   Batch Loss = 7.741316, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.204048156738281, Accuracy = 0.8648004531860352\n",
      "Training iter #16233000:   Batch Loss = 7.820300, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.203948020935059, Accuracy = 0.8645026683807373\n",
      "Training iter #16236000:   Batch Loss = 7.872686, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.203741073608398, Accuracy = 0.8645026683807373\n",
      "Training iter #16239000:   Batch Loss = 8.015533, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.203548431396484, Accuracy = 0.8645026683807373\n",
      "Training iter #16242000:   Batch Loss = 7.923658, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.203455924987793, Accuracy = 0.8648004531860352\n",
      "Training iter #16245000:   Batch Loss = 7.808225, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.20326042175293, Accuracy = 0.8645026683807373\n",
      "Training iter #16248000:   Batch Loss = 7.935476, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.202960968017578, Accuracy = 0.8648004531860352\n",
      "Training iter #16251000:   Batch Loss = 7.991028, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.202653884887695, Accuracy = 0.8648004531860352\n",
      "Training iter #16254000:   Batch Loss = 8.000715, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.20230770111084, Accuracy = 0.8650982975959778\n",
      "Training iter #16257000:   Batch Loss = 7.746094, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.202068328857422, Accuracy = 0.8650982975959778\n",
      "Training iter #16260000:   Batch Loss = 7.817577, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.201845169067383, Accuracy = 0.8648004531860352\n",
      "Training iter #16263000:   Batch Loss = 7.870709, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.201608657836914, Accuracy = 0.8648004531860352\n",
      "Training iter #16266000:   Batch Loss = 8.021257, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.201316833496094, Accuracy = 0.8648004531860352\n",
      "Training iter #16269000:   Batch Loss = 7.903023, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.20107650756836, Accuracy = 0.8650982975959778\n",
      "Training iter #16272000:   Batch Loss = 7.787268, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.200870513916016, Accuracy = 0.8653960824012756\n",
      "Training iter #16275000:   Batch Loss = 7.934137, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.200600624084473, Accuracy = 0.8648004531860352\n",
      "Training iter #16278000:   Batch Loss = 7.984842, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.200380325317383, Accuracy = 0.8650982975959778\n",
      "Training iter #16281000:   Batch Loss = 7.983462, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.200230598449707, Accuracy = 0.8653960824012756\n",
      "Training iter #16284000:   Batch Loss = 7.752399, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.200156211853027, Accuracy = 0.8656938672065735\n",
      "Training iter #16287000:   Batch Loss = 7.824245, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.199881553649902, Accuracy = 0.8656938672065735\n",
      "Training iter #16290000:   Batch Loss = 7.853631, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.199481964111328, Accuracy = 0.8653960824012756\n",
      "Training iter #16293000:   Batch Loss = 8.030163, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.199165344238281, Accuracy = 0.8659916520118713\n",
      "Training iter #16296000:   Batch Loss = 7.886483, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.198843002319336, Accuracy = 0.8659916520118713\n",
      "Training iter #16299000:   Batch Loss = 7.789380, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.198556900024414, Accuracy = 0.8659916520118713\n",
      "Training iter #16302000:   Batch Loss = 7.909727, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.198271751403809, Accuracy = 0.8659916520118713\n",
      "Training iter #16305000:   Batch Loss = 7.993729, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.198040008544922, Accuracy = 0.8659916520118713\n",
      "Training iter #16308000:   Batch Loss = 7.966679, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.197676658630371, Accuracy = 0.8656938672065735\n",
      "Training iter #16311000:   Batch Loss = 7.755618, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.197408676147461, Accuracy = 0.8659916520118713\n",
      "Training iter #16314000:   Batch Loss = 7.846354, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.19715690612793, Accuracy = 0.8659916520118713\n",
      "Training iter #16317000:   Batch Loss = 7.890613, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.196914672851562, Accuracy = 0.8659916520118713\n",
      "Training iter #16320000:   Batch Loss = 8.043514, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.196779251098633, Accuracy = 0.8662894368171692\n",
      "Training iter #16323000:   Batch Loss = 7.872302, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.196614265441895, Accuracy = 0.8665872812271118\n",
      "Training iter #16326000:   Batch Loss = 7.769023, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.196585655212402, Accuracy = 0.8665872812271118\n",
      "Training iter #16329000:   Batch Loss = 7.913685, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1964111328125, Accuracy = 0.8662894368171692\n",
      "Training iter #16332000:   Batch Loss = 7.984006, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.196224212646484, Accuracy = 0.8665872812271118\n",
      "Training iter #16335000:   Batch Loss = 7.937113, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.195927619934082, Accuracy = 0.8668850660324097\n",
      "Training iter #16338000:   Batch Loss = 7.745150, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.195698738098145, Accuracy = 0.8662894368171692\n",
      "Training iter #16341000:   Batch Loss = 7.847420, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.195372581481934, Accuracy = 0.8662894368171692\n",
      "Training iter #16344000:   Batch Loss = 7.891114, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.195056915283203, Accuracy = 0.8662894368171692\n",
      "Training iter #16347000:   Batch Loss = 8.047330, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.194940567016602, Accuracy = 0.8662894368171692\n",
      "Training iter #16350000:   Batch Loss = 7.852202, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.194872856140137, Accuracy = 0.8662894368171692\n",
      "Training iter #16353000:   Batch Loss = 7.775005, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.194804191589355, Accuracy = 0.8668850660324097\n",
      "Training iter #16356000:   Batch Loss = 7.923002, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.194632530212402, Accuracy = 0.8662894368171692\n",
      "Training iter #16359000:   Batch Loss = 7.978826, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.194421768188477, Accuracy = 0.8662894368171692\n",
      "Training iter #16362000:   Batch Loss = 7.936401, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.194061279296875, Accuracy = 0.8665872812271118\n",
      "Training iter #16365000:   Batch Loss = 7.734821, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.193833351135254, Accuracy = 0.8665872812271118\n",
      "Training iter #16368000:   Batch Loss = 7.850343, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.19346809387207, Accuracy = 0.8665872812271118\n",
      "Training iter #16371000:   Batch Loss = 7.878121, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.193132400512695, Accuracy = 0.8665872812271118\n",
      "Training iter #16374000:   Batch Loss = 8.060909, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.19289779663086, Accuracy = 0.8665872812271118\n",
      "Training iter #16377000:   Batch Loss = 7.843454, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.192648887634277, Accuracy = 0.8665872812271118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16380000:   Batch Loss = 7.765128, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.192426681518555, Accuracy = 0.8665872812271118\n",
      "Training iter #16383000:   Batch Loss = 7.930820, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.192085266113281, Accuracy = 0.8671828508377075\n",
      "Training iter #16386000:   Batch Loss = 7.964847, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.191856384277344, Accuracy = 0.8671828508377075\n",
      "Training iter #16389000:   Batch Loss = 7.954432, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.191692352294922, Accuracy = 0.8671828508377075\n",
      "Training iter #16392000:   Batch Loss = 7.740758, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.191705703735352, Accuracy = 0.8683740496635437\n",
      "Training iter #16395000:   Batch Loss = 7.854310, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.191304206848145, Accuracy = 0.8677784204483032\n",
      "Training iter #16398000:   Batch Loss = 7.897907, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.190918922424316, Accuracy = 0.8671828508377075\n",
      "Training iter #16401000:   Batch Loss = 8.049017, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.19058895111084, Accuracy = 0.8668850660324097\n",
      "Training iter #16404000:   Batch Loss = 7.805780, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1902494430542, Accuracy = 0.8671828508377075\n",
      "Training iter #16407000:   Batch Loss = 7.765645, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.189895629882812, Accuracy = 0.8668850660324097\n",
      "Training iter #16410000:   Batch Loss = 7.934467, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.189464569091797, Accuracy = 0.8668850660324097\n",
      "Training iter #16413000:   Batch Loss = 7.960187, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.189330101013184, Accuracy = 0.8674806356430054\n",
      "Training iter #16416000:   Batch Loss = 7.935987, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.189191818237305, Accuracy = 0.8683740496635437\n",
      "Training iter #16419000:   Batch Loss = 7.752399, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.189248085021973, Accuracy = 0.8680762648582458\n",
      "Training iter #16422000:   Batch Loss = 7.860639, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.188542366027832, Accuracy = 0.8671828508377075\n",
      "Training iter #16425000:   Batch Loss = 7.894207, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.188361167907715, Accuracy = 0.8671828508377075\n",
      "Training iter #16428000:   Batch Loss = 8.058270, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.18873119354248, Accuracy = 0.8671828508377075\n",
      "Training iter #16431000:   Batch Loss = 7.777987, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.189064025878906, Accuracy = 0.8671828508377075\n",
      "Training iter #16434000:   Batch Loss = 7.765307, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.188582420349121, Accuracy = 0.8671828508377075\n",
      "Training iter #16437000:   Batch Loss = 7.950557, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.187612533569336, Accuracy = 0.8671828508377075\n",
      "Training iter #16440000:   Batch Loss = 7.949052, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.187129020690918, Accuracy = 0.8665872812271118\n",
      "Training iter #16443000:   Batch Loss = 7.939113, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.187300682067871, Accuracy = 0.8668850660324097\n",
      "Training iter #16446000:   Batch Loss = 7.771618, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.187017440795898, Accuracy = 0.8686718344688416\n",
      "Training iter #16449000:   Batch Loss = 7.858323, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.186678886413574, Accuracy = 0.8680762648582458\n",
      "Training iter #16452000:   Batch Loss = 7.921332, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.18616008758545, Accuracy = 0.8674806356430054\n",
      "Training iter #16455000:   Batch Loss = 8.039145, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.18564510345459, Accuracy = 0.8671828508377075\n",
      "Training iter #16458000:   Batch Loss = 7.761973, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.185138702392578, Accuracy = 0.8674806356430054\n",
      "Training iter #16461000:   Batch Loss = 7.758498, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.184706687927246, Accuracy = 0.8674806356430054\n",
      "Training iter #16464000:   Batch Loss = 7.934290, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.184378623962402, Accuracy = 0.8671828508377075\n",
      "Training iter #16467000:   Batch Loss = 7.943897, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.184165954589844, Accuracy = 0.8671828508377075\n",
      "Training iter #16470000:   Batch Loss = 7.941142, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.183945655822754, Accuracy = 0.8674806356430054\n",
      "Training iter #16473000:   Batch Loss = 7.780931, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.18377685546875, Accuracy = 0.8674806356430054\n",
      "Training iter #16476000:   Batch Loss = 7.860624, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.183655738830566, Accuracy = 0.8671828508377075\n",
      "Training iter #16479000:   Batch Loss = 7.928006, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.183449745178223, Accuracy = 0.8680762648582458\n",
      "Training iter #16482000:   Batch Loss = 8.007847, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.183252334594727, Accuracy = 0.8677784204483032\n",
      "Training iter #16485000:   Batch Loss = 7.738077, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.183138847351074, Accuracy = 0.8671828508377075\n",
      "Training iter #16488000:   Batch Loss = 7.765820, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.18297004699707, Accuracy = 0.8671828508377075\n",
      "Training iter #16491000:   Batch Loss = 7.903464, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.182660102844238, Accuracy = 0.8677784204483032\n",
      "Training iter #16494000:   Batch Loss = 7.981459, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.182401657104492, Accuracy = 0.8677784204483032\n",
      "Training iter #16497000:   Batch Loss = 7.964271, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.182245254516602, Accuracy = 0.8677784204483032\n",
      "Training iter #16500000:   Batch Loss = 7.787714, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.182005882263184, Accuracy = 0.8683740496635437\n",
      "Training iter #16503000:   Batch Loss = 7.859451, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.181912422180176, Accuracy = 0.8683740496635437\n",
      "Training iter #16506000:   Batch Loss = 7.929054, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.181816101074219, Accuracy = 0.8686718344688416\n",
      "Training iter #16509000:   Batch Loss = 7.978767, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.181751251220703, Accuracy = 0.8686718344688416\n",
      "Training iter #16512000:   Batch Loss = 7.714379, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.181522369384766, Accuracy = 0.8680762648582458\n",
      "Training iter #16515000:   Batch Loss = 7.789492, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.18112564086914, Accuracy = 0.8680762648582458\n",
      "Training iter #16518000:   Batch Loss = 7.889008, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.18050765991211, Accuracy = 0.8683740496635437\n",
      "Training iter #16521000:   Batch Loss = 7.967535, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.179994583129883, Accuracy = 0.8680762648582458\n",
      "Training iter #16524000:   Batch Loss = 7.936557, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.179673194885254, Accuracy = 0.8680762648582458\n",
      "Training iter #16527000:   Batch Loss = 7.790127, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.179471015930176, Accuracy = 0.8674806356430054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16530000:   Batch Loss = 7.853894, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.179181098937988, Accuracy = 0.8680762648582458\n",
      "Training iter #16533000:   Batch Loss = 7.929193, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.178963661193848, Accuracy = 0.8680762648582458\n",
      "Training iter #16536000:   Batch Loss = 7.984327, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.17870807647705, Accuracy = 0.8683740496635437\n",
      "Training iter #16539000:   Batch Loss = 7.725265, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.178421020507812, Accuracy = 0.8677784204483032\n",
      "Training iter #16542000:   Batch Loss = 7.799181, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.178125381469727, Accuracy = 0.8683740496635437\n",
      "Training iter #16545000:   Batch Loss = 7.858813, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.177833557128906, Accuracy = 0.8686718344688416\n",
      "Training iter #16548000:   Batch Loss = 7.988253, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.177490234375, Accuracy = 0.8692674040794373\n",
      "Training iter #16551000:   Batch Loss = 7.905063, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.17717170715332, Accuracy = 0.8689696192741394\n",
      "Training iter #16554000:   Batch Loss = 7.779595, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.176855087280273, Accuracy = 0.8683740496635437\n",
      "Training iter #16557000:   Batch Loss = 7.908355, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.176509857177734, Accuracy = 0.8686718344688416\n",
      "Training iter #16560000:   Batch Loss = 7.940379, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.176283836364746, Accuracy = 0.8686718344688416\n",
      "Training iter #16563000:   Batch Loss = 7.973577, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.176257133483887, Accuracy = 0.8686718344688416\n",
      "Training iter #16566000:   Batch Loss = 7.709740, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.176156997680664, Accuracy = 0.8680762648582458\n",
      "Training iter #16569000:   Batch Loss = 7.790121, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.175981521606445, Accuracy = 0.8677784204483032\n",
      "Training iter #16572000:   Batch Loss = 7.842812, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.175741195678711, Accuracy = 0.8680762648582458\n",
      "Training iter #16575000:   Batch Loss = 7.980588, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.17542552947998, Accuracy = 0.8686718344688416\n",
      "Training iter #16578000:   Batch Loss = 7.882516, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.17519760131836, Accuracy = 0.8683740496635437\n",
      "Training iter #16581000:   Batch Loss = 7.770133, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.174951553344727, Accuracy = 0.8680762648582458\n",
      "Training iter #16584000:   Batch Loss = 7.901901, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.174728393554688, Accuracy = 0.8680762648582458\n",
      "Training iter #16587000:   Batch Loss = 7.945973, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.17448902130127, Accuracy = 0.8686718344688416\n",
      "Training iter #16590000:   Batch Loss = 7.963585, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.17430591583252, Accuracy = 0.8695651888847351\n",
      "Training iter #16593000:   Batch Loss = 7.727228, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.174102783203125, Accuracy = 0.8683740496635437\n",
      "Training iter #16596000:   Batch Loss = 7.798437, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.173933982849121, Accuracy = 0.8686718344688416\n",
      "Training iter #16599000:   Batch Loss = 7.830258, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.173687934875488, Accuracy = 0.8686718344688416\n",
      "Training iter #16602000:   Batch Loss = 7.992255, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.173349380493164, Accuracy = 0.8686718344688416\n",
      "Training iter #16605000:   Batch Loss = 7.861592, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.173127174377441, Accuracy = 0.8686718344688416\n",
      "Training iter #16608000:   Batch Loss = 7.767663, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.172894477844238, Accuracy = 0.8683740496635437\n",
      "Training iter #16611000:   Batch Loss = 7.901981, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1726713180542, Accuracy = 0.8683740496635437\n",
      "Training iter #16614000:   Batch Loss = 7.948912, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1724271774292, Accuracy = 0.8695651888847351\n",
      "Training iter #16617000:   Batch Loss = 7.951331, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.172069549560547, Accuracy = 0.8692674040794373\n",
      "Training iter #16620000:   Batch Loss = 7.725236, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.171728134155273, Accuracy = 0.8692674040794373\n",
      "Training iter #16623000:   Batch Loss = 7.821108, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.171392440795898, Accuracy = 0.8698630332946777\n",
      "Training iter #16626000:   Batch Loss = 7.847719, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.171075820922852, Accuracy = 0.8692674040794373\n",
      "Training iter #16629000:   Batch Loss = 8.002554, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.170802116394043, Accuracy = 0.8689696192741394\n",
      "Training iter #16632000:   Batch Loss = 7.847284, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.170661926269531, Accuracy = 0.8686718344688416\n",
      "Training iter #16635000:   Batch Loss = 7.765454, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.170332908630371, Accuracy = 0.8686718344688416\n",
      "Training iter #16638000:   Batch Loss = 7.874742, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.169858932495117, Accuracy = 0.8689696192741394\n",
      "Training iter #16641000:   Batch Loss = 7.954156, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.169668197631836, Accuracy = 0.8695651888847351\n",
      "Training iter #16644000:   Batch Loss = 7.920448, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.169342994689941, Accuracy = 0.8692674040794373\n",
      "Training iter #16647000:   Batch Loss = 7.732823, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.169112205505371, Accuracy = 0.8695651888847351\n",
      "Training iter #16650000:   Batch Loss = 7.822893, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.168730735778809, Accuracy = 0.8701608180999756\n",
      "Training iter #16653000:   Batch Loss = 7.855896, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.168428421020508, Accuracy = 0.8695651888847351\n",
      "Training iter #16656000:   Batch Loss = 8.014239, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.16826343536377, Accuracy = 0.8689696192741394\n",
      "Training iter #16659000:   Batch Loss = 7.848824, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1681547164917, Accuracy = 0.8686718344688416\n",
      "Training iter #16662000:   Batch Loss = 7.752779, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.167861938476562, Accuracy = 0.8686718344688416\n",
      "Training iter #16665000:   Batch Loss = 7.884460, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.167426109313965, Accuracy = 0.8689696192741394\n",
      "Training iter #16668000:   Batch Loss = 7.948279, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.167153358459473, Accuracy = 0.8704586029052734\n",
      "Training iter #16671000:   Batch Loss = 7.922254, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.166947364807129, Accuracy = 0.8698630332946777\n",
      "Training iter #16674000:   Batch Loss = 7.711954, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.166873931884766, Accuracy = 0.8695651888847351\n",
      "Training iter #16677000:   Batch Loss = 7.819098, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.166560173034668, Accuracy = 0.8701608180999756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16680000:   Batch Loss = 7.866561, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.166388511657715, Accuracy = 0.8698630332946777\n",
      "Training iter #16683000:   Batch Loss = 8.020232, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.166255950927734, Accuracy = 0.8695651888847351\n",
      "Training iter #16686000:   Batch Loss = 7.820091, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.166206359863281, Accuracy = 0.8695651888847351\n",
      "Training iter #16689000:   Batch Loss = 7.747918, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.165875434875488, Accuracy = 0.8692674040794373\n",
      "Training iter #16692000:   Batch Loss = 7.908086, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.165434837341309, Accuracy = 0.8695651888847351\n",
      "Training iter #16695000:   Batch Loss = 7.937028, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.16507625579834, Accuracy = 0.8704586029052734\n",
      "Training iter #16698000:   Batch Loss = 7.901846, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.164766311645508, Accuracy = 0.8701608180999756\n",
      "Training iter #16701000:   Batch Loss = 7.715559, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.164709091186523, Accuracy = 0.8695651888847351\n",
      "Training iter #16704000:   Batch Loss = 7.830216, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.164088249206543, Accuracy = 0.8704586029052734\n",
      "Training iter #16707000:   Batch Loss = 7.861630, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.16376781463623, Accuracy = 0.8701608180999756\n",
      "Training iter #16710000:   Batch Loss = 8.009695, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.16340446472168, Accuracy = 0.8707563877105713\n",
      "Training iter #16713000:   Batch Loss = 7.796259, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.163250923156738, Accuracy = 0.8704586029052734\n",
      "Training iter #16716000:   Batch Loss = 7.743680, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.162870407104492, Accuracy = 0.8701608180999756\n",
      "Training iter #16719000:   Batch Loss = 7.903958, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.162322998046875, Accuracy = 0.8701608180999756\n",
      "Training iter #16722000:   Batch Loss = 7.926991, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.162093162536621, Accuracy = 0.8710541725158691\n",
      "Training iter #16725000:   Batch Loss = 7.926076, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.162211418151855, Accuracy = 0.8704586029052734\n",
      "Training iter #16728000:   Batch Loss = 7.725026, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.162301063537598, Accuracy = 0.8701608180999756\n",
      "Training iter #16731000:   Batch Loss = 7.834638, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.162013053894043, Accuracy = 0.8704586029052734\n",
      "Training iter #16734000:   Batch Loss = 7.859567, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.161473274230957, Accuracy = 0.8695651888847351\n",
      "Training iter #16737000:   Batch Loss = 8.031521, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.161186218261719, Accuracy = 0.8698630332946777\n",
      "Training iter #16740000:   Batch Loss = 7.773220, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.160992622375488, Accuracy = 0.8701608180999756\n",
      "Training iter #16743000:   Batch Loss = 7.736645, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.160733222961426, Accuracy = 0.8701608180999756\n",
      "Training iter #16746000:   Batch Loss = 7.906739, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.160443305969238, Accuracy = 0.8695651888847351\n",
      "Training iter #16749000:   Batch Loss = 7.921541, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.160202026367188, Accuracy = 0.8698630332946777\n",
      "Training iter #16752000:   Batch Loss = 7.911811, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.159969329833984, Accuracy = 0.8695651888847351\n",
      "Training iter #16755000:   Batch Loss = 7.732265, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.159780502319336, Accuracy = 0.8704586029052734\n",
      "Training iter #16758000:   Batch Loss = 7.830574, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.159046173095703, Accuracy = 0.8695651888847351\n",
      "Training iter #16761000:   Batch Loss = 7.864798, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.15895938873291, Accuracy = 0.8695651888847351\n",
      "Training iter #16764000:   Batch Loss = 8.016195, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.159029006958008, Accuracy = 0.8695651888847351\n",
      "Training iter #16767000:   Batch Loss = 7.747256, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.159100532531738, Accuracy = 0.8689696192741394\n",
      "Training iter #16770000:   Batch Loss = 7.731212, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.158811569213867, Accuracy = 0.8692674040794373\n",
      "Training iter #16773000:   Batch Loss = 7.912701, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.15825080871582, Accuracy = 0.8689696192741394\n",
      "Training iter #16776000:   Batch Loss = 7.910885, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.157817840576172, Accuracy = 0.8701608180999756\n",
      "Training iter #16779000:   Batch Loss = 7.900713, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.157631874084473, Accuracy = 0.8707563877105713\n",
      "Training iter #16782000:   Batch Loss = 7.748258, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.157617568969727, Accuracy = 0.8704586029052734\n",
      "Training iter #16785000:   Batch Loss = 7.824661, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.157378196716309, Accuracy = 0.8698630332946777\n",
      "Training iter #16788000:   Batch Loss = 7.894419, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.157180786132812, Accuracy = 0.8698630332946777\n",
      "Training iter #16791000:   Batch Loss = 7.993896, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.157003402709961, Accuracy = 0.8689696192741394\n",
      "Training iter #16794000:   Batch Loss = 7.725768, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.156755447387695, Accuracy = 0.8689696192741394\n",
      "Training iter #16797000:   Batch Loss = 7.729481, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.15636157989502, Accuracy = 0.8692674040794373\n",
      "Training iter #16800000:   Batch Loss = 7.897480, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.155991554260254, Accuracy = 0.8695651888847351\n",
      "Training iter #16803000:   Batch Loss = 7.929986, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.155891418457031, Accuracy = 0.8701608180999756\n",
      "Training iter #16806000:   Batch Loss = 7.915663, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.155756950378418, Accuracy = 0.8698630332946777\n",
      "Training iter #16809000:   Batch Loss = 7.765047, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.155428886413574, Accuracy = 0.8698630332946777\n",
      "Training iter #16812000:   Batch Loss = 7.824371, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.15520191192627, Accuracy = 0.8710541725158691\n",
      "Training iter #16815000:   Batch Loss = 7.895944, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.155138969421387, Accuracy = 0.8710541725158691\n",
      "Training iter #16818000:   Batch Loss = 7.949683, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.154820442199707, Accuracy = 0.8713520169258118\n",
      "Training iter #16821000:   Batch Loss = 7.701247, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.154473304748535, Accuracy = 0.8713520169258118\n",
      "Training iter #16824000:   Batch Loss = 7.763958, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.154122352600098, Accuracy = 0.8713520169258118\n",
      "Training iter #16827000:   Batch Loss = 7.866983, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.153862953186035, Accuracy = 0.8713520169258118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16830000:   Batch Loss = 7.944534, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.153599739074707, Accuracy = 0.8704586029052734\n",
      "Training iter #16833000:   Batch Loss = 7.929179, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.153454780578613, Accuracy = 0.8707563877105713\n",
      "Training iter #16836000:   Batch Loss = 7.766290, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.153166770935059, Accuracy = 0.8704586029052734\n",
      "Training iter #16839000:   Batch Loss = 7.822526, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.152904510498047, Accuracy = 0.8701608180999756\n",
      "Training iter #16842000:   Batch Loss = 7.896733, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.152688980102539, Accuracy = 0.8710541725158691\n",
      "Training iter #16845000:   Batch Loss = 7.946925, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.152373313903809, Accuracy = 0.8707563877105713\n",
      "Training iter #16848000:   Batch Loss = 7.692888, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.152060508728027, Accuracy = 0.8713520169258118\n",
      "Training iter #16851000:   Batch Loss = 7.774654, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.151784896850586, Accuracy = 0.8716498017311096\n",
      "Training iter #16854000:   Batch Loss = 7.862522, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.151472091674805, Accuracy = 0.8707563877105713\n",
      "Training iter #16857000:   Batch Loss = 7.941844, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.151201248168945, Accuracy = 0.8707563877105713\n",
      "Training iter #16860000:   Batch Loss = 7.880875, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.151087760925293, Accuracy = 0.8701608180999756\n",
      "Training iter #16863000:   Batch Loss = 7.760141, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.150965690612793, Accuracy = 0.8716498017311096\n",
      "Training iter #16866000:   Batch Loss = 7.852889, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.150794982910156, Accuracy = 0.8716498017311096\n",
      "Training iter #16869000:   Batch Loss = 7.905818, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.15058708190918, Accuracy = 0.8719475865364075\n",
      "Training iter #16872000:   Batch Loss = 7.934749, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.15025806427002, Accuracy = 0.8716498017311096\n",
      "Training iter #16875000:   Batch Loss = 7.699776, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.149652481079102, Accuracy = 0.8716498017311096\n",
      "Training iter #16878000:   Batch Loss = 7.770387, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.14901351928711, Accuracy = 0.8716498017311096\n",
      "Training iter #16881000:   Batch Loss = 7.827907, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.148488998413086, Accuracy = 0.8719475865364075\n",
      "Training iter #16884000:   Batch Loss = 7.950378, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.148050308227539, Accuracy = 0.8725431561470032\n",
      "Training iter #16887000:   Batch Loss = 7.863005, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1478910446167, Accuracy = 0.8725431561470032\n",
      "Training iter #16890000:   Batch Loss = 7.758273, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.147772789001465, Accuracy = 0.8728410005569458\n",
      "Training iter #16893000:   Batch Loss = 7.881427, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1475830078125, Accuracy = 0.8722453713417053\n",
      "Training iter #16896000:   Batch Loss = 7.922301, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.147247314453125, Accuracy = 0.8725431561470032\n",
      "Training iter #16899000:   Batch Loss = 7.935256, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146895408630371, Accuracy = 0.8725431561470032\n",
      "Training iter #16902000:   Batch Loss = 7.698524, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146668434143066, Accuracy = 0.8722453713417053\n",
      "Training iter #16905000:   Batch Loss = 7.774682, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146503448486328, Accuracy = 0.8725431561470032\n",
      "Training iter #16908000:   Batch Loss = 7.824210, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146450996398926, Accuracy = 0.8722453713417053\n",
      "Training iter #16911000:   Batch Loss = 7.948428, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146346092224121, Accuracy = 0.8722453713417053\n",
      "Training iter #16914000:   Batch Loss = 7.848814, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146490097045898, Accuracy = 0.8710541725158691\n",
      "Training iter #16917000:   Batch Loss = 7.741896, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146404266357422, Accuracy = 0.8722453713417053\n",
      "Training iter #16920000:   Batch Loss = 7.878456, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146291732788086, Accuracy = 0.8725431561470032\n",
      "Training iter #16923000:   Batch Loss = 7.914723, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.146060943603516, Accuracy = 0.8728410005569458\n",
      "Training iter #16926000:   Batch Loss = 7.931745, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.14564037322998, Accuracy = 0.8728410005569458\n",
      "Training iter #16929000:   Batch Loss = 7.703842, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.145316123962402, Accuracy = 0.8728410005569458\n",
      "Training iter #16932000:   Batch Loss = 7.776603, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.144844055175781, Accuracy = 0.8725431561470032\n",
      "Training iter #16935000:   Batch Loss = 7.806323, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.144458770751953, Accuracy = 0.8725431561470032\n",
      "Training iter #16938000:   Batch Loss = 7.967319, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.144217491149902, Accuracy = 0.8716498017311096\n",
      "Training iter #16941000:   Batch Loss = 7.830426, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.144026756286621, Accuracy = 0.8722453713417053\n",
      "Training iter #16944000:   Batch Loss = 7.740879, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.143790245056152, Accuracy = 0.8719475865364075\n",
      "Training iter #16947000:   Batch Loss = 7.851109, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.14357852935791, Accuracy = 0.8722453713417053\n",
      "Training iter #16950000:   Batch Loss = 7.919508, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.143342971801758, Accuracy = 0.8728410005569458\n",
      "Training iter #16953000:   Batch Loss = 7.913020, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.14295768737793, Accuracy = 0.8728410005569458\n",
      "Training iter #16956000:   Batch Loss = 7.700890, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.142669677734375, Accuracy = 0.8728410005569458\n",
      "Training iter #16959000:   Batch Loss = 7.795472, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.14234447479248, Accuracy = 0.8725431561470032\n",
      "Training iter #16962000:   Batch Loss = 7.835083, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.14207935333252, Accuracy = 0.8725431561470032\n",
      "Training iter #16965000:   Batch Loss = 7.960552, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.142077445983887, Accuracy = 0.8722453713417053\n",
      "Training iter #16968000:   Batch Loss = 7.820537, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.141995429992676, Accuracy = 0.8728410005569458\n",
      "Training iter #16971000:   Batch Loss = 7.722496, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.141687393188477, Accuracy = 0.8719475865364075\n",
      "Training iter #16974000:   Batch Loss = 7.858039, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.141441345214844, Accuracy = 0.8725431561470032\n",
      "Training iter #16977000:   Batch Loss = 7.923168, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.141298294067383, Accuracy = 0.8725431561470032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16980000:   Batch Loss = 7.867957, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.141037940979004, Accuracy = 0.8725431561470032\n",
      "Training iter #16983000:   Batch Loss = 7.709072, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.140890121459961, Accuracy = 0.8722453713417053\n",
      "Training iter #16986000:   Batch Loss = 7.796780, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1405668258667, Accuracy = 0.8725431561470032\n",
      "Training iter #16989000:   Batch Loss = 7.838789, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.140377044677734, Accuracy = 0.8725431561470032\n",
      "Training iter #16992000:   Batch Loss = 7.975182, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.140174865722656, Accuracy = 0.8719475865364075\n",
      "Training iter #16995000:   Batch Loss = 7.803026, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.140142440795898, Accuracy = 0.8719475865364075\n",
      "Training iter #16998000:   Batch Loss = 7.726112, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.139952659606934, Accuracy = 0.8713520169258118\n",
      "Training iter #17001000:   Batch Loss = 7.859652, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.139717102050781, Accuracy = 0.8728410005569458\n",
      "Training iter #17004000:   Batch Loss = 7.917223, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.139608383178711, Accuracy = 0.8731387853622437\n",
      "Training iter #17007000:   Batch Loss = 7.877453, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.139439582824707, Accuracy = 0.8731387853622437\n",
      "Training iter #17010000:   Batch Loss = 7.689998, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.139280319213867, Accuracy = 0.8731387853622437\n",
      "Training iter #17013000:   Batch Loss = 7.799677, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.138799667358398, Accuracy = 0.8731387853622437\n",
      "Training iter #17016000:   Batch Loss = 7.817360, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.13864803314209, Accuracy = 0.8728410005569458\n",
      "Training iter #17019000:   Batch Loss = 7.989505, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.138568878173828, Accuracy = 0.8725431561470032\n",
      "Training iter #17022000:   Batch Loss = 7.793897, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.138574600219727, Accuracy = 0.8722453713417053\n",
      "Training iter #17025000:   Batch Loss = 7.719003, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.138228416442871, Accuracy = 0.8725431561470032\n",
      "Training iter #17028000:   Batch Loss = 7.874603, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.137624740600586, Accuracy = 0.8740321397781372\n",
      "Training iter #17031000:   Batch Loss = 7.902460, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.137153625488281, Accuracy = 0.8743299841880798\n",
      "Training iter #17034000:   Batch Loss = 7.890772, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.136802673339844, Accuracy = 0.8740321397781372\n",
      "Training iter #17037000:   Batch Loss = 7.696269, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.136672973632812, Accuracy = 0.8734365701675415\n",
      "Training iter #17040000:   Batch Loss = 7.804476, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.136404037475586, Accuracy = 0.8737343549728394\n",
      "Training iter #17043000:   Batch Loss = 7.837032, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.136324882507324, Accuracy = 0.8728410005569458\n",
      "Training iter #17046000:   Batch Loss = 7.981847, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.136443138122559, Accuracy = 0.8722453713417053\n",
      "Training iter #17049000:   Batch Loss = 7.759452, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.136514663696289, Accuracy = 0.8725431561470032\n",
      "Training iter #17052000:   Batch Loss = 7.718543, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.13625431060791, Accuracy = 0.8728410005569458\n",
      "Training iter #17055000:   Batch Loss = 7.877707, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.135982513427734, Accuracy = 0.8731387853622437\n",
      "Training iter #17058000:   Batch Loss = 7.888047, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.13574504852295, Accuracy = 0.8737343549728394\n",
      "Training iter #17061000:   Batch Loss = 7.874536, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.135516166687012, Accuracy = 0.8740321397781372\n",
      "Training iter #17064000:   Batch Loss = 7.703452, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.135480880737305, Accuracy = 0.8737343549728394\n",
      "Training iter #17067000:   Batch Loss = 7.810110, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.13479995727539, Accuracy = 0.8725431561470032\n",
      "Training iter #17070000:   Batch Loss = 7.832977, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.134625434875488, Accuracy = 0.8722453713417053\n",
      "Training iter #17073000:   Batch Loss = 7.996069, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.134607315063477, Accuracy = 0.8728410005569458\n",
      "Training iter #17076000:   Batch Loss = 7.737909, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.134575843811035, Accuracy = 0.8722453713417053\n",
      "Training iter #17079000:   Batch Loss = 7.716514, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.13408088684082, Accuracy = 0.8725431561470032\n",
      "Training iter #17082000:   Batch Loss = 7.888828, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.133398056030273, Accuracy = 0.8734365701675415\n",
      "Training iter #17085000:   Batch Loss = 7.889895, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.133031845092773, Accuracy = 0.8740321397781372\n",
      "Training iter #17088000:   Batch Loss = 7.878959, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.132855415344238, Accuracy = 0.8737343549728394\n",
      "Training iter #17091000:   Batch Loss = 7.711984, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.132977485656738, Accuracy = 0.8737343549728394\n",
      "Training iter #17094000:   Batch Loss = 7.805812, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.132832527160645, Accuracy = 0.8743299841880798\n",
      "Training iter #17097000:   Batch Loss = 7.852593, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.132853507995605, Accuracy = 0.8734365701675415\n",
      "Training iter #17100000:   Batch Loss = 7.974705, Accuracy = 0.9086666703224182\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.133078575134277, Accuracy = 0.8728410005569458\n",
      "Training iter #17103000:   Batch Loss = 7.718982, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.133257865905762, Accuracy = 0.8734365701675415\n",
      "Training iter #17106000:   Batch Loss = 7.710830, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.13310718536377, Accuracy = 0.8731387853622437\n",
      "Training iter #17109000:   Batch Loss = 7.876228, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.13278579711914, Accuracy = 0.8734365701675415\n",
      "Training iter #17112000:   Batch Loss = 7.883267, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.132351875305176, Accuracy = 0.8734365701675415\n",
      "Training iter #17115000:   Batch Loss = 7.873429, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.131866455078125, Accuracy = 0.8731387853622437\n",
      "Training iter #17118000:   Batch Loss = 7.720442, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.131406784057617, Accuracy = 0.8731387853622437\n",
      "Training iter #17121000:   Batch Loss = 7.799543, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.130849838256836, Accuracy = 0.8734365701675415\n",
      "Training iter #17124000:   Batch Loss = 7.868106, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.130629539489746, Accuracy = 0.8746277689933777\n",
      "Training iter #17127000:   Batch Loss = 7.948631, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.130426406860352, Accuracy = 0.8743299841880798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #17130000:   Batch Loss = 7.695543, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.130035400390625, Accuracy = 0.8749255537986755\n",
      "Training iter #17133000:   Batch Loss = 7.717715, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.129525184631348, Accuracy = 0.8749255537986755\n",
      "Training iter #17136000:   Batch Loss = 7.844810, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.129246711730957, Accuracy = 0.8749255537986755\n",
      "Training iter #17139000:   Batch Loss = 7.916574, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.129076957702637, Accuracy = 0.8746277689933777\n",
      "Training iter #17142000:   Batch Loss = 7.893297, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.128907203674316, Accuracy = 0.8752233386039734\n",
      "Training iter #17145000:   Batch Loss = 7.737692, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.128698348999023, Accuracy = 0.8752233386039734\n",
      "Training iter #17148000:   Batch Loss = 7.787465, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.128332138061523, Accuracy = 0.8752233386039734\n",
      "Training iter #17151000:   Batch Loss = 7.862268, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.127976417541504, Accuracy = 0.8758189678192139\n",
      "Training iter #17154000:   Batch Loss = 7.909128, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.127828598022461, Accuracy = 0.8749255537986755\n",
      "Training iter #17157000:   Batch Loss = 7.672747, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.127947807312012, Accuracy = 0.8749255537986755\n",
      "Training iter #17160000:   Batch Loss = 7.739480, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.127923011779785, Accuracy = 0.8758189678192139\n",
      "Training iter #17163000:   Batch Loss = 7.832226, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.127767562866211, Accuracy = 0.8755211234092712\n",
      "Training iter #17166000:   Batch Loss = 7.907367, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.127553939819336, Accuracy = 0.8752233386039734\n",
      "Training iter #17169000:   Batch Loss = 7.886454, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.12745475769043, Accuracy = 0.8755211234092712\n",
      "Training iter #17172000:   Batch Loss = 7.742535, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.127238273620605, Accuracy = 0.8752233386039734\n",
      "Training iter #17175000:   Batch Loss = 7.788372, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.126607894897461, Accuracy = 0.8758189678192139\n",
      "Training iter #17178000:   Batch Loss = 7.869575, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.126163482666016, Accuracy = 0.8752233386039734\n",
      "Training iter #17181000:   Batch Loss = 7.922519, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.125639915466309, Accuracy = 0.8755211234092712\n",
      "Training iter #17184000:   Batch Loss = 7.674435, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.125328063964844, Accuracy = 0.8758189678192139\n",
      "Training iter #17187000:   Batch Loss = 7.748353, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.125104904174805, Accuracy = 0.8752233386039734\n",
      "Training iter #17190000:   Batch Loss = 7.807306, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.124699592590332, Accuracy = 0.8749255537986755\n",
      "Training iter #17193000:   Batch Loss = 7.929061, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.124700546264648, Accuracy = 0.8749255537986755\n",
      "Training iter #17196000:   Batch Loss = 7.839927, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.124739646911621, Accuracy = 0.8755211234092712\n",
      "Training iter #17199000:   Batch Loss = 7.731990, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.12458610534668, Accuracy = 0.8752233386039734\n",
      "Training iter #17202000:   Batch Loss = 7.838112, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.124234199523926, Accuracy = 0.8752233386039734\n",
      "Training iter #17205000:   Batch Loss = 7.876506, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.124127388000488, Accuracy = 0.8743299841880798\n",
      "Training iter #17208000:   Batch Loss = 7.914883, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.124115943908691, Accuracy = 0.8752233386039734\n",
      "Training iter #17211000:   Batch Loss = 7.667050, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.124168395996094, Accuracy = 0.8752233386039734\n",
      "Training iter #17214000:   Batch Loss = 7.738761, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.123779296875, Accuracy = 0.8755211234092712\n",
      "Training iter #17217000:   Batch Loss = 7.793828, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.123076438903809, Accuracy = 0.8758189678192139\n",
      "Training iter #17220000:   Batch Loss = 7.915849, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.122365951538086, Accuracy = 0.8758189678192139\n",
      "Training iter #17223000:   Batch Loss = 7.826147, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.122115135192871, Accuracy = 0.8758189678192139\n",
      "Training iter #17226000:   Batch Loss = 7.720961, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.122194290161133, Accuracy = 0.8758189678192139\n",
      "Training iter #17229000:   Batch Loss = 7.842035, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.121957778930664, Accuracy = 0.8755211234092712\n",
      "Training iter #17232000:   Batch Loss = 7.886319, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.121722221374512, Accuracy = 0.8758189678192139\n",
      "Training iter #17235000:   Batch Loss = 7.907467, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.121536254882812, Accuracy = 0.8749255537986755\n",
      "Training iter #17238000:   Batch Loss = 7.679743, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.121394157409668, Accuracy = 0.8746277689933777\n",
      "Training iter #17241000:   Batch Loss = 7.744803, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.121119499206543, Accuracy = 0.8755211234092712\n",
      "Training iter #17244000:   Batch Loss = 7.791523, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.120903968811035, Accuracy = 0.8755211234092712\n",
      "Training iter #17247000:   Batch Loss = 7.928505, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.120490074157715, Accuracy = 0.8752233386039734\n",
      "Training iter #17250000:   Batch Loss = 7.810883, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.120250701904297, Accuracy = 0.8752233386039734\n",
      "Training iter #17253000:   Batch Loss = 7.716336, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.12000846862793, Accuracy = 0.8743299841880798\n",
      "Training iter #17256000:   Batch Loss = 7.849121, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.119656562805176, Accuracy = 0.8746277689933777\n",
      "Training iter #17259000:   Batch Loss = 7.886937, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.119303703308105, Accuracy = 0.8746277689933777\n",
      "Training iter #17262000:   Batch Loss = 7.892129, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.11899471282959, Accuracy = 0.8746277689933777\n",
      "Training iter #17265000:   Batch Loss = 7.682607, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.118690490722656, Accuracy = 0.8740321397781372\n",
      "Training iter #17268000:   Batch Loss = 7.769273, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.118270874023438, Accuracy = 0.8746277689933777\n",
      "Training iter #17271000:   Batch Loss = 7.789645, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.118022918701172, Accuracy = 0.8746277689933777\n",
      "Training iter #17274000:   Batch Loss = 7.934772, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.117687225341797, Accuracy = 0.8737343549728394\n",
      "Training iter #17277000:   Batch Loss = 7.798536, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.117546081542969, Accuracy = 0.8734365701675415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #17280000:   Batch Loss = 7.719156, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.117874145507812, Accuracy = 0.8728410005569458\n",
      "Training iter #17283000:   Batch Loss = 7.817698, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.117616653442383, Accuracy = 0.8728410005569458\n",
      "Training iter #17286000:   Batch Loss = 7.901570, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.116554260253906, Accuracy = 0.8728410005569458\n",
      "Training iter #17289000:   Batch Loss = 7.873874, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.116365432739258, Accuracy = 0.8737343549728394\n",
      "Training iter #17292000:   Batch Loss = 7.688352, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.116366386413574, Accuracy = 0.8737343549728394\n",
      "Training iter #17295000:   Batch Loss = 7.770874, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.116109848022461, Accuracy = 0.8740321397781372\n",
      "Training iter #17298000:   Batch Loss = 7.811081, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.116052627563477, Accuracy = 0.8737343549728394\n",
      "Training iter #17301000:   Batch Loss = 7.946575, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.116012573242188, Accuracy = 0.8740321397781372\n",
      "Training iter #17304000:   Batch Loss = 7.789190, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.116026878356934, Accuracy = 0.8740321397781372\n",
      "Training iter #17307000:   Batch Loss = 7.704514, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.115972518920898, Accuracy = 0.8746277689933777\n",
      "Training iter #17310000:   Batch Loss = 7.828789, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.115781784057617, Accuracy = 0.8752233386039734\n",
      "Training iter #17313000:   Batch Loss = 7.885356, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.115564346313477, Accuracy = 0.8755211234092712\n",
      "Training iter #17316000:   Batch Loss = 7.856616, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.115385055541992, Accuracy = 0.8749255537986755\n",
      "Training iter #17319000:   Batch Loss = 7.671506, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.11523723602295, Accuracy = 0.8743299841880798\n",
      "Training iter #17322000:   Batch Loss = 7.770996, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.114936828613281, Accuracy = 0.8758189678192139\n",
      "Training iter #17325000:   Batch Loss = 7.810518, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.115274429321289, Accuracy = 0.8761167526245117\n",
      "Training iter #17328000:   Batch Loss = 7.951698, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.11215591430664, Accuracy = 0.8764145374298096\n",
      "Training iter #17331000:   Batch Loss = 7.770204, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.111692428588867, Accuracy = 0.8758189678192139\n",
      "Training iter #17334000:   Batch Loss = 7.706265, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.111371994018555, Accuracy = 0.8749255537986755\n",
      "Training iter #17337000:   Batch Loss = 7.851224, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.11101245880127, Accuracy = 0.8749255537986755\n",
      "Training iter #17340000:   Batch Loss = 7.880465, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.110565185546875, Accuracy = 0.8746277689933777\n",
      "Training iter #17343000:   Batch Loss = 7.845123, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.110164642333984, Accuracy = 0.8746277689933777\n",
      "Training iter #17346000:   Batch Loss = 7.672316, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109947204589844, Accuracy = 0.8743299841880798\n",
      "Training iter #17349000:   Batch Loss = 7.774615, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.10925579071045, Accuracy = 0.8740321397781372\n",
      "Training iter #17352000:   Batch Loss = 7.805622, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109018325805664, Accuracy = 0.8752233386039734\n",
      "Training iter #17355000:   Batch Loss = 7.955413, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.108903884887695, Accuracy = 0.8755211234092712\n",
      "Training iter #17358000:   Batch Loss = 7.760975, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.108871459960938, Accuracy = 0.8764145374298096\n",
      "Training iter #17361000:   Batch Loss = 7.695910, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.108794212341309, Accuracy = 0.8764145374298096\n",
      "Training iter #17364000:   Batch Loss = 7.847245, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.10872745513916, Accuracy = 0.8770101070404053\n",
      "Training iter #17367000:   Batch Loss = 7.865560, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109146118164062, Accuracy = 0.8767123222351074\n",
      "Training iter #17370000:   Batch Loss = 7.864930, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109529495239258, Accuracy = 0.8764145374298096\n",
      "Training iter #17373000:   Batch Loss = 7.676618, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109719276428223, Accuracy = 0.8764145374298096\n",
      "Training iter #17376000:   Batch Loss = 7.773516, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109594345092773, Accuracy = 0.8767123222351074\n",
      "Training iter #17379000:   Batch Loss = 7.813879, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109539031982422, Accuracy = 0.8773078918457031\n",
      "Training iter #17382000:   Batch Loss = 7.952518, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109411239624023, Accuracy = 0.8773078918457031\n",
      "Training iter #17385000:   Batch Loss = 7.729138, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.109193801879883, Accuracy = 0.8776057362556458\n",
      "Training iter #17388000:   Batch Loss = 7.698060, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.108697891235352, Accuracy = 0.8776057362556458\n",
      "Training iter #17391000:   Batch Loss = 7.850796, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.108162879943848, Accuracy = 0.8767123222351074\n",
      "Training iter #17394000:   Batch Loss = 7.861593, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.107833862304688, Accuracy = 0.8767123222351074\n",
      "Training iter #17397000:   Batch Loss = 7.849118, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.108077049255371, Accuracy = 0.8767123222351074\n",
      "Training iter #17400000:   Batch Loss = 7.682924, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.108353614807129, Accuracy = 0.8761167526245117\n",
      "Training iter #17403000:   Batch Loss = 7.781557, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.107812881469727, Accuracy = 0.8764145374298096\n",
      "Training iter #17406000:   Batch Loss = 7.803187, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.107428550720215, Accuracy = 0.8779035210609436\n",
      "Training iter #17409000:   Batch Loss = 7.956281, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.107032775878906, Accuracy = 0.8779035210609436\n",
      "Training iter #17412000:   Batch Loss = 7.705831, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.107020378112793, Accuracy = 0.8770101070404053\n",
      "Training iter #17415000:   Batch Loss = 7.690553, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.107016563415527, Accuracy = 0.8764145374298096\n",
      "Training iter #17418000:   Batch Loss = 7.859988, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.106795310974121, Accuracy = 0.8767123222351074\n",
      "Training iter #17421000:   Batch Loss = 7.857410, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.106555938720703, Accuracy = 0.8764145374298096\n",
      "Training iter #17424000:   Batch Loss = 7.840832, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.106547355651855, Accuracy = 0.8761167526245117\n",
      "Training iter #17427000:   Batch Loss = 7.701666, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.106674194335938, Accuracy = 0.8758189678192139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #17430000:   Batch Loss = 7.771502, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.106441497802734, Accuracy = 0.8758189678192139\n",
      "Training iter #17433000:   Batch Loss = 7.836234, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.106173515319824, Accuracy = 0.8764145374298096\n",
      "Training iter #17436000:   Batch Loss = 7.937289, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.105866432189941, Accuracy = 0.8764145374298096\n",
      "Training iter #17439000:   Batch Loss = 7.694373, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1056547164917, Accuracy = 0.8773078918457031\n",
      "Training iter #17442000:   Batch Loss = 7.687912, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.105415344238281, Accuracy = 0.8770101070404053\n",
      "Training iter #17445000:   Batch Loss = 7.847272, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.10507869720459, Accuracy = 0.8767123222351074\n",
      "Training iter #17448000:   Batch Loss = 7.863500, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.104581832885742, Accuracy = 0.8764145374298096\n",
      "Training iter #17451000:   Batch Loss = 7.853856, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.104299545288086, Accuracy = 0.8764145374298096\n",
      "Training iter #17454000:   Batch Loss = 7.715795, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.104166030883789, Accuracy = 0.8767123222351074\n",
      "Training iter #17457000:   Batch Loss = 7.765748, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.104022979736328, Accuracy = 0.8764145374298096\n",
      "Training iter #17460000:   Batch Loss = 7.841902, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.103827476501465, Accuracy = 0.8767123222351074\n",
      "Training iter #17463000:   Batch Loss = 7.891284, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.103487968444824, Accuracy = 0.8764145374298096\n",
      "Training iter #17466000:   Batch Loss = 7.670928, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.10331916809082, Accuracy = 0.8764145374298096\n",
      "Training iter #17469000:   Batch Loss = 7.704311, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.103059768676758, Accuracy = 0.8761167526245117\n",
      "Training iter #17472000:   Batch Loss = 7.818475, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.102545738220215, Accuracy = 0.8764145374298096\n",
      "Training iter #17475000:   Batch Loss = 7.887825, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.101639747619629, Accuracy = 0.8764145374298096\n",
      "Training iter #17478000:   Batch Loss = 7.868498, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.1016263961792, Accuracy = 0.8761167526245117\n",
      "Training iter #17481000:   Batch Loss = 7.715767, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.101447105407715, Accuracy = 0.8770101070404053\n",
      "Training iter #17484000:   Batch Loss = 7.781509, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.101265907287598, Accuracy = 0.8770101070404053\n",
      "Training iter #17487000:   Batch Loss = 7.844172, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.100964546203613, Accuracy = 0.8776057362556458\n",
      "Training iter #17490000:   Batch Loss = 7.886037, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.100586891174316, Accuracy = 0.8776057362556458\n",
      "Training iter #17493000:   Batch Loss = 7.652323, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.100464820861816, Accuracy = 0.8776057362556458\n",
      "Training iter #17496000:   Batch Loss = 7.720715, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.100371360778809, Accuracy = 0.8779035210609436\n",
      "Training iter #17499000:   Batch Loss = 7.809803, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.100062370300293, Accuracy = 0.8776057362556458\n",
      "Training iter #17502000:   Batch Loss = 7.875347, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.099620819091797, Accuracy = 0.8779035210609436\n",
      "Training iter #17505000:   Batch Loss = 7.837134, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.099292755126953, Accuracy = 0.8782013058662415\n",
      "Training iter #17508000:   Batch Loss = 7.716840, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.099212646484375, Accuracy = 0.8779035210609436\n",
      "Training iter #17511000:   Batch Loss = 7.790792, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.098987579345703, Accuracy = 0.8779035210609436\n",
      "Training iter #17514000:   Batch Loss = 7.838308, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.098605155944824, Accuracy = 0.8776057362556458\n",
      "Training iter #17517000:   Batch Loss = 7.882586, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.098570823669434, Accuracy = 0.8776057362556458\n",
      "Training iter #17520000:   Batch Loss = 7.663679, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.098812103271484, Accuracy = 0.8770101070404053\n",
      "Training iter #17523000:   Batch Loss = 7.722062, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.098593711853027, Accuracy = 0.8782013058662415\n",
      "Training iter #17526000:   Batch Loss = 7.794150, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.098368644714355, Accuracy = 0.8784990906715393\n",
      "Training iter #17529000:   Batch Loss = 7.891603, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0979642868042, Accuracy = 0.8782013058662415\n",
      "Training iter #17532000:   Batch Loss = 7.803092, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.097846031188965, Accuracy = 0.8782013058662415\n",
      "Training iter #17535000:   Batch Loss = 7.709661, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.097753524780273, Accuracy = 0.8779035210609436\n",
      "Training iter #17538000:   Batch Loss = 7.823367, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.097589492797852, Accuracy = 0.8779035210609436\n",
      "Training iter #17541000:   Batch Loss = 7.859556, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.097206115722656, Accuracy = 0.8784990906715393\n",
      "Training iter #17544000:   Batch Loss = 7.884385, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0968599319458, Accuracy = 0.8784990906715393\n",
      "Training iter #17547000:   Batch Loss = 7.656632, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.096963882446289, Accuracy = 0.8787968754768372\n",
      "Training iter #17550000:   Batch Loss = 7.722896, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.096823692321777, Accuracy = 0.8790947198867798\n",
      "Training iter #17553000:   Batch Loss = 7.773696, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.096457481384277, Accuracy = 0.8790947198867798\n",
      "Training iter #17556000:   Batch Loss = 7.888150, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.095873832702637, Accuracy = 0.8784990906715393\n",
      "Training iter #17559000:   Batch Loss = 7.792783, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.095827102661133, Accuracy = 0.8784990906715393\n",
      "Training iter #17562000:   Batch Loss = 7.698924, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.095900535583496, Accuracy = 0.8790947198867798\n",
      "Training iter #17565000:   Batch Loss = 7.816823, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.095830917358398, Accuracy = 0.8784990906715393\n",
      "Training iter #17568000:   Batch Loss = 7.853230, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.095541954040527, Accuracy = 0.8787968754768372\n",
      "Training iter #17571000:   Batch Loss = 7.875505, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.095329284667969, Accuracy = 0.8784990906715393\n",
      "Training iter #17574000:   Batch Loss = 7.661022, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.095223426818848, Accuracy = 0.8790947198867798\n",
      "Training iter #17577000:   Batch Loss = 7.727939, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.09516716003418, Accuracy = 0.8787968754768372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #17580000:   Batch Loss = 7.759308, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.094770431518555, Accuracy = 0.8796902894973755\n",
      "Training iter #17583000:   Batch Loss = 7.905737, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.094151496887207, Accuracy = 0.8796902894973755\n",
      "Training iter #17586000:   Batch Loss = 7.778684, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.093945503234863, Accuracy = 0.8796902894973755\n",
      "Training iter #17589000:   Batch Loss = 7.696157, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.093844413757324, Accuracy = 0.8799880743026733\n",
      "Training iter #17592000:   Batch Loss = 7.814889, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.09357738494873, Accuracy = 0.8799880743026733\n",
      "Training iter #17595000:   Batch Loss = 7.862837, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.093103408813477, Accuracy = 0.8796902894973755\n",
      "Training iter #17598000:   Batch Loss = 7.862333, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.092697143554688, Accuracy = 0.8790947198867798\n",
      "Training iter #17601000:   Batch Loss = 7.660493, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.092612266540527, Accuracy = 0.8784990906715393\n",
      "Training iter #17604000:   Batch Loss = 7.745455, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.092351913452148, Accuracy = 0.8793925046920776\n",
      "Training iter #17607000:   Batch Loss = 7.774322, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.091886520385742, Accuracy = 0.8793925046920776\n",
      "Training iter #17610000:   Batch Loss = 7.891953, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0912504196167, Accuracy = 0.8793925046920776\n",
      "Training iter #17613000:   Batch Loss = 7.765957, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.091076850891113, Accuracy = 0.8793925046920776\n",
      "Training iter #17616000:   Batch Loss = 7.678924, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.091019630432129, Accuracy = 0.8793925046920776\n",
      "Training iter #17619000:   Batch Loss = 7.794494, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.090956687927246, Accuracy = 0.8793925046920776\n",
      "Training iter #17622000:   Batch Loss = 7.862180, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.09066104888916, Accuracy = 0.8793925046920776\n",
      "Training iter #17625000:   Batch Loss = 7.823066, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.09054946899414, Accuracy = 0.8790947198867798\n",
      "Training iter #17628000:   Batch Loss = 7.664678, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.090547561645508, Accuracy = 0.8790947198867798\n",
      "Training iter #17631000:   Batch Loss = 7.750732, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.090404510498047, Accuracy = 0.8784990906715393\n",
      "Training iter #17634000:   Batch Loss = 7.788761, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.090084075927734, Accuracy = 0.8779035210609436\n",
      "Training iter #17637000:   Batch Loss = 7.911351, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.089826583862305, Accuracy = 0.8779035210609436\n",
      "Training iter #17640000:   Batch Loss = 7.752671, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.089627265930176, Accuracy = 0.8779035210609436\n",
      "Training iter #17643000:   Batch Loss = 7.682885, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.089374542236328, Accuracy = 0.8773078918457031\n",
      "Training iter #17646000:   Batch Loss = 7.800856, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.08913516998291, Accuracy = 0.8787968754768372\n",
      "Training iter #17649000:   Batch Loss = 7.859949, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.088897705078125, Accuracy = 0.8787968754768372\n",
      "Training iter #17652000:   Batch Loss = 7.833348, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.088563919067383, Accuracy = 0.8782013058662415\n",
      "Training iter #17655000:   Batch Loss = 7.647774, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.088441848754883, Accuracy = 0.8779035210609436\n",
      "Training iter #17658000:   Batch Loss = 7.752955, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.088272094726562, Accuracy = 0.8782013058662415\n",
      "Training iter #17661000:   Batch Loss = 7.766644, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.087930679321289, Accuracy = 0.8784990906715393\n",
      "Training iter #17664000:   Batch Loss = 7.924240, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.087685585021973, Accuracy = 0.8784990906715393\n",
      "Training iter #17667000:   Batch Loss = 7.745093, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.08762264251709, Accuracy = 0.8779035210609436\n",
      "Training iter #17670000:   Batch Loss = 7.677376, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.087358474731445, Accuracy = 0.8779035210609436\n",
      "Training iter #17673000:   Batch Loss = 7.821776, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.087397575378418, Accuracy = 0.8787968754768372\n",
      "Training iter #17676000:   Batch Loss = 7.844714, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.08729076385498, Accuracy = 0.8782013058662415\n",
      "Training iter #17679000:   Batch Loss = 7.818448, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.087251663208008, Accuracy = 0.8782013058662415\n",
      "Training iter #17682000:   Batch Loss = 7.649748, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.087210655212402, Accuracy = 0.8782013058662415\n",
      "Training iter #17685000:   Batch Loss = 7.757605, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.086939811706543, Accuracy = 0.8784990906715393\n",
      "Training iter #17688000:   Batch Loss = 7.782063, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0867280960083, Accuracy = 0.8779035210609436\n",
      "Training iter #17691000:   Batch Loss = 7.911643, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.085929870605469, Accuracy = 0.8782013058662415\n",
      "Training iter #17694000:   Batch Loss = 7.719767, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.085911750793457, Accuracy = 0.8787968754768372\n",
      "Training iter #17697000:   Batch Loss = 7.675061, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.085651397705078, Accuracy = 0.8787968754768372\n",
      "Training iter #17700000:   Batch Loss = 7.818401, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.085259437561035, Accuracy = 0.8805837035179138\n",
      "Training iter #17703000:   Batch Loss = 7.828312, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.08492374420166, Accuracy = 0.8805837035179138\n",
      "Training iter #17706000:   Batch Loss = 7.837042, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.084481239318848, Accuracy = 0.8799880743026733\n",
      "Training iter #17709000:   Batch Loss = 7.657893, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083760261535645, Accuracy = 0.8796902894973755\n",
      "Training iter #17712000:   Batch Loss = 7.754569, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083520889282227, Accuracy = 0.8796902894973755\n",
      "Training iter #17715000:   Batch Loss = 7.785030, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083608627319336, Accuracy = 0.8793925046920776\n",
      "Training iter #17718000:   Batch Loss = 7.933054, Accuracy = 0.9120000004768372\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083653450012207, Accuracy = 0.8799880743026733\n",
      "Training iter #17721000:   Batch Loss = 7.691396, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083662986755371, Accuracy = 0.8805837035179138\n",
      "Training iter #17724000:   Batch Loss = 7.666177, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083608627319336, Accuracy = 0.8799880743026733\n",
      "Training iter #17727000:   Batch Loss = 7.831567, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.08354377746582, Accuracy = 0.8799880743026733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #17730000:   Batch Loss = 7.834224, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083287239074707, Accuracy = 0.8793925046920776\n",
      "Training iter #17733000:   Batch Loss = 7.822354, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083106994628906, Accuracy = 0.8796902894973755\n",
      "Training iter #17736000:   Batch Loss = 7.666154, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083402633666992, Accuracy = 0.8790947198867798\n",
      "Training iter #17739000:   Batch Loss = 7.751065, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.083160400390625, Accuracy = 0.8784990906715393\n",
      "Training iter #17742000:   Batch Loss = 7.789546, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.082833290100098, Accuracy = 0.8779035210609436\n",
      "Training iter #17745000:   Batch Loss = 7.902812, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.082541465759277, Accuracy = 0.8784990906715393\n",
      "Training iter #17748000:   Batch Loss = 7.677292, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.082292556762695, Accuracy = 0.8790947198867798\n",
      "Training iter #17751000:   Batch Loss = 7.655913, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.081847190856934, Accuracy = 0.8790947198867798\n",
      "Training iter #17754000:   Batch Loss = 7.825599, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.081415176391602, Accuracy = 0.8805837035179138\n",
      "Training iter #17757000:   Batch Loss = 7.823920, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.081116676330566, Accuracy = 0.8805837035179138\n",
      "Training iter #17760000:   Batch Loss = 7.810330, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.081033706665039, Accuracy = 0.8805837035179138\n",
      "Training iter #17763000:   Batch Loss = 7.677805, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.080933570861816, Accuracy = 0.8802858591079712\n",
      "Training iter #17766000:   Batch Loss = 7.748292, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.080511093139648, Accuracy = 0.8796902894973755\n",
      "Training iter #17769000:   Batch Loss = 7.815871, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.080073356628418, Accuracy = 0.8811792731285095\n",
      "Training iter #17772000:   Batch Loss = 7.893303, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.080114364624023, Accuracy = 0.8802858591079712\n",
      "Training iter #17775000:   Batch Loss = 7.657533, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.080095291137695, Accuracy = 0.8799880743026733\n",
      "Training iter #17778000:   Batch Loss = 7.678257, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.079675674438477, Accuracy = 0.8790947198867798\n",
      "Training iter #17781000:   Batch Loss = 7.808224, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.079325675964355, Accuracy = 0.8790947198867798\n",
      "Training iter #17784000:   Batch Loss = 7.839970, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.078998565673828, Accuracy = 0.8799880743026733\n",
      "Training iter #17787000:   Batch Loss = 7.826106, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.078933715820312, Accuracy = 0.8799880743026733\n",
      "Training iter #17790000:   Batch Loss = 7.693688, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07852840423584, Accuracy = 0.8799880743026733\n",
      "Training iter #17793000:   Batch Loss = 7.748195, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.078113555908203, Accuracy = 0.8802858591079712\n",
      "Training iter #17796000:   Batch Loss = 7.811273, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.078428268432617, Accuracy = 0.8802858591079712\n",
      "Training iter #17799000:   Batch Loss = 7.853440, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.078540802001953, Accuracy = 0.8799880743026733\n",
      "Training iter #17802000:   Batch Loss = 7.633079, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.078128814697266, Accuracy = 0.8802858591079712\n",
      "Training iter #17805000:   Batch Loss = 7.691297, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.077930450439453, Accuracy = 0.8802858591079712\n",
      "Training iter #17808000:   Batch Loss = 7.777909, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.077677726745605, Accuracy = 0.8799880743026733\n",
      "Training iter #17811000:   Batch Loss = 7.851347, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07730770111084, Accuracy = 0.8799880743026733\n",
      "Training iter #17814000:   Batch Loss = 7.830510, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.077187538146973, Accuracy = 0.8802858591079712\n",
      "Training iter #17817000:   Batch Loss = 7.696740, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.077340126037598, Accuracy = 0.8799880743026733\n",
      "Training iter #17820000:   Batch Loss = 7.747685, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.076801300048828, Accuracy = 0.8802858591079712\n",
      "Training iter #17823000:   Batch Loss = 7.809919, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.076072692871094, Accuracy = 0.8808814883232117\n",
      "Training iter #17826000:   Batch Loss = 7.867309, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.075511932373047, Accuracy = 0.8802858591079712\n",
      "Training iter #17829000:   Batch Loss = 7.632588, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074913024902344, Accuracy = 0.8811792731285095\n",
      "Training iter #17832000:   Batch Loss = 7.701850, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07453441619873, Accuracy = 0.8817748427391052\n",
      "Training iter #17835000:   Batch Loss = 7.783677, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074281692504883, Accuracy = 0.8817748427391052\n",
      "Training iter #17838000:   Batch Loss = 7.866715, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074421882629395, Accuracy = 0.8817748427391052\n",
      "Training iter #17841000:   Batch Loss = 7.789431, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074554443359375, Accuracy = 0.8814770579338074\n",
      "Training iter #17844000:   Batch Loss = 7.682888, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074626922607422, Accuracy = 0.8811792731285095\n",
      "Training iter #17847000:   Batch Loss = 7.775180, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074518203735352, Accuracy = 0.8811792731285095\n",
      "Training iter #17850000:   Batch Loss = 7.817576, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074323654174805, Accuracy = 0.8805837035179138\n",
      "Training iter #17853000:   Batch Loss = 7.844284, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.074074745178223, Accuracy = 0.8808814883232117\n",
      "Training iter #17856000:   Batch Loss = 7.636518, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07361888885498, Accuracy = 0.8811792731285095\n",
      "Training iter #17859000:   Batch Loss = 7.697828, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.073076248168945, Accuracy = 0.8817748427391052\n",
      "Training iter #17862000:   Batch Loss = 7.751163, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.072229385375977, Accuracy = 0.8814770579338074\n",
      "Training iter #17865000:   Batch Loss = 7.857351, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071621894836426, Accuracy = 0.8808814883232117\n",
      "Training iter #17868000:   Batch Loss = 7.775462, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071727752685547, Accuracy = 0.8808814883232117\n",
      "Training iter #17871000:   Batch Loss = 7.686035, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071708679199219, Accuracy = 0.8808814883232117\n",
      "Training iter #17874000:   Batch Loss = 7.788917, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.072500228881836, Accuracy = 0.8808814883232117\n",
      "Training iter #17877000:   Batch Loss = 7.829912, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07302474975586, Accuracy = 0.8802858591079712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #17880000:   Batch Loss = 7.851162, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.072997093200684, Accuracy = 0.8796902894973755\n",
      "Training iter #17883000:   Batch Loss = 7.635350, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.072796821594238, Accuracy = 0.8805837035179138\n",
      "Training iter #17886000:   Batch Loss = 7.698912, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07229995727539, Accuracy = 0.8805837035179138\n",
      "Training iter #17889000:   Batch Loss = 7.748868, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.072094917297363, Accuracy = 0.8799880743026733\n",
      "Training iter #17892000:   Batch Loss = 7.861579, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.072258949279785, Accuracy = 0.8787968754768372\n",
      "Training iter #17895000:   Batch Loss = 7.758066, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07228946685791, Accuracy = 0.8782013058662415\n",
      "Training iter #17898000:   Batch Loss = 7.672821, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.072111129760742, Accuracy = 0.8793925046920776\n",
      "Training iter #17901000:   Batch Loss = 7.790522, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071829795837402, Accuracy = 0.8790947198867798\n",
      "Training iter #17904000:   Batch Loss = 7.826066, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071552276611328, Accuracy = 0.8793925046920776\n",
      "Training iter #17907000:   Batch Loss = 7.836661, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07127571105957, Accuracy = 0.8796902894973755\n",
      "Training iter #17910000:   Batch Loss = 7.640931, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071399688720703, Accuracy = 0.8799880743026733\n",
      "Training iter #17913000:   Batch Loss = 7.701202, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.07140827178955, Accuracy = 0.8799880743026733\n",
      "Training iter #17916000:   Batch Loss = 7.735824, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071277618408203, Accuracy = 0.8799880743026733\n",
      "Training iter #17919000:   Batch Loss = 7.864695, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071338653564453, Accuracy = 0.8793925046920776\n",
      "Training iter #17922000:   Batch Loss = 7.746877, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.071046829223633, Accuracy = 0.8796902894973755\n",
      "Training iter #17925000:   Batch Loss = 7.674381, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.070631980895996, Accuracy = 0.8796902894973755\n",
      "Training iter #17928000:   Batch Loss = 7.767311, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.069822311401367, Accuracy = 0.8793925046920776\n",
      "Training iter #17931000:   Batch Loss = 7.835603, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.068862915039062, Accuracy = 0.8793925046920776\n",
      "Training iter #17934000:   Batch Loss = 7.822291, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.068201065063477, Accuracy = 0.8790947198867798\n",
      "Training iter #17937000:   Batch Loss = 7.643988, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06794261932373, Accuracy = 0.8790947198867798\n",
      "Training iter #17940000:   Batch Loss = 7.723418, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.067625045776367, Accuracy = 0.8793925046920776\n",
      "Training iter #17943000:   Batch Loss = 7.762550, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.069388389587402, Accuracy = 0.8793925046920776\n",
      "Training iter #17946000:   Batch Loss = 7.875293, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.069263458251953, Accuracy = 0.8784990906715393\n",
      "Training iter #17949000:   Batch Loss = 7.736100, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.068666458129883, Accuracy = 0.8787968754768372\n",
      "Training iter #17952000:   Batch Loss = 7.656747, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06846809387207, Accuracy = 0.8784990906715393\n",
      "Training iter #17955000:   Batch Loss = 7.770054, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.067784309387207, Accuracy = 0.8782013058662415\n",
      "Training iter #17958000:   Batch Loss = 7.828282, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.068025588989258, Accuracy = 0.8776057362556458\n",
      "Training iter #17961000:   Batch Loss = 7.797192, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.068033218383789, Accuracy = 0.8773078918457031\n",
      "Training iter #17964000:   Batch Loss = 7.635790, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.068005561828613, Accuracy = 0.8773078918457031\n",
      "Training iter #17967000:   Batch Loss = 7.724704, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.067408561706543, Accuracy = 0.8782013058662415\n",
      "Training iter #17970000:   Batch Loss = 7.761306, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.066935539245605, Accuracy = 0.8793925046920776\n",
      "Training iter #17973000:   Batch Loss = 7.877736, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.067341804504395, Accuracy = 0.8793925046920776\n",
      "Training iter #17976000:   Batch Loss = 7.721968, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.066762924194336, Accuracy = 0.8796902894973755\n",
      "Training iter #17979000:   Batch Loss = 7.661932, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.065707206726074, Accuracy = 0.8796902894973755\n",
      "Training iter #17982000:   Batch Loss = 7.777704, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064410209655762, Accuracy = 0.8802858591079712\n",
      "Training iter #17985000:   Batch Loss = 7.826578, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06432819366455, Accuracy = 0.8799880743026733\n",
      "Training iter #17988000:   Batch Loss = 7.791350, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064505577087402, Accuracy = 0.8799880743026733\n",
      "Training iter #17991000:   Batch Loss = 7.628411, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064752578735352, Accuracy = 0.8799880743026733\n",
      "Training iter #17994000:   Batch Loss = 7.725960, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064759254455566, Accuracy = 0.8808814883232117\n",
      "Training iter #17997000:   Batch Loss = 7.746338, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06484603881836, Accuracy = 0.8805837035179138\n",
      "Training iter #18000000:   Batch Loss = 7.892448, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064644813537598, Accuracy = 0.8805837035179138\n",
      "Training iter #18003000:   Batch Loss = 7.716076, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064597129821777, Accuracy = 0.8802858591079712\n",
      "Training iter #18006000:   Batch Loss = 7.653840, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064483642578125, Accuracy = 0.8805837035179138\n",
      "Training iter #18009000:   Batch Loss = 7.786109, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06429672241211, Accuracy = 0.8808814883232117\n",
      "Training iter #18012000:   Batch Loss = 7.811808, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06421947479248, Accuracy = 0.8805837035179138\n",
      "Training iter #18015000:   Batch Loss = 7.807826, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.064080238342285, Accuracy = 0.8805837035179138\n",
      "Training iter #18018000:   Batch Loss = 7.630429, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.063838958740234, Accuracy = 0.8802858591079712\n",
      "Training iter #18021000:   Batch Loss = 7.727324, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.063627243041992, Accuracy = 0.8805837035179138\n",
      "Training iter #18024000:   Batch Loss = 7.762337, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.063483238220215, Accuracy = 0.8805837035179138\n",
      "Training iter #18027000:   Batch Loss = 7.889082, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.063247680664062, Accuracy = 0.8799880743026733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18030000:   Batch Loss = 7.682905, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.063178062438965, Accuracy = 0.8796902894973755\n",
      "Training iter #18033000:   Batch Loss = 7.652055, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.062970161437988, Accuracy = 0.8802858591079712\n",
      "Training iter #18036000:   Batch Loss = 7.789915, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06258773803711, Accuracy = 0.8802858591079712\n",
      "Training iter #18039000:   Batch Loss = 7.809766, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.061957359313965, Accuracy = 0.8808814883232117\n",
      "Training iter #18042000:   Batch Loss = 7.792659, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.061372756958008, Accuracy = 0.8805837035179138\n",
      "Training iter #18045000:   Batch Loss = 7.641159, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.061136245727539, Accuracy = 0.8805837035179138\n",
      "Training iter #18048000:   Batch Loss = 7.733269, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.060707092285156, Accuracy = 0.8808814883232117\n",
      "Training iter #18051000:   Batch Loss = 7.756401, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.060341835021973, Accuracy = 0.8805837035179138\n",
      "Training iter #18054000:   Batch Loss = 7.892693, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.06007194519043, Accuracy = 0.8793925046920776\n",
      "Training iter #18057000:   Batch Loss = 7.662901, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.059982299804688, Accuracy = 0.8796902894973755\n",
      "Training iter #18060000:   Batch Loss = 7.646483, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.059741973876953, Accuracy = 0.8799880743026733\n",
      "Training iter #18063000:   Batch Loss = 7.806102, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.059403419494629, Accuracy = 0.8811792731285095\n",
      "Training iter #18066000:   Batch Loss = 7.801989, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.059123039245605, Accuracy = 0.8805837035179138\n",
      "Training iter #18069000:   Batch Loss = 7.792181, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.059014320373535, Accuracy = 0.8802858591079712\n",
      "Training iter #18072000:   Batch Loss = 7.656169, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.059045791625977, Accuracy = 0.8802858591079712\n",
      "Training iter #18075000:   Batch Loss = 7.732194, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.05892276763916, Accuracy = 0.8802858591079712\n",
      "Training iter #18078000:   Batch Loss = 7.780506, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.058745384216309, Accuracy = 0.8799880743026733\n",
      "Training iter #18081000:   Batch Loss = 7.876763, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.058499336242676, Accuracy = 0.8796902894973755\n",
      "Training iter #18084000:   Batch Loss = 7.652221, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.058440208435059, Accuracy = 0.8799880743026733\n",
      "Training iter #18087000:   Batch Loss = 7.646111, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.05831527709961, Accuracy = 0.8805837035179138\n",
      "Training iter #18090000:   Batch Loss = 7.793047, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.058100700378418, Accuracy = 0.8802858591079712\n",
      "Training iter #18093000:   Batch Loss = 7.799757, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.057781219482422, Accuracy = 0.8805837035179138\n",
      "Training iter #18096000:   Batch Loss = 7.793249, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.057543754577637, Accuracy = 0.8808814883232117\n",
      "Training iter #18099000:   Batch Loss = 7.663318, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.057473182678223, Accuracy = 0.8808814883232117\n",
      "Training iter #18102000:   Batch Loss = 7.731574, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.057225227355957, Accuracy = 0.8808814883232117\n",
      "Training iter #18105000:   Batch Loss = 7.783547, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.056921005249023, Accuracy = 0.8808814883232117\n",
      "Training iter #18108000:   Batch Loss = 7.851905, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.056872367858887, Accuracy = 0.8805837035179138\n",
      "Training iter #18111000:   Batch Loss = 7.630059, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.05689525604248, Accuracy = 0.8802858591079712\n",
      "Training iter #18114000:   Batch Loss = 7.652957, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.056683540344238, Accuracy = 0.8802858591079712\n",
      "Training iter #18117000:   Batch Loss = 7.766833, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.056358337402344, Accuracy = 0.8799880743026733\n",
      "Training iter #18120000:   Batch Loss = 7.832985, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.056116104125977, Accuracy = 0.8799880743026733\n",
      "Training iter #18123000:   Batch Loss = 7.812783, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.055523872375488, Accuracy = 0.8799880743026733\n",
      "Training iter #18126000:   Batch Loss = 7.670278, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.055352210998535, Accuracy = 0.8799880743026733\n",
      "Training iter #18129000:   Batch Loss = 7.731325, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.055253982543945, Accuracy = 0.8796902894973755\n",
      "Training iter #18132000:   Batch Loss = 7.780187, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.055103302001953, Accuracy = 0.8796902894973755\n",
      "Training iter #18135000:   Batch Loss = 7.826202, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.055107116699219, Accuracy = 0.8796902894973755\n",
      "Training iter #18138000:   Batch Loss = 7.612541, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.054965019226074, Accuracy = 0.8799880743026733\n",
      "Training iter #18141000:   Batch Loss = 7.672108, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.054689407348633, Accuracy = 0.8799880743026733\n",
      "Training iter #18144000:   Batch Loss = 7.759229, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.054465293884277, Accuracy = 0.8796902894973755\n",
      "Training iter #18147000:   Batch Loss = 7.822433, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.05430793762207, Accuracy = 0.8793925046920776\n",
      "Training iter #18150000:   Batch Loss = 7.790340, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.054200172424316, Accuracy = 0.8790947198867798\n",
      "Training iter #18153000:   Batch Loss = 7.673559, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.054001808166504, Accuracy = 0.8796902894973755\n",
      "Training iter #18156000:   Batch Loss = 7.722950, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.053622245788574, Accuracy = 0.8799880743026733\n",
      "Training iter #18159000:   Batch Loss = 7.779549, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.053311347961426, Accuracy = 0.8796902894973755\n",
      "Training iter #18162000:   Batch Loss = 7.832462, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.053059577941895, Accuracy = 0.8799880743026733\n",
      "Training iter #18165000:   Batch Loss = 7.621567, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.052894592285156, Accuracy = 0.8802858591079712\n",
      "Training iter #18168000:   Batch Loss = 7.681170, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.052629470825195, Accuracy = 0.8796902894973755\n",
      "Training iter #18171000:   Batch Loss = 7.737007, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.052321434020996, Accuracy = 0.8802858591079712\n",
      "Training iter #18174000:   Batch Loss = 7.836421, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.052087783813477, Accuracy = 0.8799880743026733\n",
      "Training iter #18177000:   Batch Loss = 7.762258, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.052258491516113, Accuracy = 0.8793925046920776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18180000:   Batch Loss = 7.664443, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.052234649658203, Accuracy = 0.8805837035179138\n",
      "Training iter #18183000:   Batch Loss = 7.766107, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.052002906799316, Accuracy = 0.8802858591079712\n",
      "Training iter #18186000:   Batch Loss = 7.790651, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051787376403809, Accuracy = 0.8805837035179138\n",
      "Training iter #18189000:   Batch Loss = 7.826963, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051650047302246, Accuracy = 0.8802858591079712\n",
      "Training iter #18192000:   Batch Loss = 7.608654, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051594734191895, Accuracy = 0.8802858591079712\n",
      "Training iter #18195000:   Batch Loss = 7.675603, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051365852355957, Accuracy = 0.8802858591079712\n",
      "Training iter #18198000:   Batch Loss = 7.723652, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.05126667022705, Accuracy = 0.8802858591079712\n",
      "Training iter #18201000:   Batch Loss = 7.828612, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051116943359375, Accuracy = 0.8802858591079712\n",
      "Training iter #18204000:   Batch Loss = 7.741878, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051259994506836, Accuracy = 0.8805837035179138\n",
      "Training iter #18207000:   Batch Loss = 7.656541, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051316261291504, Accuracy = 0.8805837035179138\n",
      "Training iter #18210000:   Batch Loss = 7.762631, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.051068305969238, Accuracy = 0.8808814883232117\n",
      "Training iter #18213000:   Batch Loss = 7.793845, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.050756454467773, Accuracy = 0.8802858591079712\n",
      "Training iter #18216000:   Batch Loss = 7.819379, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.050429344177246, Accuracy = 0.8802858591079712\n",
      "Training iter #18219000:   Batch Loss = 7.622179, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.05012321472168, Accuracy = 0.8802858591079712\n",
      "Training iter #18222000:   Batch Loss = 7.678621, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.049610137939453, Accuracy = 0.8811792731285095\n",
      "Training iter #18225000:   Batch Loss = 7.713654, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.049298286437988, Accuracy = 0.8805837035179138\n",
      "Training iter #18228000:   Batch Loss = 7.837174, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.049078941345215, Accuracy = 0.8808814883232117\n",
      "Training iter #18231000:   Batch Loss = 7.725751, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.047656059265137, Accuracy = 0.8805837035179138\n",
      "Training iter #18234000:   Batch Loss = 7.655567, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.047662734985352, Accuracy = 0.8802858591079712\n",
      "Training iter #18237000:   Batch Loss = 7.763876, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.048276901245117, Accuracy = 0.8808814883232117\n",
      "Training iter #18240000:   Batch Loss = 7.798103, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.048063278198242, Accuracy = 0.8808814883232117\n",
      "Training iter #18243000:   Batch Loss = 7.809228, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.047852516174316, Accuracy = 0.8811792731285095\n",
      "Training iter #18246000:   Batch Loss = 7.618558, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.047935485839844, Accuracy = 0.8811792731285095\n",
      "Training iter #18249000:   Batch Loss = 7.701761, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.047860145568848, Accuracy = 0.8811792731285095\n",
      "Training iter #18252000:   Batch Loss = 7.725329, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.047257423400879, Accuracy = 0.8808814883232117\n",
      "Training iter #18255000:   Batch Loss = 7.845375, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.046708106994629, Accuracy = 0.8802858591079712\n",
      "Training iter #18258000:   Batch Loss = 7.714206, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.046711921691895, Accuracy = 0.8805837035179138\n",
      "Training iter #18261000:   Batch Loss = 7.652781, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.046606063842773, Accuracy = 0.8805837035179138\n",
      "Training iter #18264000:   Batch Loss = 7.738024, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.046353340148926, Accuracy = 0.8814770579338074\n",
      "Training iter #18267000:   Batch Loss = 7.804644, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.046004295349121, Accuracy = 0.8811792731285095\n",
      "Training iter #18270000:   Batch Loss = 7.779432, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045793533325195, Accuracy = 0.8811792731285095\n",
      "Training iter #18273000:   Batch Loss = 7.624323, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045836448669434, Accuracy = 0.8805837035179138\n",
      "Training iter #18276000:   Batch Loss = 7.703676, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045597076416016, Accuracy = 0.8811792731285095\n",
      "Training iter #18279000:   Batch Loss = 7.732804, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045280456542969, Accuracy = 0.8808814883232117\n",
      "Training iter #18282000:   Batch Loss = 7.854964, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045034408569336, Accuracy = 0.8808814883232117\n",
      "Training iter #18285000:   Batch Loss = 7.718033, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045112609863281, Accuracy = 0.8808814883232117\n",
      "Training iter #18288000:   Batch Loss = 7.641550, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045051574707031, Accuracy = 0.8802858591079712\n",
      "Training iter #18291000:   Batch Loss = 7.747418, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.046560287475586, Accuracy = 0.8802858591079712\n",
      "Training iter #18294000:   Batch Loss = 7.799942, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.04616641998291, Accuracy = 0.8805837035179138\n",
      "Training iter #18297000:   Batch Loss = 7.781574, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045982360839844, Accuracy = 0.8802858591079712\n",
      "Training iter #18300000:   Batch Loss = 7.605054, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045838356018066, Accuracy = 0.8802858591079712\n",
      "Training iter #18303000:   Batch Loss = 7.700018, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045539855957031, Accuracy = 0.8802858591079712\n",
      "Training iter #18306000:   Batch Loss = 7.739031, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.045119285583496, Accuracy = 0.8802858591079712\n",
      "Training iter #18309000:   Batch Loss = 7.860154, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.044512748718262, Accuracy = 0.8802858591079712\n",
      "Training iter #18312000:   Batch Loss = 7.695911, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.043999671936035, Accuracy = 0.8802858591079712\n",
      "Training iter #18315000:   Batch Loss = 7.636462, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.043861389160156, Accuracy = 0.8802858591079712\n",
      "Training iter #18318000:   Batch Loss = 7.767108, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.043498039245605, Accuracy = 0.8802858591079712\n",
      "Training iter #18321000:   Batch Loss = 7.793065, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.04318618774414, Accuracy = 0.8802858591079712\n",
      "Training iter #18324000:   Batch Loss = 7.761202, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.041592597961426, Accuracy = 0.8805837035179138\n",
      "Training iter #18327000:   Batch Loss = 7.609182, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.041474342346191, Accuracy = 0.8805837035179138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18330000:   Batch Loss = 7.706680, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.041250228881836, Accuracy = 0.8808814883232117\n",
      "Training iter #18333000:   Batch Loss = 7.732124, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.040748596191406, Accuracy = 0.8811792731285095\n",
      "Training iter #18336000:   Batch Loss = 7.853337, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.040481567382812, Accuracy = 0.8805837035179138\n",
      "Training iter #18339000:   Batch Loss = 7.676334, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.041969299316406, Accuracy = 0.8799880743026733\n",
      "Training iter #18342000:   Batch Loss = 7.631121, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.041706085205078, Accuracy = 0.8805837035179138\n",
      "Training iter #18345000:   Batch Loss = 7.763545, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.041016578674316, Accuracy = 0.8811792731285095\n",
      "Training iter #18348000:   Batch Loss = 7.778629, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.04052734375, Accuracy = 0.8814770579338074\n",
      "Training iter #18351000:   Batch Loss = 7.781489, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.040446281433105, Accuracy = 0.8802858591079712\n",
      "Training iter #18354000:   Batch Loss = 7.614824, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.040616989135742, Accuracy = 0.8802858591079712\n",
      "Training iter #18357000:   Batch Loss = 7.709558, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.040602684020996, Accuracy = 0.8799880743026733\n",
      "Training iter #18360000:   Batch Loss = 7.729198, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.040191650390625, Accuracy = 0.8802858591079712\n",
      "Training iter #18363000:   Batch Loss = 7.870974, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039973258972168, Accuracy = 0.8802858591079712\n",
      "Training iter #18366000:   Batch Loss = 7.653190, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039860725402832, Accuracy = 0.8802858591079712\n",
      "Training iter #18369000:   Batch Loss = 7.623892, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.04010009765625, Accuracy = 0.8802858591079712\n",
      "Training iter #18372000:   Batch Loss = 7.766869, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039722442626953, Accuracy = 0.8799880743026733\n",
      "Training iter #18375000:   Batch Loss = 7.777740, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039538383483887, Accuracy = 0.8799880743026733\n",
      "Training iter #18378000:   Batch Loss = 7.767905, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039658546447754, Accuracy = 0.8799880743026733\n",
      "Training iter #18381000:   Batch Loss = 7.621972, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039831161499023, Accuracy = 0.8793925046920776\n",
      "Training iter #18384000:   Batch Loss = 7.703566, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.03966236114502, Accuracy = 0.8805837035179138\n",
      "Training iter #18387000:   Batch Loss = 7.734303, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039372444152832, Accuracy = 0.8805837035179138\n",
      "Training iter #18390000:   Batch Loss = 7.859608, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039079666137695, Accuracy = 0.8805837035179138\n",
      "Training iter #18393000:   Batch Loss = 7.634099, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039095878601074, Accuracy = 0.8805837035179138\n",
      "Training iter #18396000:   Batch Loss = 7.617972, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.039180755615234, Accuracy = 0.8805837035179138\n",
      "Training iter #18399000:   Batch Loss = 7.772099, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.038662910461426, Accuracy = 0.8805837035179138\n",
      "Training iter #18402000:   Batch Loss = 7.770225, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.038458824157715, Accuracy = 0.8802858591079712\n",
      "Training iter #18405000:   Batch Loss = 7.758037, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.03624439239502, Accuracy = 0.8805837035179138\n",
      "Training iter #18408000:   Batch Loss = 7.635211, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.03577995300293, Accuracy = 0.8799880743026733\n",
      "Training iter #18411000:   Batch Loss = 7.700589, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.035577774047852, Accuracy = 0.8808814883232117\n",
      "Training iter #18414000:   Batch Loss = 7.761449, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.035974502563477, Accuracy = 0.8802858591079712\n",
      "Training iter #18417000:   Batch Loss = 7.839499, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.036195755004883, Accuracy = 0.8802858591079712\n",
      "Training iter #18420000:   Batch Loss = 7.617980, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.037612915039062, Accuracy = 0.8805837035179138\n",
      "Training iter #18423000:   Batch Loss = 7.620780, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.037738800048828, Accuracy = 0.8811792731285095\n",
      "Training iter #18426000:   Batch Loss = 7.760846, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.03759479522705, Accuracy = 0.8805837035179138\n",
      "Training iter #18429000:   Batch Loss = 7.786732, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.036772727966309, Accuracy = 0.8808814883232117\n",
      "Training iter #18432000:   Batch Loss = 7.770712, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.035989761352539, Accuracy = 0.8805837035179138\n",
      "Training iter #18435000:   Batch Loss = 7.652791, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.035369873046875, Accuracy = 0.8817748427391052\n",
      "Training iter #18438000:   Batch Loss = 7.703346, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.035593032836914, Accuracy = 0.8820726871490479\n",
      "Training iter #18441000:   Batch Loss = 7.758466, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.035308837890625, Accuracy = 0.8817748427391052\n",
      "Training iter #18444000:   Batch Loss = 7.802225, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.034808158874512, Accuracy = 0.8811792731285095\n",
      "Training iter #18447000:   Batch Loss = 7.600724, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.034940719604492, Accuracy = 0.8811792731285095\n",
      "Training iter #18450000:   Batch Loss = 7.648271, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.035175323486328, Accuracy = 0.8805837035179138\n",
      "Training iter #18453000:   Batch Loss = 7.740482, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.033273696899414, Accuracy = 0.8802858591079712\n",
      "Training iter #18456000:   Batch Loss = 7.801933, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.033150672912598, Accuracy = 0.8799880743026733\n",
      "Training iter #18459000:   Batch Loss = 7.780953, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.033159255981445, Accuracy = 0.8808814883232117\n",
      "Training iter #18462000:   Batch Loss = 7.654084, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.032939910888672, Accuracy = 0.8817748427391052\n",
      "Training iter #18465000:   Batch Loss = 7.699246, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.032890319824219, Accuracy = 0.8820726871490479\n",
      "Training iter #18468000:   Batch Loss = 7.753895, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.031381607055664, Accuracy = 0.8820726871490479\n",
      "Training iter #18471000:   Batch Loss = 7.800991, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.030085563659668, Accuracy = 0.8823704719543457\n",
      "Training iter #18474000:   Batch Loss = 7.592680, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02978515625, Accuracy = 0.8820726871490479\n",
      "Training iter #18477000:   Batch Loss = 7.662429, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.029549598693848, Accuracy = 0.8817748427391052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18480000:   Batch Loss = 7.741889, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.029565811157227, Accuracy = 0.8805837035179138\n",
      "Training iter #18483000:   Batch Loss = 7.802234, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.029684066772461, Accuracy = 0.8808814883232117\n",
      "Training iter #18486000:   Batch Loss = 7.743500, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.029715538024902, Accuracy = 0.8811792731285095\n",
      "Training iter #18489000:   Batch Loss = 7.649730, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028996467590332, Accuracy = 0.8811792731285095\n",
      "Training iter #18492000:   Batch Loss = 7.725307, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02820873260498, Accuracy = 0.8823704719543457\n",
      "Training iter #18495000:   Batch Loss = 7.766881, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.027436256408691, Accuracy = 0.8814770579338074\n",
      "Training iter #18498000:   Batch Loss = 7.791320, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02711009979248, Accuracy = 0.8817748427391052\n",
      "Training iter #18501000:   Batch Loss = 7.604732, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.027711868286133, Accuracy = 0.8811792731285095\n",
      "Training iter #18504000:   Batch Loss = 7.654605, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028908729553223, Accuracy = 0.8808814883232117\n",
      "Training iter #18507000:   Batch Loss = 7.713147, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02987289428711, Accuracy = 0.8796902894973755\n",
      "Training iter #18510000:   Batch Loss = 7.806190, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.030113220214844, Accuracy = 0.8790947198867798\n",
      "Training iter #18513000:   Batch Loss = 7.727503, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.030240058898926, Accuracy = 0.8787968754768372\n",
      "Training iter #18516000:   Batch Loss = 7.652886, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.029973983764648, Accuracy = 0.8790947198867798\n",
      "Training iter #18519000:   Batch Loss = 7.744500, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.029630661010742, Accuracy = 0.8787968754768372\n",
      "Training iter #18522000:   Batch Loss = 7.776266, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.027589797973633, Accuracy = 0.8802858591079712\n",
      "Training iter #18525000:   Batch Loss = 7.794349, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.027155876159668, Accuracy = 0.8805837035179138\n",
      "Training iter #18528000:   Batch Loss = 7.603393, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.026880264282227, Accuracy = 0.8799880743026733\n",
      "Training iter #18531000:   Batch Loss = 7.658656, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.026973724365234, Accuracy = 0.8802858591079712\n",
      "Training iter #18534000:   Batch Loss = 7.710505, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.027595520019531, Accuracy = 0.8802858591079712\n",
      "Training iter #18537000:   Batch Loss = 7.803857, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028148651123047, Accuracy = 0.8799880743026733\n",
      "Training iter #18540000:   Batch Loss = 7.712895, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0287504196167, Accuracy = 0.8799880743026733\n",
      "Training iter #18543000:   Batch Loss = 7.636730, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.029023170471191, Accuracy = 0.8802858591079712\n",
      "Training iter #18546000:   Batch Loss = 7.744214, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028863906860352, Accuracy = 0.8814770579338074\n",
      "Training iter #18549000:   Batch Loss = 7.769849, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028487205505371, Accuracy = 0.8817748427391052\n",
      "Training iter #18552000:   Batch Loss = 7.790861, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028568267822266, Accuracy = 0.8817748427391052\n",
      "Training iter #18555000:   Batch Loss = 7.603893, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028484344482422, Accuracy = 0.8823704719543457\n",
      "Training iter #18558000:   Batch Loss = 7.658925, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.028212547302246, Accuracy = 0.8820726871490479\n",
      "Training iter #18561000:   Batch Loss = 7.691403, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.027620315551758, Accuracy = 0.8820726871490479\n",
      "Training iter #18564000:   Batch Loss = 7.816355, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.027070045471191, Accuracy = 0.8820726871490479\n",
      "Training iter #18567000:   Batch Loss = 7.698459, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0263032913208, Accuracy = 0.8814770579338074\n",
      "Training iter #18570000:   Batch Loss = 7.632442, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.025320053100586, Accuracy = 0.8820726871490479\n",
      "Training iter #18573000:   Batch Loss = 7.719213, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.024731636047363, Accuracy = 0.8817748427391052\n",
      "Training iter #18576000:   Batch Loss = 7.775187, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0242919921875, Accuracy = 0.8823704719543457\n",
      "Training iter #18579000:   Batch Loss = 7.773830, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.023908615112305, Accuracy = 0.8829660415649414\n",
      "Training iter #18582000:   Batch Loss = 7.599720, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02360725402832, Accuracy = 0.8817748427391052\n",
      "Training iter #18585000:   Batch Loss = 7.680075, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.023232460021973, Accuracy = 0.8817748427391052\n",
      "Training iter #18588000:   Batch Loss = 7.713516, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.023005485534668, Accuracy = 0.8811792731285095\n",
      "Training iter #18591000:   Batch Loss = 7.809465, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022839546203613, Accuracy = 0.8814770579338074\n",
      "Training iter #18594000:   Batch Loss = 7.688381, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022838592529297, Accuracy = 0.8823704719543457\n",
      "Training iter #18597000:   Batch Loss = 7.615977, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02293586730957, Accuracy = 0.8820726871490479\n",
      "Training iter #18600000:   Batch Loss = 7.721807, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022854804992676, Accuracy = 0.8820726871490479\n",
      "Training iter #18603000:   Batch Loss = 7.779212, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022867202758789, Accuracy = 0.8823704719543457\n",
      "Training iter #18606000:   Batch Loss = 7.735238, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.023451805114746, Accuracy = 0.8829660415649414\n",
      "Training iter #18609000:   Batch Loss = 7.602598, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02330493927002, Accuracy = 0.8820726871490479\n",
      "Training iter #18612000:   Batch Loss = 7.680234, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.023051261901855, Accuracy = 0.8820726871490479\n",
      "Training iter #18615000:   Batch Loss = 7.716363, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02281379699707, Accuracy = 0.8823704719543457\n",
      "Training iter #18618000:   Batch Loss = 7.820765, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02256965637207, Accuracy = 0.8829660415649414\n",
      "Training iter #18621000:   Batch Loss = 7.676567, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022370338439941, Accuracy = 0.8826682567596436\n",
      "Training iter #18624000:   Batch Loss = 7.619702, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02372932434082, Accuracy = 0.8820726871490479\n",
      "Training iter #18627000:   Batch Loss = 7.723609, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022951126098633, Accuracy = 0.8826682567596436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18630000:   Batch Loss = 7.775173, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022374153137207, Accuracy = 0.8829660415649414\n",
      "Training iter #18633000:   Batch Loss = 7.741520, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.022040367126465, Accuracy = 0.8823704719543457\n",
      "Training iter #18636000:   Batch Loss = 7.589205, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.021581649780273, Accuracy = 0.8817748427391052\n",
      "Training iter #18639000:   Batch Loss = 7.681059, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.020933151245117, Accuracy = 0.8811792731285095\n",
      "Training iter #18642000:   Batch Loss = 7.694691, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0203275680542, Accuracy = 0.8814770579338074\n",
      "Training iter #18645000:   Batch Loss = 7.833763, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.020068168640137, Accuracy = 0.8823704719543457\n",
      "Training iter #18648000:   Batch Loss = 7.671569, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.019857406616211, Accuracy = 0.8817748427391052\n",
      "Training iter #18651000:   Batch Loss = 7.612606, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.019679069519043, Accuracy = 0.8826682567596436\n",
      "Training iter #18654000:   Batch Loss = 7.738891, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.020149230957031, Accuracy = 0.8826682567596436\n",
      "Training iter #18657000:   Batch Loss = 7.763745, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.02145767211914, Accuracy = 0.8826682567596436\n",
      "Training iter #18660000:   Batch Loss = 7.751579, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.018470764160156, Accuracy = 0.8823704719543457\n",
      "Training iter #18663000:   Batch Loss = 7.590575, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.016373634338379, Accuracy = 0.8823704719543457\n",
      "Training iter #18666000:   Batch Loss = 7.685023, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.01508617401123, Accuracy = 0.8823704719543457\n",
      "Training iter #18669000:   Batch Loss = 7.712107, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.014537811279297, Accuracy = 0.8829660415649414\n",
      "Training iter #18672000:   Batch Loss = 7.837098, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.014156341552734, Accuracy = 0.8826682567596436\n",
      "Training iter #18675000:   Batch Loss = 7.641262, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.013752937316895, Accuracy = 0.8829660415649414\n",
      "Training iter #18678000:   Batch Loss = 7.611092, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.01321792602539, Accuracy = 0.8832638263702393\n",
      "Training iter #18681000:   Batch Loss = 7.745533, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012688636779785, Accuracy = 0.8835616707801819\n",
      "Training iter #18684000:   Batch Loss = 7.756281, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012022972106934, Accuracy = 0.8832638263702393\n",
      "Training iter #18687000:   Batch Loss = 7.736350, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.011171340942383, Accuracy = 0.8823704719543457\n",
      "Training iter #18690000:   Batch Loss = 7.600881, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.010475158691406, Accuracy = 0.8832638263702393\n",
      "Training iter #18693000:   Batch Loss = 7.687626, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.011059761047363, Accuracy = 0.8826682567596436\n",
      "Training iter #18696000:   Batch Loss = 7.706874, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012227058410645, Accuracy = 0.8817748427391052\n",
      "Training iter #18699000:   Batch Loss = 7.840752, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.01301097869873, Accuracy = 0.8814770579338074\n",
      "Training iter #18702000:   Batch Loss = 7.622241, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.013555526733398, Accuracy = 0.8805837035179138\n",
      "Training iter #18705000:   Batch Loss = 7.607524, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.014753341674805, Accuracy = 0.8802858591079712\n",
      "Training iter #18708000:   Batch Loss = 7.755222, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.015172004699707, Accuracy = 0.8811792731285095\n",
      "Training iter #18711000:   Batch Loss = 7.752868, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.014554023742676, Accuracy = 0.8814770579338074\n",
      "Training iter #18714000:   Batch Loss = 7.739524, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.013745307922363, Accuracy = 0.8817748427391052\n",
      "Training iter #18717000:   Batch Loss = 7.606220, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.013463973999023, Accuracy = 0.8811792731285095\n",
      "Training iter #18720000:   Batch Loss = 7.683353, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.013106346130371, Accuracy = 0.8814770579338074\n",
      "Training iter #18723000:   Batch Loss = 7.725894, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012633323669434, Accuracy = 0.8820726871490479\n",
      "Training iter #18726000:   Batch Loss = 7.820879, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.01244068145752, Accuracy = 0.8820726871490479\n",
      "Training iter #18729000:   Batch Loss = 7.607166, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012601852416992, Accuracy = 0.8826682567596436\n",
      "Training iter #18732000:   Batch Loss = 7.605521, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0129976272583, Accuracy = 0.8826682567596436\n",
      "Training iter #18735000:   Batch Loss = 7.744832, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012977600097656, Accuracy = 0.8826682567596436\n",
      "Training iter #18738000:   Batch Loss = 7.748894, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012290000915527, Accuracy = 0.8829660415649414\n",
      "Training iter #18741000:   Batch Loss = 7.733611, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.012120246887207, Accuracy = 0.8829660415649414\n",
      "Training iter #18744000:   Batch Loss = 7.611683, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.01203727722168, Accuracy = 0.8829660415649414\n",
      "Training iter #18747000:   Batch Loss = 7.677756, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.011711120605469, Accuracy = 0.8826682567596436\n",
      "Training iter #18750000:   Batch Loss = 7.733406, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.011213302612305, Accuracy = 0.8820726871490479\n",
      "Training iter #18753000:   Batch Loss = 7.800344, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.011022567749023, Accuracy = 0.8820726871490479\n",
      "Training iter #18756000:   Batch Loss = 7.590063, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.011096000671387, Accuracy = 0.8817748427391052\n",
      "Training iter #18759000:   Batch Loss = 7.612731, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.011204719543457, Accuracy = 0.8820726871490479\n",
      "Training iter #18762000:   Batch Loss = 7.717536, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.010764122009277, Accuracy = 0.8820726871490479\n",
      "Training iter #18765000:   Batch Loss = 7.774730, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.009843826293945, Accuracy = 0.8820726871490479\n",
      "Training iter #18768000:   Batch Loss = 7.749914, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.009502410888672, Accuracy = 0.8820726871490479\n",
      "Training iter #18771000:   Batch Loss = 7.628432, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.009344100952148, Accuracy = 0.8823704719543457\n",
      "Training iter #18774000:   Batch Loss = 7.668170, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.009228706359863, Accuracy = 0.8817748427391052\n",
      "Training iter #18777000:   Batch Loss = 7.725063, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.009049415588379, Accuracy = 0.8826682567596436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18780000:   Batch Loss = 7.767406, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008647918701172, Accuracy = 0.8820726871490479\n",
      "Training iter #18783000:   Batch Loss = 7.574224, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.010271072387695, Accuracy = 0.8823704719543457\n",
      "Training iter #18786000:   Batch Loss = 7.629302, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.009493827819824, Accuracy = 0.8820726871490479\n",
      "Training iter #18789000:   Batch Loss = 7.710091, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.0092191696167, Accuracy = 0.8820726871490479\n",
      "Training iter #18792000:   Batch Loss = 7.769539, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00867748260498, Accuracy = 0.8817748427391052\n",
      "Training iter #18795000:   Batch Loss = 7.742558, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00851821899414, Accuracy = 0.8823704719543457\n",
      "Training iter #18798000:   Batch Loss = 7.631925, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008426666259766, Accuracy = 0.8826682567596436\n",
      "Training iter #18801000:   Batch Loss = 7.668375, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008270263671875, Accuracy = 0.8826682567596436\n",
      "Training iter #18804000:   Batch Loss = 7.731769, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008177757263184, Accuracy = 0.8829660415649414\n",
      "Training iter #18807000:   Batch Loss = 7.780409, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008123397827148, Accuracy = 0.8826682567596436\n",
      "Training iter #18810000:   Batch Loss = 7.574983, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008325576782227, Accuracy = 0.8823704719543457\n",
      "Training iter #18813000:   Batch Loss = 7.636510, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008352279663086, Accuracy = 0.8817748427391052\n",
      "Training iter #18816000:   Batch Loss = 7.689017, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008367538452148, Accuracy = 0.8826682567596436\n",
      "Training iter #18819000:   Batch Loss = 7.787902, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.007980346679688, Accuracy = 0.8826682567596436\n",
      "Training iter #18822000:   Batch Loss = 7.703774, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008625030517578, Accuracy = 0.8820726871490479\n",
      "Training iter #18825000:   Batch Loss = 7.622637, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.008676528930664, Accuracy = 0.8817748427391052\n",
      "Training iter #18828000:   Batch Loss = 7.707272, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00850772857666, Accuracy = 0.8820726871490479\n",
      "Training iter #18831000:   Batch Loss = 7.735998, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00830364227295, Accuracy = 0.8823704719543457\n",
      "Training iter #18834000:   Batch Loss = 7.775876, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.007927894592285, Accuracy = 0.8814770579338074\n",
      "Training iter #18837000:   Batch Loss = 7.569292, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.007702827453613, Accuracy = 0.8817748427391052\n",
      "Training iter #18840000:   Batch Loss = 7.628712, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.007370948791504, Accuracy = 0.8817748427391052\n",
      "Training iter #18843000:   Batch Loss = 7.678304, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.007135391235352, Accuracy = 0.8817748427391052\n",
      "Training iter #18846000:   Batch Loss = 7.775300, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.006803512573242, Accuracy = 0.8820726871490479\n",
      "Training iter #18849000:   Batch Loss = 7.689400, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.006650924682617, Accuracy = 0.8814770579338074\n",
      "Training iter #18852000:   Batch Loss = 7.612966, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.006423950195312, Accuracy = 0.8814770579338074\n",
      "Training iter #18855000:   Batch Loss = 7.706651, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.006098747253418, Accuracy = 0.8811792731285095\n",
      "Training iter #18858000:   Batch Loss = 7.743746, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.005155563354492, Accuracy = 0.8814770579338074\n",
      "Training iter #18861000:   Batch Loss = 7.769633, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.004880905151367, Accuracy = 0.8814770579338074\n",
      "Training iter #18864000:   Batch Loss = 7.579534, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00477409362793, Accuracy = 0.8814770579338074\n",
      "Training iter #18867000:   Batch Loss = 7.633898, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.004373550415039, Accuracy = 0.8817748427391052\n",
      "Training iter #18870000:   Batch Loss = 7.674309, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.004175186157227, Accuracy = 0.8817748427391052\n",
      "Training iter #18873000:   Batch Loss = 7.783997, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00352668762207, Accuracy = 0.8820726871490479\n",
      "Training iter #18876000:   Batch Loss = 7.676326, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.003159523010254, Accuracy = 0.8820726871490479\n",
      "Training iter #18879000:   Batch Loss = 7.609612, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.002829551696777, Accuracy = 0.8823704719543457\n",
      "Training iter #18882000:   Batch Loss = 7.713566, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.002617835998535, Accuracy = 0.8826682567596436\n",
      "Training iter #18885000:   Batch Loss = 7.745624, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00243854522705, Accuracy = 0.8823704719543457\n",
      "Training iter #18888000:   Batch Loss = 7.753516, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00229263305664, Accuracy = 0.8820726871490479\n",
      "Training iter #18891000:   Batch Loss = 7.580567, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.002314567565918, Accuracy = 0.8823704719543457\n",
      "Training iter #18894000:   Batch Loss = 7.653329, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.002219200134277, Accuracy = 0.8820726871490479\n",
      "Training iter #18897000:   Batch Loss = 7.671423, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.002218246459961, Accuracy = 0.8823704719543457\n",
      "Training iter #18900000:   Batch Loss = 7.789462, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.002079963684082, Accuracy = 0.8817748427391052\n",
      "Training iter #18903000:   Batch Loss = 7.667751, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00206470489502, Accuracy = 0.8817748427391052\n",
      "Training iter #18906000:   Batch Loss = 7.612103, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.001983642578125, Accuracy = 0.8826682567596436\n",
      "Training iter #18909000:   Batch Loss = 7.687247, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00185775756836, Accuracy = 0.8826682567596436\n",
      "Training iter #18912000:   Batch Loss = 7.758806, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00162124633789, Accuracy = 0.8826682567596436\n",
      "Training iter #18915000:   Batch Loss = 7.737391, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00137710571289, Accuracy = 0.8826682567596436\n",
      "Training iter #18918000:   Batch Loss = 7.584574, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00133228302002, Accuracy = 0.8832638263702393\n",
      "Training iter #18921000:   Batch Loss = 7.657509, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.001215934753418, Accuracy = 0.8832638263702393\n",
      "Training iter #18924000:   Batch Loss = 7.688758, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.001070022583008, Accuracy = 0.8829660415649414\n",
      "Training iter #18927000:   Batch Loss = 7.796846, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.000639915466309, Accuracy = 0.8823704719543457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18930000:   Batch Loss = 7.660424, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.000370025634766, Accuracy = 0.8826682567596436\n",
      "Training iter #18933000:   Batch Loss = 7.600321, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.000102043151855, Accuracy = 0.8829660415649414\n",
      "Training iter #18936000:   Batch Loss = 7.692085, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00030517578125, Accuracy = 0.8832638263702393\n",
      "Training iter #18939000:   Batch Loss = 7.746561, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.000388145446777, Accuracy = 0.8832638263702393\n",
      "Training iter #18942000:   Batch Loss = 7.723598, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.000229835510254, Accuracy = 0.8829660415649414\n",
      "Training iter #18945000:   Batch Loss = 7.569812, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.000223159790039, Accuracy = 0.8826682567596436\n",
      "Training iter #18948000:   Batch Loss = 7.656056, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.000175476074219, Accuracy = 0.8823704719543457\n",
      "Training iter #18951000:   Batch Loss = 7.690471, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 8.00019359588623, Accuracy = 0.8820726871490479\n",
      "Training iter #18954000:   Batch Loss = 7.801165, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.999959468841553, Accuracy = 0.8820726871490479\n",
      "Training iter #18957000:   Batch Loss = 7.647736, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.999850273132324, Accuracy = 0.8817748427391052\n",
      "Training iter #18960000:   Batch Loss = 7.600767, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.999709606170654, Accuracy = 0.8820726871490479\n",
      "Training iter #18963000:   Batch Loss = 7.714355, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.999488353729248, Accuracy = 0.8820726871490479\n",
      "Training iter #18966000:   Batch Loss = 7.744066, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.999179363250732, Accuracy = 0.8817748427391052\n",
      "Training iter #18969000:   Batch Loss = 7.708881, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.99884557723999, Accuracy = 0.8811792731285095\n",
      "Training iter #18972000:   Batch Loss = 7.572843, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.997122287750244, Accuracy = 0.8814770579338074\n",
      "Training iter #18975000:   Batch Loss = 7.659459, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.997419834136963, Accuracy = 0.8823704719543457\n",
      "Training iter #18978000:   Batch Loss = 7.680263, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.996642112731934, Accuracy = 0.8826682567596436\n",
      "Training iter #18981000:   Batch Loss = 7.801529, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.99605131149292, Accuracy = 0.8832638263702393\n",
      "Training iter #18984000:   Batch Loss = 7.640788, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.995861053466797, Accuracy = 0.8832638263702393\n",
      "Training iter #18987000:   Batch Loss = 7.592412, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.995693206787109, Accuracy = 0.8829660415649414\n",
      "Training iter #18990000:   Batch Loss = 7.713685, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.995612621307373, Accuracy = 0.8829660415649414\n",
      "Training iter #18993000:   Batch Loss = 7.730052, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.995022296905518, Accuracy = 0.8829660415649414\n",
      "Training iter #18996000:   Batch Loss = 7.724423, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994711875915527, Accuracy = 0.8826682567596436\n",
      "Training iter #18999000:   Batch Loss = 7.575136, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994596481323242, Accuracy = 0.8826682567596436\n",
      "Training iter #19002000:   Batch Loss = 7.658448, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994521141052246, Accuracy = 0.8826682567596436\n",
      "Training iter #19005000:   Batch Loss = 7.684909, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994541168212891, Accuracy = 0.8823704719543457\n",
      "Training iter #19008000:   Batch Loss = 7.808779, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994353294372559, Accuracy = 0.8823704719543457\n",
      "Training iter #19011000:   Batch Loss = 7.615208, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994466304779053, Accuracy = 0.8826682567596436\n",
      "Training iter #19014000:   Batch Loss = 7.590427, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994565963745117, Accuracy = 0.8823704719543457\n",
      "Training iter #19017000:   Batch Loss = 7.717527, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9959869384765625, Accuracy = 0.8823704719543457\n",
      "Training iter #19020000:   Batch Loss = 7.726888, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.994200229644775, Accuracy = 0.8823704719543457\n",
      "Training iter #19023000:   Batch Loss = 7.714281, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993773937225342, Accuracy = 0.8826682567596436\n",
      "Training iter #19026000:   Batch Loss = 7.582270, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993663787841797, Accuracy = 0.8823704719543457\n",
      "Training iter #19029000:   Batch Loss = 7.657988, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993490695953369, Accuracy = 0.8826682567596436\n",
      "Training iter #19032000:   Batch Loss = 7.678946, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993296146392822, Accuracy = 0.8829660415649414\n",
      "Training iter #19035000:   Batch Loss = 7.805561, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9930100440979, Accuracy = 0.8826682567596436\n",
      "Training iter #19038000:   Batch Loss = 7.594640, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.992958068847656, Accuracy = 0.8826682567596436\n",
      "Training iter #19041000:   Batch Loss = 7.584503, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.992913246154785, Accuracy = 0.8826682567596436\n",
      "Training iter #19044000:   Batch Loss = 7.728096, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.992764949798584, Accuracy = 0.8829660415649414\n",
      "Training iter #19047000:   Batch Loss = 7.727125, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.99243688583374, Accuracy = 0.8823704719543457\n",
      "Training iter #19050000:   Batch Loss = 7.705783, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993007659912109, Accuracy = 0.8814770579338074\n",
      "Training iter #19053000:   Batch Loss = 7.594362, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993273735046387, Accuracy = 0.8811792731285095\n",
      "Training iter #19056000:   Batch Loss = 7.652088, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993485450744629, Accuracy = 0.8811792731285095\n",
      "Training iter #19059000:   Batch Loss = 7.709153, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993008613586426, Accuracy = 0.8817748427391052\n",
      "Training iter #19062000:   Batch Loss = 7.789459, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.991903781890869, Accuracy = 0.8820726871490479\n",
      "Training iter #19065000:   Batch Loss = 7.584601, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.992792129516602, Accuracy = 0.8817748427391052\n",
      "Training iter #19068000:   Batch Loss = 7.590526, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993995666503906, Accuracy = 0.8823704719543457\n",
      "Training iter #19071000:   Batch Loss = 7.720408, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993839263916016, Accuracy = 0.8814770579338074\n",
      "Training iter #19074000:   Batch Loss = 7.734160, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.993149757385254, Accuracy = 0.8808814883232117\n",
      "Training iter #19077000:   Batch Loss = 7.719157, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.992931365966797, Accuracy = 0.8808814883232117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19080000:   Batch Loss = 7.613243, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.992275714874268, Accuracy = 0.8802858591079712\n",
      "Training iter #19083000:   Batch Loss = 7.647750, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.991600036621094, Accuracy = 0.8814770579338074\n",
      "Training iter #19086000:   Batch Loss = 7.710166, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.991122722625732, Accuracy = 0.8814770579338074\n",
      "Training iter #19089000:   Batch Loss = 7.751984, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9906721115112305, Accuracy = 0.8808814883232117\n",
      "Training iter #19092000:   Batch Loss = 7.574499, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.990827560424805, Accuracy = 0.8817748427391052\n",
      "Training iter #19095000:   Batch Loss = 7.599622, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.990726470947266, Accuracy = 0.8823704719543457\n",
      "Training iter #19098000:   Batch Loss = 7.703575, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.989589691162109, Accuracy = 0.8832638263702393\n",
      "Training iter #19101000:   Batch Loss = 7.752642, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.989345073699951, Accuracy = 0.8823704719543457\n",
      "Training iter #19104000:   Batch Loss = 7.727633, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.989428520202637, Accuracy = 0.8808814883232117\n",
      "Training iter #19107000:   Batch Loss = 7.609862, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.989076614379883, Accuracy = 0.8808814883232117\n",
      "Training iter #19110000:   Batch Loss = 7.659740, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.988733291625977, Accuracy = 0.8823704719543457\n",
      "Training iter #19113000:   Batch Loss = 7.707323, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.988349914550781, Accuracy = 0.8829660415649414\n",
      "Training iter #19116000:   Batch Loss = 7.748268, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.987491130828857, Accuracy = 0.8826682567596436\n",
      "Training iter #19119000:   Batch Loss = 7.558657, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.987410545349121, Accuracy = 0.8826682567596436\n",
      "Training iter #19122000:   Batch Loss = 7.610385, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9875102043151855, Accuracy = 0.8823704719543457\n",
      "Training iter #19125000:   Batch Loss = 7.688844, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9876556396484375, Accuracy = 0.8829660415649414\n",
      "Training iter #19128000:   Batch Loss = 7.742745, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.988134384155273, Accuracy = 0.8829660415649414\n",
      "Training iter #19131000:   Batch Loss = 7.701320, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.98789119720459, Accuracy = 0.8835616707801819\n",
      "Training iter #19134000:   Batch Loss = 7.609717, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.987815856933594, Accuracy = 0.8838594555854797\n",
      "Training iter #19137000:   Batch Loss = 7.672310, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.987707614898682, Accuracy = 0.8838594555854797\n",
      "Training iter #19140000:   Batch Loss = 7.701159, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9867753982543945, Accuracy = 0.8844550251960754\n",
      "Training iter #19143000:   Batch Loss = 7.746080, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.986238956451416, Accuracy = 0.8847528100013733\n",
      "Training iter #19146000:   Batch Loss = 7.568080, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.986371994018555, Accuracy = 0.8844550251960754\n",
      "Training iter #19149000:   Batch Loss = 7.612251, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.986519813537598, Accuracy = 0.8844550251960754\n",
      "Training iter #19152000:   Batch Loss = 7.675910, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.986490249633789, Accuracy = 0.8844550251960754\n",
      "Training iter #19155000:   Batch Loss = 7.753242, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.986205577850342, Accuracy = 0.8844550251960754\n",
      "Training iter #19158000:   Batch Loss = 7.671438, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.985907077789307, Accuracy = 0.8835616707801819\n",
      "Training iter #19161000:   Batch Loss = 7.602464, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.985771179199219, Accuracy = 0.8838594555854797\n",
      "Training iter #19164000:   Batch Loss = 7.692345, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.985570430755615, Accuracy = 0.8844550251960754\n",
      "Training iter #19167000:   Batch Loss = 7.719240, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.985189437866211, Accuracy = 0.8844550251960754\n",
      "Training iter #19170000:   Batch Loss = 7.748819, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.984925746917725, Accuracy = 0.8847528100013733\n",
      "Training iter #19173000:   Batch Loss = 7.562670, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.985083103179932, Accuracy = 0.8841572403907776\n",
      "Training iter #19176000:   Batch Loss = 7.613023, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.985252380371094, Accuracy = 0.8838594555854797\n",
      "Training iter #19179000:   Batch Loss = 7.659096, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.985143184661865, Accuracy = 0.8835616707801819\n",
      "Training iter #19182000:   Batch Loss = 7.751711, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.984767913818359, Accuracy = 0.8838594555854797\n",
      "Training iter #19185000:   Batch Loss = 7.661426, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.984441757202148, Accuracy = 0.8832638263702393\n",
      "Training iter #19188000:   Batch Loss = 7.592657, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.984219074249268, Accuracy = 0.8841572403907776\n",
      "Training iter #19191000:   Batch Loss = 7.687886, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983971118927002, Accuracy = 0.8841572403907776\n",
      "Training iter #19194000:   Batch Loss = 7.714399, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983549118041992, Accuracy = 0.8838594555854797\n",
      "Training iter #19197000:   Batch Loss = 7.739416, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983214855194092, Accuracy = 0.8844550251960754\n",
      "Training iter #19200000:   Batch Loss = 7.562991, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983719825744629, Accuracy = 0.8835616707801819\n",
      "Training iter #19203000:   Batch Loss = 7.615951, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983598709106445, Accuracy = 0.8835616707801819\n",
      "Training iter #19206000:   Batch Loss = 7.647289, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983473777770996, Accuracy = 0.8838594555854797\n",
      "Training iter #19209000:   Batch Loss = 7.765414, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.984104633331299, Accuracy = 0.8835616707801819\n",
      "Training iter #19212000:   Batch Loss = 7.651151, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983958721160889, Accuracy = 0.8838594555854797\n",
      "Training iter #19215000:   Batch Loss = 7.591601, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983584880828857, Accuracy = 0.8838594555854797\n",
      "Training iter #19218000:   Batch Loss = 7.687716, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.983063220977783, Accuracy = 0.8844550251960754\n",
      "Training iter #19221000:   Batch Loss = 7.724435, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.982744216918945, Accuracy = 0.8844550251960754\n",
      "Training iter #19224000:   Batch Loss = 7.726193, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.982608795166016, Accuracy = 0.8847528100013733\n",
      "Training iter #19227000:   Batch Loss = 7.559681, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.982021331787109, Accuracy = 0.8844550251960754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19230000:   Batch Loss = 7.635470, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.982308387756348, Accuracy = 0.8838594555854797\n",
      "Training iter #19233000:   Batch Loss = 7.657410, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.982210636138916, Accuracy = 0.8832638263702393\n",
      "Training iter #19236000:   Batch Loss = 7.749887, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.981961250305176, Accuracy = 0.8832638263702393\n",
      "Training iter #19239000:   Batch Loss = 7.639469, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.981690406799316, Accuracy = 0.8835616707801819\n",
      "Training iter #19242000:   Batch Loss = 7.576820, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.982152938842773, Accuracy = 0.8835616707801819\n",
      "Training iter #19245000:   Batch Loss = 7.666476, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.981768608093262, Accuracy = 0.8838594555854797\n",
      "Training iter #19248000:   Batch Loss = 7.725882, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.981181621551514, Accuracy = 0.8835616707801819\n",
      "Training iter #19251000:   Batch Loss = 7.691520, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.981074333190918, Accuracy = 0.8838594555854797\n",
      "Training iter #19254000:   Batch Loss = 7.564621, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.981159210205078, Accuracy = 0.8835616707801819\n",
      "Training iter #19257000:   Batch Loss = 7.638925, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.980876922607422, Accuracy = 0.8832638263702393\n",
      "Training iter #19260000:   Batch Loss = 7.672024, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.979822635650635, Accuracy = 0.8838594555854797\n",
      "Training iter #19263000:   Batch Loss = 7.769319, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.979578495025635, Accuracy = 0.8838594555854797\n",
      "Training iter #19266000:   Batch Loss = 7.631912, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.979370594024658, Accuracy = 0.8838594555854797\n",
      "Training iter #19269000:   Batch Loss = 7.579558, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.979504108428955, Accuracy = 0.8835616707801819\n",
      "Training iter #19272000:   Batch Loss = 7.670342, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.97950553894043, Accuracy = 0.8838594555854797\n",
      "Training iter #19275000:   Batch Loss = 7.724897, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.979318141937256, Accuracy = 0.8826682567596436\n",
      "Training iter #19278000:   Batch Loss = 7.700977, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.978821754455566, Accuracy = 0.8829660415649414\n",
      "Training iter #19281000:   Batch Loss = 7.547726, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.978562355041504, Accuracy = 0.8829660415649414\n",
      "Training iter #19284000:   Batch Loss = 7.638053, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.979000091552734, Accuracy = 0.8826682567596436\n",
      "Training iter #19287000:   Batch Loss = 7.648355, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.978803634643555, Accuracy = 0.8829660415649414\n",
      "Training iter #19290000:   Batch Loss = 7.779800, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.978645324707031, Accuracy = 0.8826682567596436\n",
      "Training iter #19293000:   Batch Loss = 7.625868, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9785261154174805, Accuracy = 0.8832638263702393\n",
      "Training iter #19296000:   Batch Loss = 7.575075, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.978562355041504, Accuracy = 0.8829660415649414\n",
      "Training iter #19299000:   Batch Loss = 7.693056, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.978360652923584, Accuracy = 0.8832638263702393\n",
      "Training iter #19302000:   Batch Loss = 7.714052, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.977846145629883, Accuracy = 0.8832638263702393\n",
      "Training iter #19305000:   Batch Loss = 7.686538, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.977206230163574, Accuracy = 0.8835616707801819\n",
      "Training iter #19308000:   Batch Loss = 7.552476, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.977056503295898, Accuracy = 0.8835616707801819\n",
      "Training iter #19311000:   Batch Loss = 7.640556, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9785943031311035, Accuracy = 0.8826682567596436\n",
      "Training iter #19314000:   Batch Loss = 7.660819, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.977970600128174, Accuracy = 0.8826682567596436\n",
      "Training iter #19317000:   Batch Loss = 7.769597, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.977291584014893, Accuracy = 0.8826682567596436\n",
      "Training iter #19320000:   Batch Loss = 7.605606, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.976358890533447, Accuracy = 0.8829660415649414\n",
      "Training iter #19323000:   Batch Loss = 7.571856, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.976333141326904, Accuracy = 0.8832638263702393\n",
      "Training iter #19326000:   Batch Loss = 7.688799, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.976083278656006, Accuracy = 0.8832638263702393\n",
      "Training iter #19329000:   Batch Loss = 7.698270, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975789546966553, Accuracy = 0.8829660415649414\n",
      "Training iter #19332000:   Batch Loss = 7.700852, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975630760192871, Accuracy = 0.8829660415649414\n",
      "Training iter #19335000:   Batch Loss = 7.558211, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975710391998291, Accuracy = 0.8829660415649414\n",
      "Training iter #19338000:   Batch Loss = 7.636981, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975717544555664, Accuracy = 0.8829660415649414\n",
      "Training iter #19341000:   Batch Loss = 7.662565, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975568771362305, Accuracy = 0.8829660415649414\n",
      "Training iter #19344000:   Batch Loss = 7.785738, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9755096435546875, Accuracy = 0.8829660415649414\n",
      "Training iter #19347000:   Batch Loss = 7.580304, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975528717041016, Accuracy = 0.8829660415649414\n",
      "Training iter #19350000:   Batch Loss = 7.565229, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975450038909912, Accuracy = 0.8832638263702393\n",
      "Training iter #19353000:   Batch Loss = 7.705604, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.975279808044434, Accuracy = 0.8832638263702393\n",
      "Training iter #19356000:   Batch Loss = 7.706076, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.974953651428223, Accuracy = 0.8820726871490479\n",
      "Training iter #19359000:   Batch Loss = 7.689565, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.97468376159668, Accuracy = 0.8820726871490479\n",
      "Training iter #19362000:   Batch Loss = 7.566274, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.974581241607666, Accuracy = 0.8826682567596436\n",
      "Training iter #19365000:   Batch Loss = 7.632409, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.974370002746582, Accuracy = 0.8832638263702393\n",
      "Training iter #19368000:   Batch Loss = 7.668880, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9741435050964355, Accuracy = 0.8829660415649414\n",
      "Training iter #19371000:   Batch Loss = 7.758278, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.973934650421143, Accuracy = 0.8829660415649414\n",
      "Training iter #19374000:   Batch Loss = 7.570022, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.973846912384033, Accuracy = 0.8832638263702393\n",
      "Training iter #19377000:   Batch Loss = 7.556515, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.973612308502197, Accuracy = 0.8835616707801819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19380000:   Batch Loss = 7.698214, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.973382949829102, Accuracy = 0.8835616707801819\n",
      "Training iter #19383000:   Batch Loss = 7.699040, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.973201751708984, Accuracy = 0.8835616707801819\n",
      "Training iter #19386000:   Batch Loss = 7.679277, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.97314453125, Accuracy = 0.8838594555854797\n",
      "Training iter #19389000:   Batch Loss = 7.571776, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.973169326782227, Accuracy = 0.8838594555854797\n",
      "Training iter #19392000:   Batch Loss = 7.628926, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972881317138672, Accuracy = 0.8835616707801819\n",
      "Training iter #19395000:   Batch Loss = 7.686367, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972719192504883, Accuracy = 0.8832638263702393\n",
      "Training iter #19398000:   Batch Loss = 7.747135, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972660064697266, Accuracy = 0.8829660415649414\n",
      "Training iter #19401000:   Batch Loss = 7.553740, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972758769989014, Accuracy = 0.8829660415649414\n",
      "Training iter #19404000:   Batch Loss = 7.578099, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972580432891846, Accuracy = 0.8829660415649414\n",
      "Training iter #19407000:   Batch Loss = 7.683880, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9724531173706055, Accuracy = 0.8823704719543457\n",
      "Training iter #19410000:   Batch Loss = 7.710228, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972294807434082, Accuracy = 0.8820726871490479\n",
      "Training iter #19413000:   Batch Loss = 7.689713, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972318649291992, Accuracy = 0.8829660415649414\n",
      "Training iter #19416000:   Batch Loss = 7.587319, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972341537475586, Accuracy = 0.8829660415649414\n",
      "Training iter #19419000:   Batch Loss = 7.629841, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.972164154052734, Accuracy = 0.8823704719543457\n",
      "Training iter #19422000:   Batch Loss = 7.681286, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.971981048583984, Accuracy = 0.8820726871490479\n",
      "Training iter #19425000:   Batch Loss = 7.717646, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.971559047698975, Accuracy = 0.8817748427391052\n",
      "Training iter #19428000:   Batch Loss = 7.535944, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.971195220947266, Accuracy = 0.8817748427391052\n",
      "Training iter #19431000:   Batch Loss = 7.585334, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9708380699157715, Accuracy = 0.8817748427391052\n",
      "Training iter #19434000:   Batch Loss = 7.660315, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.970739841461182, Accuracy = 0.8817748427391052\n",
      "Training iter #19437000:   Batch Loss = 7.717657, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9706244468688965, Accuracy = 0.8820726871490479\n",
      "Training iter #19440000:   Batch Loss = 7.691601, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.970618724822998, Accuracy = 0.8817748427391052\n",
      "Training iter #19443000:   Batch Loss = 7.590754, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.970606327056885, Accuracy = 0.8823704719543457\n",
      "Training iter #19446000:   Batch Loss = 7.629782, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.970450401306152, Accuracy = 0.8823704719543457\n",
      "Training iter #19449000:   Batch Loss = 7.677314, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.970282077789307, Accuracy = 0.8823704719543457\n",
      "Training iter #19452000:   Batch Loss = 7.728424, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.968326568603516, Accuracy = 0.8823704719543457\n",
      "Training iter #19455000:   Batch Loss = 7.536247, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.96803092956543, Accuracy = 0.8823704719543457\n",
      "Training iter #19458000:   Batch Loss = 7.593387, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.967814922332764, Accuracy = 0.8826682567596436\n",
      "Training iter #19461000:   Batch Loss = 7.664912, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.968153953552246, Accuracy = 0.8820726871490479\n",
      "Training iter #19464000:   Batch Loss = 7.736085, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9675421714782715, Accuracy = 0.8823704719543457\n",
      "Training iter #19467000:   Batch Loss = 7.657516, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.967404842376709, Accuracy = 0.8826682567596436\n",
      "Training iter #19470000:   Batch Loss = 7.578339, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.967418670654297, Accuracy = 0.8823704719543457\n",
      "Training iter #19473000:   Batch Loss = 7.651091, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.967225074768066, Accuracy = 0.8823704719543457\n",
      "Training iter #19476000:   Batch Loss = 7.682051, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.967071533203125, Accuracy = 0.8826682567596436\n",
      "Training iter #19479000:   Batch Loss = 7.712146, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.966824531555176, Accuracy = 0.8826682567596436\n",
      "Training iter #19482000:   Batch Loss = 7.540037, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.966744899749756, Accuracy = 0.8826682567596436\n",
      "Training iter #19485000:   Batch Loss = 7.590017, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9672136306762695, Accuracy = 0.8823704719543457\n",
      "Training iter #19488000:   Batch Loss = 7.638282, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.967082977294922, Accuracy = 0.8823704719543457\n",
      "Training iter #19491000:   Batch Loss = 7.726000, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.966629505157471, Accuracy = 0.8823704719543457\n",
      "Training iter #19494000:   Batch Loss = 7.643333, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9657487869262695, Accuracy = 0.8826682567596436\n",
      "Training iter #19497000:   Batch Loss = 7.580067, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.965684413909912, Accuracy = 0.8823704719543457\n",
      "Training iter #19500000:   Batch Loss = 7.661870, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.965445518493652, Accuracy = 0.8823704719543457\n",
      "Training iter #19503000:   Batch Loss = 7.693569, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.965301513671875, Accuracy = 0.8820726871490479\n",
      "Training iter #19506000:   Batch Loss = 7.716713, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.964999198913574, Accuracy = 0.8817748427391052\n",
      "Training iter #19509000:   Batch Loss = 7.540559, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.964821815490723, Accuracy = 0.8817748427391052\n",
      "Training iter #19512000:   Batch Loss = 7.589823, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.965359210968018, Accuracy = 0.8823704719543457\n",
      "Training iter #19515000:   Batch Loss = 7.635277, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.965446472167969, Accuracy = 0.8823704719543457\n",
      "Training iter #19518000:   Batch Loss = 7.729069, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9653778076171875, Accuracy = 0.8823704719543457\n",
      "Training iter #19521000:   Batch Loss = 7.631166, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.965270519256592, Accuracy = 0.8826682567596436\n",
      "Training iter #19524000:   Batch Loss = 7.569479, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.965039253234863, Accuracy = 0.8823704719543457\n",
      "Training iter #19527000:   Batch Loss = 7.664628, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.964845657348633, Accuracy = 0.8823704719543457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19530000:   Batch Loss = 7.691629, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.964966773986816, Accuracy = 0.8826682567596436\n",
      "Training iter #19533000:   Batch Loss = 7.703507, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.964991569519043, Accuracy = 0.8826682567596436\n",
      "Training iter #19536000:   Batch Loss = 7.543230, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.964941501617432, Accuracy = 0.8829660415649414\n",
      "Training iter #19539000:   Batch Loss = 7.591259, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.963915824890137, Accuracy = 0.8826682567596436\n",
      "Training iter #19542000:   Batch Loss = 7.624841, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.963558673858643, Accuracy = 0.8832638263702393\n",
      "Training iter #19545000:   Batch Loss = 7.727865, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.963481426239014, Accuracy = 0.8835616707801819\n",
      "Training iter #19548000:   Batch Loss = 7.623827, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.963429927825928, Accuracy = 0.8838594555854797\n",
      "Training iter #19551000:   Batch Loss = 7.571112, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.963367462158203, Accuracy = 0.8835616707801819\n",
      "Training iter #19554000:   Batch Loss = 7.643049, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.963271617889404, Accuracy = 0.8835616707801819\n",
      "Training iter #19557000:   Batch Loss = 7.709652, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.963183879852295, Accuracy = 0.8838594555854797\n",
      "Training iter #19560000:   Batch Loss = 7.692451, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.96297025680542, Accuracy = 0.8838594555854797\n",
      "Training iter #19563000:   Batch Loss = 7.545505, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.962787628173828, Accuracy = 0.8841572403907776\n",
      "Training iter #19566000:   Batch Loss = 7.614344, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.962538242340088, Accuracy = 0.8832638263702393\n",
      "Training iter #19569000:   Batch Loss = 7.646286, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.96238899230957, Accuracy = 0.8835616707801819\n",
      "Training iter #19572000:   Batch Loss = 7.735690, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.96221923828125, Accuracy = 0.8826682567596436\n",
      "Training iter #19575000:   Batch Loss = 7.615157, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.96189022064209, Accuracy = 0.8829660415649414\n",
      "Training iter #19578000:   Batch Loss = 7.555846, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.96150016784668, Accuracy = 0.8823704719543457\n",
      "Training iter #19581000:   Batch Loss = 7.640834, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9612250328063965, Accuracy = 0.8820726871490479\n",
      "Training iter #19584000:   Batch Loss = 7.695648, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.961121082305908, Accuracy = 0.8820726871490479\n",
      "Training iter #19587000:   Batch Loss = 7.668820, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.961301326751709, Accuracy = 0.8820726871490479\n",
      "Training iter #19590000:   Batch Loss = 7.534802, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.961409091949463, Accuracy = 0.8823704719543457\n",
      "Training iter #19593000:   Batch Loss = 7.613333, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.962127685546875, Accuracy = 0.8817748427391052\n",
      "Training iter #19596000:   Batch Loss = 7.645940, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9622697830200195, Accuracy = 0.8823704719543457\n",
      "Training iter #19599000:   Batch Loss = 7.738995, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.962186336517334, Accuracy = 0.8823704719543457\n",
      "Training iter #19602000:   Batch Loss = 7.604819, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.962031841278076, Accuracy = 0.8823704719543457\n",
      "Training iter #19605000:   Batch Loss = 7.561723, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.961646556854248, Accuracy = 0.8826682567596436\n",
      "Training iter #19608000:   Batch Loss = 7.658289, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.960939407348633, Accuracy = 0.8820726871490479\n",
      "Training iter #19611000:   Batch Loss = 7.696539, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959939002990723, Accuracy = 0.8820726871490479\n",
      "Training iter #19614000:   Batch Loss = 7.659276, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.95979642868042, Accuracy = 0.8820726871490479\n",
      "Training iter #19617000:   Batch Loss = 7.531448, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959625244140625, Accuracy = 0.8820726871490479\n",
      "Training iter #19620000:   Batch Loss = 7.613234, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959294319152832, Accuracy = 0.8823704719543457\n",
      "Training iter #19623000:   Batch Loss = 7.629705, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959163665771484, Accuracy = 0.8835616707801819\n",
      "Training iter #19626000:   Batch Loss = 7.753509, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.958951950073242, Accuracy = 0.8835616707801819\n",
      "Training iter #19629000:   Batch Loss = 7.601190, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.958827972412109, Accuracy = 0.8835616707801819\n",
      "Training iter #19632000:   Batch Loss = 7.554234, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959622859954834, Accuracy = 0.8829660415649414\n",
      "Training iter #19635000:   Batch Loss = 7.659821, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959411144256592, Accuracy = 0.8832638263702393\n",
      "Training iter #19638000:   Batch Loss = 7.682304, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959261417388916, Accuracy = 0.8829660415649414\n",
      "Training iter #19641000:   Batch Loss = 7.672976, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9592509269714355, Accuracy = 0.8829660415649414\n",
      "Training iter #19644000:   Batch Loss = 7.533826, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.959125518798828, Accuracy = 0.8832638263702393\n",
      "Training iter #19647000:   Batch Loss = 7.613664, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.958899021148682, Accuracy = 0.8829660415649414\n",
      "Training iter #19650000:   Batch Loss = 7.643073, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.958681583404541, Accuracy = 0.8832638263702393\n",
      "Training iter #19653000:   Batch Loss = 7.750418, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.95847225189209, Accuracy = 0.8832638263702393\n",
      "Training iter #19656000:   Batch Loss = 7.571626, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.958227157592773, Accuracy = 0.8826682567596436\n",
      "Training iter #19659000:   Batch Loss = 7.551210, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.957851409912109, Accuracy = 0.8832638263702393\n",
      "Training iter #19662000:   Batch Loss = 7.662086, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.956785678863525, Accuracy = 0.8835616707801819\n",
      "Training iter #19665000:   Batch Loss = 7.681293, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9565815925598145, Accuracy = 0.8832638263702393\n",
      "Training iter #19668000:   Batch Loss = 7.663383, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.956601142883301, Accuracy = 0.8838594555854797\n",
      "Training iter #19671000:   Batch Loss = 7.543524, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.956532001495361, Accuracy = 0.8832638263702393\n",
      "Training iter #19674000:   Batch Loss = 7.615050, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.956203937530518, Accuracy = 0.8832638263702393\n",
      "Training iter #19677000:   Batch Loss = 7.635895, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.955821514129639, Accuracy = 0.8832638263702393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19680000:   Batch Loss = 7.750057, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.955557346343994, Accuracy = 0.8832638263702393\n",
      "Training iter #19683000:   Batch Loss = 7.553543, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.95532751083374, Accuracy = 0.8835616707801819\n",
      "Training iter #19686000:   Batch Loss = 7.544902, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.954945087432861, Accuracy = 0.8838594555854797\n",
      "Training iter #19689000:   Batch Loss = 7.677398, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.954643249511719, Accuracy = 0.8835616707801819\n",
      "Training iter #19692000:   Batch Loss = 7.678377, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.954562187194824, Accuracy = 0.8835616707801819\n",
      "Training iter #19695000:   Batch Loss = 7.660645, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.954514980316162, Accuracy = 0.8838594555854797\n",
      "Training iter #19698000:   Batch Loss = 7.552080, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.954427242279053, Accuracy = 0.8835616707801819\n",
      "Training iter #19701000:   Batch Loss = 7.615399, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.954124927520752, Accuracy = 0.8832638263702393\n",
      "Training iter #19704000:   Batch Loss = 7.657164, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.953887462615967, Accuracy = 0.8835616707801819\n",
      "Training iter #19707000:   Batch Loss = 7.733368, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.953660488128662, Accuracy = 0.8829660415649414\n",
      "Training iter #19710000:   Batch Loss = 7.546094, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.953430652618408, Accuracy = 0.8826682567596436\n",
      "Training iter #19713000:   Batch Loss = 7.548841, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9532036781311035, Accuracy = 0.8832638263702393\n",
      "Training iter #19716000:   Batch Loss = 7.667314, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.952938079833984, Accuracy = 0.8832638263702393\n",
      "Training iter #19719000:   Batch Loss = 7.678065, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.952998638153076, Accuracy = 0.8832638263702393\n",
      "Training iter #19722000:   Batch Loss = 7.659035, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.953271389007568, Accuracy = 0.8841572403907776\n",
      "Training iter #19725000:   Batch Loss = 7.559084, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.953099250793457, Accuracy = 0.8838594555854797\n",
      "Training iter #19728000:   Batch Loss = 7.611735, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9528398513793945, Accuracy = 0.8838594555854797\n",
      "Training iter #19731000:   Batch Loss = 7.658197, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.952672481536865, Accuracy = 0.8838594555854797\n",
      "Training iter #19734000:   Batch Loss = 7.715807, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9523491859436035, Accuracy = 0.8826682567596436\n",
      "Training iter #19737000:   Batch Loss = 7.530629, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.95206880569458, Accuracy = 0.8826682567596436\n",
      "Training iter #19740000:   Batch Loss = 7.549729, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.951735973358154, Accuracy = 0.8835616707801819\n",
      "Training iter #19743000:   Batch Loss = 7.645552, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.951555252075195, Accuracy = 0.8835616707801819\n",
      "Training iter #19746000:   Batch Loss = 7.703343, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.951327800750732, Accuracy = 0.8832638263702393\n",
      "Training iter #19749000:   Batch Loss = 7.674369, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9512176513671875, Accuracy = 0.8832638263702393\n",
      "Training iter #19752000:   Batch Loss = 7.565844, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.951160907745361, Accuracy = 0.8838594555854797\n",
      "Training iter #19755000:   Batch Loss = 7.612815, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9511637687683105, Accuracy = 0.8844550251960754\n",
      "Training iter #19758000:   Batch Loss = 7.652447, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.951066970825195, Accuracy = 0.8844550251960754\n",
      "Training iter #19761000:   Batch Loss = 7.694712, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.950634956359863, Accuracy = 0.8844550251960754\n",
      "Training iter #19764000:   Batch Loss = 7.516378, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.95017147064209, Accuracy = 0.8838594555854797\n",
      "Training iter #19767000:   Batch Loss = 7.565046, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.94973611831665, Accuracy = 0.8844550251960754\n",
      "Training iter #19770000:   Batch Loss = 7.642679, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.949664115905762, Accuracy = 0.8844550251960754\n",
      "Training iter #19773000:   Batch Loss = 7.695289, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.949508190155029, Accuracy = 0.8841572403907776\n",
      "Training iter #19776000:   Batch Loss = 7.654136, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.949258804321289, Accuracy = 0.8841572403907776\n",
      "Training iter #19779000:   Batch Loss = 7.569129, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9508819580078125, Accuracy = 0.8847528100013733\n",
      "Training iter #19782000:   Batch Loss = 7.606659, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.950180530548096, Accuracy = 0.8844550251960754\n",
      "Training iter #19785000:   Batch Loss = 7.650808, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.949625015258789, Accuracy = 0.8844550251960754\n",
      "Training iter #19788000:   Batch Loss = 7.698219, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.949242115020752, Accuracy = 0.8850506544113159\n",
      "Training iter #19791000:   Batch Loss = 7.525438, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.948873519897461, Accuracy = 0.8856462240219116\n",
      "Training iter #19794000:   Batch Loss = 7.572450, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.948551177978516, Accuracy = 0.8859440088272095\n",
      "Training iter #19797000:   Batch Loss = 7.622984, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.948284149169922, Accuracy = 0.8856462240219116\n",
      "Training iter #19800000:   Batch Loss = 7.704950, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9479780197143555, Accuracy = 0.8853484392166138\n",
      "Training iter #19803000:   Batch Loss = 7.632589, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.947729587554932, Accuracy = 0.8853484392166138\n",
      "Training iter #19806000:   Batch Loss = 7.560489, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.947646141052246, Accuracy = 0.8853484392166138\n",
      "Training iter #19809000:   Batch Loss = 7.642984, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9475789070129395, Accuracy = 0.8853484392166138\n",
      "Training iter #19812000:   Batch Loss = 7.660675, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9475250244140625, Accuracy = 0.8859440088272095\n",
      "Training iter #19815000:   Batch Loss = 7.696939, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.947292327880859, Accuracy = 0.8859440088272095\n",
      "Training iter #19818000:   Batch Loss = 7.513192, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.946948051452637, Accuracy = 0.8859440088272095\n",
      "Training iter #19821000:   Batch Loss = 7.573267, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.946581840515137, Accuracy = 0.8856462240219116\n",
      "Training iter #19824000:   Batch Loss = 7.611484, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.946272850036621, Accuracy = 0.8853484392166138\n",
      "Training iter #19827000:   Batch Loss = 7.699967, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.946085453033447, Accuracy = 0.8853484392166138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19830000:   Batch Loss = 7.616226, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945969104766846, Accuracy = 0.8853484392166138\n",
      "Training iter #19833000:   Batch Loss = 7.553227, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.94596004486084, Accuracy = 0.8853484392166138\n",
      "Training iter #19836000:   Batch Loss = 7.640591, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945895671844482, Accuracy = 0.8853484392166138\n",
      "Training iter #19839000:   Batch Loss = 7.664116, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945878028869629, Accuracy = 0.8856462240219116\n",
      "Training iter #19842000:   Batch Loss = 7.688601, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945738792419434, Accuracy = 0.8856462240219116\n",
      "Training iter #19845000:   Batch Loss = 7.522869, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945455551147461, Accuracy = 0.8856462240219116\n",
      "Training iter #19848000:   Batch Loss = 7.572969, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945163726806641, Accuracy = 0.8856462240219116\n",
      "Training iter #19851000:   Batch Loss = 7.606818, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945102691650391, Accuracy = 0.8859440088272095\n",
      "Training iter #19854000:   Batch Loss = 7.706242, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.944962501525879, Accuracy = 0.8856462240219116\n",
      "Training iter #19857000:   Batch Loss = 7.604276, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.944909572601318, Accuracy = 0.8853484392166138\n",
      "Training iter #19860000:   Batch Loss = 7.552578, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.945810317993164, Accuracy = 0.8850506544113159\n",
      "Training iter #19863000:   Batch Loss = 7.640167, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.946006774902344, Accuracy = 0.8850506544113159\n",
      "Training iter #19866000:   Batch Loss = 7.669589, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.946289539337158, Accuracy = 0.8859440088272095\n",
      "Training iter #19869000:   Batch Loss = 7.679067, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.946086883544922, Accuracy = 0.8853484392166138\n",
      "Training iter #19872000:   Batch Loss = 7.521393, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9455885887146, Accuracy = 0.8847528100013733\n",
      "Training iter #19875000:   Batch Loss = 7.591182, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.94505500793457, Accuracy = 0.8844550251960754\n",
      "Training iter #19878000:   Batch Loss = 7.615045, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.94473123550415, Accuracy = 0.8841572403907776\n",
      "Training iter #19881000:   Batch Loss = 7.708802, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.943999290466309, Accuracy = 0.8835616707801819\n",
      "Training iter #19884000:   Batch Loss = 7.593660, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.943291664123535, Accuracy = 0.8832638263702393\n",
      "Training iter #19887000:   Batch Loss = 7.548114, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.942315101623535, Accuracy = 0.8841572403907776\n",
      "Training iter #19890000:   Batch Loss = 7.615409, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.941390514373779, Accuracy = 0.8853484392166138\n",
      "Training iter #19893000:   Batch Loss = 7.676134, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.942356586456299, Accuracy = 0.8850506544113159\n",
      "Training iter #19896000:   Batch Loss = 7.651423, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.942454814910889, Accuracy = 0.8850506544113159\n",
      "Training iter #19899000:   Batch Loss = 7.525122, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9422454833984375, Accuracy = 0.8850506544113159\n",
      "Training iter #19902000:   Batch Loss = 7.590651, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9416327476501465, Accuracy = 0.8841572403907776\n",
      "Training iter #19905000:   Batch Loss = 7.622335, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.941019535064697, Accuracy = 0.8832638263702393\n",
      "Training iter #19908000:   Batch Loss = 7.718529, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.940649032592773, Accuracy = 0.8832638263702393\n",
      "Training iter #19911000:   Batch Loss = 7.598286, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9405293464660645, Accuracy = 0.8829660415649414\n",
      "Training iter #19914000:   Batch Loss = 7.537626, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.940389633178711, Accuracy = 0.8838594555854797\n",
      "Training iter #19917000:   Batch Loss = 7.620654, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.940306186676025, Accuracy = 0.8847528100013733\n",
      "Training iter #19920000:   Batch Loss = 7.671317, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.940139293670654, Accuracy = 0.8853484392166138\n",
      "Training iter #19923000:   Batch Loss = 7.652806, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.939859390258789, Accuracy = 0.8850506544113159\n",
      "Training iter #19926000:   Batch Loss = 7.511394, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.939456462860107, Accuracy = 0.8844550251960754\n",
      "Training iter #19929000:   Batch Loss = 7.590030, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.939892292022705, Accuracy = 0.8841572403907776\n",
      "Training iter #19932000:   Batch Loss = 7.619888, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.940503120422363, Accuracy = 0.8841572403907776\n",
      "Training iter #19935000:   Batch Loss = 7.724718, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9410786628723145, Accuracy = 0.8844550251960754\n",
      "Training iter #19938000:   Batch Loss = 7.581633, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.941439628601074, Accuracy = 0.8841572403907776\n",
      "Training iter #19941000:   Batch Loss = 7.543195, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.941301345825195, Accuracy = 0.8844550251960754\n",
      "Training iter #19944000:   Batch Loss = 7.640508, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.940659046173096, Accuracy = 0.8847528100013733\n",
      "Training iter #19947000:   Batch Loss = 7.668011, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.939783573150635, Accuracy = 0.8850506544113159\n",
      "Training iter #19950000:   Batch Loss = 7.633500, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.938681602478027, Accuracy = 0.8850506544113159\n",
      "Training iter #19953000:   Batch Loss = 7.514165, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.937751770019531, Accuracy = 0.8853484392166138\n",
      "Training iter #19956000:   Batch Loss = 7.595040, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.93698263168335, Accuracy = 0.8856462240219116\n",
      "Training iter #19959000:   Batch Loss = 7.616055, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.936920642852783, Accuracy = 0.8865395784378052\n",
      "Training iter #19962000:   Batch Loss = 7.719343, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.936774730682373, Accuracy = 0.8856462240219116\n",
      "Training iter #19965000:   Batch Loss = 7.565126, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934752464294434, Accuracy = 0.8859440088272095\n",
      "Training iter #19968000:   Batch Loss = 7.527962, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934570789337158, Accuracy = 0.8862417936325073\n",
      "Training iter #19971000:   Batch Loss = 7.638795, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934330940246582, Accuracy = 0.8859440088272095\n",
      "Training iter #19974000:   Batch Loss = 7.654671, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934201717376709, Accuracy = 0.8859440088272095\n",
      "Training iter #19977000:   Batch Loss = 7.649802, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934198379516602, Accuracy = 0.8853484392166138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19980000:   Batch Loss = 7.518382, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934184551239014, Accuracy = 0.8850506544113159\n",
      "Training iter #19983000:   Batch Loss = 7.594693, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934029579162598, Accuracy = 0.8853484392166138\n",
      "Training iter #19986000:   Batch Loss = 7.615232, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9353508949279785, Accuracy = 0.8847528100013733\n",
      "Training iter #19989000:   Batch Loss = 7.734119, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934870719909668, Accuracy = 0.8847528100013733\n",
      "Training iter #19992000:   Batch Loss = 7.545286, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.934619426727295, Accuracy = 0.8853484392166138\n",
      "Training iter #19995000:   Batch Loss = 7.520418, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.932559967041016, Accuracy = 0.8844550251960754\n",
      "Training iter #19998000:   Batch Loss = 7.645349, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.931891918182373, Accuracy = 0.8853484392166138\n",
      "Training iter #20001000:   Batch Loss = 7.660139, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.93137788772583, Accuracy = 0.8841572403907776\n",
      "Training iter #20004000:   Batch Loss = 7.641765, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9314775466918945, Accuracy = 0.8844550251960754\n",
      "Training iter #20007000:   Batch Loss = 7.524937, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.931556701660156, Accuracy = 0.8844550251960754\n",
      "Training iter #20010000:   Batch Loss = 7.588605, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.931534290313721, Accuracy = 0.8850506544113159\n",
      "Training iter #20013000:   Batch Loss = 7.617285, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.931207656860352, Accuracy = 0.8850506544113159\n",
      "Training iter #20016000:   Batch Loss = 7.720788, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930895805358887, Accuracy = 0.8850506544113159\n",
      "Training iter #20019000:   Batch Loss = 7.529923, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930789947509766, Accuracy = 0.8862417936325073\n",
      "Training iter #20022000:   Batch Loss = 7.518060, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930610656738281, Accuracy = 0.8874329924583435\n",
      "Training iter #20025000:   Batch Loss = 7.649239, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930436611175537, Accuracy = 0.8874329924583435\n",
      "Training iter #20028000:   Batch Loss = 7.651195, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930130481719971, Accuracy = 0.8871352076530457\n",
      "Training iter #20031000:   Batch Loss = 7.633676, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.929827690124512, Accuracy = 0.8868374228477478\n",
      "Training iter #20034000:   Batch Loss = 7.530946, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9296441078186035, Accuracy = 0.8862417936325073\n",
      "Training iter #20037000:   Batch Loss = 7.584660, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.929505825042725, Accuracy = 0.8865395784378052\n",
      "Training iter #20040000:   Batch Loss = 7.636699, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.931129455566406, Accuracy = 0.8856462240219116\n",
      "Training iter #20043000:   Batch Loss = 7.702868, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930911064147949, Accuracy = 0.8853484392166138\n",
      "Training iter #20046000:   Batch Loss = 7.518814, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930603981018066, Accuracy = 0.8859440088272095\n",
      "Training iter #20049000:   Batch Loss = 7.518831, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.930253028869629, Accuracy = 0.8853484392166138\n",
      "Training iter #20052000:   Batch Loss = 7.639009, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.929948806762695, Accuracy = 0.8856462240219116\n",
      "Training iter #20055000:   Batch Loss = 7.662495, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.92974853515625, Accuracy = 0.8850506544113159\n",
      "Training iter #20058000:   Batch Loss = 7.641010, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9295196533203125, Accuracy = 0.8847528100013733\n",
      "Training iter #20061000:   Batch Loss = 7.546899, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.929257869720459, Accuracy = 0.8853484392166138\n",
      "Training iter #20064000:   Batch Loss = 7.588037, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.929075717926025, Accuracy = 0.8850506544113159\n",
      "Training iter #20067000:   Batch Loss = 7.634093, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9289469718933105, Accuracy = 0.8844550251960754\n",
      "Training iter #20070000:   Batch Loss = 7.671440, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.928947925567627, Accuracy = 0.8850506544113159\n",
      "Training iter #20073000:   Batch Loss = 7.502169, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9287309646606445, Accuracy = 0.8850506544113159\n",
      "Training iter #20076000:   Batch Loss = 7.540151, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.929269790649414, Accuracy = 0.8847528100013733\n",
      "Training iter #20079000:   Batch Loss = 7.618695, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.92931604385376, Accuracy = 0.8847528100013733\n",
      "Training iter #20082000:   Batch Loss = 7.675762, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.92903995513916, Accuracy = 0.8847528100013733\n",
      "Training iter #20085000:   Batch Loss = 7.647513, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9280476570129395, Accuracy = 0.8847528100013733\n",
      "Training iter #20088000:   Batch Loss = 7.549069, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.92814826965332, Accuracy = 0.8847528100013733\n",
      "Training iter #20091000:   Batch Loss = 7.584070, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9279375076293945, Accuracy = 0.8847528100013733\n",
      "Training iter #20094000:   Batch Loss = 7.628854, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.927028179168701, Accuracy = 0.8850506544113159\n",
      "Training iter #20097000:   Batch Loss = 7.671210, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.927089214324951, Accuracy = 0.8844550251960754\n",
      "Training iter #20100000:   Batch Loss = 7.497385, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.925196647644043, Accuracy = 0.8850506544113159\n",
      "Training iter #20103000:   Batch Loss = 7.548919, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.925017833709717, Accuracy = 0.8856462240219116\n",
      "Training iter #20106000:   Batch Loss = 7.618841, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9250335693359375, Accuracy = 0.8856462240219116\n",
      "Training iter #20109000:   Batch Loss = 7.677852, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.925156593322754, Accuracy = 0.8856462240219116\n",
      "Training iter #20112000:   Batch Loss = 7.613572, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.925127029418945, Accuracy = 0.8856462240219116\n",
      "Training iter #20115000:   Batch Loss = 7.540509, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.925070285797119, Accuracy = 0.8859440088272095\n",
      "Training iter #20118000:   Batch Loss = 7.605368, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924941539764404, Accuracy = 0.8862417936325073\n",
      "Training iter #20121000:   Batch Loss = 7.638939, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924767017364502, Accuracy = 0.8862417936325073\n",
      "Training iter #20124000:   Batch Loss = 7.662771, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924574375152588, Accuracy = 0.8862417936325073\n",
      "Training iter #20127000:   Batch Loss = 7.503133, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924598217010498, Accuracy = 0.8865395784378052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20130000:   Batch Loss = 7.544456, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924478530883789, Accuracy = 0.8859440088272095\n",
      "Training iter #20133000:   Batch Loss = 7.596349, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924219131469727, Accuracy = 0.8856462240219116\n",
      "Training iter #20136000:   Batch Loss = 7.679230, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923333168029785, Accuracy = 0.8856462240219116\n",
      "Training iter #20139000:   Batch Loss = 7.599442, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923296928405762, Accuracy = 0.8856462240219116\n",
      "Training iter #20142000:   Batch Loss = 7.540251, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923472881317139, Accuracy = 0.8865395784378052\n",
      "Training iter #20145000:   Batch Loss = 7.620233, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924977779388428, Accuracy = 0.8862417936325073\n",
      "Training iter #20148000:   Batch Loss = 7.645744, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924869060516357, Accuracy = 0.8856462240219116\n",
      "Training iter #20151000:   Batch Loss = 7.665633, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923117637634277, Accuracy = 0.8853484392166138\n",
      "Training iter #20154000:   Batch Loss = 7.502237, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923638820648193, Accuracy = 0.8847528100013733\n",
      "Training iter #20157000:   Batch Loss = 7.546272, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923663139343262, Accuracy = 0.8853484392166138\n",
      "Training iter #20160000:   Batch Loss = 7.596966, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923615455627441, Accuracy = 0.8856462240219116\n",
      "Training iter #20163000:   Batch Loss = 7.677570, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923624515533447, Accuracy = 0.8859440088272095\n",
      "Training iter #20166000:   Batch Loss = 7.589124, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923522472381592, Accuracy = 0.8859440088272095\n",
      "Training iter #20169000:   Batch Loss = 7.529503, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9235429763793945, Accuracy = 0.8862417936325073\n",
      "Training iter #20172000:   Batch Loss = 7.620393, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924979209899902, Accuracy = 0.8856462240219116\n",
      "Training iter #20175000:   Batch Loss = 7.640774, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924721717834473, Accuracy = 0.8847528100013733\n",
      "Training iter #20178000:   Batch Loss = 7.660304, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924403190612793, Accuracy = 0.8841572403907776\n",
      "Training iter #20181000:   Batch Loss = 7.503612, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.924198627471924, Accuracy = 0.8841572403907776\n",
      "Training iter #20184000:   Batch Loss = 7.544920, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.923800468444824, Accuracy = 0.8844550251960754\n",
      "Training iter #20187000:   Batch Loss = 7.581870, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.922473907470703, Accuracy = 0.8856462240219116\n",
      "Training iter #20190000:   Batch Loss = 7.685696, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.920487403869629, Accuracy = 0.8859440088272095\n",
      "Training iter #20193000:   Batch Loss = 7.580262, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.920230388641357, Accuracy = 0.8859440088272095\n",
      "Training iter #20196000:   Batch Loss = 7.530553, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.92076301574707, Accuracy = 0.8859440088272095\n",
      "Training iter #20199000:   Batch Loss = 7.597992, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9207305908203125, Accuracy = 0.8850506544113159\n",
      "Training iter #20202000:   Batch Loss = 7.647233, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.920748710632324, Accuracy = 0.8850506544113159\n",
      "Training iter #20205000:   Batch Loss = 7.645955, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.920555591583252, Accuracy = 0.8850506544113159\n",
      "Training iter #20208000:   Batch Loss = 7.501154, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.922086715698242, Accuracy = 0.8847528100013733\n",
      "Training iter #20211000:   Batch Loss = 7.565493, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.921846389770508, Accuracy = 0.8850506544113159\n",
      "Training iter #20214000:   Batch Loss = 7.599578, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9215240478515625, Accuracy = 0.8856462240219116\n",
      "Training iter #20217000:   Batch Loss = 7.680776, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.921254634857178, Accuracy = 0.8862417936325073\n",
      "Training iter #20220000:   Batch Loss = 7.574448, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.921178817749023, Accuracy = 0.8862417936325073\n",
      "Training iter #20223000:   Batch Loss = 7.517145, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9209160804748535, Accuracy = 0.8862417936325073\n",
      "Training iter #20226000:   Batch Loss = 7.598606, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.92025089263916, Accuracy = 0.8856462240219116\n",
      "Training iter #20229000:   Batch Loss = 7.651487, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9197163581848145, Accuracy = 0.8853484392166138\n",
      "Training iter #20232000:   Batch Loss = 7.611455, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.918855667114258, Accuracy = 0.8853484392166138\n",
      "Training iter #20235000:   Batch Loss = 7.504500, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.919285774230957, Accuracy = 0.8850506544113159\n",
      "Training iter #20238000:   Batch Loss = 7.569583, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.918863773345947, Accuracy = 0.8853484392166138\n",
      "Training iter #20241000:   Batch Loss = 7.604026, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.918631076812744, Accuracy = 0.8853484392166138\n",
      "Training iter #20244000:   Batch Loss = 7.690225, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.918563365936279, Accuracy = 0.8850506544113159\n",
      "Training iter #20247000:   Batch Loss = 7.564031, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.918631553649902, Accuracy = 0.8850506544113159\n",
      "Training iter #20250000:   Batch Loss = 7.515712, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.91855001449585, Accuracy = 0.8850506544113159\n",
      "Training iter #20253000:   Batch Loss = 7.600029, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.918338775634766, Accuracy = 0.8853484392166138\n",
      "Training iter #20256000:   Batch Loss = 7.647845, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.918034553527832, Accuracy = 0.8853484392166138\n",
      "Training iter #20259000:   Batch Loss = 7.612892, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.917501449584961, Accuracy = 0.8853484392166138\n",
      "Training iter #20262000:   Batch Loss = 7.493201, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.916998386383057, Accuracy = 0.8853484392166138\n",
      "Training iter #20265000:   Batch Loss = 7.570226, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.916402339935303, Accuracy = 0.8853484392166138\n",
      "Training iter #20268000:   Batch Loss = 7.582508, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.915929317474365, Accuracy = 0.8856462240219116\n",
      "Training iter #20271000:   Batch Loss = 7.700941, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.916041374206543, Accuracy = 0.8853484392166138\n",
      "Training iter #20274000:   Batch Loss = 7.560784, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.916182518005371, Accuracy = 0.8853484392166138\n",
      "Training iter #20277000:   Batch Loss = 7.510238, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9161553382873535, Accuracy = 0.8853484392166138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20280000:   Batch Loss = 7.615027, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.916192531585693, Accuracy = 0.8856462240219116\n",
      "Training iter #20283000:   Batch Loss = 7.638923, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.916258811950684, Accuracy = 0.8853484392166138\n",
      "Training iter #20286000:   Batch Loss = 7.621297, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9160566329956055, Accuracy = 0.8850506544113159\n",
      "Training iter #20289000:   Batch Loss = 7.493619, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.915746688842773, Accuracy = 0.8847528100013733\n",
      "Training iter #20292000:   Batch Loss = 7.571872, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.91549015045166, Accuracy = 0.8850506544113159\n",
      "Training iter #20295000:   Batch Loss = 7.593478, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.914951324462891, Accuracy = 0.8853484392166138\n",
      "Training iter #20298000:   Batch Loss = 7.698518, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.914132118225098, Accuracy = 0.8856462240219116\n",
      "Training iter #20301000:   Batch Loss = 7.533626, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.913581848144531, Accuracy = 0.8859440088272095\n",
      "Training iter #20304000:   Batch Loss = 7.508099, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9135260581970215, Accuracy = 0.8859440088272095\n",
      "Training iter #20307000:   Batch Loss = 7.617510, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.913180351257324, Accuracy = 0.8862417936325073\n",
      "Training iter #20310000:   Batch Loss = 7.629016, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.91247034072876, Accuracy = 0.8859440088272095\n",
      "Training iter #20313000:   Batch Loss = 7.609796, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.911649227142334, Accuracy = 0.8865395784378052\n",
      "Training iter #20316000:   Batch Loss = 7.504816, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.91135311126709, Accuracy = 0.8865395784378052\n",
      "Training iter #20319000:   Batch Loss = 7.575260, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.911137104034424, Accuracy = 0.8862417936325073\n",
      "Training iter #20322000:   Batch Loss = 7.590834, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.911018371582031, Accuracy = 0.8859440088272095\n",
      "Training iter #20325000:   Batch Loss = 7.704843, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9105000495910645, Accuracy = 0.8859440088272095\n",
      "Training iter #20328000:   Batch Loss = 7.517919, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.910425186157227, Accuracy = 0.8862417936325073\n",
      "Training iter #20331000:   Batch Loss = 7.503146, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.910149574279785, Accuracy = 0.8862417936325073\n",
      "Training iter #20334000:   Batch Loss = 7.632620, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.910031318664551, Accuracy = 0.8868374228477478\n",
      "Training iter #20337000:   Batch Loss = 7.633446, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.909811496734619, Accuracy = 0.8865395784378052\n",
      "Training iter #20340000:   Batch Loss = 7.612471, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.909353733062744, Accuracy = 0.8871352076530457\n",
      "Training iter #20343000:   Batch Loss = 7.509375, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.909491062164307, Accuracy = 0.8868374228477478\n",
      "Training iter #20346000:   Batch Loss = 7.573815, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.90908670425415, Accuracy = 0.8871352076530457\n",
      "Training iter #20349000:   Batch Loss = 7.606407, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.908618450164795, Accuracy = 0.8877307772636414\n",
      "Training iter #20352000:   Batch Loss = 7.684866, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.908238410949707, Accuracy = 0.8871352076530457\n",
      "Training iter #20355000:   Batch Loss = 7.507829, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.907792568206787, Accuracy = 0.8868374228477478\n",
      "Training iter #20358000:   Batch Loss = 7.507361, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.907993316650391, Accuracy = 0.8865395784378052\n",
      "Training iter #20361000:   Batch Loss = 7.621509, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.907937049865723, Accuracy = 0.8871352076530457\n",
      "Training iter #20364000:   Batch Loss = 7.629020, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.907495498657227, Accuracy = 0.8871352076530457\n",
      "Training iter #20367000:   Batch Loss = 7.608495, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.906650066375732, Accuracy = 0.8871352076530457\n",
      "Training iter #20370000:   Batch Loss = 7.509385, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.906327247619629, Accuracy = 0.8874329924583435\n",
      "Training iter #20373000:   Batch Loss = 7.565208, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.906032085418701, Accuracy = 0.8877307772636414\n",
      "Training iter #20376000:   Batch Loss = 7.612269, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.905911922454834, Accuracy = 0.8877307772636414\n",
      "Training iter #20379000:   Batch Loss = 7.668523, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.905874729156494, Accuracy = 0.8868374228477478\n",
      "Training iter #20382000:   Batch Loss = 7.493484, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.905855655670166, Accuracy = 0.8871352076530457\n",
      "Training iter #20385000:   Batch Loss = 7.506980, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.90577507019043, Accuracy = 0.8868374228477478\n",
      "Training iter #20388000:   Batch Loss = 7.598967, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.906192779541016, Accuracy = 0.8859440088272095\n",
      "Training iter #20391000:   Batch Loss = 7.650773, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9058990478515625, Accuracy = 0.8862417936325073\n",
      "Training iter #20394000:   Batch Loss = 7.621964, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.906009674072266, Accuracy = 0.8865395784378052\n",
      "Training iter #20397000:   Batch Loss = 7.526525, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9061479568481445, Accuracy = 0.8868374228477478\n",
      "Training iter #20400000:   Batch Loss = 7.556764, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.906070232391357, Accuracy = 0.8871352076530457\n",
      "Training iter #20403000:   Batch Loss = 7.604027, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.905745983123779, Accuracy = 0.8865395784378052\n",
      "Training iter #20406000:   Batch Loss = 7.640120, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.904952049255371, Accuracy = 0.8865395784378052\n",
      "Training iter #20409000:   Batch Loss = 7.479622, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.904754161834717, Accuracy = 0.8865395784378052\n",
      "Training iter #20412000:   Batch Loss = 7.519490, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.904843330383301, Accuracy = 0.8880285620689392\n",
      "Training iter #20415000:   Batch Loss = 7.593388, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9045915603637695, Accuracy = 0.8880285620689392\n",
      "Training iter #20418000:   Batch Loss = 7.646248, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9038801193237305, Accuracy = 0.8886241912841797\n",
      "Training iter #20421000:   Batch Loss = 7.615413, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.903825759887695, Accuracy = 0.8877307772636414\n",
      "Training iter #20424000:   Batch Loss = 7.528969, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.903802871704102, Accuracy = 0.8877307772636414\n",
      "Training iter #20427000:   Batch Loss = 7.556610, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.903833389282227, Accuracy = 0.8877307772636414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20430000:   Batch Loss = 7.607790, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.903479099273682, Accuracy = 0.8877307772636414\n",
      "Training iter #20433000:   Batch Loss = 7.651009, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9033122062683105, Accuracy = 0.8880285620689392\n",
      "Training iter #20436000:   Batch Loss = 7.480395, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9030842781066895, Accuracy = 0.8877307772636414\n",
      "Training iter #20439000:   Batch Loss = 7.524952, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.902791500091553, Accuracy = 0.8877307772636414\n",
      "Training iter #20442000:   Batch Loss = 7.575360, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.902610778808594, Accuracy = 0.8874329924583435\n",
      "Training iter #20445000:   Batch Loss = 7.662085, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.90239953994751, Accuracy = 0.8874329924583435\n",
      "Training iter #20448000:   Batch Loss = 7.584702, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.902213096618652, Accuracy = 0.8874329924583435\n",
      "Training iter #20451000:   Batch Loss = 7.520013, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.902066230773926, Accuracy = 0.8874329924583435\n",
      "Training iter #20454000:   Batch Loss = 7.588861, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.90199613571167, Accuracy = 0.8874329924583435\n",
      "Training iter #20457000:   Batch Loss = 7.611135, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9019083976745605, Accuracy = 0.8874329924583435\n",
      "Training iter #20460000:   Batch Loss = 7.647462, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.901800155639648, Accuracy = 0.8865395784378052\n",
      "Training iter #20463000:   Batch Loss = 7.475745, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.9017181396484375, Accuracy = 0.8868374228477478\n",
      "Training iter #20466000:   Batch Loss = 7.518388, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.901508331298828, Accuracy = 0.8871352076530457\n",
      "Training iter #20469000:   Batch Loss = 7.566896, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.901392936706543, Accuracy = 0.8877307772636414\n",
      "Training iter #20472000:   Batch Loss = 7.652815, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.901450157165527, Accuracy = 0.8874329924583435\n",
      "Training iter #20475000:   Batch Loss = 7.572081, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.90146541595459, Accuracy = 0.8874329924583435\n",
      "Training iter #20478000:   Batch Loss = 7.510979, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.901193141937256, Accuracy = 0.8874329924583435\n",
      "Training iter #20481000:   Batch Loss = 7.589720, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.900759696960449, Accuracy = 0.8868374228477478\n",
      "Training iter #20484000:   Batch Loss = 7.616970, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.90038537979126, Accuracy = 0.8871352076530457\n",
      "Training iter #20487000:   Batch Loss = 7.640899, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.90006160736084, Accuracy = 0.8865395784378052\n",
      "Training iter #20490000:   Batch Loss = 7.483805, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.899750232696533, Accuracy = 0.8865395784378052\n",
      "Training iter #20493000:   Batch Loss = 7.522738, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.899418354034424, Accuracy = 0.8871352076530457\n",
      "Training iter #20496000:   Batch Loss = 7.561787, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.89929723739624, Accuracy = 0.8874329924583435\n",
      "Training iter #20499000:   Batch Loss = 7.658556, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8993330001831055, Accuracy = 0.8874329924583435\n",
      "Training iter #20502000:   Batch Loss = 7.562378, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.899478435516357, Accuracy = 0.8874329924583435\n",
      "Training iter #20505000:   Batch Loss = 7.508153, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.899454593658447, Accuracy = 0.8877307772636414\n",
      "Training iter #20508000:   Batch Loss = 7.594973, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8999104499816895, Accuracy = 0.8868374228477478\n",
      "Training iter #20511000:   Batch Loss = 7.619062, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.899750232696533, Accuracy = 0.8865395784378052\n",
      "Training iter #20514000:   Batch Loss = 7.627502, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.898938179016113, Accuracy = 0.8865395784378052\n",
      "Training iter #20517000:   Batch Loss = 7.485043, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.898509502410889, Accuracy = 0.8862417936325073\n",
      "Training iter #20520000:   Batch Loss = 7.539867, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8979082107543945, Accuracy = 0.8865395784378052\n",
      "Training iter #20523000:   Batch Loss = 7.561173, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.897428512573242, Accuracy = 0.8868374228477478\n",
      "Training iter #20526000:   Batch Loss = 7.663324, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.897382736206055, Accuracy = 0.8877307772636414\n",
      "Training iter #20529000:   Batch Loss = 7.555101, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.897245407104492, Accuracy = 0.8874329924583435\n",
      "Training iter #20532000:   Batch Loss = 7.510447, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.897088050842285, Accuracy = 0.8874329924583435\n",
      "Training iter #20535000:   Batch Loss = 7.571877, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8968939781188965, Accuracy = 0.8874329924583435\n",
      "Training iter #20538000:   Batch Loss = 7.629834, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.896744728088379, Accuracy = 0.8871352076530457\n",
      "Training iter #20541000:   Batch Loss = 7.610954, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.896638870239258, Accuracy = 0.8874329924583435\n",
      "Training iter #20544000:   Batch Loss = 7.487171, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.896454334259033, Accuracy = 0.8871352076530457\n",
      "Training iter #20547000:   Batch Loss = 7.543833, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.896241664886475, Accuracy = 0.8874329924583435\n",
      "Training iter #20550000:   Batch Loss = 7.574347, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.895971775054932, Accuracy = 0.8877307772636414\n",
      "Training iter #20553000:   Batch Loss = 7.668349, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.895770072937012, Accuracy = 0.8889219760894775\n",
      "Training iter #20556000:   Batch Loss = 7.548989, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8955979347229, Accuracy = 0.8889219760894775\n",
      "Training iter #20559000:   Batch Loss = 7.498078, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.895501613616943, Accuracy = 0.8880285620689392\n",
      "Training iter #20562000:   Batch Loss = 7.572486, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8954548835754395, Accuracy = 0.8886241912841797\n",
      "Training iter #20565000:   Batch Loss = 7.621625, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.895385265350342, Accuracy = 0.8889219760894775\n",
      "Training iter #20568000:   Batch Loss = 7.598586, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.895170211791992, Accuracy = 0.8889219760894775\n",
      "Training iter #20571000:   Batch Loss = 7.474285, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.894540786743164, Accuracy = 0.8877307772636414\n",
      "Training iter #20574000:   Batch Loss = 7.546864, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.894101142883301, Accuracy = 0.8880285620689392\n",
      "Training iter #20577000:   Batch Loss = 7.576845, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.893970489501953, Accuracy = 0.8877307772636414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20580000:   Batch Loss = 7.672420, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.893847942352295, Accuracy = 0.8883264064788818\n",
      "Training iter #20583000:   Batch Loss = 7.538803, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.893553256988525, Accuracy = 0.8880285620689392\n",
      "Training iter #20586000:   Batch Loss = 7.496769, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.893228530883789, Accuracy = 0.8877307772636414\n",
      "Training iter #20589000:   Batch Loss = 7.592763, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.892939567565918, Accuracy = 0.8883264064788818\n",
      "Training iter #20592000:   Batch Loss = 7.620520, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8927459716796875, Accuracy = 0.8886241912841797\n",
      "Training iter #20595000:   Batch Loss = 7.585047, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.892570972442627, Accuracy = 0.8886241912841797\n",
      "Training iter #20598000:   Batch Loss = 7.475113, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.892506122589111, Accuracy = 0.8886241912841797\n",
      "Training iter #20601000:   Batch Loss = 7.546407, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.892723560333252, Accuracy = 0.8877307772636414\n",
      "Training iter #20604000:   Batch Loss = 7.567880, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.892462730407715, Accuracy = 0.8877307772636414\n",
      "Training iter #20607000:   Batch Loss = 7.671191, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.89185094833374, Accuracy = 0.8883264064788818\n",
      "Training iter #20610000:   Batch Loss = 7.534181, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891449928283691, Accuracy = 0.8883264064788818\n",
      "Training iter #20613000:   Batch Loss = 7.490435, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.89124870300293, Accuracy = 0.8883264064788818\n",
      "Training iter #20616000:   Batch Loss = 7.591045, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891165256500244, Accuracy = 0.8898153901100159\n",
      "Training iter #20619000:   Batch Loss = 7.607989, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891075134277344, Accuracy = 0.8898153901100159\n",
      "Training iter #20622000:   Batch Loss = 7.594646, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890900611877441, Accuracy = 0.8892197608947754\n",
      "Training iter #20625000:   Batch Loss = 7.478632, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890557765960693, Accuracy = 0.8892197608947754\n",
      "Training iter #20628000:   Batch Loss = 7.545186, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890357494354248, Accuracy = 0.8883264064788818\n",
      "Training iter #20631000:   Batch Loss = 7.567988, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890793800354004, Accuracy = 0.8886241912841797\n",
      "Training iter #20634000:   Batch Loss = 7.679801, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890960693359375, Accuracy = 0.8883264064788818\n",
      "Training iter #20637000:   Batch Loss = 7.519798, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891844272613525, Accuracy = 0.8883264064788818\n",
      "Training iter #20640000:   Batch Loss = 7.499454, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8912506103515625, Accuracy = 0.8880285620689392\n",
      "Training iter #20643000:   Batch Loss = 7.596063, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8900299072265625, Accuracy = 0.8886241912841797\n",
      "Training iter #20646000:   Batch Loss = 7.607611, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.889648914337158, Accuracy = 0.8889219760894775\n",
      "Training iter #20649000:   Batch Loss = 7.591122, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890109539031982, Accuracy = 0.8898153901100159\n",
      "Training iter #20652000:   Batch Loss = 7.495053, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8900251388549805, Accuracy = 0.8889219760894775\n",
      "Training iter #20655000:   Batch Loss = 7.546562, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.889391899108887, Accuracy = 0.8892197608947754\n",
      "Training iter #20658000:   Batch Loss = 7.563381, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891191482543945, Accuracy = 0.8889219760894775\n",
      "Training iter #20661000:   Batch Loss = 7.673130, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.892116069793701, Accuracy = 0.8892197608947754\n",
      "Training iter #20664000:   Batch Loss = 7.504788, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891510009765625, Accuracy = 0.8886241912841797\n",
      "Training iter #20667000:   Batch Loss = 7.506740, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891639232635498, Accuracy = 0.8889219760894775\n",
      "Training iter #20670000:   Batch Loss = 7.607710, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891763687133789, Accuracy = 0.8883264064788818\n",
      "Training iter #20673000:   Batch Loss = 7.608204, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891670227050781, Accuracy = 0.8886241912841797\n",
      "Training iter #20676000:   Batch Loss = 7.586209, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.89141845703125, Accuracy = 0.8883264064788818\n",
      "Training iter #20679000:   Batch Loss = 7.494870, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891552925109863, Accuracy = 0.8880285620689392\n",
      "Training iter #20682000:   Batch Loss = 7.541295, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.891305923461914, Accuracy = 0.8880285620689392\n",
      "Training iter #20685000:   Batch Loss = 7.588714, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890879154205322, Accuracy = 0.8883264064788818\n",
      "Training iter #20688000:   Batch Loss = 7.656081, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.890262126922607, Accuracy = 0.8889219760894775\n",
      "Training iter #20691000:   Batch Loss = 7.495664, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.889285564422607, Accuracy = 0.8892197608947754\n",
      "Training iter #20694000:   Batch Loss = 7.504934, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.888389587402344, Accuracy = 0.8895175457000732\n",
      "Training iter #20697000:   Batch Loss = 7.595183, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.888179779052734, Accuracy = 0.8898153901100159\n",
      "Training iter #20700000:   Batch Loss = 7.612916, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.888914585113525, Accuracy = 0.8898153901100159\n",
      "Training iter #20703000:   Batch Loss = 7.597539, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.888620853424072, Accuracy = 0.8895175457000732\n",
      "Training iter #20706000:   Batch Loss = 7.521399, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.888476371765137, Accuracy = 0.8895175457000732\n",
      "Training iter #20709000:   Batch Loss = 7.540065, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.887528896331787, Accuracy = 0.8898153901100159\n",
      "Training iter #20712000:   Batch Loss = 7.588506, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.885830879211426, Accuracy = 0.8901131749153137\n",
      "Training iter #20715000:   Batch Loss = 7.625556, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.885842800140381, Accuracy = 0.8892197608947754\n",
      "Training iter #20718000:   Batch Loss = 7.481273, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.885981559753418, Accuracy = 0.8889219760894775\n",
      "Training iter #20721000:   Batch Loss = 7.492168, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.883900165557861, Accuracy = 0.8889219760894775\n",
      "Training iter #20724000:   Batch Loss = 7.583446, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.883177757263184, Accuracy = 0.8895175457000732\n",
      "Training iter #20727000:   Batch Loss = 7.630182, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.883561134338379, Accuracy = 0.8892197608947754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20730000:   Batch Loss = 7.602334, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.884106159210205, Accuracy = 0.8892197608947754\n",
      "Training iter #20733000:   Batch Loss = 7.514241, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.884131908416748, Accuracy = 0.8892197608947754\n",
      "Training iter #20736000:   Batch Loss = 7.545972, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.883803367614746, Accuracy = 0.8901131749153137\n",
      "Training iter #20739000:   Batch Loss = 7.587000, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.883389949798584, Accuracy = 0.8918999433517456\n",
      "Training iter #20742000:   Batch Loss = 7.623718, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.882855415344238, Accuracy = 0.8916021585464478\n",
      "Training iter #20745000:   Batch Loss = 7.463705, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.882756233215332, Accuracy = 0.8916021585464478\n",
      "Training iter #20748000:   Batch Loss = 7.500320, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.882263660430908, Accuracy = 0.8916021585464478\n",
      "Training iter #20751000:   Batch Loss = 7.574156, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8817338943481445, Accuracy = 0.8910065293312073\n",
      "Training iter #20754000:   Batch Loss = 7.622669, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.881300449371338, Accuracy = 0.8910065293312073\n",
      "Training iter #20757000:   Batch Loss = 7.581651, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.880605697631836, Accuracy = 0.8904109597206116\n",
      "Training iter #20760000:   Batch Loss = 7.509328, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.880220413208008, Accuracy = 0.8904109597206116\n",
      "Training iter #20763000:   Batch Loss = 7.558881, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.879897117614746, Accuracy = 0.8901131749153137\n",
      "Training iter #20766000:   Batch Loss = 7.581600, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.879578113555908, Accuracy = 0.8907087445259094\n",
      "Training iter #20769000:   Batch Loss = 7.618104, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.879274845123291, Accuracy = 0.8913043737411499\n",
      "Training iter #20772000:   Batch Loss = 7.471365, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8790740966796875, Accuracy = 0.8907087445259094\n",
      "Training iter #20775000:   Batch Loss = 7.501023, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.878911972045898, Accuracy = 0.8904109597206116\n",
      "Training iter #20778000:   Batch Loss = 7.563073, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.878549098968506, Accuracy = 0.8904109597206116\n",
      "Training iter #20781000:   Batch Loss = 7.629907, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.878057956695557, Accuracy = 0.8913043737411499\n",
      "Training iter #20784000:   Batch Loss = 7.557091, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.877745151519775, Accuracy = 0.8910065293312073\n",
      "Training iter #20787000:   Batch Loss = 7.503053, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.87750244140625, Accuracy = 0.8907087445259094\n",
      "Training iter #20790000:   Batch Loss = 7.575519, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.877230644226074, Accuracy = 0.8904109597206116\n",
      "Training iter #20793000:   Batch Loss = 7.596131, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.877026557922363, Accuracy = 0.8910065293312073\n",
      "Training iter #20796000:   Batch Loss = 7.618881, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.876841068267822, Accuracy = 0.8910065293312073\n",
      "Training iter #20799000:   Batch Loss = 7.466077, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8767805099487305, Accuracy = 0.8907087445259094\n",
      "Training iter #20802000:   Batch Loss = 7.503355, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8766961097717285, Accuracy = 0.8910065293312073\n",
      "Training iter #20805000:   Batch Loss = 7.548242, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.876763820648193, Accuracy = 0.8918999433517456\n",
      "Training iter #20808000:   Batch Loss = 7.630346, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.87833309173584, Accuracy = 0.8918999433517456\n",
      "Training iter #20811000:   Batch Loss = 7.547932, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.878292560577393, Accuracy = 0.8918999433517456\n",
      "Training iter #20814000:   Batch Loss = 7.491955, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.877806186676025, Accuracy = 0.8918999433517456\n",
      "Training iter #20817000:   Batch Loss = 7.573158, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.878677845001221, Accuracy = 0.8913043737411499\n",
      "Training iter #20820000:   Batch Loss = 7.590196, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.878401756286621, Accuracy = 0.8910065293312073\n",
      "Training iter #20823000:   Batch Loss = 7.610095, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.877971649169922, Accuracy = 0.8910065293312073\n",
      "Training iter #20826000:   Batch Loss = 7.466882, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.877749919891357, Accuracy = 0.8907087445259094\n",
      "Training iter #20829000:   Batch Loss = 7.504687, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8773722648620605, Accuracy = 0.8910065293312073\n",
      "Training iter #20832000:   Batch Loss = 7.536780, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.876856327056885, Accuracy = 0.8910065293312073\n",
      "Training iter #20835000:   Batch Loss = 7.640785, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.876396179199219, Accuracy = 0.8913043737411499\n",
      "Training iter #20838000:   Batch Loss = 7.539803, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.876117706298828, Accuracy = 0.8913043737411499\n",
      "Training iter #20841000:   Batch Loss = 7.491868, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.876782417297363, Accuracy = 0.8910065293312073\n",
      "Training iter #20844000:   Batch Loss = 7.560773, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.876552581787109, Accuracy = 0.8907087445259094\n",
      "Training iter #20847000:   Batch Loss = 7.599163, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8762335777282715, Accuracy = 0.8913043737411499\n",
      "Training iter #20850000:   Batch Loss = 7.597414, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.875878810882568, Accuracy = 0.8913043737411499\n",
      "Training iter #20853000:   Batch Loss = 7.465440, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8757853507995605, Accuracy = 0.8910065293312073\n",
      "Training iter #20856000:   Batch Loss = 7.522133, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.875703811645508, Accuracy = 0.8904109597206116\n",
      "Training iter #20859000:   Batch Loss = 7.542703, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.875521659851074, Accuracy = 0.8907087445259094\n",
      "Training iter #20862000:   Batch Loss = 7.625274, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874546051025391, Accuracy = 0.8910065293312073\n",
      "Training iter #20865000:   Batch Loss = 7.529791, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874377727508545, Accuracy = 0.8910065293312073\n",
      "Training iter #20868000:   Batch Loss = 7.479239, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874241828918457, Accuracy = 0.8904109597206116\n",
      "Training iter #20871000:   Batch Loss = 7.551306, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8739213943481445, Accuracy = 0.8907087445259094\n",
      "Training iter #20874000:   Batch Loss = 7.601873, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874464511871338, Accuracy = 0.8916021585464478\n",
      "Training iter #20877000:   Batch Loss = 7.567312, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874440670013428, Accuracy = 0.8916021585464478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20880000:   Batch Loss = 7.468209, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874423027038574, Accuracy = 0.8913043737411499\n",
      "Training iter #20883000:   Batch Loss = 7.524981, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874223709106445, Accuracy = 0.8910065293312073\n",
      "Training iter #20886000:   Batch Loss = 7.559027, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.874006271362305, Accuracy = 0.8913043737411499\n",
      "Training iter #20889000:   Batch Loss = 7.642680, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.873673439025879, Accuracy = 0.8907087445259094\n",
      "Training iter #20892000:   Batch Loss = 7.524722, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.87341833114624, Accuracy = 0.8907087445259094\n",
      "Training iter #20895000:   Batch Loss = 7.477265, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.873073577880859, Accuracy = 0.8913043737411499\n",
      "Training iter #20898000:   Batch Loss = 7.555051, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8727335929870605, Accuracy = 0.8918999433517456\n",
      "Training iter #20901000:   Batch Loss = 7.602827, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.872383117675781, Accuracy = 0.8918999433517456\n",
      "Training iter #20904000:   Batch Loss = 7.576921, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.872150897979736, Accuracy = 0.8916021585464478\n",
      "Training iter #20907000:   Batch Loss = 7.454679, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.871868133544922, Accuracy = 0.8916021585464478\n",
      "Training iter #20910000:   Batch Loss = 7.527010, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.871384620666504, Accuracy = 0.8916021585464478\n",
      "Training iter #20913000:   Batch Loss = 7.538110, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.87040901184082, Accuracy = 0.8918999433517456\n",
      "Training iter #20916000:   Batch Loss = 7.652332, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.870122909545898, Accuracy = 0.8918999433517456\n",
      "Training iter #20919000:   Batch Loss = 7.518786, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.870320796966553, Accuracy = 0.8916021585464478\n",
      "Training iter #20922000:   Batch Loss = 7.472100, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.870150089263916, Accuracy = 0.8916021585464478\n",
      "Training iter #20925000:   Batch Loss = 7.571290, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.870835304260254, Accuracy = 0.8910065293312073\n",
      "Training iter #20928000:   Batch Loss = 7.592143, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.870596885681152, Accuracy = 0.8910065293312073\n",
      "Training iter #20931000:   Batch Loss = 7.563804, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8696489334106445, Accuracy = 0.8910065293312073\n",
      "Training iter #20934000:   Batch Loss = 7.457848, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8698601722717285, Accuracy = 0.8913043737411499\n",
      "Training iter #20937000:   Batch Loss = 7.528732, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8698296546936035, Accuracy = 0.8910065293312073\n",
      "Training iter #20940000:   Batch Loss = 7.545462, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8695173263549805, Accuracy = 0.8910065293312073\n",
      "Training iter #20943000:   Batch Loss = 7.645010, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.869505405426025, Accuracy = 0.8913043737411499\n",
      "Training iter #20946000:   Batch Loss = 7.501287, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.869516849517822, Accuracy = 0.8913043737411499\n",
      "Training iter #20949000:   Batch Loss = 7.470586, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.869486331939697, Accuracy = 0.8910065293312073\n",
      "Training iter #20952000:   Batch Loss = 7.569480, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.869278907775879, Accuracy = 0.8913043737411499\n",
      "Training iter #20955000:   Batch Loss = 7.579278, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.869098663330078, Accuracy = 0.8913043737411499\n",
      "Training iter #20958000:   Batch Loss = 7.576309, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.869306564331055, Accuracy = 0.8918999433517456\n",
      "Training iter #20961000:   Batch Loss = 7.463070, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.867275714874268, Accuracy = 0.8918999433517456\n",
      "Training iter #20964000:   Batch Loss = 7.525214, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.867107391357422, Accuracy = 0.8918999433517456\n",
      "Training iter #20967000:   Batch Loss = 7.547603, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86791467666626, Accuracy = 0.8921977281570435\n",
      "Training iter #20970000:   Batch Loss = 7.655052, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.867706298828125, Accuracy = 0.8927933573722839\n",
      "Training iter #20973000:   Batch Loss = 7.479405, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866970062255859, Accuracy = 0.8924955129623413\n",
      "Training iter #20976000:   Batch Loss = 7.463008, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866958141326904, Accuracy = 0.8927933573722839\n",
      "Training iter #20979000:   Batch Loss = 7.584610, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866933822631836, Accuracy = 0.8927933573722839\n",
      "Training iter #20982000:   Batch Loss = 7.589767, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.867059230804443, Accuracy = 0.8924955129623413\n",
      "Training iter #20985000:   Batch Loss = 7.569242, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86786413192749, Accuracy = 0.8916021585464478\n",
      "Training iter #20988000:   Batch Loss = 7.470884, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8673577308654785, Accuracy = 0.8918999433517456\n",
      "Training iter #20991000:   Batch Loss = 7.521600, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8670573234558105, Accuracy = 0.8921977281570435\n",
      "Training iter #20994000:   Batch Loss = 7.553990, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8678107261657715, Accuracy = 0.8921977281570435\n",
      "Training iter #20997000:   Batch Loss = 7.630521, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.868152141571045, Accuracy = 0.8916021585464478\n",
      "Training iter #21000000:   Batch Loss = 7.469893, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.868703365325928, Accuracy = 0.8916021585464478\n",
      "Training iter #21003000:   Batch Loss = 7.469224, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.868667125701904, Accuracy = 0.8907087445259094\n",
      "Training iter #21006000:   Batch Loss = 7.579835, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.868354797363281, Accuracy = 0.8907087445259094\n",
      "Training iter #21009000:   Batch Loss = 7.583719, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.867810249328613, Accuracy = 0.8910065293312073\n",
      "Training iter #21012000:   Batch Loss = 7.560520, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866640090942383, Accuracy = 0.8913043737411499\n",
      "Training iter #21015000:   Batch Loss = 7.472036, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866372108459473, Accuracy = 0.8910065293312073\n",
      "Training iter #21018000:   Batch Loss = 7.523969, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8663105964660645, Accuracy = 0.8913043737411499\n",
      "Training iter #21021000:   Batch Loss = 7.568096, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86639404296875, Accuracy = 0.8913043737411499\n",
      "Training iter #21024000:   Batch Loss = 7.622165, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866412162780762, Accuracy = 0.8916021585464478\n",
      "Training iter #21027000:   Batch Loss = 7.456989, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.867971420288086, Accuracy = 0.8913043737411499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #21030000:   Batch Loss = 7.479812, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.867655277252197, Accuracy = 0.8916021585464478\n",
      "Training iter #21033000:   Batch Loss = 7.563756, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866420745849609, Accuracy = 0.8913043737411499\n",
      "Training iter #21036000:   Batch Loss = 7.597496, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.866064548492432, Accuracy = 0.8916021585464478\n",
      "Training iter #21039000:   Batch Loss = 7.569733, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863980770111084, Accuracy = 0.8913043737411499\n",
      "Training iter #21042000:   Batch Loss = 7.486548, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863610744476318, Accuracy = 0.8913043737411499\n",
      "Training iter #21045000:   Batch Loss = 7.525307, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863415241241455, Accuracy = 0.8910065293312073\n",
      "Training iter #21048000:   Batch Loss = 7.562678, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863351345062256, Accuracy = 0.8913043737411499\n",
      "Training iter #21051000:   Batch Loss = 7.595881, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.865193843841553, Accuracy = 0.8910065293312073\n",
      "Training iter #21054000:   Batch Loss = 7.442976, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8651533126831055, Accuracy = 0.8907087445259094\n",
      "Training iter #21057000:   Batch Loss = 7.481173, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.864981174468994, Accuracy = 0.8904109597206116\n",
      "Training iter #21060000:   Batch Loss = 7.546166, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86469030380249, Accuracy = 0.8901131749153137\n",
      "Training iter #21063000:   Batch Loss = 7.599812, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8645195960998535, Accuracy = 0.8901131749153137\n",
      "Training iter #21066000:   Batch Loss = 7.571380, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.864434719085693, Accuracy = 0.8895175457000732\n",
      "Training iter #21069000:   Batch Loss = 7.489546, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86428689956665, Accuracy = 0.8904109597206116\n",
      "Training iter #21072000:   Batch Loss = 7.519585, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.864066123962402, Accuracy = 0.8892197608947754\n",
      "Training iter #21075000:   Batch Loss = 7.558834, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863911151885986, Accuracy = 0.8898153901100159\n",
      "Training iter #21078000:   Batch Loss = 7.602668, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863819122314453, Accuracy = 0.8898153901100159\n",
      "Training iter #21081000:   Batch Loss = 7.442846, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863707065582275, Accuracy = 0.8898153901100159\n",
      "Training iter #21084000:   Batch Loss = 7.486538, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8634796142578125, Accuracy = 0.8901131749153137\n",
      "Training iter #21087000:   Batch Loss = 7.548504, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.863014221191406, Accuracy = 0.8901131749153137\n",
      "Training iter #21090000:   Batch Loss = 7.615464, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.862610816955566, Accuracy = 0.8901131749153137\n",
      "Training iter #21093000:   Batch Loss = 7.544909, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.862322807312012, Accuracy = 0.8898153901100159\n",
      "Training iter #21096000:   Batch Loss = 7.478436, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.862067699432373, Accuracy = 0.8901131749153137\n",
      "Training iter #21099000:   Batch Loss = 7.539574, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.861758708953857, Accuracy = 0.8901131749153137\n",
      "Training iter #21102000:   Batch Loss = 7.562562, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.861422061920166, Accuracy = 0.8901131749153137\n",
      "Training iter #21105000:   Batch Loss = 7.588534, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.861239910125732, Accuracy = 0.8907087445259094\n",
      "Training iter #21108000:   Batch Loss = 7.446617, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.861151695251465, Accuracy = 0.8907087445259094\n",
      "Training iter #21111000:   Batch Loss = 7.483662, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.860968112945557, Accuracy = 0.8907087445259094\n",
      "Training iter #21114000:   Batch Loss = 7.525227, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.860692024230957, Accuracy = 0.8907087445259094\n",
      "Training iter #21117000:   Batch Loss = 7.606094, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.860487461090088, Accuracy = 0.8907087445259094\n",
      "Training iter #21120000:   Batch Loss = 7.531041, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.860499382019043, Accuracy = 0.8904109597206116\n",
      "Training iter #21123000:   Batch Loss = 7.475784, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86041784286499, Accuracy = 0.8910065293312073\n",
      "Training iter #21126000:   Batch Loss = 7.548122, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86024808883667, Accuracy = 0.8907087445259094\n",
      "Training iter #21129000:   Batch Loss = 7.570772, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.86008882522583, Accuracy = 0.8910065293312073\n",
      "Training iter #21132000:   Batch Loss = 7.591310, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.860042572021484, Accuracy = 0.8910065293312073\n",
      "Training iter #21135000:   Batch Loss = 7.445386, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.859987258911133, Accuracy = 0.8907087445259094\n",
      "Training iter #21138000:   Batch Loss = 7.484056, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.859764099121094, Accuracy = 0.8907087445259094\n",
      "Training iter #21141000:   Batch Loss = 7.522402, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.859424591064453, Accuracy = 0.8910065293312073\n",
      "Training iter #21144000:   Batch Loss = 7.609747, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.859213829040527, Accuracy = 0.8910065293312073\n",
      "Training iter #21147000:   Batch Loss = 7.522351, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8591742515563965, Accuracy = 0.8907087445259094\n",
      "Training iter #21150000:   Batch Loss = 7.469486, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.859066963195801, Accuracy = 0.8907087445259094\n",
      "Training iter #21153000:   Batch Loss = 7.551278, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.858784198760986, Accuracy = 0.8904109597206116\n",
      "Training iter #21156000:   Batch Loss = 7.568930, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.85844087600708, Accuracy = 0.8907087445259094\n",
      "Training iter #21159000:   Batch Loss = 7.578619, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.858236312866211, Accuracy = 0.8910065293312073\n",
      "Training iter #21162000:   Batch Loss = 7.448740, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.858156681060791, Accuracy = 0.8913043737411499\n",
      "Training iter #21165000:   Batch Loss = 7.484333, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.857866287231445, Accuracy = 0.8913043737411499\n",
      "Training iter #21168000:   Batch Loss = 7.513322, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.85727596282959, Accuracy = 0.8910065293312073\n",
      "Training iter #21171000:   Batch Loss = 7.607965, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.857003688812256, Accuracy = 0.8913043737411499\n",
      "Training iter #21174000:   Batch Loss = 7.515702, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.85718297958374, Accuracy = 0.8910065293312073\n",
      "Training iter #21177000:   Batch Loss = 7.471334, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.857052326202393, Accuracy = 0.8913043737411499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #21180000:   Batch Loss = 7.530762, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.856698036193848, Accuracy = 0.8913043737411499\n",
      "Training iter #21183000:   Batch Loss = 7.586680, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.856513977050781, Accuracy = 0.8913043737411499\n",
      "Training iter #21186000:   Batch Loss = 7.569910, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.85648250579834, Accuracy = 0.8913043737411499\n",
      "Training iter #21189000:   Batch Loss = 7.451241, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.856562614440918, Accuracy = 0.8916021585464478\n",
      "Training iter #21192000:   Batch Loss = 7.504320, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.856371879577637, Accuracy = 0.8916021585464478\n",
      "Training iter #21195000:   Batch Loss = 7.531897, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8560309410095215, Accuracy = 0.8913043737411499\n",
      "Training iter #21198000:   Batch Loss = 7.612997, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.855983734130859, Accuracy = 0.8904109597206116\n",
      "Training iter #21201000:   Batch Loss = 7.508429, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.856005668640137, Accuracy = 0.8898153901100159\n",
      "Training iter #21204000:   Batch Loss = 7.458777, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.85587739944458, Accuracy = 0.8895175457000732\n",
      "Training iter #21207000:   Batch Loss = 7.529252, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.855551242828369, Accuracy = 0.8898153901100159\n",
      "Training iter #21210000:   Batch Loss = 7.575336, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8550124168396, Accuracy = 0.8904109597206116\n",
      "Training iter #21213000:   Batch Loss = 7.552261, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.854438304901123, Accuracy = 0.8904109597206116\n",
      "Training iter #21216000:   Batch Loss = 7.440485, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.853912353515625, Accuracy = 0.8907087445259094\n",
      "Training iter #21219000:   Batch Loss = 7.507283, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.853117942810059, Accuracy = 0.8907087445259094\n",
      "Training iter #21222000:   Batch Loss = 7.532716, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.852208614349365, Accuracy = 0.8904109597206116\n",
      "Training iter #21225000:   Batch Loss = 7.615777, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.851843357086182, Accuracy = 0.8904109597206116\n",
      "Training iter #21228000:   Batch Loss = 7.497799, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8517889976501465, Accuracy = 0.8904109597206116\n",
      "Training iter #21231000:   Batch Loss = 7.458794, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.851912021636963, Accuracy = 0.8904109597206116\n",
      "Training iter #21234000:   Batch Loss = 7.550480, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.851524353027344, Accuracy = 0.8907087445259094\n",
      "Training iter #21237000:   Batch Loss = 7.575994, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.850937366485596, Accuracy = 0.8913043737411499\n",
      "Training iter #21240000:   Batch Loss = 7.541400, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.850727081298828, Accuracy = 0.8913043737411499\n",
      "Training iter #21243000:   Batch Loss = 7.437437, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.850593090057373, Accuracy = 0.8910065293312073\n",
      "Training iter #21246000:   Batch Loss = 7.506427, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8501877784729, Accuracy = 0.8904109597206116\n",
      "Training iter #21249000:   Batch Loss = 7.517967, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849583625793457, Accuracy = 0.8910065293312073\n",
      "Training iter #21252000:   Batch Loss = 7.627873, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849299430847168, Accuracy = 0.8907087445259094\n",
      "Training iter #21255000:   Batch Loss = 7.495978, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849113464355469, Accuracy = 0.8904109597206116\n",
      "Training iter #21258000:   Batch Loss = 7.453250, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849107265472412, Accuracy = 0.8910065293312073\n",
      "Training iter #21261000:   Batch Loss = 7.547828, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.848824501037598, Accuracy = 0.8910065293312073\n",
      "Training iter #21264000:   Batch Loss = 7.563996, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.848904609680176, Accuracy = 0.8916021585464478\n",
      "Training iter #21267000:   Batch Loss = 7.552922, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849391937255859, Accuracy = 0.8913043737411499\n",
      "Training iter #21270000:   Batch Loss = 7.440250, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849620342254639, Accuracy = 0.8916021585464478\n",
      "Training iter #21273000:   Batch Loss = 7.506444, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849358081817627, Accuracy = 0.8901131749153137\n",
      "Training iter #21276000:   Batch Loss = 7.527328, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849822521209717, Accuracy = 0.8898153901100159\n",
      "Training iter #21279000:   Batch Loss = 7.626365, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849929332733154, Accuracy = 0.8898153901100159\n",
      "Training iter #21282000:   Batch Loss = 7.473344, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849673271179199, Accuracy = 0.8898153901100159\n",
      "Training iter #21285000:   Batch Loss = 7.468019, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.849145889282227, Accuracy = 0.8895175457000732\n",
      "Training iter #21288000:   Batch Loss = 7.549316, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.848212718963623, Accuracy = 0.8901131749153137\n",
      "Training iter #21291000:   Batch Loss = 7.565263, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.847545623779297, Accuracy = 0.8901131749153137\n",
      "Training iter #21294000:   Batch Loss = 7.546907, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.84700870513916, Accuracy = 0.8910065293312073\n",
      "Training iter #21297000:   Batch Loss = 7.453608, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.846685409545898, Accuracy = 0.8904109597206116\n",
      "Training iter #21300000:   Batch Loss = 7.508768, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8463239669799805, Accuracy = 0.8907087445259094\n",
      "Training iter #21303000:   Batch Loss = 7.521466, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.846041202545166, Accuracy = 0.8904109597206116\n",
      "Training iter #21306000:   Batch Loss = 7.623110, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.845802307128906, Accuracy = 0.8907087445259094\n",
      "Training iter #21309000:   Batch Loss = 7.463098, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.845585823059082, Accuracy = 0.8904109597206116\n",
      "Training iter #21312000:   Batch Loss = 7.444551, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.845386505126953, Accuracy = 0.8901131749153137\n",
      "Training iter #21315000:   Batch Loss = 7.562690, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.84581995010376, Accuracy = 0.8907087445259094\n",
      "Training iter #21318000:   Batch Loss = 7.571150, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.846022129058838, Accuracy = 0.8910065293312073\n",
      "Training iter #21321000:   Batch Loss = 7.544671, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.846084117889404, Accuracy = 0.8904109597206116\n",
      "Training iter #21324000:   Batch Loss = 7.454939, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.846073150634766, Accuracy = 0.8907087445259094\n",
      "Training iter #21327000:   Batch Loss = 7.509101, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8459906578063965, Accuracy = 0.8910065293312073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #21330000:   Batch Loss = 7.538924, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.845970153808594, Accuracy = 0.8904109597206116\n",
      "Training iter #21333000:   Batch Loss = 7.605958, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.843324661254883, Accuracy = 0.8916021585464478\n",
      "Training iter #21336000:   Batch Loss = 7.456622, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8422417640686035, Accuracy = 0.8916021585464478\n",
      "Training iter #21339000:   Batch Loss = 7.450507, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841341018676758, Accuracy = 0.8916021585464478\n",
      "Training iter #21342000:   Batch Loss = 7.553216, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841205596923828, Accuracy = 0.8918999433517456\n",
      "Training iter #21345000:   Batch Loss = 7.565515, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841531276702881, Accuracy = 0.8924955129623413\n",
      "Training iter #21348000:   Batch Loss = 7.545226, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841635704040527, Accuracy = 0.8924955129623413\n",
      "Training iter #21351000:   Batch Loss = 7.460334, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841840744018555, Accuracy = 0.8924955129623413\n",
      "Training iter #21354000:   Batch Loss = 7.506874, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.842588424682617, Accuracy = 0.8916021585464478\n",
      "Training iter #21357000:   Batch Loss = 7.541617, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.842742443084717, Accuracy = 0.8910065293312073\n",
      "Training iter #21360000:   Batch Loss = 7.593791, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.842447280883789, Accuracy = 0.8918999433517456\n",
      "Training iter #21363000:   Batch Loss = 7.443535, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.842073440551758, Accuracy = 0.8913043737411499\n",
      "Training iter #21366000:   Batch Loss = 7.448356, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841576099395752, Accuracy = 0.8907087445259094\n",
      "Training iter #21369000:   Batch Loss = 7.533064, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841272354125977, Accuracy = 0.8904109597206116\n",
      "Training iter #21372000:   Batch Loss = 7.584858, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841480255126953, Accuracy = 0.8904109597206116\n",
      "Training iter #21375000:   Batch Loss = 7.557174, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841639041900635, Accuracy = 0.8895175457000732\n",
      "Training iter #21378000:   Batch Loss = 7.467324, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841641902923584, Accuracy = 0.8892197608947754\n",
      "Training iter #21381000:   Batch Loss = 7.506857, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8416361808776855, Accuracy = 0.8892197608947754\n",
      "Training iter #21384000:   Batch Loss = 7.535897, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8408284187316895, Accuracy = 0.8892197608947754\n",
      "Training iter #21387000:   Batch Loss = 7.575203, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8406901359558105, Accuracy = 0.8892197608947754\n",
      "Training iter #21390000:   Batch Loss = 7.432678, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.842313289642334, Accuracy = 0.8889219760894775\n",
      "Training iter #21393000:   Batch Loss = 7.463628, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.844229698181152, Accuracy = 0.8883264064788818\n",
      "Training iter #21396000:   Batch Loss = 7.531264, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8447394371032715, Accuracy = 0.8883264064788818\n",
      "Training iter #21399000:   Batch Loss = 7.588582, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.844299793243408, Accuracy = 0.8889219760894775\n",
      "Training iter #21402000:   Batch Loss = 7.543347, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.844142913818359, Accuracy = 0.8892197608947754\n",
      "Training iter #21405000:   Batch Loss = 7.474326, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.843931198120117, Accuracy = 0.8895175457000732\n",
      "Training iter #21408000:   Batch Loss = 7.500935, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.844202041625977, Accuracy = 0.8892197608947754\n",
      "Training iter #21411000:   Batch Loss = 7.544769, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8444061279296875, Accuracy = 0.8898153901100159\n",
      "Training iter #21414000:   Batch Loss = 7.575438, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8445000648498535, Accuracy = 0.8898153901100159\n",
      "Training iter #21417000:   Batch Loss = 7.441984, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.844364166259766, Accuracy = 0.8895175457000732\n",
      "Training iter #21420000:   Batch Loss = 7.469628, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.844034671783447, Accuracy = 0.8892197608947754\n",
      "Training iter #21423000:   Batch Loss = 7.514422, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.843103885650635, Accuracy = 0.8895175457000732\n",
      "Training iter #21426000:   Batch Loss = 7.587639, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8425679206848145, Accuracy = 0.8901131749153137\n",
      "Training iter #21429000:   Batch Loss = 7.525353, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.842355251312256, Accuracy = 0.8904109597206116\n",
      "Training iter #21432000:   Batch Loss = 7.465166, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8423919677734375, Accuracy = 0.8907087445259094\n",
      "Training iter #21435000:   Batch Loss = 7.533926, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841914653778076, Accuracy = 0.8910065293312073\n",
      "Training iter #21438000:   Batch Loss = 7.547407, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841343402862549, Accuracy = 0.8913043737411499\n",
      "Training iter #21441000:   Batch Loss = 7.574587, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841161727905273, Accuracy = 0.8916021585464478\n",
      "Training iter #21444000:   Batch Loss = 7.429941, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.841151237487793, Accuracy = 0.8907087445259094\n",
      "Training iter #21447000:   Batch Loss = 7.466220, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8409743309021, Accuracy = 0.8904109597206116\n",
      "Training iter #21450000:   Batch Loss = 7.503335, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.840531826019287, Accuracy = 0.8904109597206116\n",
      "Training iter #21453000:   Batch Loss = 7.585167, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.839841842651367, Accuracy = 0.8907087445259094\n",
      "Training iter #21456000:   Batch Loss = 7.510014, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.839215278625488, Accuracy = 0.8901131749153137\n",
      "Training iter #21459000:   Batch Loss = 7.456673, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.838939189910889, Accuracy = 0.8898153901100159\n",
      "Training iter #21462000:   Batch Loss = 7.533108, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.839004039764404, Accuracy = 0.8898153901100159\n",
      "Training iter #21465000:   Batch Loss = 7.546548, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.839244365692139, Accuracy = 0.8898153901100159\n",
      "Training iter #21468000:   Batch Loss = 7.566520, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.839175224304199, Accuracy = 0.8898153901100159\n",
      "Training iter #21471000:   Batch Loss = 7.438083, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.838663101196289, Accuracy = 0.8895175457000732\n",
      "Training iter #21474000:   Batch Loss = 7.467942, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8382978439331055, Accuracy = 0.8895175457000732\n",
      "Training iter #21477000:   Batch Loss = 7.496189, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.838777542114258, Accuracy = 0.8895175457000732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #21480000:   Batch Loss = 7.590212, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837981700897217, Accuracy = 0.8898153901100159\n",
      "Training iter #21483000:   Batch Loss = 7.500272, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837759494781494, Accuracy = 0.8892197608947754\n",
      "Training iter #21486000:   Batch Loss = 7.465818, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.838009357452393, Accuracy = 0.8895175457000732\n",
      "Training iter #21489000:   Batch Loss = 7.534671, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837796211242676, Accuracy = 0.8892197608947754\n",
      "Training iter #21492000:   Batch Loss = 7.551712, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837575912475586, Accuracy = 0.8898153901100159\n",
      "Training iter #21495000:   Batch Loss = 7.558167, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837554454803467, Accuracy = 0.8898153901100159\n",
      "Training iter #21498000:   Batch Loss = 7.436907, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837827682495117, Accuracy = 0.8898153901100159\n",
      "Training iter #21501000:   Batch Loss = 7.485981, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837733268737793, Accuracy = 0.8898153901100159\n",
      "Training iter #21504000:   Batch Loss = 7.498634, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.837189197540283, Accuracy = 0.8901131749153137\n",
      "Training iter #21507000:   Batch Loss = 7.588959, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.836641311645508, Accuracy = 0.8898153901100159\n",
      "Training iter #21510000:   Batch Loss = 7.490783, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8365702629089355, Accuracy = 0.8898153901100159\n",
      "Training iter #21513000:   Batch Loss = 7.453292, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.836787700653076, Accuracy = 0.8895175457000732\n",
      "Training iter #21516000:   Batch Loss = 7.510553, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.836276054382324, Accuracy = 0.8895175457000732\n",
      "Training iter #21519000:   Batch Loss = 7.558521, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.836056709289551, Accuracy = 0.8895175457000732\n",
      "Training iter #21522000:   Batch Loss = 7.533835, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.835938930511475, Accuracy = 0.8895175457000732\n",
      "Training iter #21525000:   Batch Loss = 7.438450, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.835790157318115, Accuracy = 0.8895175457000732\n",
      "Training iter #21528000:   Batch Loss = 7.486987, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8356451988220215, Accuracy = 0.8892197608947754\n",
      "Training iter #21531000:   Batch Loss = 7.512267, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.835662841796875, Accuracy = 0.8895175457000732\n",
      "Training iter #21534000:   Batch Loss = 7.597812, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.835563659667969, Accuracy = 0.8898153901100159\n",
      "Training iter #21537000:   Batch Loss = 7.492959, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8353376388549805, Accuracy = 0.8895175457000732\n",
      "Training iter #21540000:   Batch Loss = 7.444505, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.835070610046387, Accuracy = 0.8892197608947754\n",
      "Training iter #21543000:   Batch Loss = 7.515432, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.834919452667236, Accuracy = 0.8901131749153137\n",
      "Training iter #21546000:   Batch Loss = 7.553885, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.834860324859619, Accuracy = 0.8901131749153137\n",
      "Training iter #21549000:   Batch Loss = 7.536909, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.83470344543457, Accuracy = 0.8901131749153137\n",
      "Training iter #21552000:   Batch Loss = 7.425477, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8347368240356445, Accuracy = 0.8895175457000732\n",
      "Training iter #21555000:   Batch Loss = 7.486686, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.834522247314453, Accuracy = 0.8895175457000732\n",
      "Training iter #21558000:   Batch Loss = 7.511181, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.834358215332031, Accuracy = 0.8898153901100159\n",
      "Training iter #21561000:   Batch Loss = 7.603479, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8340163230896, Accuracy = 0.8898153901100159\n",
      "Training iter #21564000:   Batch Loss = 7.479026, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.833947658538818, Accuracy = 0.8898153901100159\n",
      "Training iter #21567000:   Batch Loss = 7.439127, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.833775043487549, Accuracy = 0.8898153901100159\n",
      "Training iter #21570000:   Batch Loss = 7.530385, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.833677768707275, Accuracy = 0.8898153901100159\n",
      "Training iter #21573000:   Batch Loss = 7.550709, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.833557605743408, Accuracy = 0.8901131749153137\n",
      "Training iter #21576000:   Batch Loss = 7.519883, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.833406448364258, Accuracy = 0.8907087445259094\n",
      "Training iter #21579000:   Batch Loss = 7.429682, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.833265781402588, Accuracy = 0.8901131749153137\n",
      "Training iter #21582000:   Batch Loss = 7.489539, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.833089351654053, Accuracy = 0.8901131749153137\n",
      "Training iter #21585000:   Batch Loss = 7.504795, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.832963943481445, Accuracy = 0.8901131749153137\n",
      "Training iter #21588000:   Batch Loss = 7.597953, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8328857421875, Accuracy = 0.8901131749153137\n",
      "Training iter #21591000:   Batch Loss = 7.464100, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.832715034484863, Accuracy = 0.8901131749153137\n",
      "Training iter #21594000:   Batch Loss = 7.433076, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.832484722137451, Accuracy = 0.8901131749153137\n",
      "Training iter #21597000:   Batch Loss = 7.529988, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.832378387451172, Accuracy = 0.8907087445259094\n",
      "Training iter #21600000:   Batch Loss = 7.537348, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.832220077514648, Accuracy = 0.8907087445259094\n",
      "Training iter #21603000:   Batch Loss = 7.533868, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.832089424133301, Accuracy = 0.8907087445259094\n",
      "Training iter #21606000:   Batch Loss = 7.431951, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.831945896148682, Accuracy = 0.8910065293312073\n",
      "Training iter #21609000:   Batch Loss = 7.489194, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8317999839782715, Accuracy = 0.8907087445259094\n",
      "Training iter #21612000:   Batch Loss = 7.501020, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8317084312438965, Accuracy = 0.8907087445259094\n",
      "Training iter #21615000:   Batch Loss = 7.604054, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8316650390625, Accuracy = 0.8907087445259094\n",
      "Training iter #21618000:   Batch Loss = 7.450782, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8315606117248535, Accuracy = 0.8904109597206116\n",
      "Training iter #21621000:   Batch Loss = 7.425127, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.831332206726074, Accuracy = 0.8907087445259094\n",
      "Training iter #21624000:   Batch Loss = 7.534789, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.831185340881348, Accuracy = 0.8907087445259094\n",
      "Training iter #21627000:   Batch Loss = 7.539268, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8309125900268555, Accuracy = 0.8904109597206116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #21630000:   Batch Loss = 7.527998, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8306660652160645, Accuracy = 0.8907087445259094\n",
      "Training iter #21633000:   Batch Loss = 7.433271, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.830437660217285, Accuracy = 0.8907087445259094\n",
      "Training iter #21636000:   Batch Loss = 7.484667, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.830163955688477, Accuracy = 0.8907087445259094\n",
      "Training iter #21639000:   Batch Loss = 7.503248, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.829950332641602, Accuracy = 0.8907087445259094\n",
      "Training iter #21642000:   Batch Loss = 7.593837, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.829835414886475, Accuracy = 0.8913043737411499\n",
      "Training iter #21645000:   Batch Loss = 7.438340, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.829697608947754, Accuracy = 0.8913043737411499\n",
      "Training iter #21648000:   Batch Loss = 7.423428, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.82950496673584, Accuracy = 0.8910065293312073\n",
      "Training iter #21651000:   Batch Loss = 7.536219, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.829375743865967, Accuracy = 0.8910065293312073\n",
      "Training iter #21654000:   Batch Loss = 7.538394, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.829222202301025, Accuracy = 0.8910065293312073\n",
      "Training iter #21657000:   Batch Loss = 7.520114, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.829052925109863, Accuracy = 0.8910065293312073\n",
      "Training iter #21660000:   Batch Loss = 7.436551, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.828944206237793, Accuracy = 0.8910065293312073\n",
      "Training iter #21663000:   Batch Loss = 7.480999, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.828755855560303, Accuracy = 0.8907087445259094\n",
      "Training iter #21666000:   Batch Loss = 7.520854, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.828546047210693, Accuracy = 0.8907087445259094\n",
      "Training iter #21669000:   Batch Loss = 7.578895, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.828492641448975, Accuracy = 0.8910065293312073\n",
      "Training iter #21672000:   Batch Loss = 7.429339, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.828361988067627, Accuracy = 0.8907087445259094\n",
      "Training iter #21675000:   Batch Loss = 7.424017, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8281378746032715, Accuracy = 0.8910065293312073\n",
      "Training iter #21678000:   Batch Loss = 7.527042, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.827890872955322, Accuracy = 0.8913043737411499\n",
      "Training iter #21681000:   Batch Loss = 7.547745, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.827664852142334, Accuracy = 0.8913043737411499\n",
      "Training iter #21684000:   Batch Loss = 7.527311, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.827383518218994, Accuracy = 0.8913043737411499\n",
      "Training iter #21687000:   Batch Loss = 7.449800, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.82723331451416, Accuracy = 0.8913043737411499\n",
      "Training iter #21690000:   Batch Loss = 7.487357, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.827978610992432, Accuracy = 0.8910065293312073\n",
      "Training iter #21693000:   Batch Loss = 7.517602, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.827851295471191, Accuracy = 0.8910065293312073\n",
      "Training iter #21696000:   Batch Loss = 7.552597, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.826735019683838, Accuracy = 0.8913043737411499\n",
      "Training iter #21699000:   Batch Loss = 7.414093, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.826600551605225, Accuracy = 0.8910065293312073\n",
      "Training iter #21702000:   Batch Loss = 7.442998, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.826353073120117, Accuracy = 0.8913043737411499\n",
      "Training iter #21705000:   Batch Loss = 7.507823, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.826125144958496, Accuracy = 0.8913043737411499\n",
      "Training iter #21708000:   Batch Loss = 7.558154, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.825921058654785, Accuracy = 0.8913043737411499\n",
      "Training iter #21711000:   Batch Loss = 7.532340, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.825649738311768, Accuracy = 0.8913043737411499\n",
      "Training iter #21714000:   Batch Loss = 7.451062, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.825514793395996, Accuracy = 0.8913043737411499\n",
      "Training iter #21717000:   Batch Loss = 7.480379, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.82526969909668, Accuracy = 0.8910065293312073\n",
      "Training iter #21720000:   Batch Loss = 7.513364, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.825068473815918, Accuracy = 0.8910065293312073\n",
      "Training iter #21723000:   Batch Loss = 7.551894, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.824974060058594, Accuracy = 0.8907087445259094\n",
      "Training iter #21726000:   Batch Loss = 7.408906, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.824889659881592, Accuracy = 0.8910065293312073\n",
      "Training iter #21729000:   Batch Loss = 7.448421, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.824637413024902, Accuracy = 0.8910065293312073\n",
      "Training iter #21732000:   Batch Loss = 7.509913, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.824296951293945, Accuracy = 0.8907087445259094\n",
      "Training iter #21735000:   Batch Loss = 7.561657, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.824010848999023, Accuracy = 0.8910065293312073\n",
      "Training iter #21738000:   Batch Loss = 7.506170, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.823803901672363, Accuracy = 0.8910065293312073\n",
      "Training iter #21741000:   Batch Loss = 7.441832, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.823631286621094, Accuracy = 0.8904109597206116\n",
      "Training iter #21744000:   Batch Loss = 7.498575, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.823239326477051, Accuracy = 0.8910065293312073\n",
      "Training iter #21747000:   Batch Loss = 7.523217, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.822983741760254, Accuracy = 0.8913043737411499\n",
      "Training iter #21750000:   Batch Loss = 7.542360, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.822863578796387, Accuracy = 0.8913043737411499\n",
      "Training iter #21753000:   Batch Loss = 7.414820, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.822754383087158, Accuracy = 0.8918999433517456\n",
      "Training iter #21756000:   Batch Loss = 7.446016, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.822690963745117, Accuracy = 0.8913043737411499\n",
      "Training iter #21759000:   Batch Loss = 7.487241, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.822587013244629, Accuracy = 0.8910065293312073\n",
      "Training iter #21762000:   Batch Loss = 7.564005, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.822340965270996, Accuracy = 0.8907087445259094\n",
      "Training iter #21765000:   Batch Loss = 7.492071, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.822101593017578, Accuracy = 0.8910065293312073\n",
      "Training iter #21768000:   Batch Loss = 7.442142, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.821913242340088, Accuracy = 0.8913043737411499\n",
      "Training iter #21771000:   Batch Loss = 7.510111, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8215532302856445, Accuracy = 0.8916021585464478\n",
      "Training iter #21774000:   Batch Loss = 7.527920, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.821288108825684, Accuracy = 0.8921977281570435\n",
      "Training iter #21777000:   Batch Loss = 7.545231, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.821074962615967, Accuracy = 0.8924955129623413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #21780000:   Batch Loss = 7.412515, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.820919990539551, Accuracy = 0.8924955129623413\n",
      "Training iter #21783000:   Batch Loss = 7.447534, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.820667266845703, Accuracy = 0.8924955129623413\n",
      "Training iter #21786000:   Batch Loss = 7.488038, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.82033634185791, Accuracy = 0.8921977281570435\n",
      "Training iter #21789000:   Batch Loss = 7.564173, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.820035934448242, Accuracy = 0.8921977281570435\n",
      "Training iter #21792000:   Batch Loss = 7.483906, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.819769859313965, Accuracy = 0.8918999433517456\n",
      "Training iter #21795000:   Batch Loss = 7.433617, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8195295333862305, Accuracy = 0.8918999433517456\n",
      "Training iter #21798000:   Batch Loss = 7.511261, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.819230079650879, Accuracy = 0.8918999433517456\n",
      "Training iter #21801000:   Batch Loss = 7.522777, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.819027900695801, Accuracy = 0.8918999433517456\n",
      "Training iter #21804000:   Batch Loss = 7.538745, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.818813800811768, Accuracy = 0.8918999433517456\n",
      "Training iter #21807000:   Batch Loss = 7.412155, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.818618297576904, Accuracy = 0.8921977281570435\n",
      "Training iter #21810000:   Batch Loss = 7.446826, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.818361282348633, Accuracy = 0.8921977281570435\n",
      "Training iter #21813000:   Batch Loss = 7.476861, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.81827974319458, Accuracy = 0.8927933573722839\n",
      "Training iter #21816000:   Batch Loss = 7.569187, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.818249702453613, Accuracy = 0.8927933573722839\n",
      "Training iter #21819000:   Batch Loss = 7.476182, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.818163871765137, Accuracy = 0.8927933573722839\n",
      "Training iter #21822000:   Batch Loss = 7.434439, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.818045616149902, Accuracy = 0.8921977281570435\n",
      "Training iter #21825000:   Batch Loss = 7.489430, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.81784725189209, Accuracy = 0.8927933573722839\n",
      "Training iter #21828000:   Batch Loss = 7.527331, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.817707061767578, Accuracy = 0.8927933573722839\n",
      "Training iter #21831000:   Batch Loss = 7.528058, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.817532062530518, Accuracy = 0.8924955129623413\n",
      "Training iter #21834000:   Batch Loss = 7.409857, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8173909187316895, Accuracy = 0.8924955129623413\n",
      "Training iter #21837000:   Batch Loss = 7.465097, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.817132949829102, Accuracy = 0.8924955129623413\n",
      "Training iter #21840000:   Batch Loss = 7.488503, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.81679105758667, Accuracy = 0.8930911421775818\n",
      "Training iter #21843000:   Batch Loss = 7.563468, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.816351413726807, Accuracy = 0.8930911421775818\n",
      "Training iter #21846000:   Batch Loss = 7.470970, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.815969944000244, Accuracy = 0.8927933573722839\n",
      "Training iter #21849000:   Batch Loss = 7.422786, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.815629005432129, Accuracy = 0.8930911421775818\n",
      "Training iter #21852000:   Batch Loss = 7.489817, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.815292835235596, Accuracy = 0.8930911421775818\n",
      "Training iter #21855000:   Batch Loss = 7.532777, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.815099239349365, Accuracy = 0.8927933573722839\n",
      "Training iter #21858000:   Batch Loss = 7.499206, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.814972877502441, Accuracy = 0.8921977281570435\n",
      "Training iter #21861000:   Batch Loss = 7.411358, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8148627281188965, Accuracy = 0.8921977281570435\n",
      "Training iter #21864000:   Batch Loss = 7.467473, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.814693450927734, Accuracy = 0.8921977281570435\n",
      "Training iter #21867000:   Batch Loss = 7.494769, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.814611911773682, Accuracy = 0.8921977281570435\n",
      "Training iter #21870000:   Batch Loss = 7.571192, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.814440727233887, Accuracy = 0.8924955129623413\n",
      "Training iter #21873000:   Batch Loss = 7.460508, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.814178943634033, Accuracy = 0.8924955129623413\n",
      "Training iter #21876000:   Batch Loss = 7.421231, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.813836097717285, Accuracy = 0.8930911421775818\n",
      "Training iter #21879000:   Batch Loss = 7.491178, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8137526512146, Accuracy = 0.8924955129623413\n",
      "Training iter #21882000:   Batch Loss = 7.531093, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.813822269439697, Accuracy = 0.8921977281570435\n",
      "Training iter #21885000:   Batch Loss = 7.501406, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.813616752624512, Accuracy = 0.8921977281570435\n",
      "Training iter #21888000:   Batch Loss = 7.403258, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.813538551330566, Accuracy = 0.8924955129623413\n",
      "Training iter #21891000:   Batch Loss = 7.467116, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.813348770141602, Accuracy = 0.8930911421775818\n",
      "Training iter #21894000:   Batch Loss = 7.476022, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.813167095184326, Accuracy = 0.8933889269828796\n",
      "Training iter #21897000:   Batch Loss = 7.580027, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8129048347473145, Accuracy = 0.8927933573722839\n",
      "Training iter #21900000:   Batch Loss = 7.457397, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.812596321105957, Accuracy = 0.8930911421775818\n",
      "Training iter #21903000:   Batch Loss = 7.416748, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.812243938446045, Accuracy = 0.8930911421775818\n",
      "Training iter #21906000:   Batch Loss = 7.505847, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.811865329742432, Accuracy = 0.8930911421775818\n",
      "Training iter #21909000:   Batch Loss = 7.524673, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.811484336853027, Accuracy = 0.8927933573722839\n",
      "Training iter #21912000:   Batch Loss = 7.508190, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.811176300048828, Accuracy = 0.8924955129623413\n",
      "Training iter #21915000:   Batch Loss = 7.401909, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.811032772064209, Accuracy = 0.8927933573722839\n",
      "Training iter #21918000:   Batch Loss = 7.468608, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.810882568359375, Accuracy = 0.8924955129623413\n",
      "Training iter #21921000:   Batch Loss = 7.482801, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.81088924407959, Accuracy = 0.8930911421775818\n",
      "Training iter #21924000:   Batch Loss = 7.578969, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.810759544372559, Accuracy = 0.8930911421775818\n",
      "Training iter #21927000:   Batch Loss = 7.434576, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.810276031494141, Accuracy = 0.8933889269828796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #21930000:   Batch Loss = 7.413607, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.810090065002441, Accuracy = 0.8933889269828796\n",
      "Training iter #21933000:   Batch Loss = 7.508606, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809978485107422, Accuracy = 0.8927933573722839\n",
      "Training iter #21936000:   Batch Loss = 7.516082, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80989933013916, Accuracy = 0.8933889269828796\n",
      "Training iter #21939000:   Batch Loss = 7.501011, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809924125671387, Accuracy = 0.8933889269828796\n",
      "Training iter #21942000:   Batch Loss = 7.411961, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809936046600342, Accuracy = 0.8924955129623413\n",
      "Training iter #21945000:   Batch Loss = 7.468524, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809798240661621, Accuracy = 0.8930911421775818\n",
      "Training iter #21948000:   Batch Loss = 7.479902, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809727191925049, Accuracy = 0.8936867117881775\n",
      "Training iter #21951000:   Batch Loss = 7.581338, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809530258178711, Accuracy = 0.8936867117881775\n",
      "Training iter #21954000:   Batch Loss = 7.419481, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809298038482666, Accuracy = 0.8933889269828796\n",
      "Training iter #21957000:   Batch Loss = 7.409064, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.809137344360352, Accuracy = 0.8936867117881775\n",
      "Training iter #21960000:   Batch Loss = 7.520483, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.808987617492676, Accuracy = 0.8933889269828796\n",
      "Training iter #21963000:   Batch Loss = 7.522903, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.808708190917969, Accuracy = 0.8936867117881775\n",
      "Training iter #21966000:   Batch Loss = 7.501559, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.808321952819824, Accuracy = 0.8933889269828796\n",
      "Training iter #21969000:   Batch Loss = 7.415156, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80794620513916, Accuracy = 0.8927933573722839\n",
      "Training iter #21972000:   Batch Loss = 7.469015, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.807521343231201, Accuracy = 0.8927933573722839\n",
      "Training iter #21975000:   Batch Loss = 7.494082, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80726432800293, Accuracy = 0.8930911421775818\n",
      "Training iter #21978000:   Batch Loss = 7.562072, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80698299407959, Accuracy = 0.8933889269828796\n",
      "Training iter #21981000:   Batch Loss = 7.412432, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.806710720062256, Accuracy = 0.8930911421775818\n",
      "Training iter #21984000:   Batch Loss = 7.412672, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.806538105010986, Accuracy = 0.8930911421775818\n",
      "Training iter #21987000:   Batch Loss = 7.512509, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.806491851806641, Accuracy = 0.8930911421775818\n",
      "Training iter #21990000:   Batch Loss = 7.518204, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.806286334991455, Accuracy = 0.8930911421775818\n",
      "Training iter #21993000:   Batch Loss = 7.497399, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.806082725524902, Accuracy = 0.8924955129623413\n",
      "Training iter #21996000:   Batch Loss = 7.414718, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80607795715332, Accuracy = 0.8924955129623413\n",
      "Training iter #21999000:   Batch Loss = 7.463406, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.806096076965332, Accuracy = 0.8933889269828796\n",
      "Training iter #22002000:   Batch Loss = 7.497841, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.806120872497559, Accuracy = 0.8930911421775818\n",
      "Training iter #22005000:   Batch Loss = 7.548817, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.805818557739258, Accuracy = 0.8933889269828796\n",
      "Training iter #22008000:   Batch Loss = 7.400853, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.805535316467285, Accuracy = 0.8930911421775818\n",
      "Training iter #22011000:   Batch Loss = 7.411131, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80539608001709, Accuracy = 0.8933889269828796\n",
      "Training iter #22014000:   Batch Loss = 7.492358, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.805108547210693, Accuracy = 0.8933889269828796\n",
      "Training iter #22017000:   Batch Loss = 7.535231, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8047776222229, Accuracy = 0.8933889269828796\n",
      "Training iter #22020000:   Batch Loss = 7.507905, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8044586181640625, Accuracy = 0.8936867117881775\n",
      "Training iter #22023000:   Batch Loss = 7.427588, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.804187774658203, Accuracy = 0.8930911421775818\n",
      "Training iter #22026000:   Batch Loss = 7.454887, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80421781539917, Accuracy = 0.8930911421775818\n",
      "Training iter #22029000:   Batch Loss = 7.491020, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.804144382476807, Accuracy = 0.8936867117881775\n",
      "Training iter #22032000:   Batch Loss = 7.525524, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.803979396820068, Accuracy = 0.8939844965934753\n",
      "Training iter #22035000:   Batch Loss = 7.389712, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.803779125213623, Accuracy = 0.8942822813987732\n",
      "Training iter #22038000:   Batch Loss = 7.424338, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.803705215454102, Accuracy = 0.8942822813987732\n",
      "Training iter #22041000:   Batch Loss = 7.487273, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.804711818695068, Accuracy = 0.8942822813987732\n",
      "Training iter #22044000:   Batch Loss = 7.532021, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8045430183410645, Accuracy = 0.8939844965934753\n",
      "Training iter #22047000:   Batch Loss = 7.501149, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.804087162017822, Accuracy = 0.8942822813987732\n",
      "Training iter #22050000:   Batch Loss = 7.430161, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.803019046783447, Accuracy = 0.8945801258087158\n",
      "Training iter #22053000:   Batch Loss = 7.453430, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.802795886993408, Accuracy = 0.8942822813987732\n",
      "Training iter #22056000:   Batch Loss = 7.493317, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.802534580230713, Accuracy = 0.8939844965934753\n",
      "Training iter #22059000:   Batch Loss = 7.534635, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.802111625671387, Accuracy = 0.8942822813987732\n",
      "Training iter #22062000:   Batch Loss = 7.390802, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.801770210266113, Accuracy = 0.8945801258087158\n",
      "Training iter #22065000:   Batch Loss = 7.429043, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.801480770111084, Accuracy = 0.8945801258087158\n",
      "Training iter #22068000:   Batch Loss = 7.469353, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.801220893859863, Accuracy = 0.8951756954193115\n",
      "Training iter #22071000:   Batch Loss = 7.547068, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.800779342651367, Accuracy = 0.8942822813987732\n",
      "Training iter #22074000:   Batch Loss = 7.477128, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8004021644592285, Accuracy = 0.8942822813987732\n",
      "Training iter #22077000:   Batch Loss = 7.422372, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8001251220703125, Accuracy = 0.8936867117881775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22080000:   Batch Loss = 7.481139, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.8000807762146, Accuracy = 0.8936867117881775\n",
      "Training iter #22083000:   Batch Loss = 7.497376, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.80006742477417, Accuracy = 0.8939844965934753\n",
      "Training iter #22086000:   Batch Loss = 7.529270, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.799735069274902, Accuracy = 0.8936867117881775\n",
      "Training iter #22089000:   Batch Loss = 7.387085, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.79946756362915, Accuracy = 0.8936867117881775\n",
      "Training iter #22092000:   Batch Loss = 7.423506, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.799285888671875, Accuracy = 0.8939844965934753\n",
      "Training iter #22095000:   Batch Loss = 7.463084, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.799272537231445, Accuracy = 0.8939844965934753\n",
      "Training iter #22098000:   Batch Loss = 7.541613, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7987189292907715, Accuracy = 0.8936867117881775\n",
      "Training iter #22101000:   Batch Loss = 7.464686, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.798830032348633, Accuracy = 0.8939844965934753\n",
      "Training iter #22104000:   Batch Loss = 7.414687, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.798876762390137, Accuracy = 0.8942822813987732\n",
      "Training iter #22107000:   Batch Loss = 7.482240, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.799133777618408, Accuracy = 0.8939844965934753\n",
      "Training iter #22110000:   Batch Loss = 7.503927, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.799527645111084, Accuracy = 0.8939844965934753\n",
      "Training iter #22113000:   Batch Loss = 7.523643, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.799596309661865, Accuracy = 0.8930911421775818\n",
      "Training iter #22116000:   Batch Loss = 7.401105, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.799644947052002, Accuracy = 0.8924955129623413\n",
      "Training iter #22119000:   Batch Loss = 7.428560, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.79854679107666, Accuracy = 0.8924955129623413\n",
      "Training iter #22122000:   Batch Loss = 7.458960, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.797723293304443, Accuracy = 0.8927933573722839\n",
      "Training iter #22125000:   Batch Loss = 7.546565, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.797739505767822, Accuracy = 0.8933889269828796\n",
      "Training iter #22128000:   Batch Loss = 7.457528, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7980546951293945, Accuracy = 0.8939844965934753\n",
      "Training iter #22131000:   Batch Loss = 7.420182, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.797983646392822, Accuracy = 0.8942822813987732\n",
      "Training iter #22134000:   Batch Loss = 7.486834, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.796541690826416, Accuracy = 0.8948779106140137\n",
      "Training iter #22137000:   Batch Loss = 7.502966, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.79665470123291, Accuracy = 0.8948779106140137\n",
      "Training iter #22140000:   Batch Loss = 7.513263, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.796286582946777, Accuracy = 0.8951756954193115\n",
      "Training iter #22143000:   Batch Loss = 7.401057, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.796525001525879, Accuracy = 0.8948779106140137\n",
      "Training iter #22146000:   Batch Loss = 7.443279, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7963032722473145, Accuracy = 0.8948779106140137\n",
      "Training iter #22149000:   Batch Loss = 7.459194, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.795669078826904, Accuracy = 0.8954734802246094\n",
      "Training iter #22152000:   Batch Loss = 7.551443, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.795264720916748, Accuracy = 0.8963668942451477\n",
      "Training iter #22155000:   Batch Loss = 7.451167, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.794948101043701, Accuracy = 0.8963668942451477\n",
      "Training iter #22158000:   Batch Loss = 7.415247, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.794544696807861, Accuracy = 0.8963668942451477\n",
      "Training iter #22161000:   Batch Loss = 7.466932, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.79397439956665, Accuracy = 0.8963668942451477\n",
      "Training iter #22164000:   Batch Loss = 7.513497, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.793575286865234, Accuracy = 0.8966646790504456\n",
      "Training iter #22167000:   Batch Loss = 7.496890, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.793087005615234, Accuracy = 0.8963668942451477\n",
      "Training iter #22170000:   Batch Loss = 7.400780, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7927937507629395, Accuracy = 0.8957712650299072\n",
      "Training iter #22173000:   Batch Loss = 7.446628, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.792477607727051, Accuracy = 0.8963668942451477\n",
      "Training iter #22176000:   Batch Loss = 7.468792, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.792375564575195, Accuracy = 0.8957712650299072\n",
      "Training iter #22179000:   Batch Loss = 7.552427, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.792173862457275, Accuracy = 0.8960691094398499\n",
      "Training iter #22182000:   Batch Loss = 7.445998, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.791953086853027, Accuracy = 0.8951756954193115\n",
      "Training iter #22185000:   Batch Loss = 7.404587, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.791721343994141, Accuracy = 0.8954734802246094\n",
      "Training iter #22188000:   Batch Loss = 7.465808, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.791443347930908, Accuracy = 0.8954734802246094\n",
      "Training iter #22191000:   Batch Loss = 7.507760, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.791242599487305, Accuracy = 0.8960691094398499\n",
      "Training iter #22194000:   Batch Loss = 7.488441, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.79102897644043, Accuracy = 0.8957712650299072\n",
      "Training iter #22197000:   Batch Loss = 7.386368, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.79083251953125, Accuracy = 0.8954734802246094\n",
      "Training iter #22200000:   Batch Loss = 7.447380, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790440559387207, Accuracy = 0.8954734802246094\n",
      "Training iter #22203000:   Batch Loss = 7.471801, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790206432342529, Accuracy = 0.8954734802246094\n",
      "Training iter #22206000:   Batch Loss = 7.554374, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790028095245361, Accuracy = 0.8960691094398499\n",
      "Training iter #22209000:   Batch Loss = 7.435076, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789876461029053, Accuracy = 0.8954734802246094\n",
      "Training iter #22212000:   Batch Loss = 7.403633, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789734840393066, Accuracy = 0.8954734802246094\n",
      "Training iter #22215000:   Batch Loss = 7.485345, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789694309234619, Accuracy = 0.8948779106140137\n",
      "Training iter #22218000:   Batch Loss = 7.507413, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789885997772217, Accuracy = 0.8948779106140137\n",
      "Training iter #22221000:   Batch Loss = 7.475637, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789907932281494, Accuracy = 0.8951756954193115\n",
      "Training iter #22224000:   Batch Loss = 7.385103, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789966106414795, Accuracy = 0.8951756954193115\n",
      "Training iter #22227000:   Batch Loss = 7.447064, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790014266967773, Accuracy = 0.8954734802246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22230000:   Batch Loss = 7.462173, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.79021692276001, Accuracy = 0.8954734802246094\n",
      "Training iter #22233000:   Batch Loss = 7.552551, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7901930809021, Accuracy = 0.8951756954193115\n",
      "Training iter #22236000:   Batch Loss = 7.430797, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790126800537109, Accuracy = 0.8948779106140137\n",
      "Training iter #22239000:   Batch Loss = 7.398030, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790080547332764, Accuracy = 0.8945801258087158\n",
      "Training iter #22242000:   Batch Loss = 7.483895, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790155410766602, Accuracy = 0.8948779106140137\n",
      "Training iter #22245000:   Batch Loss = 7.496723, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.790172576904297, Accuracy = 0.8951756954193115\n",
      "Training iter #22248000:   Batch Loss = 7.482766, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789961814880371, Accuracy = 0.8948779106140137\n",
      "Training iter #22251000:   Batch Loss = 7.388885, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789794445037842, Accuracy = 0.8951756954193115\n",
      "Training iter #22254000:   Batch Loss = 7.444768, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789701461791992, Accuracy = 0.8951756954193115\n",
      "Training iter #22257000:   Batch Loss = 7.460941, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789840221405029, Accuracy = 0.8951756954193115\n",
      "Training iter #22260000:   Batch Loss = 7.560602, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789583683013916, Accuracy = 0.8951756954193115\n",
      "Training iter #22263000:   Batch Loss = 7.413001, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789461135864258, Accuracy = 0.8954734802246094\n",
      "Training iter #22266000:   Batch Loss = 7.389977, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789334297180176, Accuracy = 0.8954734802246094\n",
      "Training iter #22269000:   Batch Loss = 7.488609, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789272308349609, Accuracy = 0.8954734802246094\n",
      "Training iter #22272000:   Batch Loss = 7.497643, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789138317108154, Accuracy = 0.8951756954193115\n",
      "Training iter #22275000:   Batch Loss = 7.480893, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.789020538330078, Accuracy = 0.8951756954193115\n",
      "Training iter #22278000:   Batch Loss = 7.393756, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.788898468017578, Accuracy = 0.8951756954193115\n",
      "Training iter #22281000:   Batch Loss = 7.444581, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.788733005523682, Accuracy = 0.8951756954193115\n",
      "Training iter #22284000:   Batch Loss = 7.456272, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.788757801055908, Accuracy = 0.8948779106140137\n",
      "Training iter #22287000:   Batch Loss = 7.550185, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.788442611694336, Accuracy = 0.8957712650299072\n",
      "Training iter #22290000:   Batch Loss = 7.397216, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.788033485412598, Accuracy = 0.8951756954193115\n",
      "Training iter #22293000:   Batch Loss = 7.389006, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.787181854248047, Accuracy = 0.8951756954193115\n",
      "Training iter #22296000:   Batch Loss = 7.494555, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.786835670471191, Accuracy = 0.8954734802246094\n",
      "Training iter #22299000:   Batch Loss = 7.499878, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.786686897277832, Accuracy = 0.8957712650299072\n",
      "Training iter #22302000:   Batch Loss = 7.473841, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7866997718811035, Accuracy = 0.8954734802246094\n",
      "Training iter #22305000:   Batch Loss = 7.397664, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7866435050964355, Accuracy = 0.8948779106140137\n",
      "Training iter #22308000:   Batch Loss = 7.441243, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.786709308624268, Accuracy = 0.8948779106140137\n",
      "Training iter #22311000:   Batch Loss = 7.478950, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.786432266235352, Accuracy = 0.8948779106140137\n",
      "Training iter #22314000:   Batch Loss = 7.535586, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.785788059234619, Accuracy = 0.8951756954193115\n",
      "Training iter #22317000:   Batch Loss = 7.392887, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7853684425354, Accuracy = 0.8951756954193115\n",
      "Training iter #22320000:   Batch Loss = 7.393908, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.785348892211914, Accuracy = 0.8954734802246094\n",
      "Training iter #22323000:   Batch Loss = 7.488210, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.785256862640381, Accuracy = 0.8954734802246094\n",
      "Training iter #22326000:   Batch Loss = 7.503164, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784670352935791, Accuracy = 0.8960691094398499\n",
      "Training iter #22329000:   Batch Loss = 7.483971, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.78449821472168, Accuracy = 0.8957712650299072\n",
      "Training iter #22332000:   Batch Loss = 7.409586, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784441947937012, Accuracy = 0.8957712650299072\n",
      "Training iter #22335000:   Batch Loss = 7.441811, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784453392028809, Accuracy = 0.8957712650299072\n",
      "Training iter #22338000:   Batch Loss = 7.478237, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784623146057129, Accuracy = 0.8960691094398499\n",
      "Training iter #22341000:   Batch Loss = 7.512791, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784404277801514, Accuracy = 0.8957712650299072\n",
      "Training iter #22344000:   Batch Loss = 7.380335, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.78421688079834, Accuracy = 0.8957712650299072\n",
      "Training iter #22347000:   Batch Loss = 7.400762, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784215927124023, Accuracy = 0.8951756954193115\n",
      "Training iter #22350000:   Batch Loss = 7.475811, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784507751464844, Accuracy = 0.8954734802246094\n",
      "Training iter #22353000:   Batch Loss = 7.517899, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784554481506348, Accuracy = 0.8954734802246094\n",
      "Training iter #22356000:   Batch Loss = 7.490056, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7856221199035645, Accuracy = 0.8945801258087158\n",
      "Training iter #22359000:   Batch Loss = 7.409713, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7896552085876465, Accuracy = 0.8948779106140137\n",
      "Training iter #22362000:   Batch Loss = 7.441658, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7891316413879395, Accuracy = 0.8939844965934753\n",
      "Training iter #22365000:   Batch Loss = 7.476503, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.788795471191406, Accuracy = 0.8930911421775818\n",
      "Training iter #22368000:   Batch Loss = 7.511804, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.78494119644165, Accuracy = 0.8945801258087158\n",
      "Training iter #22371000:   Batch Loss = 7.378489, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784743309020996, Accuracy = 0.8948779106140137\n",
      "Training iter #22374000:   Batch Loss = 7.409070, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7843403816223145, Accuracy = 0.8954734802246094\n",
      "Training iter #22377000:   Batch Loss = 7.469769, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.78422737121582, Accuracy = 0.8951756954193115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22380000:   Batch Loss = 7.514191, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784039497375488, Accuracy = 0.8951756954193115\n",
      "Training iter #22383000:   Batch Loss = 7.478814, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.783843994140625, Accuracy = 0.8954734802246094\n",
      "Training iter #22386000:   Batch Loss = 7.411451, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.783658027648926, Accuracy = 0.8951756954193115\n",
      "Training iter #22389000:   Batch Loss = 7.456698, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.783400535583496, Accuracy = 0.8951756954193115\n",
      "Training iter #22392000:   Batch Loss = 7.478743, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.783005714416504, Accuracy = 0.8951756954193115\n",
      "Training iter #22395000:   Batch Loss = 7.505441, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.782393455505371, Accuracy = 0.8954734802246094\n",
      "Training iter #22398000:   Batch Loss = 7.386087, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.78385591506958, Accuracy = 0.8945801258087158\n",
      "Training iter #22401000:   Batch Loss = 7.407708, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784407615661621, Accuracy = 0.8948779106140137\n",
      "Training iter #22404000:   Batch Loss = 7.457248, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.784081935882568, Accuracy = 0.8951756954193115\n",
      "Training iter #22407000:   Batch Loss = 7.521948, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.783501148223877, Accuracy = 0.8954734802246094\n",
      "Training iter #22410000:   Batch Loss = 7.455180, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.783026695251465, Accuracy = 0.8957712650299072\n",
      "Training iter #22413000:   Batch Loss = 7.404332, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.781818866729736, Accuracy = 0.8966646790504456\n",
      "Training iter #22416000:   Batch Loss = 7.468816, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.782009601593018, Accuracy = 0.8960691094398499\n",
      "Training iter #22419000:   Batch Loss = 7.486326, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.782012939453125, Accuracy = 0.8954734802246094\n",
      "Training iter #22422000:   Batch Loss = 7.504983, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.781418800354004, Accuracy = 0.8960691094398499\n",
      "Training iter #22425000:   Batch Loss = 7.383079, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.781229496002197, Accuracy = 0.8963668942451477\n",
      "Training iter #22428000:   Batch Loss = 7.409411, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7810893058776855, Accuracy = 0.8960691094398499\n",
      "Training iter #22431000:   Batch Loss = 7.446689, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.780947685241699, Accuracy = 0.8960691094398499\n",
      "Training iter #22434000:   Batch Loss = 7.523197, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.780930519104004, Accuracy = 0.8960691094398499\n",
      "Training iter #22437000:   Batch Loss = 7.446989, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7808003425598145, Accuracy = 0.8951756954193115\n",
      "Training iter #22440000:   Batch Loss = 7.397612, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.780734539031982, Accuracy = 0.8954734802246094\n",
      "Training iter #22443000:   Batch Loss = 7.466032, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.781865119934082, Accuracy = 0.8945801258087158\n",
      "Training iter #22446000:   Batch Loss = 7.479743, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.781929969787598, Accuracy = 0.8942822813987732\n",
      "Training iter #22449000:   Batch Loss = 7.497983, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.781488418579102, Accuracy = 0.8948779106140137\n",
      "Training iter #22452000:   Batch Loss = 7.383077, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.781301975250244, Accuracy = 0.8942822813987732\n",
      "Training iter #22455000:   Batch Loss = 7.410181, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7810564041137695, Accuracy = 0.8942822813987732\n",
      "Training iter #22458000:   Batch Loss = 7.436877, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7809062004089355, Accuracy = 0.8942822813987732\n",
      "Training iter #22461000:   Batch Loss = 7.531199, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.779636383056641, Accuracy = 0.8954734802246094\n",
      "Training iter #22464000:   Batch Loss = 7.439719, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.779358386993408, Accuracy = 0.8948779106140137\n",
      "Training iter #22467000:   Batch Loss = 7.395330, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.779130458831787, Accuracy = 0.8954734802246094\n",
      "Training iter #22470000:   Batch Loss = 7.449924, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.77889347076416, Accuracy = 0.8954734802246094\n",
      "Training iter #22473000:   Batch Loss = 7.486669, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.778894901275635, Accuracy = 0.8957712650299072\n",
      "Training iter #22476000:   Batch Loss = 7.485887, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7787981033325195, Accuracy = 0.8957712650299072\n",
      "Training iter #22479000:   Batch Loss = 7.381644, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.778726577758789, Accuracy = 0.8957712650299072\n",
      "Training iter #22482000:   Batch Loss = 7.425465, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.778563499450684, Accuracy = 0.8960691094398499\n",
      "Training iter #22485000:   Batch Loss = 7.441069, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.778471946716309, Accuracy = 0.8954734802246094\n",
      "Training iter #22488000:   Batch Loss = 7.515582, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.778277397155762, Accuracy = 0.8954734802246094\n",
      "Training iter #22491000:   Batch Loss = 7.431202, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.778082847595215, Accuracy = 0.8954734802246094\n",
      "Training iter #22494000:   Batch Loss = 7.385941, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.777824401855469, Accuracy = 0.8954734802246094\n",
      "Training iter #22497000:   Batch Loss = 7.446983, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7776079177856445, Accuracy = 0.8954734802246094\n",
      "Training iter #22500000:   Batch Loss = 7.490659, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.777557373046875, Accuracy = 0.8951756954193115\n",
      "Training iter #22503000:   Batch Loss = 7.460865, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.777554035186768, Accuracy = 0.8957712650299072\n",
      "Training iter #22506000:   Batch Loss = 7.383081, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.777595520019531, Accuracy = 0.8957712650299072\n",
      "Training iter #22509000:   Batch Loss = 7.426969, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.777468681335449, Accuracy = 0.8960691094398499\n",
      "Training iter #22512000:   Batch Loss = 7.455059, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.777417182922363, Accuracy = 0.8960691094398499\n",
      "Training iter #22515000:   Batch Loss = 7.529363, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.777245044708252, Accuracy = 0.8954734802246094\n",
      "Training iter #22518000:   Batch Loss = 7.425561, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.775299072265625, Accuracy = 0.8954734802246094\n",
      "Training iter #22521000:   Batch Loss = 7.383885, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.775104999542236, Accuracy = 0.8957712650299072\n",
      "Training iter #22524000:   Batch Loss = 7.448039, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.775855541229248, Accuracy = 0.8957712650299072\n",
      "Training iter #22527000:   Batch Loss = 7.489806, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.775961875915527, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22530000:   Batch Loss = 7.470270, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.775691509246826, Accuracy = 0.8957712650299072\n",
      "Training iter #22533000:   Batch Loss = 7.371974, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.774781227111816, Accuracy = 0.8960691094398499\n",
      "Training iter #22536000:   Batch Loss = 7.428237, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.774501323699951, Accuracy = 0.8960691094398499\n",
      "Training iter #22539000:   Batch Loss = 7.437045, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7743239402771, Accuracy = 0.8960691094398499\n",
      "Training iter #22542000:   Batch Loss = 7.537098, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.774075031280518, Accuracy = 0.8960691094398499\n",
      "Training iter #22545000:   Batch Loss = 7.417244, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.773844242095947, Accuracy = 0.8954734802246094\n",
      "Training iter #22548000:   Batch Loss = 7.379001, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.773633003234863, Accuracy = 0.8954734802246094\n",
      "Training iter #22551000:   Batch Loss = 7.463480, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.773411750793457, Accuracy = 0.8957712650299072\n",
      "Training iter #22554000:   Batch Loss = 7.481697, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.77336311340332, Accuracy = 0.8954734802246094\n",
      "Training iter #22557000:   Batch Loss = 7.457197, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.773224830627441, Accuracy = 0.8960691094398499\n",
      "Training iter #22560000:   Batch Loss = 7.373702, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.773054122924805, Accuracy = 0.8960691094398499\n",
      "Training iter #22563000:   Batch Loss = 7.429419, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.772768020629883, Accuracy = 0.8963668942451477\n",
      "Training iter #22566000:   Batch Loss = 7.442142, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.772635459899902, Accuracy = 0.8957712650299072\n",
      "Training iter #22569000:   Batch Loss = 7.530966, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.772459983825684, Accuracy = 0.8957712650299072\n",
      "Training iter #22572000:   Batch Loss = 7.402679, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7720208168029785, Accuracy = 0.8957712650299072\n",
      "Training iter #22575000:   Batch Loss = 7.378160, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.771777629852295, Accuracy = 0.8957712650299072\n",
      "Training iter #22578000:   Batch Loss = 7.462340, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.771607398986816, Accuracy = 0.8957712650299072\n",
      "Training iter #22581000:   Batch Loss = 7.470986, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.771590709686279, Accuracy = 0.8960691094398499\n",
      "Training iter #22584000:   Batch Loss = 7.466975, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.771507740020752, Accuracy = 0.8960691094398499\n",
      "Training iter #22587000:   Batch Loss = 7.369927, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7713623046875, Accuracy = 0.8960691094398499\n",
      "Training iter #22590000:   Batch Loss = 7.426747, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7710957527160645, Accuracy = 0.8960691094398499\n",
      "Training iter #22593000:   Batch Loss = 7.442440, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.770882606506348, Accuracy = 0.8960691094398499\n",
      "Training iter #22596000:   Batch Loss = 7.537405, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.77055025100708, Accuracy = 0.8963668942451477\n",
      "Training iter #22599000:   Batch Loss = 7.390032, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.770299911499023, Accuracy = 0.8957712650299072\n",
      "Training iter #22602000:   Batch Loss = 7.371046, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.771047592163086, Accuracy = 0.8948779106140137\n",
      "Training iter #22605000:   Batch Loss = 7.475370, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.771064281463623, Accuracy = 0.8951756954193115\n",
      "Training iter #22608000:   Batch Loss = 7.481449, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.771013259887695, Accuracy = 0.8951756954193115\n",
      "Training iter #22611000:   Batch Loss = 7.460895, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.770955562591553, Accuracy = 0.8957712650299072\n",
      "Training iter #22614000:   Batch Loss = 7.378109, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.770798206329346, Accuracy = 0.8954734802246094\n",
      "Training iter #22617000:   Batch Loss = 7.423051, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7704901695251465, Accuracy = 0.8954734802246094\n",
      "Training iter #22620000:   Batch Loss = 7.448431, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.76924467086792, Accuracy = 0.8960691094398499\n",
      "Training iter #22623000:   Batch Loss = 7.517015, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768912315368652, Accuracy = 0.8957712650299072\n",
      "Training iter #22626000:   Batch Loss = 7.382208, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768647193908691, Accuracy = 0.8957712650299072\n",
      "Training iter #22629000:   Batch Loss = 7.375206, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768390655517578, Accuracy = 0.8954734802246094\n",
      "Training iter #22632000:   Batch Loss = 7.468230, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768168926239014, Accuracy = 0.8948779106140137\n",
      "Training iter #22635000:   Batch Loss = 7.476467, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768190383911133, Accuracy = 0.8963668942451477\n",
      "Training iter #22638000:   Batch Loss = 7.453234, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7681474685668945, Accuracy = 0.8963668942451477\n",
      "Training iter #22641000:   Batch Loss = 7.377703, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768097877502441, Accuracy = 0.8960691094398499\n",
      "Training iter #22644000:   Batch Loss = 7.420822, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.76905632019043, Accuracy = 0.8960691094398499\n",
      "Training iter #22647000:   Batch Loss = 7.459229, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768996715545654, Accuracy = 0.8957712650299072\n",
      "Training iter #22650000:   Batch Loss = 7.507451, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768786430358887, Accuracy = 0.8957712650299072\n",
      "Training iter #22653000:   Batch Loss = 7.369796, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768641471862793, Accuracy = 0.8957712650299072\n",
      "Training iter #22656000:   Batch Loss = 7.382542, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768486022949219, Accuracy = 0.8957712650299072\n",
      "Training iter #22659000:   Batch Loss = 7.456567, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768310546875, Accuracy = 0.8960691094398499\n",
      "Training iter #22662000:   Batch Loss = 7.489320, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.768214225769043, Accuracy = 0.8960691094398499\n",
      "Training iter #22665000:   Batch Loss = 7.461568, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.767475128173828, Accuracy = 0.8960691094398499\n",
      "Training iter #22668000:   Batch Loss = 7.390272, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.767744064331055, Accuracy = 0.8957712650299072\n",
      "Training iter #22671000:   Batch Loss = 7.424125, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.767526626586914, Accuracy = 0.8957712650299072\n",
      "Training iter #22674000:   Batch Loss = 7.455311, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.767218589782715, Accuracy = 0.8957712650299072\n",
      "Training iter #22677000:   Batch Loss = 7.486162, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.766806125640869, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22680000:   Batch Loss = 7.355600, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.766608715057373, Accuracy = 0.8948779106140137\n",
      "Training iter #22683000:   Batch Loss = 7.386379, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.766644477844238, Accuracy = 0.8945801258087158\n",
      "Training iter #22686000:   Batch Loss = 7.443588, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.767344951629639, Accuracy = 0.8951756954193115\n",
      "Training iter #22689000:   Batch Loss = 7.489037, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.767200469970703, Accuracy = 0.8954734802246094\n",
      "Training iter #22692000:   Batch Loss = 7.460970, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7670488357543945, Accuracy = 0.8954734802246094\n",
      "Training iter #22695000:   Batch Loss = 7.392920, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.766963958740234, Accuracy = 0.8954734802246094\n",
      "Training iter #22698000:   Batch Loss = 7.417115, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.766743183135986, Accuracy = 0.8954734802246094\n",
      "Training iter #22701000:   Batch Loss = 7.450387, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.766544342041016, Accuracy = 0.8954734802246094\n",
      "Training iter #22704000:   Batch Loss = 7.490677, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.765148162841797, Accuracy = 0.8954734802246094\n",
      "Training iter #22707000:   Batch Loss = 7.355273, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764916896820068, Accuracy = 0.8945801258087158\n",
      "Training iter #22710000:   Batch Loss = 7.390823, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764646530151367, Accuracy = 0.8945801258087158\n",
      "Training iter #22713000:   Batch Loss = 7.443947, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764350414276123, Accuracy = 0.8951756954193115\n",
      "Training iter #22716000:   Batch Loss = 7.506030, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764279365539551, Accuracy = 0.8951756954193115\n",
      "Training iter #22719000:   Batch Loss = 7.438205, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764186382293701, Accuracy = 0.8954734802246094\n",
      "Training iter #22722000:   Batch Loss = 7.383659, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764108180999756, Accuracy = 0.8954734802246094\n",
      "Training iter #22725000:   Batch Loss = 7.434948, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764946937561035, Accuracy = 0.8954734802246094\n",
      "Training iter #22728000:   Batch Loss = 7.453380, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764726638793945, Accuracy = 0.8954734802246094\n",
      "Training iter #22731000:   Batch Loss = 7.477359, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764423370361328, Accuracy = 0.8954734802246094\n",
      "Training iter #22734000:   Batch Loss = 7.358371, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764225006103516, Accuracy = 0.8954734802246094\n",
      "Training iter #22737000:   Batch Loss = 7.387700, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.764025688171387, Accuracy = 0.8951756954193115\n",
      "Training iter #22740000:   Batch Loss = 7.424972, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.763767242431641, Accuracy = 0.8957712650299072\n",
      "Training iter #22743000:   Batch Loss = 7.497624, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.763289451599121, Accuracy = 0.8957712650299072\n",
      "Training iter #22746000:   Batch Loss = 7.425336, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.762477874755859, Accuracy = 0.8954734802246094\n",
      "Training iter #22749000:   Batch Loss = 7.377155, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.762375831604004, Accuracy = 0.8957712650299072\n",
      "Training iter #22752000:   Batch Loss = 7.441758, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.762094497680664, Accuracy = 0.8957712650299072\n",
      "Training iter #22755000:   Batch Loss = 7.460033, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.761850833892822, Accuracy = 0.8957712650299072\n",
      "Training iter #22758000:   Batch Loss = 7.480263, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.761500835418701, Accuracy = 0.8960691094398499\n",
      "Training iter #22761000:   Batch Loss = 7.357031, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.761258602142334, Accuracy = 0.8960691094398499\n",
      "Training iter #22764000:   Batch Loss = 7.387900, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.761054039001465, Accuracy = 0.8960691094398499\n",
      "Training iter #22767000:   Batch Loss = 7.423014, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.762168884277344, Accuracy = 0.8957712650299072\n",
      "Training iter #22770000:   Batch Loss = 7.501678, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.762408256530762, Accuracy = 0.8957712650299072\n",
      "Training iter #22773000:   Batch Loss = 7.417847, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.761944770812988, Accuracy = 0.8957712650299072\n",
      "Training iter #22776000:   Batch Loss = 7.375500, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.762251853942871, Accuracy = 0.8957712650299072\n",
      "Training iter #22779000:   Batch Loss = 7.445601, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.762023448944092, Accuracy = 0.8957712650299072\n",
      "Training iter #22782000:   Batch Loss = 7.457547, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.761918067932129, Accuracy = 0.8957712650299072\n",
      "Training iter #22785000:   Batch Loss = 7.469023, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.760441780090332, Accuracy = 0.8957712650299072\n",
      "Training iter #22788000:   Batch Loss = 7.358944, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.76019287109375, Accuracy = 0.8960691094398499\n",
      "Training iter #22791000:   Batch Loss = 7.387854, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.760080814361572, Accuracy = 0.8960691094398499\n",
      "Training iter #22794000:   Batch Loss = 7.415290, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.759973526000977, Accuracy = 0.8957712650299072\n",
      "Training iter #22797000:   Batch Loss = 7.499946, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.759972095489502, Accuracy = 0.8954734802246094\n",
      "Training iter #22800000:   Batch Loss = 7.410826, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.759599208831787, Accuracy = 0.8960691094398499\n",
      "Training iter #22803000:   Batch Loss = 7.376954, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.759599208831787, Accuracy = 0.8960691094398499\n",
      "Training iter #22806000:   Batch Loss = 7.427812, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.759437561035156, Accuracy = 0.8957712650299072\n",
      "Training iter #22809000:   Batch Loss = 7.475122, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.760432243347168, Accuracy = 0.8954734802246094\n",
      "Training iter #22812000:   Batch Loss = 7.460592, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.760146141052246, Accuracy = 0.8954734802246094\n",
      "Training iter #22815000:   Batch Loss = 7.360826, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.760020732879639, Accuracy = 0.8954734802246094\n",
      "Training iter #22818000:   Batch Loss = 7.405501, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.759866237640381, Accuracy = 0.8954734802246094\n",
      "Training iter #22821000:   Batch Loss = 7.430498, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.759720802307129, Accuracy = 0.8951756954193115\n",
      "Training iter #22824000:   Batch Loss = 7.503512, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.758738994598389, Accuracy = 0.8948779106140137\n",
      "Training iter #22827000:   Batch Loss = 7.407157, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.758509159088135, Accuracy = 0.8951756954193115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22830000:   Batch Loss = 7.366927, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.758416175842285, Accuracy = 0.8951756954193115\n",
      "Training iter #22833000:   Batch Loss = 7.424991, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.758067607879639, Accuracy = 0.8951756954193115\n",
      "Training iter #22836000:   Batch Loss = 7.463312, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7578630447387695, Accuracy = 0.8951756954193115\n",
      "Training iter #22839000:   Batch Loss = 7.448206, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.757433891296387, Accuracy = 0.8951756954193115\n",
      "Training iter #22842000:   Batch Loss = 7.350558, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.757261276245117, Accuracy = 0.8951756954193115\n",
      "Training iter #22845000:   Batch Loss = 7.407638, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.758151054382324, Accuracy = 0.8945801258087158\n",
      "Training iter #22848000:   Batch Loss = 7.431757, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7581467628479, Accuracy = 0.8951756954193115\n",
      "Training iter #22851000:   Batch Loss = 7.503243, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.758056163787842, Accuracy = 0.8954734802246094\n",
      "Training iter #22854000:   Batch Loss = 7.396641, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.757612228393555, Accuracy = 0.8954734802246094\n",
      "Training iter #22857000:   Batch Loss = 7.366713, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.75748872756958, Accuracy = 0.8954734802246094\n",
      "Training iter #22860000:   Batch Loss = 7.443616, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.757259368896484, Accuracy = 0.8954734802246094\n",
      "Training iter #22863000:   Batch Loss = 7.465633, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.757116794586182, Accuracy = 0.8948779106140137\n",
      "Training iter #22866000:   Batch Loss = 7.436582, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755842208862305, Accuracy = 0.8954734802246094\n",
      "Training iter #22869000:   Batch Loss = 7.348269, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755863189697266, Accuracy = 0.8954734802246094\n",
      "Training iter #22872000:   Batch Loss = 7.406433, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755826473236084, Accuracy = 0.8951756954193115\n",
      "Training iter #22875000:   Batch Loss = 7.418332, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755634307861328, Accuracy = 0.8945801258087158\n",
      "Training iter #22878000:   Batch Loss = 7.513679, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755682945251465, Accuracy = 0.8945801258087158\n",
      "Training iter #22881000:   Batch Loss = 7.395942, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755580425262451, Accuracy = 0.8945801258087158\n",
      "Training iter #22884000:   Batch Loss = 7.362174, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.756585597991943, Accuracy = 0.8942822813987732\n",
      "Training iter #22887000:   Batch Loss = 7.441171, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.756397247314453, Accuracy = 0.8945801258087158\n",
      "Training iter #22890000:   Batch Loss = 7.455246, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.756155490875244, Accuracy = 0.8945801258087158\n",
      "Training iter #22893000:   Batch Loss = 7.446138, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755860328674316, Accuracy = 0.8948779106140137\n",
      "Training iter #22896000:   Batch Loss = 7.351145, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755615234375, Accuracy = 0.8945801258087158\n",
      "Training iter #22899000:   Batch Loss = 7.406305, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755537033081055, Accuracy = 0.8942822813987732\n",
      "Training iter #22902000:   Batch Loss = 7.425559, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755170822143555, Accuracy = 0.8954734802246094\n",
      "Training iter #22905000:   Batch Loss = 7.512188, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.755092144012451, Accuracy = 0.8951756954193115\n",
      "Training iter #22908000:   Batch Loss = 7.373969, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.754913330078125, Accuracy = 0.8951756954193115\n",
      "Training iter #22911000:   Batch Loss = 7.358860, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.754841327667236, Accuracy = 0.8951756954193115\n",
      "Training iter #22914000:   Batch Loss = 7.443077, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.754631996154785, Accuracy = 0.8951756954193115\n",
      "Training iter #22917000:   Batch Loss = 7.456727, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.754371643066406, Accuracy = 0.8954734802246094\n",
      "Training iter #22920000:   Batch Loss = 7.440178, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.754085540771484, Accuracy = 0.8954734802246094\n",
      "Training iter #22923000:   Batch Loss = 7.357502, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.753977298736572, Accuracy = 0.8957712650299072\n",
      "Training iter #22926000:   Batch Loss = 7.406440, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.753830909729004, Accuracy = 0.8954734802246094\n",
      "Training iter #22929000:   Batch Loss = 7.418575, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.753564357757568, Accuracy = 0.8954734802246094\n",
      "Training iter #22932000:   Batch Loss = 7.506771, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.75340461730957, Accuracy = 0.8954734802246094\n",
      "Training iter #22935000:   Batch Loss = 7.360779, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.753315448760986, Accuracy = 0.8954734802246094\n",
      "Training iter #22938000:   Batch Loss = 7.352524, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.753178596496582, Accuracy = 0.8954734802246094\n",
      "Training iter #22941000:   Batch Loss = 7.453845, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.752843379974365, Accuracy = 0.8951756954193115\n",
      "Training iter #22944000:   Batch Loss = 7.463022, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.752545356750488, Accuracy = 0.8951756954193115\n",
      "Training iter #22947000:   Batch Loss = 7.438399, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.752382278442383, Accuracy = 0.8951756954193115\n",
      "Training iter #22950000:   Batch Loss = 7.361419, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.752171516418457, Accuracy = 0.8954734802246094\n",
      "Training iter #22953000:   Batch Loss = 7.408317, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751913547515869, Accuracy = 0.8954734802246094\n",
      "Training iter #22956000:   Batch Loss = 7.434760, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751648426055908, Accuracy = 0.8957712650299072\n",
      "Training iter #22959000:   Batch Loss = 7.490628, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751522064208984, Accuracy = 0.8954734802246094\n",
      "Training iter #22962000:   Batch Loss = 7.356565, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751559257507324, Accuracy = 0.8957712650299072\n",
      "Training iter #22965000:   Batch Loss = 7.357902, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751457691192627, Accuracy = 0.8957712650299072\n",
      "Training iter #22968000:   Batch Loss = 7.445380, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751285552978516, Accuracy = 0.8957712650299072\n",
      "Training iter #22971000:   Batch Loss = 7.457520, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.75092887878418, Accuracy = 0.8957712650299072\n",
      "Training iter #22974000:   Batch Loss = 7.437531, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.750268459320068, Accuracy = 0.8954734802246094\n",
      "Training iter #22977000:   Batch Loss = 7.364587, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749858379364014, Accuracy = 0.8960691094398499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22980000:   Batch Loss = 7.405590, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749427318572998, Accuracy = 0.8963668942451477\n",
      "Training iter #22983000:   Batch Loss = 7.435003, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748953342437744, Accuracy = 0.8960691094398499\n",
      "Training iter #22986000:   Batch Loss = 7.477910, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748687267303467, Accuracy = 0.8960691094398499\n",
      "Training iter #22989000:   Batch Loss = 7.345850, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748935699462891, Accuracy = 0.8957712650299072\n",
      "Training iter #22992000:   Batch Loss = 7.355625, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749690055847168, Accuracy = 0.8957712650299072\n",
      "Training iter #22995000:   Batch Loss = 7.428724, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751559257507324, Accuracy = 0.8951756954193115\n",
      "Training iter #22998000:   Batch Loss = 7.474633, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751700401306152, Accuracy = 0.8951756954193115\n",
      "Training iter #23001000:   Batch Loss = 7.446823, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751206874847412, Accuracy = 0.8945801258087158\n",
      "Training iter #23004000:   Batch Loss = 7.371916, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.751156806945801, Accuracy = 0.8945801258087158\n",
      "Training iter #23007000:   Batch Loss = 7.404107, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.75069522857666, Accuracy = 0.8939844965934753\n",
      "Training iter #23010000:   Batch Loss = 7.429505, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.750434398651123, Accuracy = 0.8942822813987732\n",
      "Training iter #23013000:   Batch Loss = 7.465468, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749988555908203, Accuracy = 0.8942822813987732\n",
      "Training iter #23016000:   Batch Loss = 7.337229, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749849796295166, Accuracy = 0.8945801258087158\n",
      "Training iter #23019000:   Batch Loss = 7.367763, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749748706817627, Accuracy = 0.8945801258087158\n",
      "Training iter #23022000:   Batch Loss = 7.426822, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749341011047363, Accuracy = 0.8945801258087158\n",
      "Training iter #23025000:   Batch Loss = 7.470230, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749063968658447, Accuracy = 0.8948779106140137\n",
      "Training iter #23028000:   Batch Loss = 7.431991, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749048233032227, Accuracy = 0.8948779106140137\n",
      "Training iter #23031000:   Batch Loss = 7.373712, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749241352081299, Accuracy = 0.8948779106140137\n",
      "Training iter #23034000:   Batch Loss = 7.402454, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.749051094055176, Accuracy = 0.8948779106140137\n",
      "Training iter #23037000:   Batch Loss = 7.428047, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748796463012695, Accuracy = 0.8948779106140137\n",
      "Training iter #23040000:   Batch Loss = 7.461242, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748778343200684, Accuracy = 0.8948779106140137\n",
      "Training iter #23043000:   Batch Loss = 7.343690, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748873233795166, Accuracy = 0.8951756954193115\n",
      "Training iter #23046000:   Batch Loss = 7.372019, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748782157897949, Accuracy = 0.8951756954193115\n",
      "Training iter #23049000:   Batch Loss = 7.411598, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748480796813965, Accuracy = 0.8948779106140137\n",
      "Training iter #23052000:   Batch Loss = 7.476548, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.748199462890625, Accuracy = 0.8945801258087158\n",
      "Training iter #23055000:   Batch Loss = 7.416625, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7478461265563965, Accuracy = 0.8945801258087158\n",
      "Training iter #23058000:   Batch Loss = 7.366224, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.74766731262207, Accuracy = 0.8948779106140137\n",
      "Training iter #23061000:   Batch Loss = 7.424148, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.747315883636475, Accuracy = 0.8948779106140137\n",
      "Training iter #23064000:   Batch Loss = 7.436156, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.74704122543335, Accuracy = 0.8948779106140137\n",
      "Training iter #23067000:   Batch Loss = 7.463093, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.746749401092529, Accuracy = 0.8948779106140137\n",
      "Training iter #23070000:   Batch Loss = 7.334742, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.746626377105713, Accuracy = 0.8948779106140137\n",
      "Training iter #23073000:   Batch Loss = 7.369829, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.746551036834717, Accuracy = 0.8948779106140137\n",
      "Training iter #23076000:   Batch Loss = 7.404606, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.746234893798828, Accuracy = 0.8951756954193115\n",
      "Training iter #23079000:   Batch Loss = 7.476051, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.74598503112793, Accuracy = 0.8948779106140137\n",
      "Training iter #23082000:   Batch Loss = 7.403201, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7459821701049805, Accuracy = 0.8948779106140137\n",
      "Training iter #23085000:   Batch Loss = 7.359941, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.745968818664551, Accuracy = 0.8948779106140137\n",
      "Training iter #23088000:   Batch Loss = 7.424181, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.745718479156494, Accuracy = 0.8948779106140137\n",
      "Training iter #23091000:   Batch Loss = 7.435287, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.745399475097656, Accuracy = 0.8948779106140137\n",
      "Training iter #23094000:   Batch Loss = 7.456886, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.745337963104248, Accuracy = 0.8942822813987732\n",
      "Training iter #23097000:   Batch Loss = 7.341484, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.745247840881348, Accuracy = 0.8942822813987732\n",
      "Training iter #23100000:   Batch Loss = 7.370129, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.745001792907715, Accuracy = 0.8945801258087158\n",
      "Training iter #23103000:   Batch Loss = 7.398012, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.744700908660889, Accuracy = 0.8945801258087158\n",
      "Training iter #23106000:   Batch Loss = 7.481493, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7444257736206055, Accuracy = 0.8948779106140137\n",
      "Training iter #23109000:   Batch Loss = 7.395116, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.744345188140869, Accuracy = 0.8948779106140137\n",
      "Training iter #23112000:   Batch Loss = 7.360326, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.744073867797852, Accuracy = 0.8948779106140137\n",
      "Training iter #23115000:   Batch Loss = 7.424963, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.743795394897461, Accuracy = 0.8951756954193115\n",
      "Training iter #23118000:   Batch Loss = 7.438786, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.743542194366455, Accuracy = 0.8948779106140137\n",
      "Training iter #23121000:   Batch Loss = 7.448544, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.743860244750977, Accuracy = 0.8948779106140137\n",
      "Training iter #23124000:   Batch Loss = 7.339620, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.743934154510498, Accuracy = 0.8945801258087158\n",
      "Training iter #23127000:   Batch Loss = 7.386356, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7437872886657715, Accuracy = 0.8945801258087158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #23130000:   Batch Loss = 7.399961, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.743459701538086, Accuracy = 0.8942822813987732\n",
      "Training iter #23133000:   Batch Loss = 7.479554, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.74315071105957, Accuracy = 0.8945801258087158\n",
      "Training iter #23136000:   Batch Loss = 7.387721, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.743213653564453, Accuracy = 0.8945801258087158\n",
      "Training iter #23139000:   Batch Loss = 7.354517, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.742947578430176, Accuracy = 0.8942822813987732\n",
      "Training iter #23142000:   Batch Loss = 7.405744, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.742526054382324, Accuracy = 0.8942822813987732\n",
      "Training iter #23145000:   Batch Loss = 7.445551, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7420268058776855, Accuracy = 0.8939844965934753\n",
      "Training iter #23148000:   Batch Loss = 7.427574, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.742065906524658, Accuracy = 0.8948779106140137\n",
      "Training iter #23151000:   Batch Loss = 7.340029, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.74184513092041, Accuracy = 0.8951756954193115\n",
      "Training iter #23154000:   Batch Loss = 7.388517, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7415032386779785, Accuracy = 0.8951756954193115\n",
      "Training iter #23157000:   Batch Loss = 7.412300, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.741209030151367, Accuracy = 0.8951756954193115\n",
      "Training iter #23160000:   Batch Loss = 7.484849, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.740845203399658, Accuracy = 0.8951756954193115\n",
      "Training iter #23163000:   Batch Loss = 7.389001, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.741102695465088, Accuracy = 0.8954734802246094\n",
      "Training iter #23166000:   Batch Loss = 7.349394, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.741143703460693, Accuracy = 0.8951756954193115\n",
      "Training iter #23169000:   Batch Loss = 7.407373, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.740917205810547, Accuracy = 0.8951756954193115\n",
      "Training iter #23172000:   Batch Loss = 7.442498, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.740603923797607, Accuracy = 0.8951756954193115\n",
      "Training iter #23175000:   Batch Loss = 7.430231, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.740684509277344, Accuracy = 0.8954734802246094\n",
      "Training iter #23178000:   Batch Loss = 7.329804, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.740455627441406, Accuracy = 0.8954734802246094\n",
      "Training iter #23181000:   Batch Loss = 7.387116, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.740147590637207, Accuracy = 0.8954734802246094\n",
      "Training iter #23184000:   Batch Loss = 7.411680, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.739870548248291, Accuracy = 0.8957712650299072\n",
      "Training iter #23187000:   Batch Loss = 7.488266, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.739589691162109, Accuracy = 0.8960691094398499\n",
      "Training iter #23190000:   Batch Loss = 7.376964, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7396416664123535, Accuracy = 0.8957712650299072\n",
      "Training iter #23193000:   Batch Loss = 7.345505, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.739414691925049, Accuracy = 0.8960691094398499\n",
      "Training iter #23196000:   Batch Loss = 7.422358, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.73907995223999, Accuracy = 0.8960691094398499\n",
      "Training iter #23199000:   Batch Loss = 7.440432, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.738821983337402, Accuracy = 0.8957712650299072\n",
      "Training iter #23202000:   Batch Loss = 7.415014, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7389817237854, Accuracy = 0.8957712650299072\n",
      "Training iter #23205000:   Batch Loss = 7.331335, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7389912605285645, Accuracy = 0.8957712650299072\n",
      "Training iter #23208000:   Batch Loss = 7.390064, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.738785743713379, Accuracy = 0.8957712650299072\n",
      "Training iter #23211000:   Batch Loss = 7.402955, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.738604545593262, Accuracy = 0.8957712650299072\n",
      "Training iter #23214000:   Batch Loss = 7.484471, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.738435745239258, Accuracy = 0.8957712650299072\n",
      "Training iter #23217000:   Batch Loss = 7.365160, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7385077476501465, Accuracy = 0.8957712650299072\n",
      "Training iter #23220000:   Batch Loss = 7.342418, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.738295555114746, Accuracy = 0.8957712650299072\n",
      "Training iter #23223000:   Batch Loss = 7.421408, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.737943172454834, Accuracy = 0.8954734802246094\n",
      "Training iter #23226000:   Batch Loss = 7.429996, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.737637519836426, Accuracy = 0.8957712650299072\n",
      "Training iter #23229000:   Batch Loss = 7.424974, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.737607002258301, Accuracy = 0.8954734802246094\n",
      "Training iter #23232000:   Batch Loss = 7.335511, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.737384796142578, Accuracy = 0.8954734802246094\n",
      "Training iter #23235000:   Batch Loss = 7.388964, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.737153053283691, Accuracy = 0.8957712650299072\n",
      "Training iter #23238000:   Batch Loss = 7.398967, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.736990451812744, Accuracy = 0.8957712650299072\n",
      "Training iter #23241000:   Batch Loss = 7.488966, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.736887454986572, Accuracy = 0.8957712650299072\n",
      "Training iter #23244000:   Batch Loss = 7.350770, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.737000465393066, Accuracy = 0.8957712650299072\n",
      "Training iter #23247000:   Batch Loss = 7.335364, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.736880302429199, Accuracy = 0.8954734802246094\n",
      "Training iter #23250000:   Batch Loss = 7.427717, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7367377281188965, Accuracy = 0.8954734802246094\n",
      "Training iter #23253000:   Batch Loss = 7.433234, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7364888191223145, Accuracy = 0.8951756954193115\n",
      "Training iter #23256000:   Batch Loss = 7.421146, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.736393451690674, Accuracy = 0.8951756954193115\n",
      "Training iter #23259000:   Batch Loss = 7.340221, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.736106872558594, Accuracy = 0.8954734802246094\n",
      "Training iter #23262000:   Batch Loss = 7.384603, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7358245849609375, Accuracy = 0.8957712650299072\n",
      "Training iter #23265000:   Batch Loss = 7.400770, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.735618591308594, Accuracy = 0.8960691094398499\n",
      "Training iter #23268000:   Batch Loss = 7.478479, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.735388278961182, Accuracy = 0.8960691094398499\n",
      "Training iter #23271000:   Batch Loss = 7.341089, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7353410720825195, Accuracy = 0.8957712650299072\n",
      "Training iter #23274000:   Batch Loss = 7.333358, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.73497200012207, Accuracy = 0.8957712650299072\n",
      "Training iter #23277000:   Batch Loss = 7.427662, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7346320152282715, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #23280000:   Batch Loss = 7.433928, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.734435558319092, Accuracy = 0.8957712650299072\n",
      "Training iter #23283000:   Batch Loss = 7.413641, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.734470367431641, Accuracy = 0.8957712650299072\n",
      "Training iter #23286000:   Batch Loss = 7.341821, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.734226226806641, Accuracy = 0.8957712650299072\n",
      "Training iter #23289000:   Batch Loss = 7.382365, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.734075546264648, Accuracy = 0.8954734802246094\n",
      "Training iter #23292000:   Batch Loss = 7.415994, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.733946323394775, Accuracy = 0.8954734802246094\n",
      "Training iter #23295000:   Batch Loss = 7.465732, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7338433265686035, Accuracy = 0.8951756954193115\n",
      "Training iter #23298000:   Batch Loss = 7.334023, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.733926773071289, Accuracy = 0.8954734802246094\n",
      "Training iter #23301000:   Batch Loss = 7.334192, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.733808517456055, Accuracy = 0.8954734802246094\n",
      "Training iter #23304000:   Batch Loss = 7.418613, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.733443260192871, Accuracy = 0.8957712650299072\n",
      "Training iter #23307000:   Batch Loss = 7.441021, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7330241203308105, Accuracy = 0.8960691094398499\n",
      "Training iter #23310000:   Batch Loss = 7.420138, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7328009605407715, Accuracy = 0.8960691094398499\n",
      "Training iter #23313000:   Batch Loss = 7.353745, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.732454299926758, Accuracy = 0.8960691094398499\n",
      "Training iter #23316000:   Batch Loss = 7.388817, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.732367992401123, Accuracy = 0.8960691094398499\n",
      "Training iter #23319000:   Batch Loss = 7.412643, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7321906089782715, Accuracy = 0.8960691094398499\n",
      "Training iter #23322000:   Batch Loss = 7.443879, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.732117652893066, Accuracy = 0.8960691094398499\n",
      "Training iter #23325000:   Batch Loss = 7.320924, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.73187255859375, Accuracy = 0.8957712650299072\n",
      "Training iter #23328000:   Batch Loss = 7.348653, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.731284141540527, Accuracy = 0.8957712650299072\n",
      "Training iter #23331000:   Batch Loss = 7.404752, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730903148651123, Accuracy = 0.8957712650299072\n",
      "Training iter #23334000:   Batch Loss = 7.449332, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730723857879639, Accuracy = 0.8957712650299072\n",
      "Training iter #23337000:   Batch Loss = 7.422496, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730868339538574, Accuracy = 0.8945801258087158\n",
      "Training iter #23340000:   Batch Loss = 7.355399, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730900764465332, Accuracy = 0.8945801258087158\n",
      "Training iter #23343000:   Batch Loss = 7.380404, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730956077575684, Accuracy = 0.8945801258087158\n",
      "Training iter #23346000:   Batch Loss = 7.408633, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730901718139648, Accuracy = 0.8945801258087158\n",
      "Training iter #23349000:   Batch Loss = 7.444675, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7303948402404785, Accuracy = 0.8948779106140137\n",
      "Training iter #23352000:   Batch Loss = 7.319630, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.73000955581665, Accuracy = 0.8945801258087158\n",
      "Training iter #23355000:   Batch Loss = 7.352212, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7299370765686035, Accuracy = 0.8945801258087158\n",
      "Training iter #23358000:   Batch Loss = 7.406705, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730049133300781, Accuracy = 0.8942822813987732\n",
      "Training iter #23361000:   Batch Loss = 7.452456, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730010032653809, Accuracy = 0.8945801258087158\n",
      "Training iter #23364000:   Batch Loss = 7.400802, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730127334594727, Accuracy = 0.8948779106140137\n",
      "Training iter #23367000:   Batch Loss = 7.347090, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730042934417725, Accuracy = 0.8945801258087158\n",
      "Training iter #23370000:   Batch Loss = 7.395342, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730088710784912, Accuracy = 0.8948779106140137\n",
      "Training iter #23373000:   Batch Loss = 7.417010, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.730031490325928, Accuracy = 0.8948779106140137\n",
      "Training iter #23376000:   Batch Loss = 7.435323, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7299346923828125, Accuracy = 0.8948779106140137\n",
      "Training iter #23379000:   Batch Loss = 7.324200, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.729770183563232, Accuracy = 0.8948779106140137\n",
      "Training iter #23382000:   Batch Loss = 7.350816, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.729448318481445, Accuracy = 0.8948779106140137\n",
      "Training iter #23385000:   Batch Loss = 7.388237, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.72906494140625, Accuracy = 0.8951756954193115\n",
      "Training iter #23388000:   Batch Loss = 7.454967, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728763103485107, Accuracy = 0.8951756954193115\n",
      "Training iter #23391000:   Batch Loss = 7.386681, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728569507598877, Accuracy = 0.8954734802246094\n",
      "Training iter #23394000:   Batch Loss = 7.346778, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728296279907227, Accuracy = 0.8954734802246094\n",
      "Training iter #23397000:   Batch Loss = 7.403360, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728231906890869, Accuracy = 0.8957712650299072\n",
      "Training iter #23400000:   Batch Loss = 7.419611, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728297233581543, Accuracy = 0.8960691094398499\n",
      "Training iter #23403000:   Batch Loss = 7.437461, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728252410888672, Accuracy = 0.8954734802246094\n",
      "Training iter #23406000:   Batch Loss = 7.323342, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728255271911621, Accuracy = 0.8948779106140137\n",
      "Training iter #23409000:   Batch Loss = 7.351684, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.728060722351074, Accuracy = 0.8951756954193115\n",
      "Training iter #23412000:   Batch Loss = 7.389675, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.727741718292236, Accuracy = 0.8948779106140137\n",
      "Training iter #23415000:   Batch Loss = 7.456637, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.72749662399292, Accuracy = 0.8948779106140137\n",
      "Training iter #23418000:   Batch Loss = 7.379116, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.727277755737305, Accuracy = 0.8951756954193115\n",
      "Training iter #23421000:   Batch Loss = 7.338913, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7269287109375, Accuracy = 0.8954734802246094\n",
      "Training iter #23424000:   Batch Loss = 7.407340, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.726558685302734, Accuracy = 0.8963668942451477\n",
      "Training iter #23427000:   Batch Loss = 7.414619, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7262654304504395, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #23430000:   Batch Loss = 7.431784, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.725953578948975, Accuracy = 0.8960691094398499\n",
      "Training iter #23433000:   Batch Loss = 7.323025, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7254252433776855, Accuracy = 0.8963668942451477\n",
      "Training iter #23436000:   Batch Loss = 7.350384, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.725184440612793, Accuracy = 0.8963668942451477\n",
      "Training iter #23439000:   Batch Loss = 7.377018, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.72506046295166, Accuracy = 0.8963668942451477\n",
      "Training iter #23442000:   Batch Loss = 7.460175, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.724950790405273, Accuracy = 0.8957712650299072\n",
      "Training iter #23445000:   Batch Loss = 7.374052, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.724965572357178, Accuracy = 0.8960691094398499\n",
      "Training iter #23448000:   Batch Loss = 7.340676, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.724940299987793, Accuracy = 0.8954734802246094\n",
      "Training iter #23451000:   Batch Loss = 7.386193, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.726442813873291, Accuracy = 0.8951756954193115\n",
      "Training iter #23454000:   Batch Loss = 7.420407, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7262067794799805, Accuracy = 0.8945801258087158\n",
      "Training iter #23457000:   Batch Loss = 7.419482, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.726172924041748, Accuracy = 0.8951756954193115\n",
      "Training iter #23460000:   Batch Loss = 7.321035, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.72606086730957, Accuracy = 0.8945801258087158\n",
      "Training iter #23463000:   Batch Loss = 7.367062, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.725884914398193, Accuracy = 0.8942822813987732\n",
      "Training iter #23466000:   Batch Loss = 7.388987, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7257585525512695, Accuracy = 0.8942822813987732\n",
      "Training iter #23469000:   Batch Loss = 7.459453, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.725621223449707, Accuracy = 0.8939844965934753\n",
      "Training iter #23472000:   Batch Loss = 7.372255, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.725466728210449, Accuracy = 0.8945801258087158\n",
      "Training iter #23475000:   Batch Loss = 7.330914, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7251152992248535, Accuracy = 0.8951756954193115\n",
      "Training iter #23478000:   Batch Loss = 7.387211, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.724936485290527, Accuracy = 0.8951756954193115\n",
      "Training iter #23481000:   Batch Loss = 7.423723, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.724707126617432, Accuracy = 0.8954734802246094\n",
      "Training iter #23484000:   Batch Loss = 7.396830, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.724403381347656, Accuracy = 0.8948779106140137\n",
      "Training iter #23487000:   Batch Loss = 7.319862, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.723914623260498, Accuracy = 0.8945801258087158\n",
      "Training iter #23490000:   Batch Loss = 7.369243, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.723451137542725, Accuracy = 0.8954734802246094\n",
      "Training iter #23493000:   Batch Loss = 7.392981, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.723123073577881, Accuracy = 0.8954734802246094\n",
      "Training iter #23496000:   Batch Loss = 7.464829, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.723014831542969, Accuracy = 0.8948779106140137\n",
      "Training iter #23499000:   Batch Loss = 7.360821, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.722981929779053, Accuracy = 0.8951756954193115\n",
      "Training iter #23502000:   Batch Loss = 7.329804, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.722868919372559, Accuracy = 0.8948779106140137\n",
      "Training iter #23505000:   Batch Loss = 7.388397, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.722684860229492, Accuracy = 0.8957712650299072\n",
      "Training iter #23508000:   Batch Loss = 7.422599, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.722468376159668, Accuracy = 0.8963668942451477\n",
      "Training iter #23511000:   Batch Loss = 7.398326, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.722143173217773, Accuracy = 0.8960691094398499\n",
      "Training iter #23514000:   Batch Loss = 7.314387, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.720317840576172, Accuracy = 0.8963668942451477\n",
      "Training iter #23517000:   Batch Loss = 7.368487, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7197585105896, Accuracy = 0.8966646790504456\n",
      "Training iter #23520000:   Batch Loss = 7.377655, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719338893890381, Accuracy = 0.8963668942451477\n",
      "Training iter #23523000:   Batch Loss = 7.470710, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719311714172363, Accuracy = 0.8963668942451477\n",
      "Training iter #23526000:   Batch Loss = 7.358780, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719295024871826, Accuracy = 0.8960691094398499\n",
      "Training iter #23529000:   Batch Loss = 7.330547, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.72113561630249, Accuracy = 0.8954734802246094\n",
      "Training iter #23532000:   Batch Loss = 7.399500, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.722408294677734, Accuracy = 0.8951756954193115\n",
      "Training iter #23535000:   Batch Loss = 7.417696, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.723620414733887, Accuracy = 0.8942822813987732\n",
      "Training iter #23538000:   Batch Loss = 7.404275, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.723569393157959, Accuracy = 0.8942822813987732\n",
      "Training iter #23541000:   Batch Loss = 7.326292, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.723066806793213, Accuracy = 0.8945801258087158\n",
      "Training iter #23544000:   Batch Loss = 7.371379, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.721297740936279, Accuracy = 0.8948779106140137\n",
      "Training iter #23547000:   Batch Loss = 7.384940, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7205095291137695, Accuracy = 0.8960691094398499\n",
      "Training iter #23550000:   Batch Loss = 7.467642, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.720129489898682, Accuracy = 0.8960691094398499\n",
      "Training iter #23553000:   Batch Loss = 7.339670, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.720087051391602, Accuracy = 0.8957712650299072\n",
      "Training iter #23556000:   Batch Loss = 7.329136, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719609260559082, Accuracy = 0.8960691094398499\n",
      "Training iter #23559000:   Batch Loss = 7.404633, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.718908309936523, Accuracy = 0.8960691094398499\n",
      "Training iter #23562000:   Batch Loss = 7.423644, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.718447208404541, Accuracy = 0.8954734802246094\n",
      "Training iter #23565000:   Batch Loss = 7.399990, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.718276023864746, Accuracy = 0.8945801258087158\n",
      "Training iter #23568000:   Batch Loss = 7.326897, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.71809720993042, Accuracy = 0.8951756954193115\n",
      "Training iter #23571000:   Batch Loss = 7.370540, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.717822551727295, Accuracy = 0.8954734802246094\n",
      "Training iter #23574000:   Batch Loss = 7.381551, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.718492031097412, Accuracy = 0.8951756954193115\n",
      "Training iter #23577000:   Batch Loss = 7.467311, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719021797180176, Accuracy = 0.8951756954193115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #23580000:   Batch Loss = 7.334377, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719257831573486, Accuracy = 0.8951756954193115\n",
      "Training iter #23583000:   Batch Loss = 7.320167, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719367027282715, Accuracy = 0.8954734802246094\n",
      "Training iter #23586000:   Batch Loss = 7.413474, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719761371612549, Accuracy = 0.8948779106140137\n",
      "Training iter #23589000:   Batch Loss = 7.420937, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7199273109436035, Accuracy = 0.8942822813987732\n",
      "Training iter #23592000:   Batch Loss = 7.398581, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719812393188477, Accuracy = 0.8939844965934753\n",
      "Training iter #23595000:   Batch Loss = 7.325243, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719770431518555, Accuracy = 0.8933889269828796\n",
      "Training iter #23598000:   Batch Loss = 7.370790, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7195048332214355, Accuracy = 0.8933889269828796\n",
      "Training iter #23601000:   Batch Loss = 7.392769, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719534873962402, Accuracy = 0.8942822813987732\n",
      "Training iter #23604000:   Batch Loss = 7.448534, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719605922698975, Accuracy = 0.8939844965934753\n",
      "Training iter #23607000:   Batch Loss = 7.328637, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719484329223633, Accuracy = 0.8936867117881775\n",
      "Training iter #23610000:   Batch Loss = 7.322719, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719417572021484, Accuracy = 0.8939844965934753\n",
      "Training iter #23613000:   Batch Loss = 7.405799, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719135761260986, Accuracy = 0.8936867117881775\n",
      "Training iter #23616000:   Batch Loss = 7.417066, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.719183921813965, Accuracy = 0.8945801258087158\n",
      "Training iter #23619000:   Batch Loss = 7.394970, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.718929767608643, Accuracy = 0.8954734802246094\n",
      "Training iter #23622000:   Batch Loss = 7.326246, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.71854829788208, Accuracy = 0.8948779106140137\n",
      "Training iter #23625000:   Batch Loss = 7.366642, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.718185901641846, Accuracy = 0.8945801258087158\n",
      "Training iter #23628000:   Batch Loss = 7.394945, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.717983245849609, Accuracy = 0.8948779106140137\n",
      "Training iter #23631000:   Batch Loss = 7.438329, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.716429710388184, Accuracy = 0.8945801258087158\n",
      "Training iter #23634000:   Batch Loss = 7.318479, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7163801193237305, Accuracy = 0.8942822813987732\n",
      "Training iter #23637000:   Batch Loss = 7.319334, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.717127799987793, Accuracy = 0.8945801258087158\n",
      "Training iter #23640000:   Batch Loss = 7.388460, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.717303276062012, Accuracy = 0.8942822813987732\n",
      "Training iter #23643000:   Batch Loss = 7.430335, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7169060707092285, Accuracy = 0.8936867117881775\n",
      "Training iter #23646000:   Batch Loss = 7.402315, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.716493129730225, Accuracy = 0.8939844965934753\n",
      "Training iter #23649000:   Batch Loss = 7.335433, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.716278553009033, Accuracy = 0.8936867117881775\n",
      "Training iter #23652000:   Batch Loss = 7.359748, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.715979099273682, Accuracy = 0.8936867117881775\n",
      "Training iter #23655000:   Batch Loss = 7.387601, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.715447902679443, Accuracy = 0.8939844965934753\n",
      "Training iter #23658000:   Batch Loss = 7.419697, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7154998779296875, Accuracy = 0.8942822813987732\n",
      "Training iter #23661000:   Batch Loss = 7.309701, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7156524658203125, Accuracy = 0.8936867117881775\n",
      "Training iter #23664000:   Batch Loss = 7.330513, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7154974937438965, Accuracy = 0.8936867117881775\n",
      "Training iter #23667000:   Batch Loss = 7.386307, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.715271949768066, Accuracy = 0.8933889269828796\n",
      "Training iter #23670000:   Batch Loss = 7.428737, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7150983810424805, Accuracy = 0.8936867117881775\n",
      "Training iter #23673000:   Batch Loss = 7.395908, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.714969158172607, Accuracy = 0.8936867117881775\n",
      "Training iter #23676000:   Batch Loss = 7.336833, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.714557647705078, Accuracy = 0.8939844965934753\n",
      "Training iter #23679000:   Batch Loss = 7.357672, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.714364051818848, Accuracy = 0.8939844965934753\n",
      "Training iter #23682000:   Batch Loss = 7.389240, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.714132785797119, Accuracy = 0.8936867117881775\n",
      "Training iter #23685000:   Batch Loss = 7.426650, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.713987350463867, Accuracy = 0.8939844965934753\n",
      "Training iter #23688000:   Batch Loss = 7.310237, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.713723182678223, Accuracy = 0.8939844965934753\n",
      "Training iter #23691000:   Batch Loss = 7.334085, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.713162422180176, Accuracy = 0.8942822813987732\n",
      "Training iter #23694000:   Batch Loss = 7.369831, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.712642669677734, Accuracy = 0.8942822813987732\n",
      "Training iter #23697000:   Batch Loss = 7.441552, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7124457359313965, Accuracy = 0.8945801258087158\n",
      "Training iter #23700000:   Batch Loss = 7.375380, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.71239709854126, Accuracy = 0.8945801258087158\n",
      "Training iter #23703000:   Batch Loss = 7.329983, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.712106227874756, Accuracy = 0.8945801258087158\n",
      "Training iter #23706000:   Batch Loss = 7.379611, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.711989402770996, Accuracy = 0.8942822813987732\n",
      "Training iter #23709000:   Batch Loss = 7.391705, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.711942672729492, Accuracy = 0.8939844965934753\n",
      "Training iter #23712000:   Batch Loss = 7.421460, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7118916511535645, Accuracy = 0.8939844965934753\n",
      "Training iter #23715000:   Batch Loss = 7.306778, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.711696147918701, Accuracy = 0.8939844965934753\n",
      "Training iter #23718000:   Batch Loss = 7.329834, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.71129035949707, Accuracy = 0.8942822813987732\n",
      "Training iter #23721000:   Batch Loss = 7.364861, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.710864543914795, Accuracy = 0.8942822813987732\n",
      "Training iter #23724000:   Batch Loss = 7.437305, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.710754871368408, Accuracy = 0.8942822813987732\n",
      "Training iter #23727000:   Batch Loss = 7.363513, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.710666656494141, Accuracy = 0.8939844965934753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #23730000:   Batch Loss = 7.322052, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.710413932800293, Accuracy = 0.8939844965934753\n",
      "Training iter #23733000:   Batch Loss = 7.380110, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7101216316223145, Accuracy = 0.8939844965934753\n",
      "Training iter #23736000:   Batch Loss = 7.394658, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.709936141967773, Accuracy = 0.8939844965934753\n",
      "Training iter #23739000:   Batch Loss = 7.415167, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.709593296051025, Accuracy = 0.8939844965934753\n",
      "Training iter #23742000:   Batch Loss = 7.312729, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.70919942855835, Accuracy = 0.8945801258087158\n",
      "Training iter #23745000:   Batch Loss = 7.332408, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.709005832672119, Accuracy = 0.8945801258087158\n",
      "Training iter #23748000:   Batch Loss = 7.360996, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.708991050720215, Accuracy = 0.8945801258087158\n",
      "Training iter #23751000:   Batch Loss = 7.440159, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.709001541137695, Accuracy = 0.8942822813987732\n",
      "Training iter #23754000:   Batch Loss = 7.356368, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.708909034729004, Accuracy = 0.8942822813987732\n",
      "Training iter #23757000:   Batch Loss = 7.319659, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.708845138549805, Accuracy = 0.8942822813987732\n",
      "Training iter #23760000:   Batch Loss = 7.384993, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.708635330200195, Accuracy = 0.8942822813987732\n",
      "Training iter #23763000:   Batch Loss = 7.394667, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.708619117736816, Accuracy = 0.8942822813987732\n",
      "Training iter #23766000:   Batch Loss = 7.406118, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.708500862121582, Accuracy = 0.8942822813987732\n",
      "Training iter #23769000:   Batch Loss = 7.312434, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.708263397216797, Accuracy = 0.8945801258087158\n",
      "Training iter #23772000:   Batch Loss = 7.345766, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.707950115203857, Accuracy = 0.8945801258087158\n",
      "Training iter #23775000:   Batch Loss = 7.361576, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7078447341918945, Accuracy = 0.8945801258087158\n",
      "Training iter #23778000:   Batch Loss = 7.444827, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.707810401916504, Accuracy = 0.8948779106140137\n",
      "Training iter #23781000:   Batch Loss = 7.352230, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.707667350769043, Accuracy = 0.8942822813987732\n",
      "Training iter #23784000:   Batch Loss = 7.321354, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.707326412200928, Accuracy = 0.8942822813987732\n",
      "Training iter #23787000:   Batch Loss = 7.366456, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.707155704498291, Accuracy = 0.8942822813987732\n",
      "Training iter #23790000:   Batch Loss = 7.403189, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.707040786743164, Accuracy = 0.8942822813987732\n",
      "Training iter #23793000:   Batch Loss = 7.392050, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.706736087799072, Accuracy = 0.8939844965934753\n",
      "Training iter #23796000:   Batch Loss = 7.312781, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.70641565322876, Accuracy = 0.8945801258087158\n",
      "Training iter #23799000:   Batch Loss = 7.348150, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.706257343292236, Accuracy = 0.8945801258087158\n",
      "Training iter #23802000:   Batch Loss = 7.369371, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.70622444152832, Accuracy = 0.8942822813987732\n",
      "Training iter #23805000:   Batch Loss = 7.444083, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.70590877532959, Accuracy = 0.8945801258087158\n",
      "Training iter #23808000:   Batch Loss = 7.348112, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7060627937316895, Accuracy = 0.8939844965934753\n",
      "Training iter #23811000:   Batch Loss = 7.312827, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.706089973449707, Accuracy = 0.8936867117881775\n",
      "Training iter #23814000:   Batch Loss = 7.365170, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.705695152282715, Accuracy = 0.8939844965934753\n",
      "Training iter #23817000:   Batch Loss = 7.398685, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.70526123046875, Accuracy = 0.8951756954193115\n",
      "Training iter #23820000:   Batch Loss = 7.386651, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.705166339874268, Accuracy = 0.8945801258087158\n",
      "Training iter #23823000:   Batch Loss = 7.303962, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.704973220825195, Accuracy = 0.8951756954193115\n",
      "Training iter #23826000:   Batch Loss = 7.349727, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.70464563369751, Accuracy = 0.8954734802246094\n",
      "Training iter #23829000:   Batch Loss = 7.372044, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.704451560974121, Accuracy = 0.8954734802246094\n",
      "Training iter #23832000:   Batch Loss = 7.445907, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.704319953918457, Accuracy = 0.8957712650299072\n",
      "Training iter #23835000:   Batch Loss = 7.337645, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.704105377197266, Accuracy = 0.8948779106140137\n",
      "Training iter #23838000:   Batch Loss = 7.311453, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.704090118408203, Accuracy = 0.8945801258087158\n",
      "Training iter #23841000:   Batch Loss = 7.382171, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.7038140296936035, Accuracy = 0.8945801258087158\n",
      "Training iter #23844000:   Batch Loss = 7.399004, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.703738212585449, Accuracy = 0.8945801258087158\n",
      "Training iter #23847000:   Batch Loss = 7.375060, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.703601837158203, Accuracy = 0.8942822813987732\n",
      "Training iter #23850000:   Batch Loss = 7.304248, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.703171730041504, Accuracy = 0.8951756954193115\n",
      "Training iter #23853000:   Batch Loss = 7.349583, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.702972412109375, Accuracy = 0.8948779106140137\n",
      "Training iter #23856000:   Batch Loss = 7.362740, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.702871322631836, Accuracy = 0.8948779106140137\n",
      "Training iter #23859000:   Batch Loss = 7.444323, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.702765464782715, Accuracy = 0.8951756954193115\n",
      "Training iter #23862000:   Batch Loss = 7.329271, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.702678203582764, Accuracy = 0.8951756954193115\n",
      "Training iter #23865000:   Batch Loss = 7.307994, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.702766418457031, Accuracy = 0.8945801258087158\n",
      "Training iter #23868000:   Batch Loss = 7.380862, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.702094078063965, Accuracy = 0.8948779106140137\n",
      "Training iter #23871000:   Batch Loss = 7.390617, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.701971530914307, Accuracy = 0.8948779106140137\n",
      "Training iter #23874000:   Batch Loss = 7.380471, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.701843738555908, Accuracy = 0.8948779106140137\n",
      "Training iter #23877000:   Batch Loss = 7.307786, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.701649188995361, Accuracy = 0.8948779106140137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #23880000:   Batch Loss = 7.348173, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.701383113861084, Accuracy = 0.8954734802246094\n",
      "Training iter #23883000:   Batch Loss = 7.360704, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.701259136199951, Accuracy = 0.8954734802246094\n",
      "Training iter #23886000:   Batch Loss = 7.448209, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.701376438140869, Accuracy = 0.8951756954193115\n",
      "Training iter #23889000:   Batch Loss = 7.325871, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.701131820678711, Accuracy = 0.8948779106140137\n",
      "Training iter #23892000:   Batch Loss = 7.299404, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.700502872467041, Accuracy = 0.8951756954193115\n",
      "Training iter #23895000:   Batch Loss = 7.384859, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.700292587280273, Accuracy = 0.8945801258087158\n",
      "Training iter #23898000:   Batch Loss = 7.395542, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.700232028961182, Accuracy = 0.8945801258087158\n",
      "Training iter #23901000:   Batch Loss = 7.378957, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.700041770935059, Accuracy = 0.8948779106140137\n",
      "Training iter #23904000:   Batch Loss = 7.305204, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.699760913848877, Accuracy = 0.8945801258087158\n",
      "Training iter #23907000:   Batch Loss = 7.347695, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6993842124938965, Accuracy = 0.8945801258087158\n",
      "Training iter #23910000:   Batch Loss = 7.357153, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6991167068481445, Accuracy = 0.8942822813987732\n",
      "Training iter #23913000:   Batch Loss = 7.438313, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.699419021606445, Accuracy = 0.8945801258087158\n",
      "Training iter #23916000:   Batch Loss = 7.312501, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.699458599090576, Accuracy = 0.8954734802246094\n",
      "Training iter #23919000:   Batch Loss = 7.299146, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.699126720428467, Accuracy = 0.8954734802246094\n",
      "Training iter #23922000:   Batch Loss = 7.391568, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.699362754821777, Accuracy = 0.8954734802246094\n",
      "Training iter #23925000:   Batch Loss = 7.397439, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.699166774749756, Accuracy = 0.8951756954193115\n",
      "Training iter #23928000:   Batch Loss = 7.374577, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.698885917663574, Accuracy = 0.8945801258087158\n",
      "Training iter #23931000:   Batch Loss = 7.313344, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.698479652404785, Accuracy = 0.8945801258087158\n",
      "Training iter #23934000:   Batch Loss = 7.345156, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.698153495788574, Accuracy = 0.8948779106140137\n",
      "Training iter #23937000:   Batch Loss = 7.375738, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.698145866394043, Accuracy = 0.8948779106140137\n",
      "Training iter #23940000:   Batch Loss = 7.423537, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.697951316833496, Accuracy = 0.8951756954193115\n",
      "Training iter #23943000:   Batch Loss = 7.308394, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.697690963745117, Accuracy = 0.8951756954193115\n",
      "Training iter #23946000:   Batch Loss = 7.303562, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6971893310546875, Accuracy = 0.8945801258087158\n",
      "Training iter #23949000:   Batch Loss = 7.385677, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.697061061859131, Accuracy = 0.8942822813987732\n",
      "Training iter #23952000:   Batch Loss = 7.399495, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.697013854980469, Accuracy = 0.8945801258087158\n",
      "Training iter #23955000:   Batch Loss = 7.381200, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.696689128875732, Accuracy = 0.8945801258087158\n",
      "Training iter #23958000:   Batch Loss = 7.322507, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.696234703063965, Accuracy = 0.8948779106140137\n",
      "Training iter #23961000:   Batch Loss = 7.349157, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.695714473724365, Accuracy = 0.8951756954193115\n",
      "Training iter #23964000:   Batch Loss = 7.374313, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.69550085067749, Accuracy = 0.8954734802246094\n",
      "Training iter #23967000:   Batch Loss = 7.403584, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.695798873901367, Accuracy = 0.8951756954193115\n",
      "Training iter #23970000:   Batch Loss = 7.295575, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.695662975311279, Accuracy = 0.8951756954193115\n",
      "Training iter #23973000:   Batch Loss = 7.310596, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.695445537567139, Accuracy = 0.8954734802246094\n",
      "Training iter #23976000:   Batch Loss = 7.374499, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.695391654968262, Accuracy = 0.8948779106140137\n",
      "Training iter #23979000:   Batch Loss = 7.410730, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.695303440093994, Accuracy = 0.8948779106140137\n",
      "Training iter #23982000:   Batch Loss = 7.384130, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.695262908935547, Accuracy = 0.8948779106140137\n",
      "Training iter #23985000:   Batch Loss = 7.319340, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.69498348236084, Accuracy = 0.8948779106140137\n",
      "Training iter #23988000:   Batch Loss = 7.344095, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.694423198699951, Accuracy = 0.8948779106140137\n",
      "Training iter #23991000:   Batch Loss = 7.369281, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.694208145141602, Accuracy = 0.8948779106140137\n",
      "Training iter #23994000:   Batch Loss = 7.402062, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.694218635559082, Accuracy = 0.8948779106140137\n",
      "Training iter #23997000:   Batch Loss = 7.289591, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6941351890563965, Accuracy = 0.8951756954193115\n",
      "Training iter #24000000:   Batch Loss = 7.315534, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6939592361450195, Accuracy = 0.8951756954193115\n",
      "Training iter #24003000:   Batch Loss = 7.368984, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693844795227051, Accuracy = 0.8951756954193115\n",
      "Training iter #24006000:   Batch Loss = 7.409513, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693774223327637, Accuracy = 0.8951756954193115\n",
      "Training iter #24009000:   Batch Loss = 7.368474, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693637847900391, Accuracy = 0.8951756954193115\n",
      "Training iter #24012000:   Batch Loss = 7.318507, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693360805511475, Accuracy = 0.8948779106140137\n",
      "Training iter #24015000:   Batch Loss = 7.356869, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693294048309326, Accuracy = 0.8939844965934753\n",
      "Training iter #24018000:   Batch Loss = 7.367282, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693172931671143, Accuracy = 0.8942822813987732\n",
      "Training iter #24021000:   Batch Loss = 7.396398, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693233489990234, Accuracy = 0.8945801258087158\n",
      "Training iter #24024000:   Batch Loss = 7.297391, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.693017959594727, Accuracy = 0.8951756954193115\n",
      "Training iter #24027000:   Batch Loss = 7.314771, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.692887783050537, Accuracy = 0.8948779106140137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24030000:   Batch Loss = 7.357925, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.692878723144531, Accuracy = 0.8945801258087158\n",
      "Training iter #24033000:   Batch Loss = 7.414184, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.692717552185059, Accuracy = 0.8942822813987732\n",
      "Training iter #24036000:   Batch Loss = 7.348255, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.692415714263916, Accuracy = 0.8948779106140137\n",
      "Training iter #24039000:   Batch Loss = 7.314847, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.692016124725342, Accuracy = 0.8948779106140137\n",
      "Training iter #24042000:   Batch Loss = 7.365897, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.691553115844727, Accuracy = 0.8948779106140137\n",
      "Training iter #24045000:   Batch Loss = 7.376463, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.691311359405518, Accuracy = 0.8945801258087158\n",
      "Training iter #24048000:   Batch Loss = 7.398407, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.691129684448242, Accuracy = 0.8945801258087158\n",
      "Training iter #24051000:   Batch Loss = 7.294981, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.690920829772949, Accuracy = 0.8942822813987732\n",
      "Training iter #24054000:   Batch Loss = 7.315630, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.690714359283447, Accuracy = 0.8945801258087158\n",
      "Training iter #24057000:   Batch Loss = 7.349801, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.690540790557861, Accuracy = 0.8942822813987732\n",
      "Training iter #24060000:   Batch Loss = 7.415996, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6901631355285645, Accuracy = 0.8939844965934753\n",
      "Training iter #24063000:   Batch Loss = 7.341524, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.689934253692627, Accuracy = 0.8939844965934753\n",
      "Training iter #24066000:   Batch Loss = 7.306377, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.689597129821777, Accuracy = 0.8942822813987732\n",
      "Training iter #24069000:   Batch Loss = 7.366227, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.689245700836182, Accuracy = 0.8945801258087158\n",
      "Training iter #24072000:   Batch Loss = 7.372430, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6891326904296875, Accuracy = 0.8942822813987732\n",
      "Training iter #24075000:   Batch Loss = 7.390924, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.689050674438477, Accuracy = 0.8942822813987732\n",
      "Training iter #24078000:   Batch Loss = 7.294446, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6889543533325195, Accuracy = 0.8948779106140137\n",
      "Training iter #24081000:   Batch Loss = 7.317058, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.689071178436279, Accuracy = 0.8948779106140137\n",
      "Training iter #24084000:   Batch Loss = 7.341918, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6895928382873535, Accuracy = 0.8951756954193115\n",
      "Training iter #24087000:   Batch Loss = 7.422741, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.689570426940918, Accuracy = 0.8948779106140137\n",
      "Training iter #24090000:   Batch Loss = 7.336514, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.689244747161865, Accuracy = 0.8948779106140137\n",
      "Training iter #24093000:   Batch Loss = 7.309885, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6889190673828125, Accuracy = 0.8945801258087158\n",
      "Training iter #24096000:   Batch Loss = 7.350097, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.688317775726318, Accuracy = 0.8942822813987732\n",
      "Training iter #24099000:   Batch Loss = 7.377166, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.687917709350586, Accuracy = 0.8945801258087158\n",
      "Training iter #24102000:   Batch Loss = 7.379176, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.687441349029541, Accuracy = 0.8945801258087158\n",
      "Training iter #24105000:   Batch Loss = 7.291105, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.687262535095215, Accuracy = 0.8945801258087158\n",
      "Training iter #24108000:   Batch Loss = 7.332175, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.687580108642578, Accuracy = 0.8945801258087158\n",
      "Training iter #24111000:   Batch Loss = 7.345165, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6875901222229, Accuracy = 0.8942822813987732\n",
      "Training iter #24114000:   Batch Loss = 7.409004, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.687501430511475, Accuracy = 0.8942822813987732\n",
      "Training iter #24117000:   Batch Loss = 7.328460, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.687097549438477, Accuracy = 0.8945801258087158\n",
      "Training iter #24120000:   Batch Loss = 7.296810, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.686856269836426, Accuracy = 0.8951756954193115\n",
      "Training iter #24123000:   Batch Loss = 7.347718, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.686577320098877, Accuracy = 0.8948779106140137\n",
      "Training iter #24126000:   Batch Loss = 7.383523, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.686374664306641, Accuracy = 0.8951756954193115\n",
      "Training iter #24129000:   Batch Loss = 7.358368, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.686312675476074, Accuracy = 0.8951756954193115\n",
      "Training iter #24132000:   Batch Loss = 7.292480, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.686257839202881, Accuracy = 0.8945801258087158\n",
      "Training iter #24135000:   Batch Loss = 7.335024, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.685960292816162, Accuracy = 0.8951756954193115\n",
      "Training iter #24138000:   Batch Loss = 7.355900, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6857452392578125, Accuracy = 0.8948779106140137\n",
      "Training iter #24141000:   Batch Loss = 7.420791, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.685414791107178, Accuracy = 0.8951756954193115\n",
      "Training iter #24144000:   Batch Loss = 7.324728, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.685046195983887, Accuracy = 0.8948779106140137\n",
      "Training iter #24147000:   Batch Loss = 7.293960, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684741973876953, Accuracy = 0.8948779106140137\n",
      "Training iter #24150000:   Batch Loss = 7.348342, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684495449066162, Accuracy = 0.8948779106140137\n",
      "Training iter #24153000:   Batch Loss = 7.380552, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684452056884766, Accuracy = 0.8948779106140137\n",
      "Training iter #24156000:   Batch Loss = 7.360815, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.68447208404541, Accuracy = 0.8951756954193115\n",
      "Training iter #24159000:   Batch Loss = 7.280962, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684443950653076, Accuracy = 0.8954734802246094\n",
      "Training iter #24162000:   Batch Loss = 7.331699, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684380531311035, Accuracy = 0.8957712650299072\n",
      "Training iter #24165000:   Batch Loss = 7.338514, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684408664703369, Accuracy = 0.8954734802246094\n",
      "Training iter #24168000:   Batch Loss = 7.426530, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684329509735107, Accuracy = 0.8954734802246094\n",
      "Training iter #24171000:   Batch Loss = 7.320231, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.684057235717773, Accuracy = 0.8957712650299072\n",
      "Training iter #24174000:   Batch Loss = 7.292745, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6836724281311035, Accuracy = 0.8954734802246094\n",
      "Training iter #24177000:   Batch Loss = 7.362319, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6834001541137695, Accuracy = 0.8954734802246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24180000:   Batch Loss = 7.375000, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.683465957641602, Accuracy = 0.8948779106140137\n",
      "Training iter #24183000:   Batch Loss = 7.355690, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6837263107299805, Accuracy = 0.8942822813987732\n",
      "Training iter #24186000:   Batch Loss = 7.285323, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6839213371276855, Accuracy = 0.8942822813987732\n",
      "Training iter #24189000:   Batch Loss = 7.332763, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.683752059936523, Accuracy = 0.8951756954193115\n",
      "Training iter #24192000:   Batch Loss = 7.342284, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.683720588684082, Accuracy = 0.8948779106140137\n",
      "Training iter #24195000:   Batch Loss = 7.420227, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6836347579956055, Accuracy = 0.8948779106140137\n",
      "Training iter #24198000:   Batch Loss = 7.307771, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.683408737182617, Accuracy = 0.8945801258087158\n",
      "Training iter #24201000:   Batch Loss = 7.289013, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.683186054229736, Accuracy = 0.8945801258087158\n",
      "Training iter #24204000:   Batch Loss = 7.361571, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6826934814453125, Accuracy = 0.8951756954193115\n",
      "Training iter #24207000:   Batch Loss = 7.368790, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.682276725769043, Accuracy = 0.8948779106140137\n",
      "Training iter #24210000:   Batch Loss = 7.363424, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.682106971740723, Accuracy = 0.8951756954193115\n",
      "Training iter #24213000:   Batch Loss = 7.282190, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.68209171295166, Accuracy = 0.8954734802246094\n",
      "Training iter #24216000:   Batch Loss = 7.330954, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.681992530822754, Accuracy = 0.8954734802246094\n",
      "Training iter #24219000:   Batch Loss = 7.342079, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6820454597473145, Accuracy = 0.8951756954193115\n",
      "Training iter #24222000:   Batch Loss = 7.422431, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6817946434021, Accuracy = 0.8951756954193115\n",
      "Training iter #24225000:   Batch Loss = 7.291532, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.681427478790283, Accuracy = 0.8951756954193115\n",
      "Training iter #24228000:   Batch Loss = 7.282634, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.681118965148926, Accuracy = 0.8954734802246094\n",
      "Training iter #24231000:   Batch Loss = 7.372633, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.681032657623291, Accuracy = 0.8954734802246094\n",
      "Training iter #24234000:   Batch Loss = 7.378273, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6808953285217285, Accuracy = 0.8954734802246094\n",
      "Training iter #24237000:   Batch Loss = 7.358223, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.681004524230957, Accuracy = 0.8954734802246094\n",
      "Training iter #24240000:   Batch Loss = 7.289062, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.681015491485596, Accuracy = 0.8954734802246094\n",
      "Training iter #24243000:   Batch Loss = 7.326901, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.680967807769775, Accuracy = 0.8954734802246094\n",
      "Training iter #24246000:   Batch Loss = 7.350218, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.681081771850586, Accuracy = 0.8957712650299072\n",
      "Training iter #24249000:   Batch Loss = 7.403919, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.680954456329346, Accuracy = 0.8954734802246094\n",
      "Training iter #24252000:   Batch Loss = 7.285851, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.680650234222412, Accuracy = 0.8951756954193115\n",
      "Training iter #24255000:   Batch Loss = 7.285387, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.680301189422607, Accuracy = 0.8951756954193115\n",
      "Training iter #24258000:   Batch Loss = 7.365693, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.680053234100342, Accuracy = 0.8951756954193115\n",
      "Training iter #24261000:   Batch Loss = 7.374767, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.679896354675293, Accuracy = 0.8954734802246094\n",
      "Training iter #24264000:   Batch Loss = 7.351890, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.679816722869873, Accuracy = 0.8954734802246094\n",
      "Training iter #24267000:   Batch Loss = 7.288135, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.679599285125732, Accuracy = 0.8954734802246094\n",
      "Training iter #24270000:   Batch Loss = 7.325386, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6796064376831055, Accuracy = 0.8957712650299072\n",
      "Training iter #24273000:   Batch Loss = 7.355396, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.679843902587891, Accuracy = 0.8957712650299072\n",
      "Training iter #24276000:   Batch Loss = 7.395212, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.679696559906006, Accuracy = 0.8957712650299072\n",
      "Training iter #24279000:   Batch Loss = 7.276663, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6795806884765625, Accuracy = 0.8957712650299072\n",
      "Training iter #24282000:   Batch Loss = 7.287043, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6794304847717285, Accuracy = 0.8954734802246094\n",
      "Training iter #24285000:   Batch Loss = 7.354626, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.679296493530273, Accuracy = 0.8954734802246094\n",
      "Training iter #24288000:   Batch Loss = 7.387127, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.679126739501953, Accuracy = 0.8954734802246094\n",
      "Training iter #24291000:   Batch Loss = 7.358056, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.678982257843018, Accuracy = 0.8954734802246094\n",
      "Training iter #24294000:   Batch Loss = 7.298424, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.678742408752441, Accuracy = 0.8957712650299072\n",
      "Training iter #24297000:   Batch Loss = 7.328399, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.678524017333984, Accuracy = 0.8957712650299072\n",
      "Training iter #24300000:   Batch Loss = 7.351580, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677919387817383, Accuracy = 0.8954734802246094\n",
      "Training iter #24303000:   Batch Loss = 7.377450, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677468776702881, Accuracy = 0.8954734802246094\n",
      "Training iter #24306000:   Batch Loss = 7.267464, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676929950714111, Accuracy = 0.8957712650299072\n",
      "Training iter #24309000:   Batch Loss = 7.293668, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676288604736328, Accuracy = 0.8957712650299072\n",
      "Training iter #24312000:   Batch Loss = 7.344185, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677093982696533, Accuracy = 0.8954734802246094\n",
      "Training iter #24315000:   Batch Loss = 7.385715, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6772589683532715, Accuracy = 0.8954734802246094\n",
      "Training iter #24318000:   Batch Loss = 7.357045, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677202224731445, Accuracy = 0.8954734802246094\n",
      "Training iter #24321000:   Batch Loss = 7.299875, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677201747894287, Accuracy = 0.8957712650299072\n",
      "Training iter #24324000:   Batch Loss = 7.322681, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677274227142334, Accuracy = 0.8960691094398499\n",
      "Training iter #24327000:   Batch Loss = 7.346725, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677330493927002, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24330000:   Batch Loss = 7.381287, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677222728729248, Accuracy = 0.8960691094398499\n",
      "Training iter #24333000:   Batch Loss = 7.267262, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677145481109619, Accuracy = 0.8960691094398499\n",
      "Training iter #24336000:   Batch Loss = 7.297554, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677083969116211, Accuracy = 0.8960691094398499\n",
      "Training iter #24339000:   Batch Loss = 7.341123, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.677066802978516, Accuracy = 0.8957712650299072\n",
      "Training iter #24342000:   Batch Loss = 7.399910, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676912307739258, Accuracy = 0.8957712650299072\n",
      "Training iter #24345000:   Batch Loss = 7.336899, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676655292510986, Accuracy = 0.8951756954193115\n",
      "Training iter #24348000:   Batch Loss = 7.291797, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676431179046631, Accuracy = 0.8951756954193115\n",
      "Training iter #24351000:   Batch Loss = 7.337091, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676663875579834, Accuracy = 0.8951756954193115\n",
      "Training iter #24354000:   Batch Loss = 7.348973, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6767754554748535, Accuracy = 0.8951756954193115\n",
      "Training iter #24357000:   Batch Loss = 7.378385, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676549434661865, Accuracy = 0.8951756954193115\n",
      "Training iter #24360000:   Batch Loss = 7.270362, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676449298858643, Accuracy = 0.8954734802246094\n",
      "Training iter #24363000:   Batch Loss = 7.295335, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676608562469482, Accuracy = 0.8951756954193115\n",
      "Training iter #24366000:   Batch Loss = 7.328463, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676656246185303, Accuracy = 0.8954734802246094\n",
      "Training iter #24369000:   Batch Loss = 7.391857, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.676499366760254, Accuracy = 0.8954734802246094\n",
      "Training iter #24372000:   Batch Loss = 7.325065, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.675869464874268, Accuracy = 0.8954734802246094\n",
      "Training iter #24375000:   Batch Loss = 7.285368, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.675319194793701, Accuracy = 0.8951756954193115\n",
      "Training iter #24378000:   Batch Loss = 7.342490, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.675008296966553, Accuracy = 0.8948779106140137\n",
      "Training iter #24381000:   Batch Loss = 7.354189, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.674844264984131, Accuracy = 0.8948779106140137\n",
      "Training iter #24384000:   Batch Loss = 7.373463, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.675078392028809, Accuracy = 0.8948779106140137\n",
      "Training iter #24387000:   Batch Loss = 7.269795, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6751604080200195, Accuracy = 0.8939844965934753\n",
      "Training iter #24390000:   Batch Loss = 7.296350, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6752190589904785, Accuracy = 0.8939844965934753\n",
      "Training iter #24393000:   Batch Loss = 7.327657, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6753716468811035, Accuracy = 0.8939844965934753\n",
      "Training iter #24396000:   Batch Loss = 7.397791, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.67519474029541, Accuracy = 0.8936867117881775\n",
      "Training iter #24399000:   Batch Loss = 7.318367, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.675045967102051, Accuracy = 0.8939844965934753\n",
      "Training iter #24402000:   Batch Loss = 7.283439, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.674861431121826, Accuracy = 0.8939844965934753\n",
      "Training iter #24405000:   Batch Loss = 7.345748, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.674554347991943, Accuracy = 0.8936867117881775\n",
      "Training iter #24408000:   Batch Loss = 7.352034, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.674208164215088, Accuracy = 0.8936867117881775\n",
      "Training iter #24411000:   Batch Loss = 7.364432, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.673795700073242, Accuracy = 0.8939844965934753\n",
      "Training iter #24414000:   Batch Loss = 7.271268, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.673506259918213, Accuracy = 0.8945801258087158\n",
      "Training iter #24417000:   Batch Loss = 7.295685, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.672873020172119, Accuracy = 0.8942822813987732\n",
      "Training iter #24420000:   Batch Loss = 7.322517, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.672268867492676, Accuracy = 0.8948779106140137\n",
      "Training iter #24423000:   Batch Loss = 7.397463, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.671911716461182, Accuracy = 0.8948779106140137\n",
      "Training iter #24426000:   Batch Loss = 7.313496, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.67133092880249, Accuracy = 0.8942822813987732\n",
      "Training iter #24429000:   Batch Loss = 7.284693, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.670917510986328, Accuracy = 0.8945801258087158\n",
      "Training iter #24432000:   Batch Loss = 7.328728, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.670596599578857, Accuracy = 0.8939844965934753\n",
      "Training iter #24435000:   Batch Loss = 7.368268, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.670350551605225, Accuracy = 0.8939844965934753\n",
      "Training iter #24438000:   Batch Loss = 7.355216, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.670255184173584, Accuracy = 0.8936867117881775\n",
      "Training iter #24441000:   Batch Loss = 7.272768, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.670019149780273, Accuracy = 0.8936867117881775\n",
      "Training iter #24444000:   Batch Loss = 7.312477, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.669928073883057, Accuracy = 0.8936867117881775\n",
      "Training iter #24447000:   Batch Loss = 7.333052, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.669953346252441, Accuracy = 0.8939844965934753\n",
      "Training iter #24450000:   Batch Loss = 7.396777, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.669810771942139, Accuracy = 0.8939844965934753\n",
      "Training iter #24453000:   Batch Loss = 7.310167, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.669520854949951, Accuracy = 0.8939844965934753\n",
      "Training iter #24456000:   Batch Loss = 7.276658, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.669333457946777, Accuracy = 0.8939844965934753\n",
      "Training iter #24459000:   Batch Loss = 7.326293, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.668898105621338, Accuracy = 0.8942822813987732\n",
      "Training iter #24462000:   Batch Loss = 7.357184, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.668686866760254, Accuracy = 0.8945801258087158\n",
      "Training iter #24465000:   Batch Loss = 7.345279, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.668577671051025, Accuracy = 0.8945801258087158\n",
      "Training iter #24468000:   Batch Loss = 7.262740, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6685791015625, Accuracy = 0.8942822813987732\n",
      "Training iter #24471000:   Batch Loss = 7.313795, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.668479919433594, Accuracy = 0.8945801258087158\n",
      "Training iter #24474000:   Batch Loss = 7.333608, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.668331623077393, Accuracy = 0.8945801258087158\n",
      "Training iter #24477000:   Batch Loss = 7.396470, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.667991638183594, Accuracy = 0.8945801258087158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24480000:   Batch Loss = 7.299907, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.667932987213135, Accuracy = 0.8942822813987732\n",
      "Training iter #24483000:   Batch Loss = 7.276355, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.667902946472168, Accuracy = 0.8942822813987732\n",
      "Training iter #24486000:   Batch Loss = 7.342718, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.667830467224121, Accuracy = 0.8942822813987732\n",
      "Training iter #24489000:   Batch Loss = 7.358749, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.667686462402344, Accuracy = 0.8942822813987732\n",
      "Training iter #24492000:   Batch Loss = 7.335899, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.667382717132568, Accuracy = 0.8942822813987732\n",
      "Training iter #24495000:   Batch Loss = 7.261228, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.667098045349121, Accuracy = 0.8945801258087158\n",
      "Training iter #24498000:   Batch Loss = 7.312379, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.666896820068359, Accuracy = 0.8945801258087158\n",
      "Training iter #24501000:   Batch Loss = 7.319841, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.666803359985352, Accuracy = 0.8945801258087158\n",
      "Training iter #24504000:   Batch Loss = 7.403936, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.666701793670654, Accuracy = 0.8942822813987732\n",
      "Training iter #24507000:   Batch Loss = 7.299155, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.666712760925293, Accuracy = 0.8942822813987732\n",
      "Training iter #24510000:   Batch Loss = 7.273413, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.666697025299072, Accuracy = 0.8942822813987732\n",
      "Training iter #24513000:   Batch Loss = 7.342041, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6664276123046875, Accuracy = 0.8942822813987732\n",
      "Training iter #24516000:   Batch Loss = 7.350671, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.666059494018555, Accuracy = 0.8942822813987732\n",
      "Training iter #24519000:   Batch Loss = 7.343464, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6657185554504395, Accuracy = 0.8942822813987732\n",
      "Training iter #24522000:   Batch Loss = 7.263275, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.665486812591553, Accuracy = 0.8945801258087158\n",
      "Training iter #24525000:   Batch Loss = 7.310544, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664979457855225, Accuracy = 0.8948779106140137\n",
      "Training iter #24528000:   Batch Loss = 7.325576, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664569854736328, Accuracy = 0.8951756954193115\n",
      "Training iter #24531000:   Batch Loss = 7.400353, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664676189422607, Accuracy = 0.8948779106140137\n",
      "Training iter #24534000:   Batch Loss = 7.280684, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664752006530762, Accuracy = 0.8951756954193115\n",
      "Training iter #24537000:   Batch Loss = 7.269262, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664788246154785, Accuracy = 0.8948779106140137\n",
      "Training iter #24540000:   Batch Loss = 7.342762, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664711952209473, Accuracy = 0.8948779106140137\n",
      "Training iter #24543000:   Batch Loss = 7.355885, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664412975311279, Accuracy = 0.8948779106140137\n",
      "Training iter #24546000:   Batch Loss = 7.338495, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664076328277588, Accuracy = 0.8948779106140137\n",
      "Training iter #24549000:   Batch Loss = 7.269097, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.663938522338867, Accuracy = 0.8945801258087158\n",
      "Training iter #24552000:   Batch Loss = 7.310681, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664085388183594, Accuracy = 0.8945801258087158\n",
      "Training iter #24555000:   Batch Loss = 7.318296, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664399147033691, Accuracy = 0.8939844965934753\n",
      "Training iter #24558000:   Batch Loss = 7.394379, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664360046386719, Accuracy = 0.8939844965934753\n",
      "Training iter #24561000:   Batch Loss = 7.267889, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664465427398682, Accuracy = 0.8939844965934753\n",
      "Training iter #24564000:   Batch Loss = 7.266367, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.663966655731201, Accuracy = 0.8936867117881775\n",
      "Training iter #24567000:   Batch Loss = 7.351469, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664703369140625, Accuracy = 0.8933889269828796\n",
      "Training iter #24570000:   Batch Loss = 7.365122, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.665745258331299, Accuracy = 0.8927933573722839\n",
      "Training iter #24573000:   Batch Loss = 7.336856, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.666059494018555, Accuracy = 0.8930911421775818\n",
      "Training iter #24576000:   Batch Loss = 7.277136, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.66567850112915, Accuracy = 0.8930911421775818\n",
      "Training iter #24579000:   Batch Loss = 7.313722, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.664558410644531, Accuracy = 0.8930911421775818\n",
      "Training iter #24582000:   Batch Loss = 7.333811, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6637797355651855, Accuracy = 0.8921977281570435\n",
      "Training iter #24585000:   Batch Loss = 7.380381, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6634087562561035, Accuracy = 0.8921977281570435\n",
      "Training iter #24588000:   Batch Loss = 7.279590, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.663281440734863, Accuracy = 0.8921977281570435\n",
      "Training iter #24591000:   Batch Loss = 7.274375, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.662717342376709, Accuracy = 0.8924955129623413\n",
      "Training iter #24594000:   Batch Loss = 7.344090, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.66156530380249, Accuracy = 0.8939844965934753\n",
      "Training iter #24597000:   Batch Loss = 7.356812, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.661038398742676, Accuracy = 0.8942822813987732\n",
      "Training iter #24600000:   Batch Loss = 7.337803, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.660670280456543, Accuracy = 0.8939844965934753\n",
      "Training iter #24603000:   Batch Loss = 7.275667, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.660219192504883, Accuracy = 0.8945801258087158\n",
      "Training iter #24606000:   Batch Loss = 7.307581, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659825325012207, Accuracy = 0.8948779106140137\n",
      "Training iter #24609000:   Batch Loss = 7.332624, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6597065925598145, Accuracy = 0.8948779106140137\n",
      "Training iter #24612000:   Batch Loss = 7.367832, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659778594970703, Accuracy = 0.8948779106140137\n",
      "Training iter #24615000:   Batch Loss = 7.264183, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.660135746002197, Accuracy = 0.8948779106140137\n",
      "Training iter #24618000:   Batch Loss = 7.267345, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.660950660705566, Accuracy = 0.8945801258087158\n",
      "Training iter #24621000:   Batch Loss = 7.328607, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.661195278167725, Accuracy = 0.8942822813987732\n",
      "Training iter #24624000:   Batch Loss = 7.369438, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.661109924316406, Accuracy = 0.8942822813987732\n",
      "Training iter #24627000:   Batch Loss = 7.343014, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.66023063659668, Accuracy = 0.8945801258087158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24630000:   Batch Loss = 7.280286, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.660073757171631, Accuracy = 0.8942822813987732\n",
      "Training iter #24633000:   Batch Loss = 7.309388, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659939289093018, Accuracy = 0.8942822813987732\n",
      "Training iter #24636000:   Batch Loss = 7.327308, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659962177276611, Accuracy = 0.8936867117881775\n",
      "Training iter #24639000:   Batch Loss = 7.357688, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659585952758789, Accuracy = 0.8933889269828796\n",
      "Training iter #24642000:   Batch Loss = 7.253804, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6594953536987305, Accuracy = 0.8936867117881775\n",
      "Training iter #24645000:   Batch Loss = 7.275689, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.65955114364624, Accuracy = 0.8933889269828796\n",
      "Training iter #24648000:   Batch Loss = 7.326571, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.660001277923584, Accuracy = 0.8933889269828796\n",
      "Training iter #24651000:   Batch Loss = 7.367906, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659735202789307, Accuracy = 0.8936867117881775\n",
      "Training iter #24654000:   Batch Loss = 7.329949, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659077167510986, Accuracy = 0.8942822813987732\n",
      "Training iter #24657000:   Batch Loss = 7.282937, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.658315658569336, Accuracy = 0.8939844965934753\n",
      "Training iter #24660000:   Batch Loss = 7.306806, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657690525054932, Accuracy = 0.8942822813987732\n",
      "Training iter #24663000:   Batch Loss = 7.324885, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657810211181641, Accuracy = 0.8945801258087158\n",
      "Training iter #24666000:   Batch Loss = 7.355158, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.658205509185791, Accuracy = 0.8945801258087158\n",
      "Training iter #24669000:   Batch Loss = 7.258089, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.658995151519775, Accuracy = 0.8942822813987732\n",
      "Training iter #24672000:   Batch Loss = 7.278678, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.659244537353516, Accuracy = 0.8939844965934753\n",
      "Training iter #24675000:   Batch Loss = 7.314870, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6592488288879395, Accuracy = 0.8939844965934753\n",
      "Training iter #24678000:   Batch Loss = 7.371621, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.658357620239258, Accuracy = 0.8942822813987732\n",
      "Training iter #24681000:   Batch Loss = 7.316956, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657856464385986, Accuracy = 0.8948779106140137\n",
      "Training iter #24684000:   Batch Loss = 7.276144, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657491207122803, Accuracy = 0.8942822813987732\n",
      "Training iter #24687000:   Batch Loss = 7.324473, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657271862030029, Accuracy = 0.8942822813987732\n",
      "Training iter #24690000:   Batch Loss = 7.331570, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657176494598389, Accuracy = 0.8942822813987732\n",
      "Training iter #24693000:   Batch Loss = 7.355872, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657510757446289, Accuracy = 0.8936867117881775\n",
      "Training iter #24696000:   Batch Loss = 7.250289, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.658329486846924, Accuracy = 0.8945801258087158\n",
      "Training iter #24699000:   Batch Loss = 7.276130, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.65852165222168, Accuracy = 0.8942822813987732\n",
      "Training iter #24702000:   Batch Loss = 7.310023, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.658539295196533, Accuracy = 0.8942822813987732\n",
      "Training iter #24705000:   Batch Loss = 7.372869, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.658315181732178, Accuracy = 0.8942822813987732\n",
      "Training iter #24708000:   Batch Loss = 7.303625, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6578521728515625, Accuracy = 0.8942822813987732\n",
      "Training iter #24711000:   Batch Loss = 7.270177, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.656972885131836, Accuracy = 0.8948779106140137\n",
      "Training iter #24714000:   Batch Loss = 7.324815, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.656750202178955, Accuracy = 0.8945801258087158\n",
      "Training iter #24717000:   Batch Loss = 7.330631, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.656680107116699, Accuracy = 0.8945801258087158\n",
      "Training iter #24720000:   Batch Loss = 7.350282, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.656497001647949, Accuracy = 0.8942822813987732\n",
      "Training iter #24723000:   Batch Loss = 7.256268, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.656698703765869, Accuracy = 0.8945801258087158\n",
      "Training iter #24726000:   Batch Loss = 7.275759, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657290935516357, Accuracy = 0.8939844965934753\n",
      "Training iter #24729000:   Batch Loss = 7.303729, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.657166004180908, Accuracy = 0.8942822813987732\n",
      "Training iter #24732000:   Batch Loss = 7.378478, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.656888961791992, Accuracy = 0.8945801258087158\n",
      "Training iter #24735000:   Batch Loss = 7.297586, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6561503410339355, Accuracy = 0.8945801258087158\n",
      "Training iter #24738000:   Batch Loss = 7.270248, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.655313968658447, Accuracy = 0.8951756954193115\n",
      "Training iter #24741000:   Batch Loss = 7.325229, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6549601554870605, Accuracy = 0.8951756954193115\n",
      "Training iter #24744000:   Batch Loss = 7.332043, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.654764175415039, Accuracy = 0.8951756954193115\n",
      "Training iter #24747000:   Batch Loss = 7.342421, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.654762268066406, Accuracy = 0.8948779106140137\n",
      "Training iter #24750000:   Batch Loss = 7.253998, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.654719829559326, Accuracy = 0.8951756954193115\n",
      "Training iter #24753000:   Batch Loss = 7.291023, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6552605628967285, Accuracy = 0.8948779106140137\n",
      "Training iter #24756000:   Batch Loss = 7.304219, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.655392646789551, Accuracy = 0.8945801258087158\n",
      "Training iter #24759000:   Batch Loss = 7.373508, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.655150890350342, Accuracy = 0.8945801258087158\n",
      "Training iter #24762000:   Batch Loss = 7.291635, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.654580116271973, Accuracy = 0.8945801258087158\n",
      "Training iter #24765000:   Batch Loss = 7.264355, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.654146194458008, Accuracy = 0.8945801258087158\n",
      "Training iter #24768000:   Batch Loss = 7.306870, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.653315544128418, Accuracy = 0.8948779106140137\n",
      "Training iter #24771000:   Batch Loss = 7.339489, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.653177261352539, Accuracy = 0.8951756954193115\n",
      "Training iter #24774000:   Batch Loss = 7.323773, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.653366565704346, Accuracy = 0.8954734802246094\n",
      "Training iter #24777000:   Batch Loss = 7.254211, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.653481960296631, Accuracy = 0.8948779106140137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24780000:   Batch Loss = 7.293860, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.653409481048584, Accuracy = 0.8945801258087158\n",
      "Training iter #24783000:   Batch Loss = 7.315634, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.653159141540527, Accuracy = 0.8945801258087158\n",
      "Training iter #24786000:   Batch Loss = 7.377969, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.652928352355957, Accuracy = 0.8945801258087158\n",
      "Training iter #24789000:   Batch Loss = 7.291927, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.652338981628418, Accuracy = 0.8945801258087158\n",
      "Training iter #24792000:   Batch Loss = 7.259440, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.651689052581787, Accuracy = 0.8945801258087158\n",
      "Training iter #24795000:   Batch Loss = 7.308072, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.65132474899292, Accuracy = 0.8948779106140137\n",
      "Training iter #24798000:   Batch Loss = 7.336484, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.651285648345947, Accuracy = 0.8951756954193115\n",
      "Training iter #24801000:   Batch Loss = 7.326102, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.651476860046387, Accuracy = 0.8948779106140137\n",
      "Training iter #24804000:   Batch Loss = 7.245334, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650731563568115, Accuracy = 0.8948779106140137\n",
      "Training iter #24807000:   Batch Loss = 7.293350, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650730609893799, Accuracy = 0.8948779106140137\n",
      "Training iter #24810000:   Batch Loss = 7.314024, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650790214538574, Accuracy = 0.8945801258087158\n",
      "Training iter #24813000:   Batch Loss = 7.379470, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650606155395508, Accuracy = 0.8945801258087158\n",
      "Training iter #24816000:   Batch Loss = 7.280896, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.649931907653809, Accuracy = 0.8948779106140137\n",
      "Training iter #24819000:   Batch Loss = 7.255914, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650576114654541, Accuracy = 0.8948779106140137\n",
      "Training iter #24822000:   Batch Loss = 7.322760, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6501240730285645, Accuracy = 0.8954734802246094\n",
      "Training iter #24825000:   Batch Loss = 7.335673, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650173187255859, Accuracy = 0.8960691094398499\n",
      "Training iter #24828000:   Batch Loss = 7.315148, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650411605834961, Accuracy = 0.8951756954193115\n",
      "Training iter #24831000:   Batch Loss = 7.246000, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650440216064453, Accuracy = 0.8948779106140137\n",
      "Training iter #24834000:   Batch Loss = 7.294815, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650511741638184, Accuracy = 0.8945801258087158\n",
      "Training iter #24837000:   Batch Loss = 7.304684, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.650224685668945, Accuracy = 0.8951756954193115\n",
      "Training iter #24840000:   Batch Loss = 7.375643, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.649923324584961, Accuracy = 0.8954734802246094\n",
      "Training iter #24843000:   Batch Loss = 7.271109, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.649509906768799, Accuracy = 0.8954734802246094\n",
      "Training iter #24846000:   Batch Loss = 7.253528, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6492156982421875, Accuracy = 0.8954734802246094\n",
      "Training iter #24849000:   Batch Loss = 7.321341, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.64900541305542, Accuracy = 0.8954734802246094\n",
      "Training iter #24852000:   Batch Loss = 7.329237, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.649013519287109, Accuracy = 0.8957712650299072\n",
      "Training iter #24855000:   Batch Loss = 7.323057, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.649093151092529, Accuracy = 0.8954734802246094\n",
      "Training iter #24858000:   Batch Loss = 7.248906, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.649156093597412, Accuracy = 0.8951756954193115\n",
      "Training iter #24861000:   Batch Loss = 7.293319, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6494340896606445, Accuracy = 0.8945801258087158\n",
      "Training iter #24864000:   Batch Loss = 7.300492, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.649085521697998, Accuracy = 0.8945801258087158\n",
      "Training iter #24867000:   Batch Loss = 7.375750, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.648629188537598, Accuracy = 0.8945801258087158\n",
      "Training iter #24870000:   Batch Loss = 7.258052, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.648097991943359, Accuracy = 0.8954734802246094\n",
      "Training iter #24873000:   Batch Loss = 7.245576, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.647680759429932, Accuracy = 0.8951756954193115\n",
      "Training iter #24876000:   Batch Loss = 7.326607, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6472320556640625, Accuracy = 0.8954734802246094\n",
      "Training iter #24879000:   Batch Loss = 7.333477, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6477508544921875, Accuracy = 0.8951756954193115\n",
      "Training iter #24882000:   Batch Loss = 7.320351, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.647857189178467, Accuracy = 0.8957712650299072\n",
      "Training iter #24885000:   Batch Loss = 7.252815, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.647675037384033, Accuracy = 0.8951756954193115\n",
      "Training iter #24888000:   Batch Loss = 7.289640, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.648152828216553, Accuracy = 0.8948779106140137\n",
      "Training iter #24891000:   Batch Loss = 7.302095, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.647800445556641, Accuracy = 0.8951756954193115\n",
      "Training iter #24894000:   Batch Loss = 7.365099, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.647176265716553, Accuracy = 0.8954734802246094\n",
      "Training iter #24897000:   Batch Loss = 7.250111, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.646594047546387, Accuracy = 0.8957712650299072\n",
      "Training iter #24900000:   Batch Loss = 7.243845, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.646353244781494, Accuracy = 0.8954734802246094\n",
      "Training iter #24903000:   Batch Loss = 7.325412, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.646148681640625, Accuracy = 0.8954734802246094\n",
      "Training iter #24906000:   Batch Loss = 7.333423, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.645954132080078, Accuracy = 0.8954734802246094\n",
      "Training iter #24909000:   Batch Loss = 7.313557, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.646126747131348, Accuracy = 0.8951756954193115\n",
      "Training iter #24912000:   Batch Loss = 7.253475, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.646486759185791, Accuracy = 0.8951756954193115\n",
      "Training iter #24915000:   Batch Loss = 7.287724, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.646665096282959, Accuracy = 0.8951756954193115\n",
      "Training iter #24918000:   Batch Loss = 7.314884, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.646608352661133, Accuracy = 0.8951756954193115\n",
      "Training iter #24921000:   Batch Loss = 7.353480, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.645646095275879, Accuracy = 0.8951756954193115\n",
      "Training iter #24924000:   Batch Loss = 7.243984, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.644858360290527, Accuracy = 0.8957712650299072\n",
      "Training iter #24927000:   Batch Loss = 7.245687, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.644278049468994, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24930000:   Batch Loss = 7.317065, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6442365646362305, Accuracy = 0.8957712650299072\n",
      "Training iter #24933000:   Batch Loss = 7.339485, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.644421577453613, Accuracy = 0.8954734802246094\n",
      "Training iter #24936000:   Batch Loss = 7.318075, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.644573211669922, Accuracy = 0.8957712650299072\n",
      "Training iter #24939000:   Batch Loss = 7.262949, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.645272731781006, Accuracy = 0.8951756954193115\n",
      "Training iter #24942000:   Batch Loss = 7.293569, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.645272731781006, Accuracy = 0.8948779106140137\n",
      "Training iter #24945000:   Batch Loss = 7.311030, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.645056247711182, Accuracy = 0.8948779106140137\n",
      "Training iter #24948000:   Batch Loss = 7.334435, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.644552230834961, Accuracy = 0.8951756954193115\n",
      "Training iter #24951000:   Batch Loss = 7.233023, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.643279075622559, Accuracy = 0.8951756954193115\n",
      "Training iter #24954000:   Batch Loss = 7.256400, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.643054485321045, Accuracy = 0.8954734802246094\n",
      "Training iter #24957000:   Batch Loss = 7.305018, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.643039226531982, Accuracy = 0.8951756954193115\n",
      "Training iter #24960000:   Batch Loss = 7.345944, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.642879009246826, Accuracy = 0.8954734802246094\n",
      "Training iter #24963000:   Batch Loss = 7.319576, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.642786026000977, Accuracy = 0.8954734802246094\n",
      "Training iter #24966000:   Batch Loss = 7.263692, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.643484592437744, Accuracy = 0.8948779106140137\n",
      "Training iter #24969000:   Batch Loss = 7.286817, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.643195629119873, Accuracy = 0.8945801258087158\n",
      "Training iter #24972000:   Batch Loss = 7.306351, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.642910957336426, Accuracy = 0.8945801258087158\n",
      "Training iter #24975000:   Batch Loss = 7.334341, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6415181159973145, Accuracy = 0.8948779106140137\n",
      "Training iter #24978000:   Batch Loss = 7.232269, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.640354156494141, Accuracy = 0.8957712650299072\n",
      "Training iter #24981000:   Batch Loss = 7.259776, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.639880180358887, Accuracy = 0.8960691094398499\n",
      "Training iter #24984000:   Batch Loss = 7.308406, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6396307945251465, Accuracy = 0.8963668942451477\n",
      "Training iter #24987000:   Batch Loss = 7.350588, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.639952659606934, Accuracy = 0.8966646790504456\n",
      "Training iter #24990000:   Batch Loss = 7.302059, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.640637397766113, Accuracy = 0.8957712650299072\n",
      "Training iter #24993000:   Batch Loss = 7.256262, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.642096519470215, Accuracy = 0.8951756954193115\n",
      "Training iter #24996000:   Batch Loss = 7.297476, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.642760753631592, Accuracy = 0.8948779106140137\n",
      "Training iter #24999000:   Batch Loss = 7.314707, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.64273738861084, Accuracy = 0.8945801258087158\n",
      "Training iter #25002000:   Batch Loss = 7.327642, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6416497230529785, Accuracy = 0.8948779106140137\n",
      "Training iter #25005000:   Batch Loss = 7.235803, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.639763832092285, Accuracy = 0.8951756954193115\n",
      "Training iter #25008000:   Batch Loss = 7.258489, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.639427661895752, Accuracy = 0.8948779106140137\n",
      "Training iter #25011000:   Batch Loss = 7.293908, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.63946008682251, Accuracy = 0.8942822813987732\n",
      "Training iter #25014000:   Batch Loss = 7.354308, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.638857364654541, Accuracy = 0.8948779106140137\n",
      "Training iter #25017000:   Batch Loss = 7.290538, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.638592720031738, Accuracy = 0.8948779106140137\n",
      "Training iter #25020000:   Batch Loss = 7.256027, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.638080596923828, Accuracy = 0.8948779106140137\n",
      "Training iter #25023000:   Batch Loss = 7.307165, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.637661933898926, Accuracy = 0.8948779106140137\n",
      "Training iter #25026000:   Batch Loss = 7.316681, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.637446880340576, Accuracy = 0.8951756954193115\n",
      "Training iter #25029000:   Batch Loss = 7.329719, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6374616622924805, Accuracy = 0.8948779106140137\n",
      "Training iter #25032000:   Batch Loss = 7.235540, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.637547492980957, Accuracy = 0.8954734802246094\n",
      "Training iter #25035000:   Batch Loss = 7.258166, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.63846492767334, Accuracy = 0.8954734802246094\n",
      "Training iter #25038000:   Batch Loss = 7.294895, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.638761520385742, Accuracy = 0.8954734802246094\n",
      "Training iter #25041000:   Batch Loss = 7.354999, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6389970779418945, Accuracy = 0.8948779106140137\n",
      "Training iter #25044000:   Batch Loss = 7.281914, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6406474113464355, Accuracy = 0.8948779106140137\n",
      "Training iter #25047000:   Batch Loss = 7.248499, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.640623569488525, Accuracy = 0.8951756954193115\n",
      "Training iter #25050000:   Batch Loss = 7.311076, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.640176773071289, Accuracy = 0.8954734802246094\n",
      "Training iter #25053000:   Batch Loss = 7.312035, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.639960289001465, Accuracy = 0.8957712650299072\n",
      "Training iter #25056000:   Batch Loss = 7.324117, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.638082027435303, Accuracy = 0.8957712650299072\n",
      "Training iter #25059000:   Batch Loss = 7.235756, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.637085914611816, Accuracy = 0.8954734802246094\n",
      "Training iter #25062000:   Batch Loss = 7.256713, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.636669635772705, Accuracy = 0.8954734802246094\n",
      "Training iter #25065000:   Batch Loss = 7.283783, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.636404037475586, Accuracy = 0.8957712650299072\n",
      "Training iter #25068000:   Batch Loss = 7.356979, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.637119770050049, Accuracy = 0.8945801258087158\n",
      "Training iter #25071000:   Batch Loss = 7.278032, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.636046886444092, Accuracy = 0.8942822813987732\n",
      "Training iter #25074000:   Batch Loss = 7.249432, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.635407447814941, Accuracy = 0.8948779106140137\n",
      "Training iter #25077000:   Batch Loss = 7.289217, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.635084629058838, Accuracy = 0.8951756954193115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25080000:   Batch Loss = 7.316647, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6351776123046875, Accuracy = 0.8948779106140137\n",
      "Training iter #25083000:   Batch Loss = 7.313142, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.63676643371582, Accuracy = 0.8951756954193115\n",
      "Training iter #25086000:   Batch Loss = 7.233898, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.636481285095215, Accuracy = 0.8945801258087158\n",
      "Training iter #25089000:   Batch Loss = 7.274185, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.636234760284424, Accuracy = 0.8945801258087158\n",
      "Training iter #25092000:   Batch Loss = 7.292971, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.636203289031982, Accuracy = 0.8948779106140137\n",
      "Training iter #25095000:   Batch Loss = 7.350625, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.635952949523926, Accuracy = 0.8945801258087158\n",
      "Training iter #25098000:   Batch Loss = 7.279133, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6359710693359375, Accuracy = 0.8942822813987732\n",
      "Training iter #25101000:   Batch Loss = 7.241430, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.63569450378418, Accuracy = 0.8942822813987732\n",
      "Training iter #25104000:   Batch Loss = 7.291315, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6352314949035645, Accuracy = 0.8942822813987732\n",
      "Training iter #25107000:   Batch Loss = 7.320294, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.634949684143066, Accuracy = 0.8945801258087158\n",
      "Training iter #25110000:   Batch Loss = 7.293167, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.63510799407959, Accuracy = 0.8945801258087158\n",
      "Training iter #25113000:   Batch Loss = 7.231951, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.634757995605469, Accuracy = 0.8948779106140137\n",
      "Training iter #25116000:   Batch Loss = 7.274899, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6326584815979, Accuracy = 0.8951756954193115\n",
      "Training iter #25119000:   Batch Loss = 7.296045, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.631422519683838, Accuracy = 0.8957712650299072\n",
      "Training iter #25122000:   Batch Loss = 7.358776, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.631277084350586, Accuracy = 0.8951756954193115\n",
      "Training iter #25125000:   Batch Loss = 7.264054, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.630917072296143, Accuracy = 0.8954734802246094\n",
      "Training iter #25128000:   Batch Loss = 7.240352, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6315131187438965, Accuracy = 0.8948779106140137\n",
      "Training iter #25131000:   Batch Loss = 7.291918, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.631256580352783, Accuracy = 0.8951756954193115\n",
      "Training iter #25134000:   Batch Loss = 7.322352, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.632534027099609, Accuracy = 0.8936867117881775\n",
      "Training iter #25137000:   Batch Loss = 7.301386, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.632949352264404, Accuracy = 0.8933889269828796\n",
      "Training iter #25140000:   Batch Loss = 7.228175, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6326398849487305, Accuracy = 0.8933889269828796\n",
      "Training iter #25143000:   Batch Loss = 7.277078, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.632460117340088, Accuracy = 0.8933889269828796\n",
      "Training iter #25146000:   Batch Loss = 7.287755, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.632292747497559, Accuracy = 0.8933889269828796\n",
      "Training iter #25149000:   Batch Loss = 7.358577, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.631500720977783, Accuracy = 0.8942822813987732\n",
      "Training iter #25152000:   Batch Loss = 7.266782, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.631789207458496, Accuracy = 0.8939844965934753\n",
      "Training iter #25155000:   Batch Loss = 7.237114, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.630362033843994, Accuracy = 0.8954734802246094\n",
      "Training iter #25158000:   Batch Loss = 7.306936, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.630666255950928, Accuracy = 0.8957712650299072\n",
      "Training iter #25161000:   Batch Loss = 7.318141, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.629039764404297, Accuracy = 0.8957712650299072\n",
      "Training iter #25164000:   Batch Loss = 7.308212, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.628377437591553, Accuracy = 0.8963668942451477\n",
      "Training iter #25167000:   Batch Loss = 7.227522, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.62894344329834, Accuracy = 0.8957712650299072\n",
      "Training iter #25170000:   Batch Loss = 7.276483, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6283955574035645, Accuracy = 0.8969624638557434\n",
      "Training iter #25173000:   Batch Loss = 7.293676, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.627269268035889, Accuracy = 0.8966646790504456\n",
      "Training iter #25176000:   Batch Loss = 7.355172, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.626424312591553, Accuracy = 0.8969624638557434\n",
      "Training iter #25179000:   Batch Loss = 7.246930, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.627354145050049, Accuracy = 0.8972602486610413\n",
      "Training iter #25182000:   Batch Loss = 7.235151, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6267290115356445, Accuracy = 0.8975580930709839\n",
      "Training iter #25185000:   Batch Loss = 7.307354, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.626475811004639, Accuracy = 0.8978558778762817\n",
      "Training iter #25188000:   Batch Loss = 7.320343, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.626132965087891, Accuracy = 0.8981536626815796\n",
      "Training iter #25191000:   Batch Loss = 7.302989, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.625929832458496, Accuracy = 0.8978558778762817\n",
      "Training iter #25194000:   Batch Loss = 7.235432, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.626096248626709, Accuracy = 0.8972602486610413\n",
      "Training iter #25197000:   Batch Loss = 7.275983, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.626595973968506, Accuracy = 0.8963668942451477\n",
      "Training iter #25200000:   Batch Loss = 7.289112, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.628445148468018, Accuracy = 0.8966646790504456\n",
      "Training iter #25203000:   Batch Loss = 7.354697, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6289873123168945, Accuracy = 0.8960691094398499\n",
      "Training iter #25206000:   Batch Loss = 7.234737, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.628835201263428, Accuracy = 0.8963668942451477\n",
      "Training iter #25209000:   Batch Loss = 7.229357, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6286492347717285, Accuracy = 0.8960691094398499\n",
      "Training iter #25212000:   Batch Loss = 7.314967, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.628693580627441, Accuracy = 0.8954734802246094\n",
      "Training iter #25215000:   Batch Loss = 7.322343, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.627678394317627, Accuracy = 0.8954734802246094\n",
      "Training iter #25218000:   Batch Loss = 7.300105, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.626617908477783, Accuracy = 0.8957712650299072\n",
      "Training iter #25221000:   Batch Loss = 7.236899, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.626294136047363, Accuracy = 0.8957712650299072\n",
      "Training iter #25224000:   Batch Loss = 7.284388, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.625999927520752, Accuracy = 0.8957712650299072\n",
      "Training iter #25227000:   Batch Loss = 7.297252, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.625237941741943, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25230000:   Batch Loss = 7.337136, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624372959136963, Accuracy = 0.8960691094398499\n",
      "Training iter #25233000:   Batch Loss = 7.230865, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622675895690918, Accuracy = 0.8963668942451477\n",
      "Training iter #25236000:   Batch Loss = 7.231783, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.62116003036499, Accuracy = 0.8969624638557434\n",
      "Training iter #25239000:   Batch Loss = 7.306525, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6212897300720215, Accuracy = 0.8966646790504456\n",
      "Training iter #25242000:   Batch Loss = 7.317178, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.621706008911133, Accuracy = 0.8957712650299072\n",
      "Training iter #25245000:   Batch Loss = 7.298995, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6217851638793945, Accuracy = 0.8954734802246094\n",
      "Training iter #25248000:   Batch Loss = 7.236950, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622140884399414, Accuracy = 0.8966646790504456\n",
      "Training iter #25251000:   Batch Loss = 7.289892, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.623799800872803, Accuracy = 0.8957712650299072\n",
      "Training iter #25254000:   Batch Loss = 7.295946, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.62416410446167, Accuracy = 0.8954734802246094\n",
      "Training iter #25257000:   Batch Loss = 7.328787, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624326229095459, Accuracy = 0.8951756954193115\n",
      "Training iter #25260000:   Batch Loss = 7.222892, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624025344848633, Accuracy = 0.8951756954193115\n",
      "Training iter #25263000:   Batch Loss = 7.229666, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624594211578369, Accuracy = 0.8954734802246094\n",
      "Training iter #25266000:   Batch Loss = 7.290421, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624518871307373, Accuracy = 0.8954734802246094\n",
      "Training iter #25269000:   Batch Loss = 7.327767, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624323844909668, Accuracy = 0.8954734802246094\n",
      "Training iter #25272000:   Batch Loss = 7.304478, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624143123626709, Accuracy = 0.8957712650299072\n",
      "Training iter #25275000:   Batch Loss = 7.244722, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.623485088348389, Accuracy = 0.8960691094398499\n",
      "Training iter #25278000:   Batch Loss = 7.270707, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622910976409912, Accuracy = 0.8969624638557434\n",
      "Training iter #25281000:   Batch Loss = 7.290860, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6229352951049805, Accuracy = 0.8960691094398499\n",
      "Training iter #25284000:   Batch Loss = 7.313524, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622869491577148, Accuracy = 0.8966646790504456\n",
      "Training iter #25287000:   Batch Loss = 7.217960, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622705936431885, Accuracy = 0.8969624638557434\n",
      "Training iter #25290000:   Batch Loss = 7.247790, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.621816635131836, Accuracy = 0.8966646790504456\n",
      "Training iter #25293000:   Batch Loss = 7.287364, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620354175567627, Accuracy = 0.8975580930709839\n",
      "Training iter #25296000:   Batch Loss = 7.328916, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620297431945801, Accuracy = 0.8972602486610413\n",
      "Training iter #25299000:   Batch Loss = 7.298592, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620192050933838, Accuracy = 0.8975580930709839\n",
      "Training iter #25302000:   Batch Loss = 7.244740, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620264053344727, Accuracy = 0.8978558778762817\n",
      "Training iter #25305000:   Batch Loss = 7.268785, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620225429534912, Accuracy = 0.8972602486610413\n",
      "Training iter #25308000:   Batch Loss = 7.291632, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620098114013672, Accuracy = 0.8966646790504456\n",
      "Training iter #25311000:   Batch Loss = 7.315747, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.62142276763916, Accuracy = 0.8960691094398499\n",
      "Training iter #25314000:   Batch Loss = 7.216755, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622124195098877, Accuracy = 0.8957712650299072\n",
      "Training iter #25317000:   Batch Loss = 7.250061, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.621702194213867, Accuracy = 0.8951756954193115\n",
      "Training iter #25320000:   Batch Loss = 7.274997, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622313976287842, Accuracy = 0.8945801258087158\n",
      "Training iter #25323000:   Batch Loss = 7.337273, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.623263359069824, Accuracy = 0.8945801258087158\n",
      "Training iter #25326000:   Batch Loss = 7.281229, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.624365329742432, Accuracy = 0.8936867117881775\n",
      "Training iter #25329000:   Batch Loss = 7.258286, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.622868061065674, Accuracy = 0.8939844965934753\n",
      "Training iter #25332000:   Batch Loss = 7.286227, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.621521472930908, Accuracy = 0.8936867117881775\n",
      "Training iter #25335000:   Batch Loss = 7.291124, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620611190795898, Accuracy = 0.8933889269828796\n",
      "Training iter #25338000:   Batch Loss = 7.314230, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620248317718506, Accuracy = 0.8942822813987732\n",
      "Training iter #25341000:   Batch Loss = 7.225743, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.619894981384277, Accuracy = 0.8936867117881775\n",
      "Training iter #25344000:   Batch Loss = 7.240555, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.618532657623291, Accuracy = 0.8942822813987732\n",
      "Training iter #25347000:   Batch Loss = 7.271356, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.618221282958984, Accuracy = 0.8942822813987732\n",
      "Training iter #25350000:   Batch Loss = 7.334029, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6194562911987305, Accuracy = 0.8942822813987732\n",
      "Training iter #25353000:   Batch Loss = 7.268509, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.619731903076172, Accuracy = 0.8939844965934753\n",
      "Training iter #25356000:   Batch Loss = 7.235221, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620230197906494, Accuracy = 0.8945801258087158\n",
      "Training iter #25359000:   Batch Loss = 7.284194, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620970726013184, Accuracy = 0.8945801258087158\n",
      "Training iter #25362000:   Batch Loss = 7.293639, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.621066570281982, Accuracy = 0.8942822813987732\n",
      "Training iter #25365000:   Batch Loss = 7.310799, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.620201587677002, Accuracy = 0.8939844965934753\n",
      "Training iter #25368000:   Batch Loss = 7.226225, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.618671417236328, Accuracy = 0.8939844965934753\n",
      "Training iter #25371000:   Batch Loss = 7.240762, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.617541790008545, Accuracy = 0.8948779106140137\n",
      "Training iter #25374000:   Batch Loss = 7.268005, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.616891860961914, Accuracy = 0.8948779106140137\n",
      "Training iter #25377000:   Batch Loss = 7.336817, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.616529941558838, Accuracy = 0.8945801258087158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25380000:   Batch Loss = 7.260878, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.616368770599365, Accuracy = 0.8945801258087158\n",
      "Training iter #25383000:   Batch Loss = 7.232525, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.616119384765625, Accuracy = 0.8942822813987732\n",
      "Training iter #25386000:   Batch Loss = 7.288496, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.61577033996582, Accuracy = 0.8945801258087158\n",
      "Training iter #25389000:   Batch Loss = 7.292643, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.615593433380127, Accuracy = 0.8948779106140137\n",
      "Training iter #25392000:   Batch Loss = 7.300070, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.615509510040283, Accuracy = 0.8948779106140137\n",
      "Training iter #25395000:   Batch Loss = 7.225169, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.615406036376953, Accuracy = 0.8945801258087158\n",
      "Training iter #25398000:   Batch Loss = 7.253304, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.615181922912598, Accuracy = 0.8945801258087158\n",
      "Training iter #25401000:   Batch Loss = 7.268464, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6152143478393555, Accuracy = 0.8942822813987732\n",
      "Training iter #25404000:   Batch Loss = 7.339639, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.615331649780273, Accuracy = 0.8933889269828796\n",
      "Training iter #25407000:   Batch Loss = 7.256574, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.615273952484131, Accuracy = 0.8936867117881775\n",
      "Training iter #25410000:   Batch Loss = 7.232836, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.615323066711426, Accuracy = 0.8939844965934753\n",
      "Training iter #25413000:   Batch Loss = 7.270440, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6151275634765625, Accuracy = 0.8942822813987732\n",
      "Training iter #25416000:   Batch Loss = 7.302474, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.614829063415527, Accuracy = 0.8945801258087158\n",
      "Training iter #25419000:   Batch Loss = 7.287309, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.614858150482178, Accuracy = 0.8945801258087158\n",
      "Training iter #25422000:   Batch Loss = 7.226765, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.614687442779541, Accuracy = 0.8948779106140137\n",
      "Training iter #25425000:   Batch Loss = 7.255132, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.614370822906494, Accuracy = 0.8951756954193115\n",
      "Training iter #25428000:   Batch Loss = 7.274331, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6139936447143555, Accuracy = 0.8951756954193115\n",
      "Training iter #25431000:   Batch Loss = 7.338158, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.613724708557129, Accuracy = 0.8951756954193115\n",
      "Training iter #25434000:   Batch Loss = 7.253480, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.613436698913574, Accuracy = 0.8951756954193115\n",
      "Training iter #25437000:   Batch Loss = 7.223756, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.613121509552002, Accuracy = 0.8951756954193115\n",
      "Training iter #25440000:   Batch Loss = 7.269133, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.612778186798096, Accuracy = 0.8951756954193115\n",
      "Training iter #25443000:   Batch Loss = 7.297410, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.612717151641846, Accuracy = 0.8951756954193115\n",
      "Training iter #25446000:   Batch Loss = 7.283521, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.612715721130371, Accuracy = 0.8951756954193115\n",
      "Training iter #25449000:   Batch Loss = 7.219239, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6125993728637695, Accuracy = 0.8951756954193115\n",
      "Training iter #25452000:   Batch Loss = 7.257028, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.612390518188477, Accuracy = 0.8948779106140137\n",
      "Training iter #25455000:   Batch Loss = 7.277134, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.612255573272705, Accuracy = 0.8945801258087158\n",
      "Training iter #25458000:   Batch Loss = 7.334143, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.612213611602783, Accuracy = 0.8945801258087158\n",
      "Training iter #25461000:   Batch Loss = 7.243190, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.612156391143799, Accuracy = 0.8948779106140137\n",
      "Training iter #25464000:   Batch Loss = 7.221665, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.611760139465332, Accuracy = 0.8951756954193115\n",
      "Training iter #25467000:   Batch Loss = 7.285501, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.611268997192383, Accuracy = 0.8951756954193115\n",
      "Training iter #25470000:   Batch Loss = 7.297128, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.610868453979492, Accuracy = 0.8948779106140137\n",
      "Training iter #25473000:   Batch Loss = 7.276909, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.610505104064941, Accuracy = 0.8957712650299072\n",
      "Training iter #25476000:   Batch Loss = 7.219844, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.610478401184082, Accuracy = 0.8954734802246094\n",
      "Training iter #25479000:   Batch Loss = 7.255625, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.610036849975586, Accuracy = 0.8951756954193115\n",
      "Training iter #25482000:   Batch Loss = 7.266747, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.608941555023193, Accuracy = 0.8954734802246094\n",
      "Training iter #25485000:   Batch Loss = 7.331656, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.610171794891357, Accuracy = 0.8945801258087158\n",
      "Training iter #25488000:   Batch Loss = 7.235960, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.609870910644531, Accuracy = 0.8948779106140137\n",
      "Training iter #25491000:   Batch Loss = 7.218648, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.609807014465332, Accuracy = 0.8945801258087158\n",
      "Training iter #25494000:   Batch Loss = 7.284079, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60968017578125, Accuracy = 0.8948779106140137\n",
      "Training iter #25497000:   Batch Loss = 7.290011, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.609514236450195, Accuracy = 0.8948779106140137\n",
      "Training iter #25500000:   Batch Loss = 7.282784, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60941743850708, Accuracy = 0.8945801258087158\n",
      "Training iter #25503000:   Batch Loss = 7.222149, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.609293460845947, Accuracy = 0.8945801258087158\n",
      "Training iter #25506000:   Batch Loss = 7.254919, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.609152793884277, Accuracy = 0.8945801258087158\n",
      "Training iter #25509000:   Batch Loss = 7.263029, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6089911460876465, Accuracy = 0.8945801258087158\n",
      "Training iter #25512000:   Batch Loss = 7.335176, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.608705997467041, Accuracy = 0.8948779106140137\n",
      "Training iter #25515000:   Batch Loss = 7.234046, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.608412265777588, Accuracy = 0.8957712650299072\n",
      "Training iter #25518000:   Batch Loss = 7.210432, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.608066558837891, Accuracy = 0.8957712650299072\n",
      "Training iter #25521000:   Batch Loss = 7.288972, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6078619956970215, Accuracy = 0.8960691094398499\n",
      "Training iter #25524000:   Batch Loss = 7.295347, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6076812744140625, Accuracy = 0.8960691094398499\n",
      "Training iter #25527000:   Batch Loss = 7.281578, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.607588291168213, Accuracy = 0.8960691094398499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25530000:   Batch Loss = 7.221362, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6074934005737305, Accuracy = 0.8957712650299072\n",
      "Training iter #25533000:   Batch Loss = 7.255756, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.607396602630615, Accuracy = 0.8954734802246094\n",
      "Training iter #25536000:   Batch Loss = 7.260306, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.607722282409668, Accuracy = 0.8948779106140137\n",
      "Training iter #25539000:   Batch Loss = 7.325183, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60733699798584, Accuracy = 0.8948779106140137\n",
      "Training iter #25542000:   Batch Loss = 7.221933, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.607351303100586, Accuracy = 0.8951756954193115\n",
      "Training iter #25545000:   Batch Loss = 7.209974, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.607213973999023, Accuracy = 0.8951756954193115\n",
      "Training iter #25548000:   Batch Loss = 7.289653, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.607097148895264, Accuracy = 0.8954734802246094\n",
      "Training iter #25551000:   Batch Loss = 7.296690, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.606841087341309, Accuracy = 0.8951756954193115\n",
      "Training iter #25554000:   Batch Loss = 7.277236, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.606517791748047, Accuracy = 0.8945801258087158\n",
      "Training iter #25557000:   Batch Loss = 7.224408, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6063008308410645, Accuracy = 0.8948779106140137\n",
      "Training iter #25560000:   Batch Loss = 7.254100, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.606045246124268, Accuracy = 0.8948779106140137\n",
      "Training iter #25563000:   Batch Loss = 7.275224, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.605815410614014, Accuracy = 0.8948779106140137\n",
      "Training iter #25566000:   Batch Loss = 7.312142, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6055989265441895, Accuracy = 0.8945801258087158\n",
      "Training iter #25569000:   Batch Loss = 7.217690, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6052093505859375, Accuracy = 0.8948779106140137\n",
      "Training iter #25572000:   Batch Loss = 7.212204, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.604928493499756, Accuracy = 0.8948779106140137\n",
      "Training iter #25575000:   Batch Loss = 7.285444, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.604778289794922, Accuracy = 0.8951756954193115\n",
      "Training iter #25578000:   Batch Loss = 7.297333, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.604559898376465, Accuracy = 0.8945801258087158\n",
      "Training iter #25581000:   Batch Loss = 7.281553, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.604381561279297, Accuracy = 0.8945801258087158\n",
      "Training iter #25584000:   Batch Loss = 7.230517, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.604178428649902, Accuracy = 0.8945801258087158\n",
      "Training iter #25587000:   Batch Loss = 7.258683, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60398530960083, Accuracy = 0.8945801258087158\n",
      "Training iter #25590000:   Batch Loss = 7.273934, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.603816509246826, Accuracy = 0.8948779106140137\n",
      "Training iter #25593000:   Batch Loss = 7.295222, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60357666015625, Accuracy = 0.8948779106140137\n",
      "Training iter #25596000:   Batch Loss = 7.203420, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.603395462036133, Accuracy = 0.8948779106140137\n",
      "Training iter #25599000:   Batch Loss = 7.218596, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.603211879730225, Accuracy = 0.8948779106140137\n",
      "Training iter #25602000:   Batch Loss = 7.275793, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.603050708770752, Accuracy = 0.8948779106140137\n",
      "Training iter #25605000:   Batch Loss = 7.307016, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60296106338501, Accuracy = 0.8948779106140137\n",
      "Training iter #25608000:   Batch Loss = 7.283710, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.602919578552246, Accuracy = 0.8945801258087158\n",
      "Training iter #25611000:   Batch Loss = 7.228467, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.602944850921631, Accuracy = 0.8942822813987732\n",
      "Training iter #25614000:   Batch Loss = 7.253653, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.602883815765381, Accuracy = 0.8948779106140137\n",
      "Training iter #25617000:   Batch Loss = 7.267867, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60274076461792, Accuracy = 0.8951756954193115\n",
      "Training iter #25620000:   Batch Loss = 7.293775, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.6024346351623535, Accuracy = 0.8948779106140137\n",
      "Training iter #25623000:   Batch Loss = 7.195734, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.60220193862915, Accuracy = 0.8948779106140137\n",
      "Training iter #25626000:   Batch Loss = 7.225741, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.602063179016113, Accuracy = 0.8948779106140137\n",
      "Training iter #25629000:   Batch Loss = 7.270324, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.601905345916748, Accuracy = 0.8951756954193115\n",
      "Training iter #25632000:   Batch Loss = 7.307632, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.601750373840332, Accuracy = 0.8948779106140137\n",
      "Training iter #25635000:   Batch Loss = 7.269017, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.601657390594482, Accuracy = 0.8945801258087158\n",
      "Training iter #25638000:   Batch Loss = 7.224792, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.601607322692871, Accuracy = 0.8945801258087158\n",
      "Training iter #25641000:   Batch Loss = 7.261896, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600489139556885, Accuracy = 0.8948779106140137\n",
      "Training iter #25644000:   Batch Loss = 7.266382, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600225925445557, Accuracy = 0.8954734802246094\n",
      "Training iter #25647000:   Batch Loss = 7.289023, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.601185321807861, Accuracy = 0.8951756954193115\n",
      "Training iter #25650000:   Batch Loss = 7.201660, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600890159606934, Accuracy = 0.8948779106140137\n",
      "Training iter #25653000:   Batch Loss = 7.223023, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600546360015869, Accuracy = 0.8948779106140137\n",
      "Training iter #25656000:   Batch Loss = 7.262375, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600170135498047, Accuracy = 0.8948779106140137\n",
      "Training iter #25659000:   Batch Loss = 7.311013, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.599837303161621, Accuracy = 0.8954734802246094\n",
      "Training iter #25662000:   Batch Loss = 7.250145, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600005626678467, Accuracy = 0.8948779106140137\n",
      "Training iter #25665000:   Batch Loss = 7.219303, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600050926208496, Accuracy = 0.8951756954193115\n",
      "Training iter #25668000:   Batch Loss = 7.269171, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600034713745117, Accuracy = 0.8954734802246094\n",
      "Training iter #25671000:   Batch Loss = 7.273849, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.600087642669678, Accuracy = 0.8954734802246094\n",
      "Training iter #25674000:   Batch Loss = 7.289804, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.599619388580322, Accuracy = 0.8954734802246094\n",
      "Training iter #25677000:   Batch Loss = 7.200595, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.599290370941162, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25680000:   Batch Loss = 7.222486, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.598595142364502, Accuracy = 0.8963668942451477\n",
      "Training iter #25683000:   Batch Loss = 7.256477, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.598451614379883, Accuracy = 0.8966646790504456\n",
      "Training iter #25686000:   Batch Loss = 7.313922, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.598357677459717, Accuracy = 0.8966646790504456\n",
      "Training iter #25689000:   Batch Loss = 7.245690, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.598359107971191, Accuracy = 0.8960691094398499\n",
      "Training iter #25692000:   Batch Loss = 7.211730, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.598341941833496, Accuracy = 0.8957712650299072\n",
      "Training iter #25695000:   Batch Loss = 7.269525, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5982441902160645, Accuracy = 0.8960691094398499\n",
      "Training iter #25698000:   Batch Loss = 7.270421, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.598110675811768, Accuracy = 0.8963668942451477\n",
      "Training iter #25701000:   Batch Loss = 7.283723, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.59776496887207, Accuracy = 0.8960691094398499\n",
      "Training iter #25704000:   Batch Loss = 7.200421, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.59746789932251, Accuracy = 0.8963668942451477\n",
      "Training iter #25707000:   Batch Loss = 7.222950, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.597201347351074, Accuracy = 0.8963668942451477\n",
      "Training iter #25710000:   Batch Loss = 7.249690, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.596936225891113, Accuracy = 0.8963668942451477\n",
      "Training iter #25713000:   Batch Loss = 7.318247, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.596723556518555, Accuracy = 0.8963668942451477\n",
      "Training iter #25716000:   Batch Loss = 7.240859, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5965118408203125, Accuracy = 0.8966646790504456\n",
      "Training iter #25719000:   Batch Loss = 7.211941, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.596318244934082, Accuracy = 0.8966646790504456\n",
      "Training iter #25722000:   Batch Loss = 7.254003, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.596133232116699, Accuracy = 0.8963668942451477\n",
      "Training iter #25725000:   Batch Loss = 7.274971, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.596060276031494, Accuracy = 0.8966646790504456\n",
      "Training iter #25728000:   Batch Loss = 7.272312, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595908164978027, Accuracy = 0.8966646790504456\n",
      "Training iter #25731000:   Batch Loss = 7.198474, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595810413360596, Accuracy = 0.8966646790504456\n",
      "Training iter #25734000:   Batch Loss = 7.237341, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595649242401123, Accuracy = 0.8966646790504456\n",
      "Training iter #25737000:   Batch Loss = 7.252007, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595541000366211, Accuracy = 0.8966646790504456\n",
      "Training iter #25740000:   Batch Loss = 7.304454, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595529556274414, Accuracy = 0.8966646790504456\n",
      "Training iter #25743000:   Batch Loss = 7.236778, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595544815063477, Accuracy = 0.8960691094398499\n",
      "Training iter #25746000:   Batch Loss = 7.205122, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595553874969482, Accuracy = 0.8960691094398499\n",
      "Training iter #25749000:   Batch Loss = 7.251638, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595466136932373, Accuracy = 0.8957712650299072\n",
      "Training iter #25752000:   Batch Loss = 7.282539, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.595183372497559, Accuracy = 0.8960691094398499\n",
      "Training iter #25755000:   Batch Loss = 7.255755, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.594764709472656, Accuracy = 0.8966646790504456\n",
      "Training iter #25758000:   Batch Loss = 7.198244, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.594435214996338, Accuracy = 0.8969624638557434\n",
      "Training iter #25761000:   Batch Loss = 7.238314, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.594052314758301, Accuracy = 0.8972602486610413\n",
      "Training iter #25764000:   Batch Loss = 7.262645, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.593710422515869, Accuracy = 0.8975580930709839\n",
      "Training iter #25767000:   Batch Loss = 7.314909, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.593563079833984, Accuracy = 0.8972602486610413\n",
      "Training iter #25770000:   Batch Loss = 7.229664, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5934367179870605, Accuracy = 0.8972602486610413\n",
      "Training iter #25773000:   Batch Loss = 7.203460, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.593339443206787, Accuracy = 0.8966646790504456\n",
      "Training iter #25776000:   Batch Loss = 7.252574, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.593193054199219, Accuracy = 0.8963668942451477\n",
      "Training iter #25779000:   Batch Loss = 7.279290, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592954158782959, Accuracy = 0.8963668942451477\n",
      "Training iter #25782000:   Batch Loss = 7.262321, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5927534103393555, Accuracy = 0.8966646790504456\n",
      "Training iter #25785000:   Batch Loss = 7.190879, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592620849609375, Accuracy = 0.8966646790504456\n",
      "Training iter #25788000:   Batch Loss = 7.238030, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592533588409424, Accuracy = 0.8969624638557434\n",
      "Training iter #25791000:   Batch Loss = 7.244470, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592229843139648, Accuracy = 0.8966646790504456\n",
      "Training iter #25794000:   Batch Loss = 7.314847, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592316150665283, Accuracy = 0.8966646790504456\n",
      "Training iter #25797000:   Batch Loss = 7.224889, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592314720153809, Accuracy = 0.8954734802246094\n",
      "Training iter #25800000:   Batch Loss = 7.198910, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592312335968018, Accuracy = 0.8951756954193115\n",
      "Training iter #25803000:   Batch Loss = 7.267618, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592206954956055, Accuracy = 0.8948779106140137\n",
      "Training iter #25806000:   Batch Loss = 7.274684, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592008590698242, Accuracy = 0.8957712650299072\n",
      "Training iter #25809000:   Batch Loss = 7.258378, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592072486877441, Accuracy = 0.8954734802246094\n",
      "Training iter #25812000:   Batch Loss = 7.192317, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.592034816741943, Accuracy = 0.8951756954193115\n",
      "Training iter #25815000:   Batch Loss = 7.238976, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.591887474060059, Accuracy = 0.8957712650299072\n",
      "Training iter #25818000:   Batch Loss = 7.246820, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5917181968688965, Accuracy = 0.8957712650299072\n",
      "Training iter #25821000:   Batch Loss = 7.309618, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.59172248840332, Accuracy = 0.8954734802246094\n",
      "Training iter #25824000:   Batch Loss = 7.214454, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.591710090637207, Accuracy = 0.8951756954193115\n",
      "Training iter #25827000:   Batch Loss = 7.198225, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.591665267944336, Accuracy = 0.8951756954193115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25830000:   Batch Loss = 7.266941, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.591468811035156, Accuracy = 0.8948779106140137\n",
      "Training iter #25833000:   Batch Loss = 7.270508, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.591100215911865, Accuracy = 0.8954734802246094\n",
      "Training iter #25836000:   Batch Loss = 7.264483, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.59067964553833, Accuracy = 0.8954734802246094\n",
      "Training iter #25839000:   Batch Loss = 7.194688, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.59027099609375, Accuracy = 0.8957712650299072\n",
      "Training iter #25842000:   Batch Loss = 7.237055, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5898237228393555, Accuracy = 0.8957712650299072\n",
      "Training iter #25845000:   Batch Loss = 7.245331, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.589634895324707, Accuracy = 0.8960691094398499\n",
      "Training iter #25848000:   Batch Loss = 7.310763, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.589980125427246, Accuracy = 0.8957712650299072\n",
      "Training iter #25851000:   Batch Loss = 7.198800, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.59023904800415, Accuracy = 0.8957712650299072\n",
      "Training iter #25854000:   Batch Loss = 7.191607, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.590413570404053, Accuracy = 0.8960691094398499\n",
      "Training iter #25857000:   Batch Loss = 7.274053, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.590481758117676, Accuracy = 0.8951756954193115\n",
      "Training iter #25860000:   Batch Loss = 7.280243, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.590157508850098, Accuracy = 0.8957712650299072\n",
      "Training iter #25863000:   Batch Loss = 7.260454, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.58963680267334, Accuracy = 0.8960691094398499\n",
      "Training iter #25866000:   Batch Loss = 7.201465, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.589158058166504, Accuracy = 0.8963668942451477\n",
      "Training iter #25869000:   Batch Loss = 7.235871, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.588639259338379, Accuracy = 0.8963668942451477\n",
      "Training iter #25872000:   Batch Loss = 7.252960, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.588142395019531, Accuracy = 0.8972602486610413\n",
      "Training iter #25875000:   Batch Loss = 7.293735, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.587924480438232, Accuracy = 0.8972602486610413\n",
      "Training iter #25878000:   Batch Loss = 7.193246, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5879130363464355, Accuracy = 0.8966646790504456\n",
      "Training iter #25881000:   Batch Loss = 7.194266, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.587966442108154, Accuracy = 0.8963668942451477\n",
      "Training iter #25884000:   Batch Loss = 7.267450, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.588042259216309, Accuracy = 0.8963668942451477\n",
      "Training iter #25887000:   Batch Loss = 7.275471, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.587852478027344, Accuracy = 0.8960691094398499\n",
      "Training iter #25890000:   Batch Loss = 7.254968, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.587634086608887, Accuracy = 0.8963668942451477\n",
      "Training iter #25893000:   Batch Loss = 7.200120, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5873918533325195, Accuracy = 0.8963668942451477\n",
      "Training iter #25896000:   Batch Loss = 7.234892, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.587120056152344, Accuracy = 0.8963668942451477\n",
      "Training iter #25899000:   Batch Loss = 7.256695, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.586902618408203, Accuracy = 0.8966646790504456\n",
      "Training iter #25902000:   Batch Loss = 7.286757, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.586785793304443, Accuracy = 0.8963668942451477\n",
      "Training iter #25905000:   Batch Loss = 7.185544, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.586724281311035, Accuracy = 0.8960691094398499\n",
      "Training iter #25908000:   Batch Loss = 7.192859, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5866923332214355, Accuracy = 0.8957712650299072\n",
      "Training iter #25911000:   Batch Loss = 7.257486, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.586657524108887, Accuracy = 0.8957712650299072\n",
      "Training iter #25914000:   Batch Loss = 7.286023, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5863871574401855, Accuracy = 0.8957712650299072\n",
      "Training iter #25917000:   Batch Loss = 7.259371, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.586115837097168, Accuracy = 0.8957712650299072\n",
      "Training iter #25920000:   Batch Loss = 7.207827, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5858683586120605, Accuracy = 0.8957712650299072\n",
      "Training iter #25923000:   Batch Loss = 7.237995, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5856194496154785, Accuracy = 0.8957712650299072\n",
      "Training iter #25926000:   Batch Loss = 7.253295, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5854973793029785, Accuracy = 0.8963668942451477\n",
      "Training iter #25929000:   Batch Loss = 7.271304, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585687160491943, Accuracy = 0.8963668942451477\n",
      "Training iter #25932000:   Batch Loss = 7.178025, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585831642150879, Accuracy = 0.8960691094398499\n",
      "Training iter #25935000:   Batch Loss = 7.201186, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585897445678711, Accuracy = 0.8960691094398499\n",
      "Training iter #25938000:   Batch Loss = 7.248498, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585970401763916, Accuracy = 0.8957712650299072\n",
      "Training iter #25941000:   Batch Loss = 7.284815, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585973739624023, Accuracy = 0.8954734802246094\n",
      "Training iter #25944000:   Batch Loss = 7.257930, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585946559906006, Accuracy = 0.8960691094398499\n",
      "Training iter #25947000:   Batch Loss = 7.207423, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585894584655762, Accuracy = 0.8960691094398499\n",
      "Training iter #25950000:   Batch Loss = 7.233397, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585745334625244, Accuracy = 0.8963668942451477\n",
      "Training iter #25953000:   Batch Loss = 7.247563, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585544586181641, Accuracy = 0.8960691094398499\n",
      "Training iter #25956000:   Batch Loss = 7.274148, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5853400230407715, Accuracy = 0.8963668942451477\n",
      "Training iter #25959000:   Batch Loss = 7.177562, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585191249847412, Accuracy = 0.8963668942451477\n",
      "Training iter #25962000:   Batch Loss = 7.206106, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.585066318511963, Accuracy = 0.8954734802246094\n",
      "Training iter #25965000:   Batch Loss = 7.245100, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.584918975830078, Accuracy = 0.8954734802246094\n",
      "Training iter #25968000:   Batch Loss = 7.297511, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.584609508514404, Accuracy = 0.8960691094398499\n",
      "Training iter #25971000:   Batch Loss = 7.240218, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.584127902984619, Accuracy = 0.8963668942451477\n",
      "Training iter #25974000:   Batch Loss = 7.200966, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.583644390106201, Accuracy = 0.8963668942451477\n",
      "Training iter #25977000:   Batch Loss = 7.242134, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.583187580108643, Accuracy = 0.8963668942451477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25980000:   Batch Loss = 7.249235, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.58288049697876, Accuracy = 0.8963668942451477\n",
      "Training iter #25983000:   Batch Loss = 7.270338, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.582706928253174, Accuracy = 0.8966646790504456\n",
      "Training iter #25986000:   Batch Loss = 7.180398, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.582744598388672, Accuracy = 0.8966646790504456\n",
      "Training iter #25989000:   Batch Loss = 7.202950, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.582734107971191, Accuracy = 0.8966646790504456\n",
      "Training iter #25992000:   Batch Loss = 7.237185, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.582727909088135, Accuracy = 0.8966646790504456\n",
      "Training iter #25995000:   Batch Loss = 7.289650, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.582579612731934, Accuracy = 0.8966646790504456\n",
      "Training iter #25998000:   Batch Loss = 7.227380, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.58243465423584, Accuracy = 0.8966646790504456\n",
      "Training iter #26001000:   Batch Loss = 7.194328, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5822601318359375, Accuracy = 0.8963668942451477\n",
      "Training iter #26004000:   Batch Loss = 7.247218, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5820231437683105, Accuracy = 0.8966646790504456\n",
      "Training iter #26007000:   Batch Loss = 7.253662, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.581946849822998, Accuracy = 0.8966646790504456\n",
      "Training iter #26010000:   Batch Loss = 7.267479, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.581882476806641, Accuracy = 0.8966646790504456\n",
      "Training iter #26013000:   Batch Loss = 7.182194, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.581881523132324, Accuracy = 0.8963668942451477\n",
      "Training iter #26016000:   Batch Loss = 7.202539, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.58184814453125, Accuracy = 0.8957712650299072\n",
      "Training iter #26019000:   Batch Loss = 7.235920, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.581796646118164, Accuracy = 0.8957712650299072\n",
      "Training iter #26022000:   Batch Loss = 7.293662, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.581813812255859, Accuracy = 0.8963668942451477\n",
      "Training iter #26025000:   Batch Loss = 7.221409, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5817790031433105, Accuracy = 0.8960691094398499\n",
      "Training iter #26028000:   Batch Loss = 7.192357, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5815935134887695, Accuracy = 0.8960691094398499\n",
      "Training iter #26031000:   Batch Loss = 7.250693, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.58128547668457, Accuracy = 0.8969624638557434\n",
      "Training iter #26034000:   Batch Loss = 7.251111, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.581021785736084, Accuracy = 0.8966646790504456\n",
      "Training iter #26037000:   Batch Loss = 7.259124, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.580931663513184, Accuracy = 0.8963668942451477\n",
      "Training iter #26040000:   Batch Loss = 7.182717, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.580795764923096, Accuracy = 0.8957712650299072\n",
      "Training iter #26043000:   Batch Loss = 7.201633, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.580563545227051, Accuracy = 0.8957712650299072\n",
      "Training iter #26046000:   Batch Loss = 7.230717, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.580393314361572, Accuracy = 0.8960691094398499\n",
      "Training iter #26049000:   Batch Loss = 7.295327, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.580131530761719, Accuracy = 0.8963668942451477\n",
      "Training iter #26052000:   Batch Loss = 7.217119, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.579806327819824, Accuracy = 0.8963668942451477\n",
      "Training iter #26055000:   Batch Loss = 7.193407, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.579339981079102, Accuracy = 0.8969624638557434\n",
      "Training iter #26058000:   Batch Loss = 7.233555, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5788702964782715, Accuracy = 0.8978558778762817\n",
      "Training iter #26061000:   Batch Loss = 7.267597, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.578654766082764, Accuracy = 0.8981536626815796\n",
      "Training iter #26064000:   Batch Loss = 7.251622, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.579158306121826, Accuracy = 0.8975580930709839\n",
      "Training iter #26067000:   Batch Loss = 7.184952, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.579675674438477, Accuracy = 0.8957712650299072\n",
      "Training iter #26070000:   Batch Loss = 7.218399, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.579898357391357, Accuracy = 0.8957712650299072\n",
      "Training iter #26073000:   Batch Loss = 7.240280, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.580422878265381, Accuracy = 0.8957712650299072\n",
      "Training iter #26076000:   Batch Loss = 7.292553, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.580053329467773, Accuracy = 0.8972602486610413\n",
      "Training iter #26079000:   Batch Loss = 7.213582, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.579720973968506, Accuracy = 0.8975580930709839\n",
      "Training iter #26082000:   Batch Loss = 7.186702, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.579361438751221, Accuracy = 0.8969624638557434\n",
      "Training iter #26085000:   Batch Loss = 7.231589, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.578875541687012, Accuracy = 0.8972602486610413\n",
      "Training iter #26088000:   Batch Loss = 7.257703, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.578425884246826, Accuracy = 0.8972602486610413\n",
      "Training iter #26091000:   Batch Loss = 7.243977, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.578022480010986, Accuracy = 0.8963668942451477\n",
      "Training iter #26094000:   Batch Loss = 7.178886, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.577792644500732, Accuracy = 0.8966646790504456\n",
      "Training iter #26097000:   Batch Loss = 7.219693, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.577563285827637, Accuracy = 0.8960691094398499\n",
      "Training iter #26100000:   Batch Loss = 7.241167, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.577375411987305, Accuracy = 0.8963668942451477\n",
      "Training iter #26103000:   Batch Loss = 7.291269, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.577171325683594, Accuracy = 0.8963668942451477\n",
      "Training iter #26106000:   Batch Loss = 7.204072, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.577053546905518, Accuracy = 0.8966646790504456\n",
      "Training iter #26109000:   Batch Loss = 7.185955, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.576888084411621, Accuracy = 0.8963668942451477\n",
      "Training iter #26112000:   Batch Loss = 7.248247, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.576574802398682, Accuracy = 0.8963668942451477\n",
      "Training iter #26115000:   Batch Loss = 7.258871, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5764288902282715, Accuracy = 0.8966646790504456\n",
      "Training iter #26118000:   Batch Loss = 7.239395, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.57634162902832, Accuracy = 0.8963668942451477\n",
      "Training iter #26121000:   Batch Loss = 7.176573, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.576426029205322, Accuracy = 0.8963668942451477\n",
      "Training iter #26124000:   Batch Loss = 7.218280, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.576429843902588, Accuracy = 0.8960691094398499\n",
      "Training iter #26127000:   Batch Loss = 7.225510, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.576427459716797, Accuracy = 0.8957712650299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #26130000:   Batch Loss = 7.293041, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.576224327087402, Accuracy = 0.8957712650299072\n",
      "Training iter #26133000:   Batch Loss = 7.203194, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.576002597808838, Accuracy = 0.8960691094398499\n",
      "Training iter #26136000:   Batch Loss = 7.182062, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5757856369018555, Accuracy = 0.8960691094398499\n",
      "Training iter #26139000:   Batch Loss = 7.247308, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575575828552246, Accuracy = 0.8969624638557434\n",
      "Training iter #26142000:   Batch Loss = 7.251831, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575498104095459, Accuracy = 0.8969624638557434\n",
      "Training iter #26145000:   Batch Loss = 7.244992, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575485706329346, Accuracy = 0.8969624638557434\n",
      "Training iter #26148000:   Batch Loss = 7.177007, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575593948364258, Accuracy = 0.8960691094398499\n",
      "Training iter #26151000:   Batch Loss = 7.218361, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575629234313965, Accuracy = 0.8954734802246094\n",
      "Training iter #26154000:   Batch Loss = 7.229034, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575638294219971, Accuracy = 0.8957712650299072\n",
      "Training iter #26157000:   Batch Loss = 7.290497, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575443744659424, Accuracy = 0.8957712650299072\n",
      "Training iter #26160000:   Batch Loss = 7.187503, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.575286388397217, Accuracy = 0.8960691094398499\n",
      "Training iter #26163000:   Batch Loss = 7.179999, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.574911117553711, Accuracy = 0.8966646790504456\n",
      "Training iter #26166000:   Batch Loss = 7.247714, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.57473087310791, Accuracy = 0.8969624638557434\n",
      "Training iter #26169000:   Batch Loss = 7.257786, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.574471950531006, Accuracy = 0.8972602486610413\n",
      "Training iter #26172000:   Batch Loss = 7.241605, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.574452877044678, Accuracy = 0.8969624638557434\n",
      "Training iter #26175000:   Batch Loss = 7.183115, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.574490070343018, Accuracy = 0.8966646790504456\n",
      "Training iter #26178000:   Batch Loss = 7.221670, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.574512004852295, Accuracy = 0.8960691094398499\n",
      "Training iter #26181000:   Batch Loss = 7.222240, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.574544429779053, Accuracy = 0.8954734802246094\n",
      "Training iter #26184000:   Batch Loss = 7.284121, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.574154376983643, Accuracy = 0.8957712650299072\n",
      "Training iter #26187000:   Batch Loss = 7.176572, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.573765754699707, Accuracy = 0.8966646790504456\n",
      "Training iter #26190000:   Batch Loss = 7.173231, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.573307991027832, Accuracy = 0.8966646790504456\n",
      "Training iter #26193000:   Batch Loss = 7.252713, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.572931289672852, Accuracy = 0.8972602486610413\n",
      "Training iter #26196000:   Batch Loss = 7.264416, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.572875499725342, Accuracy = 0.8975580930709839\n",
      "Training iter #26199000:   Batch Loss = 7.239564, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.573170185089111, Accuracy = 0.8972602486610413\n",
      "Training iter #26202000:   Batch Loss = 7.184764, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5733537673950195, Accuracy = 0.8972602486610413\n",
      "Training iter #26205000:   Batch Loss = 7.222143, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.573563575744629, Accuracy = 0.8966646790504456\n",
      "Training iter #26208000:   Batch Loss = 7.235325, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.573692321777344, Accuracy = 0.8966646790504456\n",
      "Training iter #26211000:   Batch Loss = 7.272122, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.573616981506348, Accuracy = 0.8966646790504456\n",
      "Training iter #26214000:   Batch Loss = 7.173622, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.573312282562256, Accuracy = 0.8972602486610413\n",
      "Training iter #26217000:   Batch Loss = 7.176927, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.572844982147217, Accuracy = 0.8975580930709839\n",
      "Training iter #26220000:   Batch Loss = 7.246243, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.572453022003174, Accuracy = 0.8975580930709839\n",
      "Training iter #26223000:   Batch Loss = 7.257233, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.572011470794678, Accuracy = 0.8975580930709839\n",
      "Training iter #26226000:   Batch Loss = 7.240131, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.57183837890625, Accuracy = 0.8975580930709839\n",
      "Training iter #26229000:   Batch Loss = 7.184101, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5716400146484375, Accuracy = 0.8978558778762817\n",
      "Training iter #26232000:   Batch Loss = 7.215949, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.571614742279053, Accuracy = 0.8972602486610413\n",
      "Training iter #26235000:   Batch Loss = 7.234789, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.571465492248535, Accuracy = 0.8969624638557434\n",
      "Training iter #26238000:   Batch Loss = 7.260952, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.571053981781006, Accuracy = 0.8969624638557434\n",
      "Training iter #26241000:   Batch Loss = 7.166301, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.570744037628174, Accuracy = 0.8972602486610413\n",
      "Training iter #26244000:   Batch Loss = 7.174377, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.570345401763916, Accuracy = 0.8975580930709839\n",
      "Training iter #26247000:   Batch Loss = 7.232217, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.570173740386963, Accuracy = 0.8975580930709839\n",
      "Training iter #26250000:   Batch Loss = 7.267551, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.570533752441406, Accuracy = 0.8972602486610413\n",
      "Training iter #26253000:   Batch Loss = 7.243700, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.571173191070557, Accuracy = 0.8969624638557434\n",
      "Training iter #26256000:   Batch Loss = 7.189715, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.571465969085693, Accuracy = 0.8972602486610413\n",
      "Training iter #26259000:   Batch Loss = 7.219941, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.571462631225586, Accuracy = 0.8972602486610413\n",
      "Training iter #26262000:   Batch Loss = 7.229882, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.57121467590332, Accuracy = 0.8972602486610413\n",
      "Training iter #26265000:   Batch Loss = 7.252977, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.570711135864258, Accuracy = 0.8969624638557434\n",
      "Training iter #26268000:   Batch Loss = 7.160106, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.570301055908203, Accuracy = 0.8969624638557434\n",
      "Training iter #26271000:   Batch Loss = 7.185225, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.569897651672363, Accuracy = 0.8969624638557434\n",
      "Training iter #26274000:   Batch Loss = 7.230045, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5695600509643555, Accuracy = 0.8969624638557434\n",
      "Training iter #26277000:   Batch Loss = 7.267209, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.56930685043335, Accuracy = 0.8969624638557434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #26280000:   Batch Loss = 7.231790, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.569141387939453, Accuracy = 0.8969624638557434\n",
      "Training iter #26283000:   Batch Loss = 7.188998, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.568897247314453, Accuracy = 0.8969624638557434\n",
      "Training iter #26286000:   Batch Loss = 7.216558, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.568623065948486, Accuracy = 0.8969624638557434\n",
      "Training iter #26289000:   Batch Loss = 7.226282, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.568608283996582, Accuracy = 0.8969624638557434\n",
      "Training iter #26292000:   Batch Loss = 7.249622, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.568262577056885, Accuracy = 0.8975580930709839\n",
      "Training iter #26295000:   Batch Loss = 7.165043, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.567953109741211, Accuracy = 0.8978558778762817\n",
      "Training iter #26298000:   Batch Loss = 7.187937, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.567546844482422, Accuracy = 0.8981536626815796\n",
      "Training iter #26301000:   Batch Loss = 7.223702, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.567355155944824, Accuracy = 0.8978558778762817\n",
      "Training iter #26304000:   Batch Loss = 7.269680, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.567236423492432, Accuracy = 0.8981536626815796\n",
      "Training iter #26307000:   Batch Loss = 7.221428, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.567117214202881, Accuracy = 0.8981536626815796\n",
      "Training iter #26310000:   Batch Loss = 7.182856, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.566976070404053, Accuracy = 0.8984514474868774\n",
      "Training iter #26313000:   Batch Loss = 7.229552, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.566833972930908, Accuracy = 0.8984514474868774\n",
      "Training iter #26316000:   Batch Loss = 7.232476, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.566706657409668, Accuracy = 0.8981536626815796\n",
      "Training iter #26319000:   Batch Loss = 7.250719, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.566167831420898, Accuracy = 0.8981536626815796\n",
      "Training iter #26322000:   Batch Loss = 7.160765, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.565865993499756, Accuracy = 0.8978558778762817\n",
      "Training iter #26325000:   Batch Loss = 7.185187, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.565555095672607, Accuracy = 0.8978558778762817\n",
      "Training iter #26328000:   Batch Loss = 7.218975, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5654730796813965, Accuracy = 0.8975580930709839\n",
      "Training iter #26331000:   Batch Loss = 7.272350, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.56535530090332, Accuracy = 0.8975580930709839\n",
      "Training iter #26334000:   Batch Loss = 7.206186, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.565311908721924, Accuracy = 0.8972602486610413\n",
      "Training iter #26337000:   Batch Loss = 7.177361, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5652241706848145, Accuracy = 0.8975580930709839\n",
      "Training iter #26340000:   Batch Loss = 7.230287, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.565131664276123, Accuracy = 0.8972602486610413\n",
      "Training iter #26343000:   Batch Loss = 7.230779, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.565048694610596, Accuracy = 0.8972602486610413\n",
      "Training iter #26346000:   Batch Loss = 7.245540, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564749717712402, Accuracy = 0.8972602486610413\n",
      "Training iter #26349000:   Batch Loss = 7.165717, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564592361450195, Accuracy = 0.8972602486610413\n",
      "Training iter #26352000:   Batch Loss = 7.186324, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564351558685303, Accuracy = 0.8972602486610413\n",
      "Training iter #26355000:   Batch Loss = 7.213523, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5642619132995605, Accuracy = 0.8972602486610413\n",
      "Training iter #26358000:   Batch Loss = 7.276054, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5643110275268555, Accuracy = 0.8972602486610413\n",
      "Training iter #26361000:   Batch Loss = 7.201375, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564488887786865, Accuracy = 0.8972602486610413\n",
      "Training iter #26364000:   Batch Loss = 7.176891, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564590930938721, Accuracy = 0.8972602486610413\n",
      "Training iter #26367000:   Batch Loss = 7.230359, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564581871032715, Accuracy = 0.8972602486610413\n",
      "Training iter #26370000:   Batch Loss = 7.235661, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564473628997803, Accuracy = 0.8972602486610413\n",
      "Training iter #26373000:   Batch Loss = 7.239063, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564149856567383, Accuracy = 0.8972602486610413\n",
      "Training iter #26376000:   Batch Loss = 7.163435, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.563920021057129, Accuracy = 0.8972602486610413\n",
      "Training iter #26379000:   Batch Loss = 7.199766, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.563619613647461, Accuracy = 0.8972602486610413\n",
      "Training iter #26382000:   Batch Loss = 7.213339, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.563510894775391, Accuracy = 0.8972602486610413\n",
      "Training iter #26385000:   Batch Loss = 7.270219, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.56386137008667, Accuracy = 0.8975580930709839\n",
      "Training iter #26388000:   Batch Loss = 7.196207, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564310550689697, Accuracy = 0.8972602486610413\n",
      "Training iter #26391000:   Batch Loss = 7.172577, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.56451416015625, Accuracy = 0.8972602486610413\n",
      "Training iter #26394000:   Batch Loss = 7.212389, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.56440544128418, Accuracy = 0.8972602486610413\n",
      "Training iter #26397000:   Batch Loss = 7.241141, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.564120769500732, Accuracy = 0.8972602486610413\n",
      "Training iter #26400000:   Batch Loss = 7.223490, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.563860893249512, Accuracy = 0.8972602486610413\n",
      "Training iter #26403000:   Batch Loss = 7.163459, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5637407302856445, Accuracy = 0.8972602486610413\n",
      "Training iter #26406000:   Batch Loss = 7.202011, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.563620567321777, Accuracy = 0.8969624638557434\n",
      "Training iter #26409000:   Batch Loss = 7.222987, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.563298225402832, Accuracy = 0.8969624638557434\n",
      "Training iter #26412000:   Batch Loss = 7.273109, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.562898635864258, Accuracy = 0.8972602486610413\n",
      "Training iter #26415000:   Batch Loss = 7.194530, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.562429428100586, Accuracy = 0.8981536626815796\n",
      "Training iter #26418000:   Batch Loss = 7.168942, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.56205415725708, Accuracy = 0.8981536626815796\n",
      "Training iter #26421000:   Batch Loss = 7.213535, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.561673164367676, Accuracy = 0.8981536626815796\n",
      "Training iter #26424000:   Batch Loss = 7.238091, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.561314582824707, Accuracy = 0.8981536626815796\n",
      "Training iter #26427000:   Batch Loss = 7.226551, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.561039447784424, Accuracy = 0.8981536626815796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #26430000:   Batch Loss = 7.156319, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.560848236083984, Accuracy = 0.8978558778762817\n",
      "Training iter #26433000:   Batch Loss = 7.202520, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.560461521148682, Accuracy = 0.8981536626815796\n",
      "Training iter #26436000:   Batch Loss = 7.219788, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.560377597808838, Accuracy = 0.8978558778762817\n",
      "Training iter #26439000:   Batch Loss = 7.271764, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.56103515625, Accuracy = 0.8984514474868774\n",
      "Training iter #26442000:   Batch Loss = 7.185871, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.561429977416992, Accuracy = 0.8981536626815796\n",
      "Training iter #26445000:   Batch Loss = 7.165028, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.561269283294678, Accuracy = 0.8981536626815796\n",
      "Training iter #26448000:   Batch Loss = 7.231075, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.561123847961426, Accuracy = 0.8978558778762817\n",
      "Training iter #26451000:   Batch Loss = 7.240367, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.561005592346191, Accuracy = 0.8978558778762817\n",
      "Training iter #26454000:   Batch Loss = 7.219963, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.560827732086182, Accuracy = 0.8972602486610413\n",
      "Training iter #26457000:   Batch Loss = 7.157121, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5607075691223145, Accuracy = 0.8972602486610413\n",
      "Training iter #26460000:   Batch Loss = 7.202537, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.560458660125732, Accuracy = 0.8972602486610413\n",
      "Training iter #26463000:   Batch Loss = 7.209855, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.560208320617676, Accuracy = 0.8969624638557434\n",
      "Training iter #26466000:   Batch Loss = 7.273318, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.560312747955322, Accuracy = 0.8972602486610413\n",
      "Training iter #26469000:   Batch Loss = 7.178030, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559792518615723, Accuracy = 0.8969624638557434\n",
      "Training iter #26472000:   Batch Loss = 7.163589, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559598445892334, Accuracy = 0.8969624638557434\n",
      "Training iter #26475000:   Batch Loss = 7.229899, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559584617614746, Accuracy = 0.8969624638557434\n",
      "Training iter #26478000:   Batch Loss = 7.232654, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559515476226807, Accuracy = 0.8972602486610413\n",
      "Training iter #26481000:   Batch Loss = 7.225668, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559683799743652, Accuracy = 0.8975580930709839\n",
      "Training iter #26484000:   Batch Loss = 7.159314, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559745788574219, Accuracy = 0.8972602486610413\n",
      "Training iter #26487000:   Batch Loss = 7.201303, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559988975524902, Accuracy = 0.8966646790504456\n",
      "Training iter #26490000:   Batch Loss = 7.205132, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559929847717285, Accuracy = 0.8963668942451477\n",
      "Training iter #26493000:   Batch Loss = 7.269311, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.559298038482666, Accuracy = 0.8963668942451477\n",
      "Training iter #26496000:   Batch Loss = 7.164057, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.558373928070068, Accuracy = 0.8969624638557434\n",
      "Training iter #26499000:   Batch Loss = 7.156270, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5576982498168945, Accuracy = 0.8972602486610413\n",
      "Training iter #26502000:   Batch Loss = 7.230626, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.557337760925293, Accuracy = 0.8972602486610413\n",
      "Training iter #26505000:   Batch Loss = 7.239227, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.557022571563721, Accuracy = 0.8972602486610413\n",
      "Training iter #26508000:   Batch Loss = 7.224217, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.556765079498291, Accuracy = 0.8972602486610413\n",
      "Training iter #26511000:   Batch Loss = 7.164652, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.556689262390137, Accuracy = 0.8975580930709839\n",
      "Training iter #26514000:   Batch Loss = 7.199779, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5565185546875, Accuracy = 0.8972602486610413\n",
      "Training iter #26517000:   Batch Loss = 7.206715, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.556532859802246, Accuracy = 0.8975580930709839\n",
      "Training iter #26520000:   Batch Loss = 7.258164, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.556832313537598, Accuracy = 0.8972602486610413\n",
      "Training iter #26523000:   Batch Loss = 7.157566, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5569658279418945, Accuracy = 0.8969624638557434\n",
      "Training iter #26526000:   Batch Loss = 7.154353, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.556941986083984, Accuracy = 0.8969624638557434\n",
      "Training iter #26529000:   Batch Loss = 7.228019, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.556921482086182, Accuracy = 0.8969624638557434\n",
      "Training iter #26532000:   Batch Loss = 7.235815, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5570969581604, Accuracy = 0.8972602486610413\n",
      "Training iter #26535000:   Batch Loss = 7.217630, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.557308673858643, Accuracy = 0.8975580930709839\n",
      "Training iter #26538000:   Batch Loss = 7.164714, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.557279586791992, Accuracy = 0.8978558778762817\n",
      "Training iter #26541000:   Batch Loss = 7.199205, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.55674934387207, Accuracy = 0.8978558778762817\n",
      "Training iter #26544000:   Batch Loss = 7.217587, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.555582046508789, Accuracy = 0.8981536626815796\n",
      "Training iter #26547000:   Batch Loss = 7.249024, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.555527210235596, Accuracy = 0.8975580930709839\n",
      "Training iter #26550000:   Batch Loss = 7.152318, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.55526876449585, Accuracy = 0.8975580930709839\n",
      "Training iter #26553000:   Batch Loss = 7.156348, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.554847240447998, Accuracy = 0.8975580930709839\n",
      "Training iter #26556000:   Batch Loss = 7.220800, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.554564952850342, Accuracy = 0.8972602486610413\n",
      "Training iter #26559000:   Batch Loss = 7.241302, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5543413162231445, Accuracy = 0.8975580930709839\n",
      "Training iter #26562000:   Batch Loss = 7.222081, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.554141998291016, Accuracy = 0.8978558778762817\n",
      "Training iter #26565000:   Batch Loss = 7.171489, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5539655685424805, Accuracy = 0.8978558778762817\n",
      "Training iter #26568000:   Batch Loss = 7.204448, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5538249015808105, Accuracy = 0.8978558778762817\n",
      "Training iter #26571000:   Batch Loss = 7.213851, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553804397583008, Accuracy = 0.8978558778762817\n",
      "Training iter #26574000:   Batch Loss = 7.232841, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553852081298828, Accuracy = 0.8975580930709839\n",
      "Training iter #26577000:   Batch Loss = 7.143560, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553761959075928, Accuracy = 0.8978558778762817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #26580000:   Batch Loss = 7.165457, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5537943840026855, Accuracy = 0.8978558778762817\n",
      "Training iter #26583000:   Batch Loss = 7.210043, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553983211517334, Accuracy = 0.8972602486610413\n",
      "Training iter #26586000:   Batch Loss = 7.246086, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.55396842956543, Accuracy = 0.8972602486610413\n",
      "Training iter #26589000:   Batch Loss = 7.220556, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553864002227783, Accuracy = 0.8972602486610413\n",
      "Training iter #26592000:   Batch Loss = 7.170638, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553567409515381, Accuracy = 0.8975580930709839\n",
      "Training iter #26595000:   Batch Loss = 7.197957, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553575038909912, Accuracy = 0.8975580930709839\n",
      "Training iter #26598000:   Batch Loss = 7.208899, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5535993576049805, Accuracy = 0.8975580930709839\n",
      "Training iter #26601000:   Batch Loss = 7.232947, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553608417510986, Accuracy = 0.8975580930709839\n",
      "Training iter #26604000:   Batch Loss = 7.142632, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5537261962890625, Accuracy = 0.8969624638557434\n",
      "Training iter #26607000:   Batch Loss = 7.169089, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553709506988525, Accuracy = 0.8969624638557434\n",
      "Training iter #26610000:   Batch Loss = 7.213304, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.553229808807373, Accuracy = 0.8972602486610413\n",
      "Training iter #26613000:   Batch Loss = 7.251157, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.552774429321289, Accuracy = 0.8972602486610413\n",
      "Training iter #26616000:   Batch Loss = 7.206305, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.552501201629639, Accuracy = 0.8966646790504456\n",
      "Training iter #26619000:   Batch Loss = 7.163865, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.552120208740234, Accuracy = 0.8966646790504456\n",
      "Training iter #26622000:   Batch Loss = 7.203804, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.551748752593994, Accuracy = 0.8966646790504456\n",
      "Training iter #26625000:   Batch Loss = 7.213029, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.551451206207275, Accuracy = 0.8972602486610413\n",
      "Training iter #26628000:   Batch Loss = 7.225176, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5513105392456055, Accuracy = 0.8972602486610413\n",
      "Training iter #26631000:   Batch Loss = 7.145191, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.551122188568115, Accuracy = 0.8969624638557434\n",
      "Training iter #26634000:   Batch Loss = 7.167831, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.550989627838135, Accuracy = 0.8966646790504456\n",
      "Training iter #26637000:   Batch Loss = 7.201329, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.55082368850708, Accuracy = 0.8972602486610413\n",
      "Training iter #26640000:   Batch Loss = 7.251821, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.55040168762207, Accuracy = 0.8975580930709839\n",
      "Training iter #26643000:   Batch Loss = 7.191277, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5499653816223145, Accuracy = 0.8972602486610413\n",
      "Training iter #26646000:   Batch Loss = 7.163065, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5495781898498535, Accuracy = 0.8972602486610413\n",
      "Training iter #26649000:   Batch Loss = 7.208734, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54930305480957, Accuracy = 0.8972602486610413\n",
      "Training iter #26652000:   Batch Loss = 7.215638, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54923152923584, Accuracy = 0.8972602486610413\n",
      "Training iter #26655000:   Batch Loss = 7.226364, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.549423694610596, Accuracy = 0.8969624638557434\n",
      "Training iter #26658000:   Batch Loss = 7.147147, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.549416542053223, Accuracy = 0.8969624638557434\n",
      "Training iter #26661000:   Batch Loss = 7.166838, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54929256439209, Accuracy = 0.8969624638557434\n",
      "Training iter #26664000:   Batch Loss = 7.199561, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.549110412597656, Accuracy = 0.8975580930709839\n",
      "Training iter #26667000:   Batch Loss = 7.253471, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.548890590667725, Accuracy = 0.8975580930709839\n",
      "Training iter #26670000:   Batch Loss = 7.185178, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.548697471618652, Accuracy = 0.8972602486610413\n",
      "Training iter #26673000:   Batch Loss = 7.156492, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54847526550293, Accuracy = 0.8972602486610413\n",
      "Training iter #26676000:   Batch Loss = 7.212676, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5482401847839355, Accuracy = 0.8972602486610413\n",
      "Training iter #26679000:   Batch Loss = 7.211259, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5480756759643555, Accuracy = 0.8975580930709839\n",
      "Training iter #26682000:   Batch Loss = 7.222286, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54789400100708, Accuracy = 0.8975580930709839\n",
      "Training iter #26685000:   Batch Loss = 7.146976, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5477447509765625, Accuracy = 0.8975580930709839\n",
      "Training iter #26688000:   Batch Loss = 7.165516, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.547508239746094, Accuracy = 0.8972602486610413\n",
      "Training iter #26691000:   Batch Loss = 7.194094, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.547279357910156, Accuracy = 0.8972602486610413\n",
      "Training iter #26694000:   Batch Loss = 7.255018, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.546969890594482, Accuracy = 0.8975580930709839\n",
      "Training iter #26697000:   Batch Loss = 7.181797, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.546763896942139, Accuracy = 0.8972602486610413\n",
      "Training iter #26700000:   Batch Loss = 7.156996, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.546555042266846, Accuracy = 0.8972602486610413\n",
      "Training iter #26703000:   Batch Loss = 7.193009, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5464582443237305, Accuracy = 0.8969624638557434\n",
      "Training iter #26706000:   Batch Loss = 7.214843, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.546431541442871, Accuracy = 0.8975580930709839\n",
      "Training iter #26709000:   Batch Loss = 7.213089, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.546021938323975, Accuracy = 0.8975580930709839\n",
      "Training iter #26712000:   Batch Loss = 7.144923, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5460004806518555, Accuracy = 0.8981536626815796\n",
      "Training iter #26715000:   Batch Loss = 7.182364, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545863628387451, Accuracy = 0.8978558778762817\n",
      "Training iter #26718000:   Batch Loss = 7.200217, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545687675476074, Accuracy = 0.8978558778762817\n",
      "Training iter #26721000:   Batch Loss = 7.247521, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545373439788818, Accuracy = 0.8984514474868774\n",
      "Training iter #26724000:   Batch Loss = 7.177804, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545088291168213, Accuracy = 0.8984514474868774\n",
      "Training iter #26727000:   Batch Loss = 7.150985, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.544832229614258, Accuracy = 0.8981536626815796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #26730000:   Batch Loss = 7.193559, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.544641971588135, Accuracy = 0.8978558778762817\n",
      "Training iter #26733000:   Batch Loss = 7.221101, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.544748306274414, Accuracy = 0.8978558778762817\n",
      "Training iter #26736000:   Batch Loss = 7.197062, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545022010803223, Accuracy = 0.8981536626815796\n",
      "Training iter #26739000:   Batch Loss = 7.141473, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5453877449035645, Accuracy = 0.8975580930709839\n",
      "Training iter #26742000:   Batch Loss = 7.183533, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545605182647705, Accuracy = 0.8972602486610413\n",
      "Training iter #26745000:   Batch Loss = 7.203251, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545711040496826, Accuracy = 0.8966646790504456\n",
      "Training iter #26748000:   Batch Loss = 7.251469, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.545306205749512, Accuracy = 0.8975580930709839\n",
      "Training iter #26751000:   Batch Loss = 7.168153, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.544768810272217, Accuracy = 0.8981536626815796\n",
      "Training iter #26754000:   Batch Loss = 7.149374, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5442118644714355, Accuracy = 0.8981536626815796\n",
      "Training iter #26757000:   Batch Loss = 7.194383, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54374361038208, Accuracy = 0.8978558778762817\n",
      "Training iter #26760000:   Batch Loss = 7.220181, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543682098388672, Accuracy = 0.8984514474868774\n",
      "Training iter #26763000:   Batch Loss = 7.202567, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543828010559082, Accuracy = 0.8981536626815796\n",
      "Training iter #26766000:   Batch Loss = 7.139555, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.544373512268066, Accuracy = 0.8978558778762817\n",
      "Training iter #26769000:   Batch Loss = 7.182623, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.544492721557617, Accuracy = 0.8975580930709839\n",
      "Training iter #26772000:   Batch Loss = 7.187449, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.544258117675781, Accuracy = 0.8969624638557434\n",
      "Training iter #26775000:   Batch Loss = 7.250910, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543957710266113, Accuracy = 0.8969624638557434\n",
      "Training iter #26778000:   Batch Loss = 7.166706, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543733596801758, Accuracy = 0.8972602486610413\n",
      "Training iter #26781000:   Batch Loss = 7.146718, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543473720550537, Accuracy = 0.8975580930709839\n",
      "Training iter #26784000:   Batch Loss = 7.205818, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543335437774658, Accuracy = 0.8972602486610413\n",
      "Training iter #26787000:   Batch Loss = 7.215703, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543574810028076, Accuracy = 0.8969624638557434\n",
      "Training iter #26790000:   Batch Loss = 7.207083, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543936729431152, Accuracy = 0.8969624638557434\n",
      "Training iter #26793000:   Batch Loss = 7.139784, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5443010330200195, Accuracy = 0.8966646790504456\n",
      "Training iter #26796000:   Batch Loss = 7.183585, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54437255859375, Accuracy = 0.8963668942451477\n",
      "Training iter #26799000:   Batch Loss = 7.189395, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.54425048828125, Accuracy = 0.8963668942451477\n",
      "Training iter #26802000:   Batch Loss = 7.248033, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.543553829193115, Accuracy = 0.8966646790504456\n",
      "Training iter #26805000:   Batch Loss = 7.152970, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.542698860168457, Accuracy = 0.8972602486610413\n",
      "Training iter #26808000:   Batch Loss = 7.144301, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.541965007781982, Accuracy = 0.8981536626815796\n",
      "Training iter #26811000:   Batch Loss = 7.209114, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.541443347930908, Accuracy = 0.8978558778762817\n",
      "Training iter #26814000:   Batch Loss = 7.219999, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5409016609191895, Accuracy = 0.8975580930709839\n",
      "Training iter #26817000:   Batch Loss = 7.204745, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.540849685668945, Accuracy = 0.8975580930709839\n",
      "Training iter #26820000:   Batch Loss = 7.146753, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.541407585144043, Accuracy = 0.8975580930709839\n",
      "Training iter #26823000:   Batch Loss = 7.183487, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.541797161102295, Accuracy = 0.8975580930709839\n",
      "Training iter #26826000:   Batch Loss = 7.185548, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.541691780090332, Accuracy = 0.8975580930709839\n",
      "Training iter #26829000:   Batch Loss = 7.246404, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.541229724884033, Accuracy = 0.8978558778762817\n",
      "Training iter #26832000:   Batch Loss = 7.141552, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.540314197540283, Accuracy = 0.8972602486610413\n",
      "Training iter #26835000:   Batch Loss = 7.139136, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.539617538452148, Accuracy = 0.8975580930709839\n",
      "Training iter #26838000:   Batch Loss = 7.213877, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.539069175720215, Accuracy = 0.8975580930709839\n",
      "Training iter #26841000:   Batch Loss = 7.222009, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5385518074035645, Accuracy = 0.8978558778762817\n",
      "Training iter #26844000:   Batch Loss = 7.201005, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.538383483886719, Accuracy = 0.8978558778762817\n",
      "Training iter #26847000:   Batch Loss = 7.147855, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.53815221786499, Accuracy = 0.8975580930709839\n",
      "Training iter #26850000:   Batch Loss = 7.185062, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.537932395935059, Accuracy = 0.8975580930709839\n",
      "Training iter #26853000:   Batch Loss = 7.194013, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.537962913513184, Accuracy = 0.8969624638557434\n",
      "Training iter #26856000:   Batch Loss = 7.230777, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.538025856018066, Accuracy = 0.8972602486610413\n",
      "Training iter #26859000:   Batch Loss = 7.138470, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.537833213806152, Accuracy = 0.8975580930709839\n",
      "Training iter #26862000:   Batch Loss = 7.141350, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.536831378936768, Accuracy = 0.8978558778762817\n",
      "Training iter #26865000:   Batch Loss = 7.207025, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5375823974609375, Accuracy = 0.8978558778762817\n",
      "Training iter #26868000:   Batch Loss = 7.220337, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.537517070770264, Accuracy = 0.8978558778762817\n",
      "Training iter #26871000:   Batch Loss = 7.200646, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5377960205078125, Accuracy = 0.8975580930709839\n",
      "Training iter #26874000:   Batch Loss = 7.149173, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.537811756134033, Accuracy = 0.8972602486610413\n",
      "Training iter #26877000:   Batch Loss = 7.185201, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.536706924438477, Accuracy = 0.8972602486610413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #26880000:   Batch Loss = 7.196603, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.535869598388672, Accuracy = 0.8972602486610413\n",
      "Training iter #26883000:   Batch Loss = 7.221794, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.535550594329834, Accuracy = 0.8981536626815796\n",
      "Training iter #26886000:   Batch Loss = 7.131399, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.535128593444824, Accuracy = 0.8981536626815796\n",
      "Training iter #26889000:   Batch Loss = 7.140392, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.534679412841797, Accuracy = 0.8981536626815796\n",
      "Training iter #26892000:   Batch Loss = 7.194253, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5363240242004395, Accuracy = 0.8981536626815796\n",
      "Training iter #26895000:   Batch Loss = 7.228420, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.536264419555664, Accuracy = 0.8981536626815796\n",
      "Training iter #26898000:   Batch Loss = 7.204500, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.536395072937012, Accuracy = 0.8981536626815796\n",
      "Training iter #26901000:   Batch Loss = 7.155583, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.536388397216797, Accuracy = 0.8975580930709839\n",
      "Training iter #26904000:   Batch Loss = 7.178717, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.536271572113037, Accuracy = 0.8975580930709839\n",
      "Training iter #26907000:   Batch Loss = 7.189132, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.53596305847168, Accuracy = 0.8975580930709839\n",
      "Training iter #26910000:   Batch Loss = 7.214280, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.535192966461182, Accuracy = 0.8978558778762817\n",
      "Training iter #26913000:   Batch Loss = 7.126449, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.534552574157715, Accuracy = 0.8975580930709839\n",
      "Training iter #26916000:   Batch Loss = 7.150373, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.533937454223633, Accuracy = 0.8975580930709839\n",
      "Training iter #26919000:   Batch Loss = 7.192221, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.533563137054443, Accuracy = 0.8975580930709839\n",
      "Training iter #26922000:   Batch Loss = 7.227291, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.533658027648926, Accuracy = 0.8972602486610413\n",
      "Training iter #26925000:   Batch Loss = 7.197940, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5339789390563965, Accuracy = 0.8972602486610413\n",
      "Training iter #26928000:   Batch Loss = 7.154084, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.533957481384277, Accuracy = 0.8972602486610413\n",
      "Training iter #26931000:   Batch Loss = 7.176484, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.532739162445068, Accuracy = 0.8975580930709839\n",
      "Training iter #26934000:   Batch Loss = 7.202950, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.534590721130371, Accuracy = 0.8972602486610413\n",
      "Training iter #26937000:   Batch Loss = 7.212365, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5346150398254395, Accuracy = 0.8978558778762817\n",
      "Training iter #26940000:   Batch Loss = 7.126674, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.535005569458008, Accuracy = 0.8969624638557434\n",
      "Training iter #26943000:   Batch Loss = 7.153094, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.534745216369629, Accuracy = 0.8969624638557434\n",
      "Training iter #26946000:   Batch Loss = 7.184659, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.534088134765625, Accuracy = 0.8969624638557434\n",
      "Training iter #26949000:   Batch Loss = 7.236765, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.534185886383057, Accuracy = 0.8969624638557434\n",
      "Training iter #26952000:   Batch Loss = 7.185991, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.532374858856201, Accuracy = 0.8975580930709839\n",
      "Training iter #26955000:   Batch Loss = 7.147563, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.53158712387085, Accuracy = 0.8975580930709839\n",
      "Training iter #26958000:   Batch Loss = 7.189973, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5308356285095215, Accuracy = 0.8975580930709839\n",
      "Training iter #26961000:   Batch Loss = 7.193547, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.530132293701172, Accuracy = 0.8981536626815796\n",
      "Training iter #26964000:   Batch Loss = 7.211594, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.528724193572998, Accuracy = 0.8990470767021179\n",
      "Training iter #26967000:   Batch Loss = 7.126310, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.527757167816162, Accuracy = 0.8993448615074158\n",
      "Training iter #26970000:   Batch Loss = 7.148288, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.527085304260254, Accuracy = 0.8993448615074158\n",
      "Training iter #26973000:   Batch Loss = 7.181756, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.526758193969727, Accuracy = 0.8996426463127136\n",
      "Training iter #26976000:   Batch Loss = 7.235220, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.526626110076904, Accuracy = 0.8996426463127136\n",
      "Training iter #26979000:   Batch Loss = 7.173994, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.526841163635254, Accuracy = 0.8990470767021179\n",
      "Training iter #26982000:   Batch Loss = 7.141484, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.527030944824219, Accuracy = 0.8987492322921753\n",
      "Training iter #26985000:   Batch Loss = 7.189011, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.527072906494141, Accuracy = 0.8975580930709839\n",
      "Training iter #26988000:   Batch Loss = 7.190816, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.527192115783691, Accuracy = 0.8978558778762817\n",
      "Training iter #26991000:   Batch Loss = 7.206486, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.527007102966309, Accuracy = 0.8978558778762817\n",
      "Training iter #26994000:   Batch Loss = 7.130649, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.526764392852783, Accuracy = 0.8972602486610413\n",
      "Training iter #26997000:   Batch Loss = 7.149882, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.526415824890137, Accuracy = 0.8966646790504456\n",
      "Training iter #27000000:   Batch Loss = 7.178776, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.526124477386475, Accuracy = 0.8966646790504456\n",
      "Training iter #27003000:   Batch Loss = 7.236247, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5259222984313965, Accuracy = 0.8969624638557434\n",
      "Training iter #27006000:   Batch Loss = 7.165228, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.525544166564941, Accuracy = 0.8972602486610413\n",
      "Training iter #27009000:   Batch Loss = 7.139117, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5254082679748535, Accuracy = 0.8972602486610413\n",
      "Training iter #27012000:   Batch Loss = 7.192368, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5253005027771, Accuracy = 0.8975580930709839\n",
      "Training iter #27015000:   Batch Loss = 7.191539, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.52518892288208, Accuracy = 0.8972602486610413\n",
      "Training iter #27018000:   Batch Loss = 7.199713, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5248494148254395, Accuracy = 0.8969624638557434\n",
      "Training iter #27021000:   Batch Loss = 7.129163, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524764537811279, Accuracy = 0.8978558778762817\n",
      "Training iter #27024000:   Batch Loss = 7.161842, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524643898010254, Accuracy = 0.8975580930709839\n",
      "Training iter #27027000:   Batch Loss = 7.178891, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524814605712891, Accuracy = 0.8975580930709839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27030000:   Batch Loss = 7.237743, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524585723876953, Accuracy = 0.8978558778762817\n",
      "Training iter #27033000:   Batch Loss = 7.161346, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5246171951293945, Accuracy = 0.8978558778762817\n",
      "Training iter #27036000:   Batch Loss = 7.139527, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524637699127197, Accuracy = 0.8978558778762817\n",
      "Training iter #27039000:   Batch Loss = 7.174495, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524460315704346, Accuracy = 0.8978558778762817\n",
      "Training iter #27042000:   Batch Loss = 7.202370, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524203777313232, Accuracy = 0.8978558778762817\n",
      "Training iter #27045000:   Batch Loss = 7.189134, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523980617523193, Accuracy = 0.8981536626815796\n",
      "Training iter #27048000:   Batch Loss = 7.128895, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523894786834717, Accuracy = 0.8984514474868774\n",
      "Training iter #27051000:   Batch Loss = 7.164164, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.52379035949707, Accuracy = 0.8984514474868774\n",
      "Training iter #27054000:   Batch Loss = 7.179879, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.52386474609375, Accuracy = 0.8984514474868774\n",
      "Training iter #27057000:   Batch Loss = 7.233333, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524013042449951, Accuracy = 0.8984514474868774\n",
      "Training iter #27060000:   Batch Loss = 7.157003, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524238586425781, Accuracy = 0.8978558778762817\n",
      "Training iter #27063000:   Batch Loss = 7.134139, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.52604341506958, Accuracy = 0.8975580930709839\n",
      "Training iter #27066000:   Batch Loss = 7.173860, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.525947570800781, Accuracy = 0.8975580930709839\n",
      "Training iter #27069000:   Batch Loss = 7.199186, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.525821208953857, Accuracy = 0.8975580930709839\n",
      "Training iter #27072000:   Batch Loss = 7.186426, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.525472640991211, Accuracy = 0.8975580930709839\n",
      "Training iter #27075000:   Batch Loss = 7.122452, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523414611816406, Accuracy = 0.8981536626815796\n",
      "Training iter #27078000:   Batch Loss = 7.164812, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523085594177246, Accuracy = 0.8981536626815796\n",
      "Training iter #27081000:   Batch Loss = 7.181962, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523037433624268, Accuracy = 0.8981536626815796\n",
      "Training iter #27084000:   Batch Loss = 7.230234, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5231614112854, Accuracy = 0.8978558778762817\n",
      "Training iter #27087000:   Batch Loss = 7.148296, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523332595825195, Accuracy = 0.8978558778762817\n",
      "Training iter #27090000:   Batch Loss = 7.131687, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523288249969482, Accuracy = 0.8978558778762817\n",
      "Training iter #27093000:   Batch Loss = 7.190086, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523626327514648, Accuracy = 0.8975580930709839\n",
      "Training iter #27096000:   Batch Loss = 7.198903, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523031711578369, Accuracy = 0.8981536626815796\n",
      "Training iter #27099000:   Batch Loss = 7.181162, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522906303405762, Accuracy = 0.8978558778762817\n",
      "Training iter #27102000:   Batch Loss = 7.122468, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522872447967529, Accuracy = 0.8975580930709839\n",
      "Training iter #27105000:   Batch Loss = 7.164785, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522816181182861, Accuracy = 0.8978558778762817\n",
      "Training iter #27108000:   Batch Loss = 7.169952, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522785663604736, Accuracy = 0.8978558778762817\n",
      "Training iter #27111000:   Batch Loss = 7.227609, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524643421173096, Accuracy = 0.8972602486610413\n",
      "Training iter #27114000:   Batch Loss = 7.141944, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524627685546875, Accuracy = 0.8966646790504456\n",
      "Training iter #27117000:   Batch Loss = 7.129833, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524415493011475, Accuracy = 0.8969624638557434\n",
      "Training iter #27120000:   Batch Loss = 7.188338, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524271011352539, Accuracy = 0.8969624638557434\n",
      "Training iter #27123000:   Batch Loss = 7.192333, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.524060249328613, Accuracy = 0.8969624638557434\n",
      "Training iter #27126000:   Batch Loss = 7.185285, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523694038391113, Accuracy = 0.8975580930709839\n",
      "Training iter #27129000:   Batch Loss = 7.124261, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.523371696472168, Accuracy = 0.8975580930709839\n",
      "Training iter #27132000:   Batch Loss = 7.163442, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.52299165725708, Accuracy = 0.8975580930709839\n",
      "Training iter #27135000:   Batch Loss = 7.166030, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522610187530518, Accuracy = 0.8975580930709839\n",
      "Training iter #27138000:   Batch Loss = 7.231489, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522365093231201, Accuracy = 0.8975580930709839\n",
      "Training iter #27141000:   Batch Loss = 7.132965, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5222296714782715, Accuracy = 0.8978558778762817\n",
      "Training iter #27144000:   Batch Loss = 7.122600, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.521988868713379, Accuracy = 0.8975580930709839\n",
      "Training iter #27147000:   Batch Loss = 7.189741, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522067546844482, Accuracy = 0.8975580930709839\n",
      "Training iter #27150000:   Batch Loss = 7.197342, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522718906402588, Accuracy = 0.8972602486610413\n",
      "Training iter #27153000:   Batch Loss = 7.183716, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522729873657227, Accuracy = 0.8975580930709839\n",
      "Training iter #27156000:   Batch Loss = 7.129087, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522609710693359, Accuracy = 0.8975580930709839\n",
      "Training iter #27159000:   Batch Loss = 7.163558, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.522419452667236, Accuracy = 0.8978558778762817\n",
      "Training iter #27162000:   Batch Loss = 7.162517, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.52214241027832, Accuracy = 0.8975580930709839\n",
      "Training iter #27165000:   Batch Loss = 7.218960, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.521812915802002, Accuracy = 0.8975580930709839\n",
      "Training iter #27168000:   Batch Loss = 7.122387, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.52154016494751, Accuracy = 0.8978558778762817\n",
      "Training iter #27171000:   Batch Loss = 7.120849, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.520960807800293, Accuracy = 0.8978558778762817\n",
      "Training iter #27174000:   Batch Loss = 7.189034, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.520208358764648, Accuracy = 0.8978558778762817\n",
      "Training iter #27177000:   Batch Loss = 7.199757, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.519838333129883, Accuracy = 0.8975580930709839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27180000:   Batch Loss = 7.179185, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.519556045532227, Accuracy = 0.8975580930709839\n",
      "Training iter #27183000:   Batch Loss = 7.130299, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.519378185272217, Accuracy = 0.8972602486610413\n",
      "Training iter #27186000:   Batch Loss = 7.162223, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.519242763519287, Accuracy = 0.8969624638557434\n",
      "Training iter #27189000:   Batch Loss = 7.175320, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.519259929656982, Accuracy = 0.8966646790504456\n",
      "Training iter #27192000:   Batch Loss = 7.208947, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.519283294677734, Accuracy = 0.8969624638557434\n",
      "Training iter #27195000:   Batch Loss = 7.119347, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.519123554229736, Accuracy = 0.8975580930709839\n",
      "Training iter #27198000:   Batch Loss = 7.123467, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.518815040588379, Accuracy = 0.8975580930709839\n",
      "Training iter #27201000:   Batch Loss = 7.187081, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.518383979797363, Accuracy = 0.8981536626815796\n",
      "Training iter #27204000:   Batch Loss = 7.199552, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517986297607422, Accuracy = 0.8981536626815796\n",
      "Training iter #27207000:   Batch Loss = 7.183016, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517672538757324, Accuracy = 0.8981536626815796\n",
      "Training iter #27210000:   Batch Loss = 7.135767, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5174384117126465, Accuracy = 0.8981536626815796\n",
      "Training iter #27213000:   Batch Loss = 7.167154, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517230033874512, Accuracy = 0.8984514474868774\n",
      "Training iter #27216000:   Batch Loss = 7.174798, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517144680023193, Accuracy = 0.8975580930709839\n",
      "Training iter #27219000:   Batch Loss = 7.194208, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517216205596924, Accuracy = 0.8972602486610413\n",
      "Training iter #27222000:   Batch Loss = 7.110984, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517372131347656, Accuracy = 0.8966646790504456\n",
      "Training iter #27225000:   Batch Loss = 7.129620, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517683982849121, Accuracy = 0.8966646790504456\n",
      "Training iter #27228000:   Batch Loss = 7.177215, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517930507659912, Accuracy = 0.8969624638557434\n",
      "Training iter #27231000:   Batch Loss = 7.207921, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.518203258514404, Accuracy = 0.8972602486610413\n",
      "Training iter #27234000:   Batch Loss = 7.183835, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.518404483795166, Accuracy = 0.8972602486610413\n",
      "Training iter #27237000:   Batch Loss = 7.134804, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5185418128967285, Accuracy = 0.8972602486610413\n",
      "Training iter #27240000:   Batch Loss = 7.162545, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5182929039001465, Accuracy = 0.8963668942451477\n",
      "Training iter #27243000:   Batch Loss = 7.168884, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.517124652862549, Accuracy = 0.8966646790504456\n",
      "Training iter #27246000:   Batch Loss = 7.193284, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.516044616699219, Accuracy = 0.8969624638557434\n",
      "Training iter #27249000:   Batch Loss = 7.107925, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.515517234802246, Accuracy = 0.8972602486610413\n",
      "Training iter #27252000:   Batch Loss = 7.134233, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.515110969543457, Accuracy = 0.8963668942451477\n",
      "Training iter #27255000:   Batch Loss = 7.173791, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.514890193939209, Accuracy = 0.8963668942451477\n",
      "Training iter #27258000:   Batch Loss = 7.209310, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5147552490234375, Accuracy = 0.8975580930709839\n",
      "Training iter #27261000:   Batch Loss = 7.172171, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.514734268188477, Accuracy = 0.8981536626815796\n",
      "Training iter #27264000:   Batch Loss = 7.132958, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.514660358428955, Accuracy = 0.8978558778762817\n",
      "Training iter #27267000:   Batch Loss = 7.166099, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.514502048492432, Accuracy = 0.8978558778762817\n",
      "Training iter #27270000:   Batch Loss = 7.167938, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.514288425445557, Accuracy = 0.8969624638557434\n",
      "Training iter #27273000:   Batch Loss = 7.187878, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5140557289123535, Accuracy = 0.8966646790504456\n",
      "Training iter #27276000:   Batch Loss = 7.112349, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.513827323913574, Accuracy = 0.8969624638557434\n",
      "Training iter #27279000:   Batch Loss = 7.132552, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.513439178466797, Accuracy = 0.8972602486610413\n",
      "Training iter #27282000:   Batch Loss = 7.167768, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5133256912231445, Accuracy = 0.8969624638557434\n",
      "Training iter #27285000:   Batch Loss = 7.211567, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.513154983520508, Accuracy = 0.8969624638557434\n",
      "Training iter #27288000:   Batch Loss = 7.154321, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5129852294921875, Accuracy = 0.8972602486610413\n",
      "Training iter #27291000:   Batch Loss = 7.128380, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.512720584869385, Accuracy = 0.8972602486610413\n",
      "Training iter #27294000:   Batch Loss = 7.172449, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.512553691864014, Accuracy = 0.8975580930709839\n",
      "Training iter #27297000:   Batch Loss = 7.173480, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.51251745223999, Accuracy = 0.8975580930709839\n",
      "Training iter #27300000:   Batch Loss = 7.188356, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5125885009765625, Accuracy = 0.8975580930709839\n",
      "Training iter #27303000:   Batch Loss = 7.112946, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.512510776519775, Accuracy = 0.8969624638557434\n",
      "Training iter #27306000:   Batch Loss = 7.131886, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.512279033660889, Accuracy = 0.8963668942451477\n",
      "Training iter #27309000:   Batch Loss = 7.163663, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.512148857116699, Accuracy = 0.8969624638557434\n",
      "Training iter #27312000:   Batch Loss = 7.212602, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.511873722076416, Accuracy = 0.8969624638557434\n",
      "Training iter #27315000:   Batch Loss = 7.149661, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.511590003967285, Accuracy = 0.8969624638557434\n",
      "Training iter #27318000:   Batch Loss = 7.121969, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.511286735534668, Accuracy = 0.8969624638557434\n",
      "Training iter #27321000:   Batch Loss = 7.173530, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.511020660400391, Accuracy = 0.8972602486610413\n",
      "Training iter #27324000:   Batch Loss = 7.169091, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.510822772979736, Accuracy = 0.8975580930709839\n",
      "Training iter #27327000:   Batch Loss = 7.183197, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.51084041595459, Accuracy = 0.8972602486610413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27330000:   Batch Loss = 7.112570, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5108537673950195, Accuracy = 0.8969624638557434\n",
      "Training iter #27333000:   Batch Loss = 7.131999, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.510770797729492, Accuracy = 0.8969624638557434\n",
      "Training iter #27336000:   Batch Loss = 7.157248, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5108489990234375, Accuracy = 0.8963668942451477\n",
      "Training iter #27339000:   Batch Loss = 7.216808, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.510729789733887, Accuracy = 0.8963668942451477\n",
      "Training iter #27342000:   Batch Loss = 7.145008, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.510605335235596, Accuracy = 0.8969624638557434\n",
      "Training iter #27345000:   Batch Loss = 7.122345, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5103912353515625, Accuracy = 0.8969624638557434\n",
      "Training iter #27348000:   Batch Loss = 7.157415, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.510049343109131, Accuracy = 0.8972602486610413\n",
      "Training iter #27351000:   Batch Loss = 7.172660, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.509869575500488, Accuracy = 0.8969624638557434\n",
      "Training iter #27354000:   Batch Loss = 7.173622, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.509729385375977, Accuracy = 0.8972602486610413\n",
      "Training iter #27357000:   Batch Loss = 7.110504, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.509588241577148, Accuracy = 0.8972602486610413\n",
      "Training iter #27360000:   Batch Loss = 7.146224, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.509407997131348, Accuracy = 0.8966646790504456\n",
      "Training iter #27363000:   Batch Loss = 7.158527, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.509304046630859, Accuracy = 0.8966646790504456\n",
      "Training iter #27366000:   Batch Loss = 7.202924, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5090837478637695, Accuracy = 0.8972602486610413\n",
      "Training iter #27369000:   Batch Loss = 7.142306, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.509033679962158, Accuracy = 0.8972602486610413\n",
      "Training iter #27372000:   Batch Loss = 7.116592, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.508922576904297, Accuracy = 0.8969624638557434\n",
      "Training iter #27375000:   Batch Loss = 7.155114, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.508786678314209, Accuracy = 0.8966646790504456\n",
      "Training iter #27378000:   Batch Loss = 7.183134, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.50872802734375, Accuracy = 0.8966646790504456\n",
      "Training iter #27381000:   Batch Loss = 7.159670, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.508589744567871, Accuracy = 0.8963668942451477\n",
      "Training iter #27384000:   Batch Loss = 7.110814, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.508751392364502, Accuracy = 0.8957712650299072\n",
      "Training iter #27387000:   Batch Loss = 7.147036, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.508572578430176, Accuracy = 0.8957712650299072\n",
      "Training iter #27390000:   Batch Loss = 7.164911, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.508397579193115, Accuracy = 0.8960691094398499\n",
      "Training iter #27393000:   Batch Loss = 7.211087, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5080695152282715, Accuracy = 0.8960691094398499\n",
      "Training iter #27396000:   Batch Loss = 7.134641, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.507895469665527, Accuracy = 0.8966646790504456\n",
      "Training iter #27399000:   Batch Loss = 7.114667, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.507603168487549, Accuracy = 0.8969624638557434\n",
      "Training iter #27402000:   Batch Loss = 7.156015, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.507099151611328, Accuracy = 0.8972602486610413\n",
      "Training iter #27405000:   Batch Loss = 7.180184, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5071210861206055, Accuracy = 0.8969624638557434\n",
      "Training iter #27408000:   Batch Loss = 7.165088, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.507178783416748, Accuracy = 0.8966646790504456\n",
      "Training iter #27411000:   Batch Loss = 7.105059, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.507129669189453, Accuracy = 0.8963668942451477\n",
      "Training iter #27414000:   Batch Loss = 7.146771, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.506979942321777, Accuracy = 0.8960691094398499\n",
      "Training iter #27417000:   Batch Loss = 7.147881, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.50678014755249, Accuracy = 0.8966646790504456\n",
      "Training iter #27420000:   Batch Loss = 7.209948, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.506539821624756, Accuracy = 0.8966646790504456\n",
      "Training iter #27423000:   Batch Loss = 7.130625, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.506430149078369, Accuracy = 0.8972602486610413\n",
      "Training iter #27426000:   Batch Loss = 7.110971, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.506257057189941, Accuracy = 0.8972602486610413\n",
      "Training iter #27429000:   Batch Loss = 7.169794, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.506003379821777, Accuracy = 0.8972602486610413\n",
      "Training iter #27432000:   Batch Loss = 7.176450, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.505938529968262, Accuracy = 0.8969624638557434\n",
      "Training iter #27435000:   Batch Loss = 7.161834, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.505895614624023, Accuracy = 0.8966646790504456\n",
      "Training iter #27438000:   Batch Loss = 7.105011, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.505883693695068, Accuracy = 0.8966646790504456\n",
      "Training iter #27441000:   Batch Loss = 7.147501, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.505638122558594, Accuracy = 0.8960691094398499\n",
      "Training iter #27444000:   Batch Loss = 7.149324, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.505380630493164, Accuracy = 0.8963668942451477\n",
      "Training iter #27447000:   Batch Loss = 7.205366, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5047454833984375, Accuracy = 0.8966646790504456\n",
      "Training iter #27450000:   Batch Loss = 7.122327, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.504161357879639, Accuracy = 0.8972602486610413\n",
      "Training iter #27453000:   Batch Loss = 7.110393, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.503762245178223, Accuracy = 0.8972602486610413\n",
      "Training iter #27456000:   Batch Loss = 7.169311, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.503422260284424, Accuracy = 0.8975580930709839\n",
      "Training iter #27459000:   Batch Loss = 7.172902, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.50332498550415, Accuracy = 0.8975580930709839\n",
      "Training iter #27462000:   Batch Loss = 7.166492, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.503352165222168, Accuracy = 0.8975580930709839\n",
      "Training iter #27465000:   Batch Loss = 7.106829, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.503373622894287, Accuracy = 0.8969624638557434\n",
      "Training iter #27468000:   Batch Loss = 7.145548, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.503416061401367, Accuracy = 0.8969624638557434\n",
      "Training iter #27471000:   Batch Loss = 7.147049, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.503415584564209, Accuracy = 0.8966646790504456\n",
      "Training iter #27474000:   Batch Loss = 7.205564, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.503020763397217, Accuracy = 0.8969624638557434\n",
      "Training iter #27477000:   Batch Loss = 7.108145, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.502649784088135, Accuracy = 0.8969624638557434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27480000:   Batch Loss = 7.104189, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.502253532409668, Accuracy = 0.8972602486610413\n",
      "Training iter #27483000:   Batch Loss = 7.174431, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501906394958496, Accuracy = 0.8972602486610413\n",
      "Training iter #27486000:   Batch Loss = 7.181450, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501748561859131, Accuracy = 0.8972602486610413\n",
      "Training iter #27489000:   Batch Loss = 7.163537, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501979827880859, Accuracy = 0.8969624638557434\n",
      "Training iter #27492000:   Batch Loss = 7.112967, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.502169132232666, Accuracy = 0.8966646790504456\n",
      "Training iter #27495000:   Batch Loss = 7.144134, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5023040771484375, Accuracy = 0.8966646790504456\n",
      "Training iter #27498000:   Batch Loss = 7.153490, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.502388954162598, Accuracy = 0.8969624638557434\n",
      "Training iter #27501000:   Batch Loss = 7.190142, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5020751953125, Accuracy = 0.8966646790504456\n",
      "Training iter #27504000:   Batch Loss = 7.103812, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.50179386138916, Accuracy = 0.8963668942451477\n",
      "Training iter #27507000:   Batch Loss = 7.106060, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501421928405762, Accuracy = 0.8969624638557434\n",
      "Training iter #27510000:   Batch Loss = 7.168417, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501118183135986, Accuracy = 0.8969624638557434\n",
      "Training iter #27513000:   Batch Loss = 7.177633, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501096248626709, Accuracy = 0.8969624638557434\n",
      "Training iter #27516000:   Batch Loss = 7.158602, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501387119293213, Accuracy = 0.8969624638557434\n",
      "Training iter #27519000:   Batch Loss = 7.111690, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501537799835205, Accuracy = 0.8966646790504456\n",
      "Training iter #27522000:   Batch Loss = 7.143525, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501641273498535, Accuracy = 0.8966646790504456\n",
      "Training iter #27525000:   Batch Loss = 7.156434, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501559257507324, Accuracy = 0.8966646790504456\n",
      "Training iter #27528000:   Batch Loss = 7.184441, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.501046657562256, Accuracy = 0.8969624638557434\n",
      "Training iter #27531000:   Batch Loss = 7.097179, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.500595569610596, Accuracy = 0.8969624638557434\n",
      "Training iter #27534000:   Batch Loss = 7.105079, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.500036716461182, Accuracy = 0.8966646790504456\n",
      "Training iter #27537000:   Batch Loss = 7.157598, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499575138092041, Accuracy = 0.8969624638557434\n",
      "Training iter #27540000:   Batch Loss = 7.186226, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499317169189453, Accuracy = 0.8972602486610413\n",
      "Training iter #27543000:   Batch Loss = 7.162350, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499295711517334, Accuracy = 0.8975580930709839\n",
      "Training iter #27546000:   Batch Loss = 7.117253, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499171733856201, Accuracy = 0.8978558778762817\n",
      "Training iter #27549000:   Batch Loss = 7.146325, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.498987674713135, Accuracy = 0.8975580930709839\n",
      "Training iter #27552000:   Batch Loss = 7.153581, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4991774559021, Accuracy = 0.8969624638557434\n",
      "Training iter #27555000:   Batch Loss = 7.171031, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499021530151367, Accuracy = 0.8966646790504456\n",
      "Training iter #27558000:   Batch Loss = 7.091184, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.498662948608398, Accuracy = 0.8972602486610413\n",
      "Training iter #27561000:   Batch Loss = 7.111960, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4985551834106445, Accuracy = 0.8969624638557434\n",
      "Training iter #27564000:   Batch Loss = 7.152202, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.498633861541748, Accuracy = 0.8960691094398499\n",
      "Training iter #27567000:   Batch Loss = 7.185893, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4988932609558105, Accuracy = 0.8963668942451477\n",
      "Training iter #27570000:   Batch Loss = 7.160079, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499448299407959, Accuracy = 0.8966646790504456\n",
      "Training iter #27573000:   Batch Loss = 7.116706, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499912261962891, Accuracy = 0.8963668942451477\n",
      "Training iter #27576000:   Batch Loss = 7.142266, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.500241279602051, Accuracy = 0.8960691094398499\n",
      "Training iter #27579000:   Batch Loss = 7.147568, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.5000715255737305, Accuracy = 0.8960691094398499\n",
      "Training iter #27582000:   Batch Loss = 7.172832, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499502182006836, Accuracy = 0.8963668942451477\n",
      "Training iter #27585000:   Batch Loss = 7.090779, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.499134540557861, Accuracy = 0.8960691094398499\n",
      "Training iter #27588000:   Batch Loss = 7.116169, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4987030029296875, Accuracy = 0.8966646790504456\n",
      "Training iter #27591000:   Batch Loss = 7.148949, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.498115539550781, Accuracy = 0.8963668942451477\n",
      "Training iter #27594000:   Batch Loss = 7.196923, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.497585296630859, Accuracy = 0.8963668942451477\n",
      "Training iter #27597000:   Batch Loss = 7.144881, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.497626781463623, Accuracy = 0.8969624638557434\n",
      "Training iter #27600000:   Batch Loss = 7.110418, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.497454643249512, Accuracy = 0.8972602486610413\n",
      "Training iter #27603000:   Batch Loss = 7.148256, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4971842765808105, Accuracy = 0.8975580930709839\n",
      "Training iter #27606000:   Batch Loss = 7.149198, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.494746208190918, Accuracy = 0.8981536626815796\n",
      "Training iter #27609000:   Batch Loss = 7.170298, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.492670059204102, Accuracy = 0.8990470767021179\n",
      "Training iter #27612000:   Batch Loss = 7.096919, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490394592285156, Accuracy = 0.8990470767021179\n",
      "Training iter #27615000:   Batch Loss = 7.111259, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.489661693572998, Accuracy = 0.8993448615074158\n",
      "Training iter #27618000:   Batch Loss = 7.145891, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.489259243011475, Accuracy = 0.8987492322921753\n",
      "Training iter #27621000:   Batch Loss = 7.188889, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.489538192749023, Accuracy = 0.8993448615074158\n",
      "Training iter #27624000:   Batch Loss = 7.136093, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.49110221862793, Accuracy = 0.8984514474868774\n",
      "Training iter #27627000:   Batch Loss = 7.105291, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.492039203643799, Accuracy = 0.8969624638557434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27630000:   Batch Loss = 7.155332, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.494537353515625, Accuracy = 0.8969624638557434\n",
      "Training iter #27633000:   Batch Loss = 7.163585, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4939680099487305, Accuracy = 0.8972602486610413\n",
      "Training iter #27636000:   Batch Loss = 7.170954, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.491271018981934, Accuracy = 0.8987492322921753\n",
      "Training iter #27639000:   Batch Loss = 7.099072, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.48877477645874, Accuracy = 0.8981536626815796\n",
      "Training iter #27642000:   Batch Loss = 7.111697, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488124847412109, Accuracy = 0.8972602486610413\n",
      "Training iter #27645000:   Batch Loss = 7.146245, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487854480743408, Accuracy = 0.8963668942451477\n",
      "Training iter #27648000:   Batch Loss = 7.199422, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488844871520996, Accuracy = 0.8969624638557434\n",
      "Training iter #27651000:   Batch Loss = 7.145580, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490389823913574, Accuracy = 0.8969624638557434\n",
      "Training iter #27654000:   Batch Loss = 7.105116, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490851879119873, Accuracy = 0.8975580930709839\n",
      "Training iter #27657000:   Batch Loss = 7.166898, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.491021156311035, Accuracy = 0.8975580930709839\n",
      "Training iter #27660000:   Batch Loss = 7.160985, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490969657897949, Accuracy = 0.8969624638557434\n",
      "Training iter #27663000:   Batch Loss = 7.166627, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490649700164795, Accuracy = 0.8972602486610413\n",
      "Training iter #27666000:   Batch Loss = 7.104030, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.492264747619629, Accuracy = 0.8972602486610413\n",
      "Training iter #27669000:   Batch Loss = 7.111632, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.494174003601074, Accuracy = 0.8966646790504456\n",
      "Training iter #27672000:   Batch Loss = 7.141366, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.494555950164795, Accuracy = 0.8966646790504456\n",
      "Training iter #27675000:   Batch Loss = 7.197619, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.494571208953857, Accuracy = 0.8972602486610413\n",
      "Training iter #27678000:   Batch Loss = 7.140216, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.493716239929199, Accuracy = 0.8972602486610413\n",
      "Training iter #27681000:   Batch Loss = 7.107703, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.492645740509033, Accuracy = 0.8957712650299072\n",
      "Training iter #27684000:   Batch Loss = 7.139319, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.491977214813232, Accuracy = 0.8963668942451477\n",
      "Training iter #27687000:   Batch Loss = 7.175636, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4908037185668945, Accuracy = 0.8966646790504456\n",
      "Training iter #27690000:   Batch Loss = 7.158031, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.489656448364258, Accuracy = 0.8966646790504456\n",
      "Training iter #27693000:   Batch Loss = 7.102196, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.489349365234375, Accuracy = 0.8969624638557434\n",
      "Training iter #27696000:   Batch Loss = 7.128366, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4890947341918945, Accuracy = 0.8966646790504456\n",
      "Training iter #27699000:   Batch Loss = 7.144648, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488964557647705, Accuracy = 0.8963668942451477\n",
      "Training iter #27702000:   Batch Loss = 7.192413, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487911224365234, Accuracy = 0.8966646790504456\n",
      "Training iter #27705000:   Batch Loss = 7.130380, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487534046173096, Accuracy = 0.8969624638557434\n",
      "Training iter #27708000:   Batch Loss = 7.099936, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488499164581299, Accuracy = 0.8972602486610413\n",
      "Training iter #27711000:   Batch Loss = 7.136728, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488711833953857, Accuracy = 0.8978558778762817\n",
      "Training iter #27714000:   Batch Loss = 7.165367, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488846778869629, Accuracy = 0.8981536626815796\n",
      "Training iter #27717000:   Batch Loss = 7.149253, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488752365112305, Accuracy = 0.8981536626815796\n",
      "Training iter #27720000:   Batch Loss = 7.091640, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488588333129883, Accuracy = 0.8978558778762817\n",
      "Training iter #27723000:   Batch Loss = 7.128046, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488090515136719, Accuracy = 0.8975580930709839\n",
      "Training iter #27726000:   Batch Loss = 7.145365, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487584590911865, Accuracy = 0.8975580930709839\n",
      "Training iter #27729000:   Batch Loss = 7.190633, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487401962280273, Accuracy = 0.8981536626815796\n",
      "Training iter #27732000:   Batch Loss = 7.120984, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487884044647217, Accuracy = 0.8978558778762817\n",
      "Training iter #27735000:   Batch Loss = 7.110450, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.48762321472168, Accuracy = 0.8975580930709839\n",
      "Training iter #27738000:   Batch Loss = 7.151814, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.48862886428833, Accuracy = 0.8966646790504456\n",
      "Training iter #27741000:   Batch Loss = 7.164700, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490956783294678, Accuracy = 0.8963668942451477\n",
      "Training iter #27744000:   Batch Loss = 7.144783, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.492537498474121, Accuracy = 0.8957712650299072\n",
      "Training iter #27747000:   Batch Loss = 7.101519, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.492780685424805, Accuracy = 0.8954734802246094\n",
      "Training iter #27750000:   Batch Loss = 7.140375, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490662574768066, Accuracy = 0.8957712650299072\n",
      "Training iter #27753000:   Batch Loss = 7.131485, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490383148193359, Accuracy = 0.8966646790504456\n",
      "Training iter #27756000:   Batch Loss = 7.190496, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490361213684082, Accuracy = 0.8963668942451477\n",
      "Training iter #27759000:   Batch Loss = 7.117262, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.490442276000977, Accuracy = 0.8969624638557434\n",
      "Training iter #27762000:   Batch Loss = 7.109491, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4900736808776855, Accuracy = 0.8963668942451477\n",
      "Training iter #27765000:   Batch Loss = 7.151776, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.491196155548096, Accuracy = 0.8960691094398499\n",
      "Training iter #27768000:   Batch Loss = 7.175498, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.491580009460449, Accuracy = 0.8969624638557434\n",
      "Training iter #27771000:   Batch Loss = 7.151282, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.49326229095459, Accuracy = 0.8969624638557434\n",
      "Training iter #27774000:   Batch Loss = 7.098685, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.494469165802002, Accuracy = 0.8966646790504456\n",
      "Training iter #27777000:   Batch Loss = 7.131989, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.495063304901123, Accuracy = 0.8969624638557434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27780000:   Batch Loss = 7.137475, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.494853973388672, Accuracy = 0.8966646790504456\n",
      "Training iter #27783000:   Batch Loss = 7.192276, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.492800235748291, Accuracy = 0.8975580930709839\n",
      "Training iter #27786000:   Batch Loss = 7.109442, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.489782333374023, Accuracy = 0.8990470767021179\n",
      "Training iter #27789000:   Batch Loss = 7.094686, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.488137245178223, Accuracy = 0.8996426463127136\n",
      "Training iter #27792000:   Batch Loss = 7.154231, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487337589263916, Accuracy = 0.8999404311180115\n",
      "Training iter #27795000:   Batch Loss = 7.173014, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.486618995666504, Accuracy = 0.900536060333252\n",
      "Training iter #27798000:   Batch Loss = 7.150502, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487415790557861, Accuracy = 0.9002382159233093\n",
      "Training iter #27801000:   Batch Loss = 7.097547, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.487543106079102, Accuracy = 0.8999404311180115\n",
      "Training iter #27804000:   Batch Loss = 7.131418, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.486608028411865, Accuracy = 0.900536060333252\n",
      "Training iter #27807000:   Batch Loss = 7.133800, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.486080646514893, Accuracy = 0.9002382159233093\n",
      "Training iter #27810000:   Batch Loss = 7.183745, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.486018180847168, Accuracy = 0.900536060333252\n",
      "Training iter #27813000:   Batch Loss = 7.096325, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.485959529876709, Accuracy = 0.900536060333252\n",
      "Training iter #27816000:   Batch Loss = 7.087401, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.485978603363037, Accuracy = 0.9002382159233093\n",
      "Training iter #27819000:   Batch Loss = 7.157290, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.486068248748779, Accuracy = 0.8996426463127136\n",
      "Training iter #27822000:   Batch Loss = 7.179559, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.485828876495361, Accuracy = 0.8990470767021179\n",
      "Training iter #27825000:   Batch Loss = 7.147378, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4846649169921875, Accuracy = 0.8990470767021179\n",
      "Training iter #27828000:   Batch Loss = 7.098309, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.483465194702148, Accuracy = 0.8987492322921753\n",
      "Training iter #27831000:   Batch Loss = 7.132025, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482653617858887, Accuracy = 0.8984514474868774\n",
      "Training iter #27834000:   Batch Loss = 7.142771, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482182502746582, Accuracy = 0.8984514474868774\n",
      "Training iter #27837000:   Batch Loss = 7.173134, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482094764709473, Accuracy = 0.8993448615074158\n",
      "Training iter #27840000:   Batch Loss = 7.089706, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482162952423096, Accuracy = 0.8996426463127136\n",
      "Training iter #27843000:   Batch Loss = 7.090557, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482177734375, Accuracy = 0.8999404311180115\n",
      "Training iter #27846000:   Batch Loss = 7.150610, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482349395751953, Accuracy = 0.8999404311180115\n",
      "Training iter #27849000:   Batch Loss = 7.164930, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.48247766494751, Accuracy = 0.8996426463127136\n",
      "Training iter #27852000:   Batch Loss = 7.150201, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482613563537598, Accuracy = 0.8993448615074158\n",
      "Training iter #27855000:   Batch Loss = 7.096613, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482684135437012, Accuracy = 0.8993448615074158\n",
      "Training iter #27858000:   Batch Loss = 7.128082, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4831342697143555, Accuracy = 0.8990470767021179\n",
      "Training iter #27861000:   Batch Loss = 7.141645, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4832963943481445, Accuracy = 0.8990470767021179\n",
      "Training iter #27864000:   Batch Loss = 7.161943, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.483313083648682, Accuracy = 0.8993448615074158\n",
      "Training iter #27867000:   Batch Loss = 7.082614, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4832587242126465, Accuracy = 0.8990470767021179\n",
      "Training iter #27870000:   Batch Loss = 7.088450, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4827880859375, Accuracy = 0.8996426463127136\n",
      "Training iter #27873000:   Batch Loss = 7.138194, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.48292350769043, Accuracy = 0.8996426463127136\n",
      "Training iter #27876000:   Batch Loss = 7.173653, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.483160972595215, Accuracy = 0.8993448615074158\n",
      "Training iter #27879000:   Batch Loss = 7.150931, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.483124732971191, Accuracy = 0.8990470767021179\n",
      "Training iter #27882000:   Batch Loss = 7.101323, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.483255863189697, Accuracy = 0.8981536626815796\n",
      "Training iter #27885000:   Batch Loss = 7.130738, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482968330383301, Accuracy = 0.8981536626815796\n",
      "Training iter #27888000:   Batch Loss = 7.136275, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482436180114746, Accuracy = 0.8984514474868774\n",
      "Training iter #27891000:   Batch Loss = 7.155440, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.482184410095215, Accuracy = 0.8987492322921753\n",
      "Training iter #27894000:   Batch Loss = 7.077219, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4819135665893555, Accuracy = 0.8990470767021179\n",
      "Training iter #27897000:   Batch Loss = 7.095469, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.481590270996094, Accuracy = 0.8987492322921753\n",
      "Training iter #27900000:   Batch Loss = 7.135910, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.481348037719727, Accuracy = 0.8987492322921753\n",
      "Training iter #27903000:   Batch Loss = 7.170462, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.481186389923096, Accuracy = 0.8987492322921753\n",
      "Training iter #27906000:   Batch Loss = 7.140043, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4810261726379395, Accuracy = 0.8987492322921753\n",
      "Training iter #27909000:   Batch Loss = 7.100154, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.480785846710205, Accuracy = 0.8984514474868774\n",
      "Training iter #27912000:   Batch Loss = 7.127167, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4805521965026855, Accuracy = 0.8984514474868774\n",
      "Training iter #27915000:   Batch Loss = 7.132859, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.48034143447876, Accuracy = 0.8987492322921753\n",
      "Training iter #27918000:   Batch Loss = 7.148808, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.480041980743408, Accuracy = 0.8987492322921753\n",
      "Training iter #27921000:   Batch Loss = 7.081347, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.479753017425537, Accuracy = 0.8987492322921753\n",
      "Training iter #27924000:   Batch Loss = 7.098351, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.479747295379639, Accuracy = 0.8984514474868774\n",
      "Training iter #27927000:   Batch Loss = 7.133194, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4793925285339355, Accuracy = 0.8984514474868774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27930000:   Batch Loss = 7.171234, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4790568351745605, Accuracy = 0.8987492322921753\n",
      "Training iter #27933000:   Batch Loss = 7.130428, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.478777885437012, Accuracy = 0.8993448615074158\n",
      "Training iter #27936000:   Batch Loss = 7.094222, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.478461742401123, Accuracy = 0.8993448615074158\n",
      "Training iter #27939000:   Batch Loss = 7.135597, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.478342056274414, Accuracy = 0.8993448615074158\n",
      "Training iter #27942000:   Batch Loss = 7.138059, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.478240489959717, Accuracy = 0.8996426463127136\n",
      "Training iter #27945000:   Batch Loss = 7.152936, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477962493896484, Accuracy = 0.8996426463127136\n",
      "Training iter #27948000:   Batch Loss = 7.076729, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477970600128174, Accuracy = 0.8990470767021179\n",
      "Training iter #27951000:   Batch Loss = 7.094579, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477405071258545, Accuracy = 0.8993448615074158\n",
      "Training iter #27954000:   Batch Loss = 7.127567, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4773125648498535, Accuracy = 0.8993448615074158\n",
      "Training iter #27957000:   Batch Loss = 7.174355, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477525234222412, Accuracy = 0.8984514474868774\n",
      "Training iter #27960000:   Batch Loss = 7.116881, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477688789367676, Accuracy = 0.8984514474868774\n",
      "Training iter #27963000:   Batch Loss = 7.090042, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4776835441589355, Accuracy = 0.8981536626815796\n",
      "Training iter #27966000:   Batch Loss = 7.136990, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.47760009765625, Accuracy = 0.8978558778762817\n",
      "Training iter #27969000:   Batch Loss = 7.137919, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477550029754639, Accuracy = 0.8978558778762817\n",
      "Training iter #27972000:   Batch Loss = 7.148023, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477458477020264, Accuracy = 0.8981536626815796\n",
      "Training iter #27975000:   Batch Loss = 7.079118, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4772467613220215, Accuracy = 0.8978558778762817\n",
      "Training iter #27978000:   Batch Loss = 7.095430, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.477034091949463, Accuracy = 0.8978558778762817\n",
      "Training iter #27981000:   Batch Loss = 7.122239, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4767537117004395, Accuracy = 0.8978558778762817\n",
      "Training iter #27984000:   Batch Loss = 7.175519, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.476369380950928, Accuracy = 0.8984514474868774\n",
      "Training iter #27987000:   Batch Loss = 7.112676, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.475748062133789, Accuracy = 0.8984514474868774\n",
      "Training iter #27990000:   Batch Loss = 7.089302, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.475398540496826, Accuracy = 0.8990470767021179\n",
      "Training iter #27993000:   Batch Loss = 7.136841, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4748969078063965, Accuracy = 0.8990470767021179\n",
      "Training iter #27996000:   Batch Loss = 7.142101, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.474589824676514, Accuracy = 0.8990470767021179\n",
      "Training iter #27999000:   Batch Loss = 7.142697, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.474290370941162, Accuracy = 0.8993448615074158\n",
      "Training iter #28002000:   Batch Loss = 7.076654, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.47401237487793, Accuracy = 0.8993448615074158\n",
      "Training iter #28005000:   Batch Loss = 7.108542, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473765850067139, Accuracy = 0.8993448615074158\n",
      "Training iter #28008000:   Batch Loss = 7.121608, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473489761352539, Accuracy = 0.8993448615074158\n",
      "Training iter #28011000:   Batch Loss = 7.169003, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473340034484863, Accuracy = 0.8996426463127136\n",
      "Training iter #28014000:   Batch Loss = 7.108133, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473181247711182, Accuracy = 0.8996426463127136\n",
      "Training iter #28017000:   Batch Loss = 7.084918, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473158359527588, Accuracy = 0.8996426463127136\n",
      "Training iter #28020000:   Batch Loss = 7.119298, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473040580749512, Accuracy = 0.8996426463127136\n",
      "Training iter #28023000:   Batch Loss = 7.150344, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.47292423248291, Accuracy = 0.8993448615074158\n",
      "Training iter #28026000:   Batch Loss = 7.130501, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473400115966797, Accuracy = 0.8987492322921753\n",
      "Training iter #28029000:   Batch Loss = 7.077658, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473537445068359, Accuracy = 0.8981536626815796\n",
      "Training iter #28032000:   Batch Loss = 7.110353, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473654270172119, Accuracy = 0.8984514474868774\n",
      "Training iter #28035000:   Batch Loss = 7.128904, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473329544067383, Accuracy = 0.8981536626815796\n",
      "Training iter #28038000:   Batch Loss = 7.170926, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.472992420196533, Accuracy = 0.8981536626815796\n",
      "Training iter #28041000:   Batch Loss = 7.104614, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.473136901855469, Accuracy = 0.8978558778762817\n",
      "Training iter #28044000:   Batch Loss = 7.081529, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.472761631011963, Accuracy = 0.8975580930709839\n",
      "Training iter #28047000:   Batch Loss = 7.120136, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4724884033203125, Accuracy = 0.8975580930709839\n",
      "Training iter #28050000:   Batch Loss = 7.148796, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4722514152526855, Accuracy = 0.8978558778762817\n",
      "Training iter #28053000:   Batch Loss = 7.130898, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.472074508666992, Accuracy = 0.8981536626815796\n",
      "Training iter #28056000:   Batch Loss = 7.071548, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.472064971923828, Accuracy = 0.8981536626815796\n",
      "Training iter #28059000:   Batch Loss = 7.109862, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.47269868850708, Accuracy = 0.8987492322921753\n",
      "Training iter #28062000:   Batch Loss = 7.125260, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.472376823425293, Accuracy = 0.8978558778762817\n",
      "Training iter #28065000:   Batch Loss = 7.169547, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.471960067749023, Accuracy = 0.8978558778762817\n",
      "Training iter #28068000:   Batch Loss = 7.097559, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4716081619262695, Accuracy = 0.8978558778762817\n",
      "Training iter #28071000:   Batch Loss = 7.078095, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.471207141876221, Accuracy = 0.8984514474868774\n",
      "Training iter #28074000:   Batch Loss = 7.133776, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.470923900604248, Accuracy = 0.8990470767021179\n",
      "Training iter #28077000:   Batch Loss = 7.145955, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.470708847045898, Accuracy = 0.8996426463127136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28080000:   Batch Loss = 7.125274, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.470467567443848, Accuracy = 0.8996426463127136\n",
      "Training iter #28083000:   Batch Loss = 7.071832, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.470229148864746, Accuracy = 0.8990470767021179\n",
      "Training iter #28086000:   Batch Loss = 7.111105, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.46993350982666, Accuracy = 0.8990470767021179\n",
      "Training iter #28089000:   Batch Loss = 7.114811, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.469639301300049, Accuracy = 0.8987492322921753\n",
      "Training iter #28092000:   Batch Loss = 7.166364, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.469465732574463, Accuracy = 0.8990470767021179\n",
      "Training iter #28095000:   Batch Loss = 7.090490, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.469180107116699, Accuracy = 0.8993448615074158\n",
      "Training iter #28098000:   Batch Loss = 7.077603, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4688920974731445, Accuracy = 0.8987492322921753\n",
      "Training iter #28101000:   Batch Loss = 7.132268, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.468841552734375, Accuracy = 0.8987492322921753\n",
      "Training iter #28104000:   Batch Loss = 7.140858, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.468749046325684, Accuracy = 0.8987492322921753\n",
      "Training iter #28107000:   Batch Loss = 7.131644, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.468595027923584, Accuracy = 0.8990470767021179\n",
      "Training iter #28110000:   Batch Loss = 7.073766, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.46836519241333, Accuracy = 0.8990470767021179\n",
      "Training iter #28113000:   Batch Loss = 7.109469, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.46804141998291, Accuracy = 0.8990470767021179\n",
      "Training iter #28116000:   Batch Loss = 7.109862, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.467775344848633, Accuracy = 0.8987492322921753\n",
      "Training iter #28119000:   Batch Loss = 7.166537, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.467692852020264, Accuracy = 0.8987492322921753\n",
      "Training iter #28122000:   Batch Loss = 7.076940, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.467597961425781, Accuracy = 0.8984514474868774\n",
      "Training iter #28125000:   Batch Loss = 7.070539, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.467395305633545, Accuracy = 0.8987492322921753\n",
      "Training iter #28128000:   Batch Loss = 7.134929, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.467301845550537, Accuracy = 0.8987492322921753\n",
      "Training iter #28131000:   Batch Loss = 7.142637, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.467282772064209, Accuracy = 0.8987492322921753\n",
      "Training iter #28134000:   Batch Loss = 7.129457, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.467202663421631, Accuracy = 0.8981536626815796\n",
      "Training iter #28137000:   Batch Loss = 7.078920, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.466948509216309, Accuracy = 0.8981536626815796\n",
      "Training iter #28140000:   Batch Loss = 7.107881, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.466587543487549, Accuracy = 0.8978558778762817\n",
      "Training iter #28143000:   Batch Loss = 7.113136, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.466183662414551, Accuracy = 0.8978558778762817\n",
      "Training iter #28146000:   Batch Loss = 7.157922, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4655022621154785, Accuracy = 0.8978558778762817\n",
      "Training iter #28149000:   Batch Loss = 7.072399, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.465379238128662, Accuracy = 0.8978558778762817\n",
      "Training iter #28152000:   Batch Loss = 7.068786, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.465219020843506, Accuracy = 0.8978558778762817\n",
      "Training iter #28155000:   Batch Loss = 7.132506, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.465526580810547, Accuracy = 0.8981536626815796\n",
      "Training iter #28158000:   Batch Loss = 7.140506, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.465423107147217, Accuracy = 0.8981536626815796\n",
      "Training iter #28161000:   Batch Loss = 7.124972, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.465338230133057, Accuracy = 0.8984514474868774\n",
      "Training iter #28164000:   Batch Loss = 7.079005, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.465155601501465, Accuracy = 0.8987492322921753\n",
      "Training iter #28167000:   Batch Loss = 7.107036, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.46492338180542, Accuracy = 0.8990470767021179\n",
      "Training iter #28170000:   Batch Loss = 7.122560, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4646430015563965, Accuracy = 0.8990470767021179\n",
      "Training iter #28173000:   Batch Loss = 7.149994, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464366436004639, Accuracy = 0.8990470767021179\n",
      "Training iter #28176000:   Batch Loss = 7.065703, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464310646057129, Accuracy = 0.8990470767021179\n",
      "Training iter #28179000:   Batch Loss = 7.070547, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464105606079102, Accuracy = 0.8990470767021179\n",
      "Training iter #28182000:   Batch Loss = 7.125578, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.463939189910889, Accuracy = 0.8990470767021179\n",
      "Training iter #28185000:   Batch Loss = 7.145452, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464131832122803, Accuracy = 0.8987492322921753\n",
      "Training iter #28188000:   Batch Loss = 7.128320, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464321613311768, Accuracy = 0.8984514474868774\n",
      "Training iter #28191000:   Batch Loss = 7.084071, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464277267456055, Accuracy = 0.8987492322921753\n",
      "Training iter #28194000:   Batch Loss = 7.112497, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464202404022217, Accuracy = 0.8987492322921753\n",
      "Training iter #28197000:   Batch Loss = 7.119295, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4639573097229, Accuracy = 0.8987492322921753\n",
      "Training iter #28200000:   Batch Loss = 7.135416, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.463749408721924, Accuracy = 0.8984514474868774\n",
      "Training iter #28203000:   Batch Loss = 7.058382, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464354991912842, Accuracy = 0.8981536626815796\n",
      "Training iter #28206000:   Batch Loss = 7.076071, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464153289794922, Accuracy = 0.8984514474868774\n",
      "Training iter #28209000:   Batch Loss = 7.116549, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4638142585754395, Accuracy = 0.8984514474868774\n",
      "Training iter #28212000:   Batch Loss = 7.149264, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.463571548461914, Accuracy = 0.8984514474868774\n",
      "Training iter #28215000:   Batch Loss = 7.127415, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.463263034820557, Accuracy = 0.8984514474868774\n",
      "Training iter #28218000:   Batch Loss = 7.083372, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.462915897369385, Accuracy = 0.8981536626815796\n",
      "Training iter #28221000:   Batch Loss = 7.108749, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4626240730285645, Accuracy = 0.8981536626815796\n",
      "Training iter #28224000:   Batch Loss = 7.114913, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.462452411651611, Accuracy = 0.8981536626815796\n",
      "Training iter #28227000:   Batch Loss = 7.136467, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.462121963500977, Accuracy = 0.8993448615074158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28230000:   Batch Loss = 7.057503, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.462223529815674, Accuracy = 0.8987492322921753\n",
      "Training iter #28233000:   Batch Loss = 7.079134, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.462209701538086, Accuracy = 0.8993448615074158\n",
      "Training iter #28236000:   Batch Loss = 7.119586, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461991310119629, Accuracy = 0.8990470767021179\n",
      "Training iter #28239000:   Batch Loss = 7.153413, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461832523345947, Accuracy = 0.8987492322921753\n",
      "Training iter #28242000:   Batch Loss = 7.114838, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461783409118652, Accuracy = 0.8987492322921753\n",
      "Training iter #28245000:   Batch Loss = 7.077038, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461630821228027, Accuracy = 0.8984514474868774\n",
      "Training iter #28248000:   Batch Loss = 7.112509, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461527347564697, Accuracy = 0.8987492322921753\n",
      "Training iter #28251000:   Batch Loss = 7.118062, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461395740509033, Accuracy = 0.8990470767021179\n",
      "Training iter #28254000:   Batch Loss = 7.129889, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461315155029297, Accuracy = 0.8990470767021179\n",
      "Training iter #28257000:   Batch Loss = 7.059750, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.461082935333252, Accuracy = 0.8996426463127136\n",
      "Training iter #28260000:   Batch Loss = 7.078034, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.460763454437256, Accuracy = 0.8996426463127136\n",
      "Training iter #28263000:   Batch Loss = 7.110577, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.460484504699707, Accuracy = 0.8996426463127136\n",
      "Training iter #28266000:   Batch Loss = 7.152548, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.459938049316406, Accuracy = 0.8990470767021179\n",
      "Training iter #28269000:   Batch Loss = 7.101038, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4596734046936035, Accuracy = 0.8990470767021179\n",
      "Training iter #28272000:   Batch Loss = 7.076442, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.45943021774292, Accuracy = 0.8993448615074158\n",
      "Training iter #28275000:   Batch Loss = 7.116598, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.459300994873047, Accuracy = 0.8993448615074158\n",
      "Training iter #28278000:   Batch Loss = 7.120859, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.459140777587891, Accuracy = 0.8990470767021179\n",
      "Training iter #28281000:   Batch Loss = 7.130821, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.459097385406494, Accuracy = 0.8990470767021179\n",
      "Training iter #28284000:   Batch Loss = 7.061113, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.458987712860107, Accuracy = 0.8996426463127136\n",
      "Training iter #28287000:   Batch Loss = 7.076712, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.458798408508301, Accuracy = 0.8996426463127136\n",
      "Training iter #28290000:   Batch Loss = 7.108942, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.459110736846924, Accuracy = 0.8990470767021179\n",
      "Training iter #28293000:   Batch Loss = 7.162125, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455146789550781, Accuracy = 0.8993448615074158\n",
      "Training iter #28296000:   Batch Loss = 7.099383, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.454851150512695, Accuracy = 0.8999404311180115\n",
      "Training iter #28299000:   Batch Loss = 7.074131, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455409526824951, Accuracy = 0.9008338451385498\n",
      "Training iter #28302000:   Batch Loss = 7.124770, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455258846282959, Accuracy = 0.9011316299438477\n",
      "Training iter #28305000:   Batch Loss = 7.156538, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455452919006348, Accuracy = 0.9008338451385498\n",
      "Training iter #28308000:   Batch Loss = 7.131578, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.456155300140381, Accuracy = 0.9002382159233093\n",
      "Training iter #28311000:   Batch Loss = 7.062910, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.457535266876221, Accuracy = 0.8987492322921753\n",
      "Training iter #28314000:   Batch Loss = 7.076294, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.458713531494141, Accuracy = 0.8981536626815796\n",
      "Training iter #28317000:   Batch Loss = 7.110946, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.46077299118042, Accuracy = 0.8972602486610413\n",
      "Training iter #28320000:   Batch Loss = 7.170529, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.462165355682373, Accuracy = 0.8975580930709839\n",
      "Training iter #28323000:   Batch Loss = 7.099847, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.463802814483643, Accuracy = 0.8975580930709839\n",
      "Training iter #28326000:   Batch Loss = 7.072188, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.464195251464844, Accuracy = 0.8981536626815796\n",
      "Training iter #28329000:   Batch Loss = 7.106418, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.462296009063721, Accuracy = 0.8966646790504456\n",
      "Training iter #28332000:   Batch Loss = 7.135868, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.457622051239014, Accuracy = 0.8972602486610413\n",
      "Training iter #28335000:   Batch Loss = 7.126134, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.456739902496338, Accuracy = 0.8975580930709839\n",
      "Training iter #28338000:   Batch Loss = 7.066031, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.458300590515137, Accuracy = 0.8975580930709839\n",
      "Training iter #28341000:   Batch Loss = 7.094438, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.459147930145264, Accuracy = 0.8981536626815796\n",
      "Training iter #28344000:   Batch Loss = 7.118512, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.459162712097168, Accuracy = 0.8975580930709839\n",
      "Training iter #28347000:   Batch Loss = 7.158966, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.458182334899902, Accuracy = 0.8978558778762817\n",
      "Training iter #28350000:   Batch Loss = 7.096371, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.458224773406982, Accuracy = 0.8978558778762817\n",
      "Training iter #28353000:   Batch Loss = 7.070026, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455882549285889, Accuracy = 0.8978558778762817\n",
      "Training iter #28356000:   Batch Loss = 7.110790, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455134868621826, Accuracy = 0.8978558778762817\n",
      "Training iter #28359000:   Batch Loss = 7.141765, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.454876899719238, Accuracy = 0.8978558778762817\n",
      "Training iter #28362000:   Batch Loss = 7.110785, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.454770088195801, Accuracy = 0.8975580930709839\n",
      "Training iter #28365000:   Batch Loss = 7.058434, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.454475402832031, Accuracy = 0.8981536626815796\n",
      "Training iter #28368000:   Batch Loss = 7.096646, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453898906707764, Accuracy = 0.8981536626815796\n",
      "Training iter #28371000:   Batch Loss = 7.116608, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453670024871826, Accuracy = 0.8990470767021179\n",
      "Training iter #28374000:   Batch Loss = 7.158230, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453701019287109, Accuracy = 0.8981536626815796\n",
      "Training iter #28377000:   Batch Loss = 7.085754, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453993797302246, Accuracy = 0.8987492322921753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28380000:   Batch Loss = 7.065662, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453699111938477, Accuracy = 0.8987492322921753\n",
      "Training iter #28383000:   Batch Loss = 7.106504, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455246448516846, Accuracy = 0.8990470767021179\n",
      "Training iter #28386000:   Batch Loss = 7.138993, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455416202545166, Accuracy = 0.8990470767021179\n",
      "Training iter #28389000:   Batch Loss = 7.112362, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.455145835876465, Accuracy = 0.8990470767021179\n",
      "Training iter #28392000:   Batch Loss = 7.056705, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4546308517456055, Accuracy = 0.8990470767021179\n",
      "Training iter #28395000:   Batch Loss = 7.102316, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.454044818878174, Accuracy = 0.8990470767021179\n",
      "Training iter #28398000:   Batch Loss = 7.099782, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453444957733154, Accuracy = 0.8990470767021179\n",
      "Training iter #28401000:   Batch Loss = 7.156303, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453075885772705, Accuracy = 0.8993448615074158\n",
      "Training iter #28404000:   Batch Loss = 7.082383, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453404426574707, Accuracy = 0.8984514474868774\n",
      "Training iter #28407000:   Batch Loss = 7.061967, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4535298347473145, Accuracy = 0.8987492322921753\n",
      "Training iter #28410000:   Batch Loss = 7.116295, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.453549385070801, Accuracy = 0.8996426463127136\n",
      "Training iter #28413000:   Batch Loss = 7.129711, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.452887058258057, Accuracy = 0.8996426463127136\n",
      "Training iter #28416000:   Batch Loss = 7.115058, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.452187538146973, Accuracy = 0.9002382159233093\n",
      "Training iter #28419000:   Batch Loss = 7.056419, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.451665878295898, Accuracy = 0.900536060333252\n",
      "Training iter #28422000:   Batch Loss = 7.103222, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.451356410980225, Accuracy = 0.900536060333252\n",
      "Training iter #28425000:   Batch Loss = 7.100243, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.451146125793457, Accuracy = 0.900536060333252\n",
      "Training iter #28428000:   Batch Loss = 7.153706, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.450875759124756, Accuracy = 0.900536060333252\n",
      "Training iter #28431000:   Batch Loss = 7.067041, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.450587272644043, Accuracy = 0.9011316299438477\n",
      "Training iter #28434000:   Batch Loss = 7.060134, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.450356960296631, Accuracy = 0.9008338451385498\n",
      "Training iter #28437000:   Batch Loss = 7.120009, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.449974060058594, Accuracy = 0.9002382159233093\n",
      "Training iter #28440000:   Batch Loss = 7.134277, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.449498176574707, Accuracy = 0.9002382159233093\n",
      "Training iter #28443000:   Batch Loss = 7.114172, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.44915771484375, Accuracy = 0.900536060333252\n",
      "Training iter #28446000:   Batch Loss = 7.062684, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.448816776275635, Accuracy = 0.9002382159233093\n",
      "Training iter #28449000:   Batch Loss = 7.094876, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.448448657989502, Accuracy = 0.9002382159233093\n",
      "Training iter #28452000:   Batch Loss = 7.098459, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.447792053222656, Accuracy = 0.900536060333252\n",
      "Training iter #28455000:   Batch Loss = 7.148364, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.447147846221924, Accuracy = 0.9008338451385498\n",
      "Training iter #28458000:   Batch Loss = 7.056460, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.446645736694336, Accuracy = 0.9008338451385498\n",
      "Training iter #28461000:   Batch Loss = 7.062620, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.446175575256348, Accuracy = 0.900536060333252\n",
      "Training iter #28464000:   Batch Loss = 7.120777, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4456610679626465, Accuracy = 0.9002382159233093\n",
      "Training iter #28467000:   Batch Loss = 7.131453, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.445271015167236, Accuracy = 0.8999404311180115\n",
      "Training iter #28470000:   Batch Loss = 7.111264, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.444926738739014, Accuracy = 0.8993448615074158\n",
      "Training iter #28473000:   Batch Loss = 7.063796, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.444578647613525, Accuracy = 0.8999404311180115\n",
      "Training iter #28476000:   Batch Loss = 7.095572, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.444262504577637, Accuracy = 0.8999404311180115\n",
      "Training iter #28479000:   Batch Loss = 7.105339, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.443867206573486, Accuracy = 0.8999404311180115\n",
      "Training iter #28482000:   Batch Loss = 7.135061, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.443562030792236, Accuracy = 0.9002382159233093\n",
      "Training iter #28485000:   Batch Loss = 7.053407, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.44328498840332, Accuracy = 0.900536060333252\n",
      "Training iter #28488000:   Batch Loss = 7.056793, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.443026542663574, Accuracy = 0.9008338451385498\n",
      "Training iter #28491000:   Batch Loss = 7.114036, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.442790508270264, Accuracy = 0.9011316299438477\n",
      "Training iter #28494000:   Batch Loss = 7.126819, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.442445755004883, Accuracy = 0.9011316299438477\n",
      "Training iter #28497000:   Batch Loss = 7.109709, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.442190170288086, Accuracy = 0.9014294147491455\n",
      "Training iter #28500000:   Batch Loss = 7.062548, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.441924095153809, Accuracy = 0.9011316299438477\n",
      "Training iter #28503000:   Batch Loss = 7.096073, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.441678047180176, Accuracy = 0.9011316299438477\n",
      "Training iter #28506000:   Batch Loss = 7.106422, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.441427707672119, Accuracy = 0.9011316299438477\n",
      "Training iter #28509000:   Batch Loss = 7.126478, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.441228866577148, Accuracy = 0.9011316299438477\n",
      "Training iter #28512000:   Batch Loss = 7.046880, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.441037178039551, Accuracy = 0.9011316299438477\n",
      "Training iter #28515000:   Batch Loss = 7.054478, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.440764904022217, Accuracy = 0.9011316299438477\n",
      "Training iter #28518000:   Batch Loss = 7.101944, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.44050407409668, Accuracy = 0.9014294147491455\n",
      "Training iter #28521000:   Batch Loss = 7.135256, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.440298557281494, Accuracy = 0.9014294147491455\n",
      "Training iter #28524000:   Batch Loss = 7.112120, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.440196514129639, Accuracy = 0.9014294147491455\n",
      "Training iter #28527000:   Batch Loss = 7.066667, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.439998626708984, Accuracy = 0.9014294147491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28530000:   Batch Loss = 7.093863, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.439780235290527, Accuracy = 0.9011316299438477\n",
      "Training iter #28533000:   Batch Loss = 7.100473, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.439546585083008, Accuracy = 0.9017271995544434\n",
      "Training iter #28536000:   Batch Loss = 7.118042, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.439277172088623, Accuracy = 0.9014294147491455\n",
      "Training iter #28539000:   Batch Loss = 7.042057, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.439037322998047, Accuracy = 0.9014294147491455\n",
      "Training iter #28542000:   Batch Loss = 7.061694, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438782691955566, Accuracy = 0.9014294147491455\n",
      "Training iter #28545000:   Batch Loss = 7.100173, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438554763793945, Accuracy = 0.9014294147491455\n",
      "Training iter #28548000:   Batch Loss = 7.133333, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438432693481445, Accuracy = 0.9014294147491455\n",
      "Training iter #28551000:   Batch Loss = 7.106538, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438451290130615, Accuracy = 0.9014294147491455\n",
      "Training iter #28554000:   Batch Loss = 7.066133, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438417911529541, Accuracy = 0.9017271995544434\n",
      "Training iter #28557000:   Batch Loss = 7.089788, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438348293304443, Accuracy = 0.9014294147491455\n",
      "Training iter #28560000:   Batch Loss = 7.099452, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438282489776611, Accuracy = 0.9014294147491455\n",
      "Training iter #28563000:   Batch Loss = 7.118653, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.438124656677246, Accuracy = 0.9014294147491455\n",
      "Training iter #28566000:   Batch Loss = 7.045585, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.437921047210693, Accuracy = 0.9017271995544434\n",
      "Training iter #28569000:   Batch Loss = 7.064002, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.437681198120117, Accuracy = 0.9017271995544434\n",
      "Training iter #28572000:   Batch Loss = 7.094212, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.437459945678711, Accuracy = 0.9017271995544434\n",
      "Training iter #28575000:   Batch Loss = 7.136388, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.437289714813232, Accuracy = 0.9017271995544434\n",
      "Training iter #28578000:   Batch Loss = 7.093533, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.437145709991455, Accuracy = 0.9017271995544434\n",
      "Training iter #28581000:   Batch Loss = 7.060926, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.436946868896484, Accuracy = 0.902025043964386\n",
      "Training iter #28584000:   Batch Loss = 7.099208, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.436840057373047, Accuracy = 0.9014294147491455\n",
      "Training iter #28587000:   Batch Loss = 7.101625, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.436744213104248, Accuracy = 0.9017271995544434\n",
      "Training iter #28590000:   Batch Loss = 7.117790, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4366536140441895, Accuracy = 0.9017271995544434\n",
      "Training iter #28593000:   Batch Loss = 7.042204, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.43668794631958, Accuracy = 0.9014294147491455\n",
      "Training iter #28596000:   Batch Loss = 7.060155, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.436641693115234, Accuracy = 0.9017271995544434\n",
      "Training iter #28599000:   Batch Loss = 7.091692, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.436366081237793, Accuracy = 0.9017271995544434\n",
      "Training iter #28602000:   Batch Loss = 7.137775, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.435817718505859, Accuracy = 0.902025043964386\n",
      "Training iter #28605000:   Batch Loss = 7.081980, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4356207847595215, Accuracy = 0.902025043964386\n",
      "Training iter #28608000:   Batch Loss = 7.055603, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.435476779937744, Accuracy = 0.9017271995544434\n",
      "Training iter #28611000:   Batch Loss = 7.097875, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.435349464416504, Accuracy = 0.9017271995544434\n",
      "Training iter #28614000:   Batch Loss = 7.102201, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.435244083404541, Accuracy = 0.9017271995544434\n",
      "Training iter #28617000:   Batch Loss = 7.113367, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.435152053833008, Accuracy = 0.9014294147491455\n",
      "Training iter #28620000:   Batch Loss = 7.045814, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.435014724731445, Accuracy = 0.9014294147491455\n",
      "Training iter #28623000:   Batch Loss = 7.061010, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.434804439544678, Accuracy = 0.9017271995544434\n",
      "Training iter #28626000:   Batch Loss = 7.087636, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.434622764587402, Accuracy = 0.9017271995544434\n",
      "Training iter #28629000:   Batch Loss = 7.138849, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.434396743774414, Accuracy = 0.9017271995544434\n",
      "Training iter #28632000:   Batch Loss = 7.076677, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.434177875518799, Accuracy = 0.9017271995544434\n",
      "Training iter #28635000:   Batch Loss = 7.054174, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.433868885040283, Accuracy = 0.902025043964386\n",
      "Training iter #28638000:   Batch Loss = 7.101792, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4336771965026855, Accuracy = 0.902025043964386\n",
      "Training iter #28641000:   Batch Loss = 7.102410, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4335150718688965, Accuracy = 0.902025043964386\n",
      "Training iter #28644000:   Batch Loss = 7.107885, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.433389663696289, Accuracy = 0.9023228287696838\n",
      "Training iter #28647000:   Batch Loss = 7.044023, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.43327522277832, Accuracy = 0.9023228287696838\n",
      "Training iter #28650000:   Batch Loss = 7.073563, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.43312931060791, Accuracy = 0.9023228287696838\n",
      "Training iter #28653000:   Batch Loss = 7.088274, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.433026313781738, Accuracy = 0.9023228287696838\n",
      "Training iter #28656000:   Batch Loss = 7.139984, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.432892799377441, Accuracy = 0.9023228287696838\n",
      "Training iter #28659000:   Batch Loss = 7.073170, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.432858943939209, Accuracy = 0.902025043964386\n",
      "Training iter #28662000:   Batch Loss = 7.054438, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.43276309967041, Accuracy = 0.902025043964386\n",
      "Training iter #28665000:   Batch Loss = 7.084461, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.432526588439941, Accuracy = 0.9023228287696838\n",
      "Training iter #28668000:   Batch Loss = 7.113173, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.432004928588867, Accuracy = 0.9023228287696838\n",
      "Training iter #28671000:   Batch Loss = 7.097973, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.431630611419678, Accuracy = 0.9023228287696838\n",
      "Training iter #28674000:   Batch Loss = 7.044806, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.431466102600098, Accuracy = 0.902025043964386\n",
      "Training iter #28677000:   Batch Loss = 7.075731, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.431310176849365, Accuracy = 0.902025043964386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28680000:   Batch Loss = 7.089664, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.431184768676758, Accuracy = 0.9017271995544434\n",
      "Training iter #28683000:   Batch Loss = 7.137142, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.431149005889893, Accuracy = 0.9023228287696838\n",
      "Training iter #28686000:   Batch Loss = 7.069941, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.431114673614502, Accuracy = 0.902025043964386\n",
      "Training iter #28689000:   Batch Loss = 7.048938, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.43101692199707, Accuracy = 0.902025043964386\n",
      "Training iter #28692000:   Batch Loss = 7.083607, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.430819034576416, Accuracy = 0.9023228287696838\n",
      "Training iter #28695000:   Batch Loss = 7.111096, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.430637836456299, Accuracy = 0.9023228287696838\n",
      "Training iter #28698000:   Batch Loss = 7.095150, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.430367469787598, Accuracy = 0.9026206135749817\n",
      "Training iter #28701000:   Batch Loss = 7.039437, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.430086135864258, Accuracy = 0.902025043964386\n",
      "Training iter #28704000:   Batch Loss = 7.076177, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.429864406585693, Accuracy = 0.902025043964386\n",
      "Training iter #28707000:   Batch Loss = 7.091707, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.429631233215332, Accuracy = 0.902025043964386\n",
      "Training iter #28710000:   Batch Loss = 7.134562, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4293532371521, Accuracy = 0.902025043964386\n",
      "Training iter #28713000:   Batch Loss = 7.061959, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.429136753082275, Accuracy = 0.902025043964386\n",
      "Training iter #28716000:   Batch Loss = 7.045676, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428892612457275, Accuracy = 0.902025043964386\n",
      "Training iter #28719000:   Batch Loss = 7.098253, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428652763366699, Accuracy = 0.9017271995544434\n",
      "Training iter #28722000:   Batch Loss = 7.110319, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4285712242126465, Accuracy = 0.9014294147491455\n",
      "Training iter #28725000:   Batch Loss = 7.090633, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428526878356934, Accuracy = 0.9014294147491455\n",
      "Training iter #28728000:   Batch Loss = 7.039213, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428456783294678, Accuracy = 0.9014294147491455\n",
      "Training iter #28731000:   Batch Loss = 7.076188, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428308486938477, Accuracy = 0.9017271995544434\n",
      "Training iter #28734000:   Batch Loss = 7.080319, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428170204162598, Accuracy = 0.9017271995544434\n",
      "Training iter #28737000:   Batch Loss = 7.131300, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428086280822754, Accuracy = 0.9017271995544434\n",
      "Training iter #28740000:   Batch Loss = 7.055923, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.42803955078125, Accuracy = 0.9017271995544434\n",
      "Training iter #28743000:   Batch Loss = 7.045081, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427921772003174, Accuracy = 0.9014294147491455\n",
      "Training iter #28746000:   Batch Loss = 7.096611, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427789688110352, Accuracy = 0.9014294147491455\n",
      "Training iter #28749000:   Batch Loss = 7.104640, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427700996398926, Accuracy = 0.9011316299438477\n",
      "Training iter #28752000:   Batch Loss = 7.095172, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427558898925781, Accuracy = 0.9011316299438477\n",
      "Training iter #28755000:   Batch Loss = 7.040627, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4274983406066895, Accuracy = 0.9014294147491455\n",
      "Training iter #28758000:   Batch Loss = 7.075615, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427280902862549, Accuracy = 0.9008338451385498\n",
      "Training iter #28761000:   Batch Loss = 7.076470, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427097797393799, Accuracy = 0.9008338451385498\n",
      "Training iter #28764000:   Batch Loss = 7.131603, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426599025726318, Accuracy = 0.9014294147491455\n",
      "Training iter #28767000:   Batch Loss = 7.047042, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426591396331787, Accuracy = 0.9014294147491455\n",
      "Training iter #28770000:   Batch Loss = 7.038210, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426553249359131, Accuracy = 0.9017271995544434\n",
      "Training iter #28773000:   Batch Loss = 7.098070, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426463603973389, Accuracy = 0.9014294147491455\n",
      "Training iter #28776000:   Batch Loss = 7.107553, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426363468170166, Accuracy = 0.9014294147491455\n",
      "Training iter #28779000:   Batch Loss = 7.094249, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426201820373535, Accuracy = 0.9011316299438477\n",
      "Training iter #28782000:   Batch Loss = 7.045421, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.425972938537598, Accuracy = 0.9011316299438477\n",
      "Training iter #28785000:   Batch Loss = 7.075135, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.425745964050293, Accuracy = 0.9011316299438477\n",
      "Training iter #28788000:   Batch Loss = 7.075766, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.425624370574951, Accuracy = 0.9011316299438477\n",
      "Training iter #28791000:   Batch Loss = 7.124022, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.425610065460205, Accuracy = 0.9011316299438477\n",
      "Training iter #28794000:   Batch Loss = 7.038003, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.425517559051514, Accuracy = 0.9011316299438477\n",
      "Training iter #28797000:   Batch Loss = 7.036913, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4253692626953125, Accuracy = 0.9011316299438477\n",
      "Training iter #28800000:   Batch Loss = 7.096751, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427457809448242, Accuracy = 0.9011316299438477\n",
      "Training iter #28803000:   Batch Loss = 7.106185, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428613662719727, Accuracy = 0.9017271995544434\n",
      "Training iter #28806000:   Batch Loss = 7.091497, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428956985473633, Accuracy = 0.9017271995544434\n",
      "Training iter #28809000:   Batch Loss = 7.047005, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.429574012756348, Accuracy = 0.9014294147491455\n",
      "Training iter #28812000:   Batch Loss = 7.078578, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.429475784301758, Accuracy = 0.9014294147491455\n",
      "Training iter #28815000:   Batch Loss = 7.087173, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.429318904876709, Accuracy = 0.9011316299438477\n",
      "Training iter #28818000:   Batch Loss = 7.114928, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4293389320373535, Accuracy = 0.9011316299438477\n",
      "Training iter #28821000:   Batch Loss = 7.034924, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.429378986358643, Accuracy = 0.9011316299438477\n",
      "Training iter #28824000:   Batch Loss = 7.038728, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428831577301025, Accuracy = 0.9014294147491455\n",
      "Training iter #28827000:   Batch Loss = 7.094481, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428575038909912, Accuracy = 0.9014294147491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28830000:   Batch Loss = 7.106122, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.428255081176758, Accuracy = 0.9011316299438477\n",
      "Training iter #28833000:   Batch Loss = 7.094483, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4278082847595215, Accuracy = 0.9017271995544434\n",
      "Training iter #28836000:   Batch Loss = 7.051036, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427591800689697, Accuracy = 0.9017271995544434\n",
      "Training iter #28839000:   Batch Loss = 7.090645, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427455902099609, Accuracy = 0.9017271995544434\n",
      "Training iter #28842000:   Batch Loss = 7.086104, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427352428436279, Accuracy = 0.9017271995544434\n",
      "Training iter #28845000:   Batch Loss = 7.102148, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427298545837402, Accuracy = 0.9017271995544434\n",
      "Training iter #28848000:   Batch Loss = 7.027609, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427280426025391, Accuracy = 0.9017271995544434\n",
      "Training iter #28851000:   Batch Loss = 7.045086, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427158832550049, Accuracy = 0.9017271995544434\n",
      "Training iter #28854000:   Batch Loss = 7.084934, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427037715911865, Accuracy = 0.9014294147491455\n",
      "Training iter #28857000:   Batch Loss = 7.112987, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427242279052734, Accuracy = 0.9014294147491455\n",
      "Training iter #28860000:   Batch Loss = 7.094934, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427191734313965, Accuracy = 0.9011316299438477\n",
      "Training iter #28863000:   Batch Loss = 7.050159, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427290916442871, Accuracy = 0.9011316299438477\n",
      "Training iter #28866000:   Batch Loss = 7.078783, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426986217498779, Accuracy = 0.9011316299438477\n",
      "Training iter #28869000:   Batch Loss = 7.080912, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.427017688751221, Accuracy = 0.9008338451385498\n",
      "Training iter #28872000:   Batch Loss = 7.100667, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426366806030273, Accuracy = 0.9017271995544434\n",
      "Training iter #28875000:   Batch Loss = 7.025094, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4263105392456055, Accuracy = 0.9017271995544434\n",
      "Training iter #28878000:   Batch Loss = 7.050951, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426311492919922, Accuracy = 0.9017271995544434\n",
      "Training iter #28881000:   Batch Loss = 7.082850, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.426405429840088, Accuracy = 0.9017271995544434\n",
      "Training iter #28884000:   Batch Loss = 7.113609, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.425909042358398, Accuracy = 0.9017271995544434\n",
      "Training iter #28887000:   Batch Loss = 7.084523, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.425343990325928, Accuracy = 0.9017271995544434\n",
      "Training iter #28890000:   Batch Loss = 7.048592, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424783706665039, Accuracy = 0.9017271995544434\n",
      "Training iter #28893000:   Batch Loss = 7.078242, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424007415771484, Accuracy = 0.9017271995544434\n",
      "Training iter #28896000:   Batch Loss = 7.080166, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423452377319336, Accuracy = 0.902025043964386\n",
      "Training iter #28899000:   Batch Loss = 7.096360, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423903465270996, Accuracy = 0.9014294147491455\n",
      "Training iter #28902000:   Batch Loss = 7.029290, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4245195388793945, Accuracy = 0.9008338451385498\n",
      "Training iter #28905000:   Batch Loss = 7.045951, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424622535705566, Accuracy = 0.9008338451385498\n",
      "Training iter #28908000:   Batch Loss = 7.079598, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424077033996582, Accuracy = 0.9014294147491455\n",
      "Training iter #28911000:   Batch Loss = 7.115224, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423783779144287, Accuracy = 0.9011316299438477\n",
      "Training iter #28914000:   Batch Loss = 7.067320, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423627853393555, Accuracy = 0.9014294147491455\n",
      "Training iter #28917000:   Batch Loss = 7.045953, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423731803894043, Accuracy = 0.900536060333252\n",
      "Training iter #28920000:   Batch Loss = 7.083857, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4237470626831055, Accuracy = 0.900536060333252\n",
      "Training iter #28923000:   Batch Loss = 7.085386, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423745155334473, Accuracy = 0.900536060333252\n",
      "Training iter #28926000:   Batch Loss = 7.097521, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423811435699463, Accuracy = 0.900536060333252\n",
      "Training iter #28929000:   Batch Loss = 7.029433, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423888683319092, Accuracy = 0.900536060333252\n",
      "Training iter #28932000:   Batch Loss = 7.043931, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423938751220703, Accuracy = 0.900536060333252\n",
      "Training iter #28935000:   Batch Loss = 7.076109, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424107551574707, Accuracy = 0.8999404311180115\n",
      "Training iter #28938000:   Batch Loss = 7.117023, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424368858337402, Accuracy = 0.8999404311180115\n",
      "Training iter #28941000:   Batch Loss = 7.063847, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4242072105407715, Accuracy = 0.9002382159233093\n",
      "Training iter #28944000:   Batch Loss = 7.039327, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424010753631592, Accuracy = 0.900536060333252\n",
      "Training iter #28947000:   Batch Loss = 7.085230, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.424107074737549, Accuracy = 0.900536060333252\n",
      "Training iter #28950000:   Batch Loss = 7.081955, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423932075500488, Accuracy = 0.900536060333252\n",
      "Training iter #28953000:   Batch Loss = 7.093406, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423727035522461, Accuracy = 0.9008338451385498\n",
      "Training iter #28956000:   Batch Loss = 7.029003, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423870086669922, Accuracy = 0.9002382159233093\n",
      "Training iter #28959000:   Batch Loss = 7.043530, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423626899719238, Accuracy = 0.9002382159233093\n",
      "Training iter #28962000:   Batch Loss = 7.069459, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.423737049102783, Accuracy = 0.9002382159233093\n",
      "Training iter #28965000:   Batch Loss = 7.119485, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.422798156738281, Accuracy = 0.9002382159233093\n",
      "Training iter #28968000:   Batch Loss = 7.059365, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.421932697296143, Accuracy = 0.900536060333252\n",
      "Training iter #28971000:   Batch Loss = 7.038981, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.421679496765137, Accuracy = 0.9002382159233093\n",
      "Training iter #28974000:   Batch Loss = 7.069614, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4214253425598145, Accuracy = 0.9002382159233093\n",
      "Training iter #28977000:   Batch Loss = 7.085071, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.42120885848999, Accuracy = 0.9002382159233093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28980000:   Batch Loss = 7.084945, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.420814037322998, Accuracy = 0.900536060333252\n",
      "Training iter #28983000:   Batch Loss = 7.026761, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.420401096343994, Accuracy = 0.9008338451385498\n",
      "Training iter #28986000:   Batch Loss = 7.058124, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4200873374938965, Accuracy = 0.9011316299438477\n",
      "Training iter #28989000:   Batch Loss = 7.071178, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.419858932495117, Accuracy = 0.9011316299438477\n",
      "Training iter #28992000:   Batch Loss = 7.107438, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.419721603393555, Accuracy = 0.9011316299438477\n",
      "Training iter #28995000:   Batch Loss = 7.056538, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.419595718383789, Accuracy = 0.9011316299438477\n",
      "Training iter #28998000:   Batch Loss = 7.033123, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4195122718811035, Accuracy = 0.9008338451385498\n",
      "Training iter #29001000:   Batch Loss = 7.068497, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.419497489929199, Accuracy = 0.9008338451385498\n",
      "Training iter #29004000:   Batch Loss = 7.095577, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.419771671295166, Accuracy = 0.900536060333252\n",
      "Training iter #29007000:   Batch Loss = 7.071672, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.420109748840332, Accuracy = 0.9002382159233093\n",
      "Training iter #29010000:   Batch Loss = 7.027784, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.420328140258789, Accuracy = 0.9002382159233093\n",
      "Training iter #29013000:   Batch Loss = 7.058852, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.42038631439209, Accuracy = 0.9002382159233093\n",
      "Training iter #29016000:   Batch Loss = 7.075696, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.420355319976807, Accuracy = 0.900536060333252\n",
      "Training iter #29019000:   Batch Loss = 7.114666, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.420235633850098, Accuracy = 0.900536060333252\n",
      "Training iter #29022000:   Batch Loss = 7.049977, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.420009613037109, Accuracy = 0.900536060333252\n",
      "Training iter #29025000:   Batch Loss = 7.031075, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.419760227203369, Accuracy = 0.900536060333252\n",
      "Training iter #29028000:   Batch Loss = 7.067906, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.419417858123779, Accuracy = 0.900536060333252\n",
      "Training iter #29031000:   Batch Loss = 7.092902, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4188923835754395, Accuracy = 0.9008338451385498\n",
      "Training iter #29034000:   Batch Loss = 7.076057, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.417786598205566, Accuracy = 0.9014294147491455\n",
      "Training iter #29037000:   Batch Loss = 7.022225, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.417538642883301, Accuracy = 0.9017271995544434\n",
      "Training iter #29040000:   Batch Loss = 7.058410, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.417293071746826, Accuracy = 0.9017271995544434\n",
      "Training iter #29043000:   Batch Loss = 7.060244, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.417023181915283, Accuracy = 0.9017271995544434\n",
      "Training iter #29046000:   Batch Loss = 7.112785, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.417789459228516, Accuracy = 0.9014294147491455\n",
      "Training iter #29049000:   Batch Loss = 7.046002, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418281078338623, Accuracy = 0.9011316299438477\n",
      "Training iter #29052000:   Batch Loss = 7.027678, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418555736541748, Accuracy = 0.9014294147491455\n",
      "Training iter #29055000:   Batch Loss = 7.080498, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418915748596191, Accuracy = 0.9011316299438477\n",
      "Training iter #29058000:   Batch Loss = 7.090603, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.41895866394043, Accuracy = 0.9008338451385498\n",
      "Training iter #29061000:   Batch Loss = 7.073522, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418878078460693, Accuracy = 0.900536060333252\n",
      "Training iter #29064000:   Batch Loss = 7.022659, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418704986572266, Accuracy = 0.900536060333252\n",
      "Training iter #29067000:   Batch Loss = 7.059560, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418614864349365, Accuracy = 0.900536060333252\n",
      "Training iter #29070000:   Batch Loss = 7.061107, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418680191040039, Accuracy = 0.9002382159233093\n",
      "Training iter #29073000:   Batch Loss = 7.110987, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418591499328613, Accuracy = 0.9002382159233093\n",
      "Training iter #29076000:   Batch Loss = 7.037973, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.418494701385498, Accuracy = 0.9002382159233093\n",
      "Training iter #29079000:   Batch Loss = 7.027137, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4182515144348145, Accuracy = 0.900536060333252\n",
      "Training iter #29082000:   Batch Loss = 7.080672, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.417890548706055, Accuracy = 0.900536060333252\n",
      "Training iter #29085000:   Batch Loss = 7.085596, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.417339324951172, Accuracy = 0.9008338451385498\n",
      "Training iter #29088000:   Batch Loss = 7.077702, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.416285514831543, Accuracy = 0.8999404311180115\n",
      "Training iter #29091000:   Batch Loss = 7.024459, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.415977954864502, Accuracy = 0.9002382159233093\n",
      "Training iter #29094000:   Batch Loss = 7.057786, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.415673732757568, Accuracy = 0.9008338451385498\n",
      "Training iter #29097000:   Batch Loss = 7.061121, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.415320873260498, Accuracy = 0.900536060333252\n",
      "Training iter #29100000:   Batch Loss = 7.110485, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.415070533752441, Accuracy = 0.900536060333252\n",
      "Training iter #29103000:   Batch Loss = 7.024498, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414888381958008, Accuracy = 0.900536060333252\n",
      "Training iter #29106000:   Batch Loss = 7.021558, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414684772491455, Accuracy = 0.9008338451385498\n",
      "Training iter #29109000:   Batch Loss = 7.083719, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414433002471924, Accuracy = 0.900536060333252\n",
      "Training iter #29112000:   Batch Loss = 7.089005, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414421081542969, Accuracy = 0.900536060333252\n",
      "Training iter #29115000:   Batch Loss = 7.075458, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414424896240234, Accuracy = 0.9011316299438477\n",
      "Training iter #29118000:   Batch Loss = 7.030067, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414409637451172, Accuracy = 0.9008338451385498\n",
      "Training iter #29121000:   Batch Loss = 7.056448, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414342403411865, Accuracy = 0.9008338451385498\n",
      "Training iter #29124000:   Batch Loss = 7.066330, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414299488067627, Accuracy = 0.9008338451385498\n",
      "Training iter #29127000:   Batch Loss = 7.097392, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.414063930511475, Accuracy = 0.9008338451385498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #29130000:   Batch Loss = 7.020867, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.41390323638916, Accuracy = 0.9008338451385498\n",
      "Training iter #29133000:   Batch Loss = 7.022359, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.413671493530273, Accuracy = 0.9008338451385498\n",
      "Training iter #29136000:   Batch Loss = 7.078674, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.413364410400391, Accuracy = 0.9014294147491455\n",
      "Training iter #29139000:   Batch Loss = 7.085920, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.413278579711914, Accuracy = 0.9011316299438477\n",
      "Training iter #29142000:   Batch Loss = 7.071649, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.413355350494385, Accuracy = 0.9014294147491455\n",
      "Training iter #29145000:   Batch Loss = 7.028817, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.413452625274658, Accuracy = 0.9014294147491455\n",
      "Training iter #29148000:   Batch Loss = 7.057191, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.413384437561035, Accuracy = 0.9011316299438477\n",
      "Training iter #29151000:   Batch Loss = 7.069138, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.413641452789307, Accuracy = 0.900536060333252\n",
      "Training iter #29154000:   Batch Loss = 7.092399, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.412687301635742, Accuracy = 0.9014294147491455\n",
      "Training iter #29157000:   Batch Loss = 7.014544, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.412253379821777, Accuracy = 0.9014294147491455\n",
      "Training iter #29160000:   Batch Loss = 7.021251, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.411793231964111, Accuracy = 0.9014294147491455\n",
      "Training iter #29163000:   Batch Loss = 7.068013, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.411357879638672, Accuracy = 0.9014294147491455\n",
      "Training iter #29166000:   Batch Loss = 7.093929, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.411212921142578, Accuracy = 0.9014294147491455\n",
      "Training iter #29169000:   Batch Loss = 7.074737, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4113054275512695, Accuracy = 0.9011316299438477\n",
      "Training iter #29172000:   Batch Loss = 7.033571, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.411369800567627, Accuracy = 0.9011316299438477\n",
      "Training iter #29175000:   Batch Loss = 7.058867, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.411373615264893, Accuracy = 0.9011316299438477\n",
      "Training iter #29178000:   Batch Loss = 7.062610, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.411345481872559, Accuracy = 0.9011316299438477\n",
      "Training iter #29181000:   Batch Loss = 7.081101, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.410315990447998, Accuracy = 0.902025043964386\n",
      "Training iter #29184000:   Batch Loss = 7.009506, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.41004753112793, Accuracy = 0.9017271995544434\n",
      "Training iter #29187000:   Batch Loss = 7.025314, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.409786224365234, Accuracy = 0.9023228287696838\n",
      "Training iter #29190000:   Batch Loss = 7.064020, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.409467697143555, Accuracy = 0.9023228287696838\n",
      "Training iter #29193000:   Batch Loss = 7.092858, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.409343242645264, Accuracy = 0.9017271995544434\n",
      "Training iter #29196000:   Batch Loss = 7.072043, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.409224510192871, Accuracy = 0.9017271995544434\n",
      "Training iter #29199000:   Batch Loss = 7.032911, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.409048557281494, Accuracy = 0.9023228287696838\n",
      "Training iter #29202000:   Batch Loss = 7.056844, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408860206604004, Accuracy = 0.9026206135749817\n",
      "Training iter #29205000:   Batch Loss = 7.061443, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4088239669799805, Accuracy = 0.9023228287696838\n",
      "Training iter #29208000:   Batch Loss = 7.082078, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408666133880615, Accuracy = 0.902025043964386\n",
      "Training iter #29211000:   Batch Loss = 7.008946, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408658027648926, Accuracy = 0.902025043964386\n",
      "Training iter #29214000:   Batch Loss = 7.028881, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408712387084961, Accuracy = 0.9017271995544434\n",
      "Training iter #29217000:   Batch Loss = 7.060612, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408666133880615, Accuracy = 0.9008338451385498\n",
      "Training iter #29220000:   Batch Loss = 7.102103, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408663749694824, Accuracy = 0.900536060333252\n",
      "Training iter #29223000:   Batch Loss = 7.058991, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408784866333008, Accuracy = 0.9014294147491455\n",
      "Training iter #29226000:   Batch Loss = 7.027369, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408787727355957, Accuracy = 0.9014294147491455\n",
      "Training iter #29229000:   Batch Loss = 7.061020, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408698081970215, Accuracy = 0.9014294147491455\n",
      "Training iter #29232000:   Batch Loss = 7.062963, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408659934997559, Accuracy = 0.9008338451385498\n",
      "Training iter #29235000:   Batch Loss = 7.080812, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408481121063232, Accuracy = 0.9014294147491455\n",
      "Training iter #29238000:   Batch Loss = 7.010580, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.408211708068848, Accuracy = 0.9023228287696838\n",
      "Training iter #29241000:   Batch Loss = 7.025923, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.407892227172852, Accuracy = 0.902025043964386\n",
      "Training iter #29244000:   Batch Loss = 7.057776, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.407569408416748, Accuracy = 0.9017271995544434\n",
      "Training iter #29247000:   Batch Loss = 7.096599, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.407347679138184, Accuracy = 0.9017271995544434\n",
      "Training iter #29250000:   Batch Loss = 7.047932, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.407149314880371, Accuracy = 0.902025043964386\n",
      "Training iter #29253000:   Batch Loss = 7.022733, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.406944274902344, Accuracy = 0.9023228287696838\n",
      "Training iter #29256000:   Batch Loss = 7.063688, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.406615257263184, Accuracy = 0.9023228287696838\n",
      "Training iter #29259000:   Batch Loss = 7.065217, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.406260967254639, Accuracy = 0.9026206135749817\n",
      "Training iter #29262000:   Batch Loss = 7.077975, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4063825607299805, Accuracy = 0.902025043964386\n",
      "Training iter #29265000:   Batch Loss = 7.012284, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.406763076782227, Accuracy = 0.9023228287696838\n",
      "Training iter #29268000:   Batch Loss = 7.025266, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.406966209411621, Accuracy = 0.902025043964386\n",
      "Training iter #29271000:   Batch Loss = 7.056016, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.406948089599609, Accuracy = 0.9023228287696838\n",
      "Training iter #29274000:   Batch Loss = 7.101130, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4057464599609375, Accuracy = 0.902025043964386\n",
      "Training iter #29277000:   Batch Loss = 7.042825, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405765533447266, Accuracy = 0.9017271995544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #29280000:   Batch Loss = 7.020807, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4057817459106445, Accuracy = 0.902025043964386\n",
      "Training iter #29283000:   Batch Loss = 7.067381, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405616760253906, Accuracy = 0.902025043964386\n",
      "Training iter #29286000:   Batch Loss = 7.062657, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405584812164307, Accuracy = 0.9017271995544434\n",
      "Training iter #29289000:   Batch Loss = 7.071949, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405494213104248, Accuracy = 0.9014294147491455\n",
      "Training iter #29292000:   Batch Loss = 7.012116, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405388832092285, Accuracy = 0.9014294147491455\n",
      "Training iter #29295000:   Batch Loss = 7.023791, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405274391174316, Accuracy = 0.9017271995544434\n",
      "Training iter #29298000:   Batch Loss = 7.053130, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405116081237793, Accuracy = 0.9011316299438477\n",
      "Training iter #29301000:   Batch Loss = 7.101717, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405115127563477, Accuracy = 0.9017271995544434\n",
      "Training iter #29304000:   Batch Loss = 7.040110, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405041217803955, Accuracy = 0.9014294147491455\n",
      "Training iter #29307000:   Batch Loss = 7.021547, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4054789543151855, Accuracy = 0.900536060333252\n",
      "Training iter #29310000:   Batch Loss = 7.050252, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.40703010559082, Accuracy = 0.9002382159233093\n",
      "Training iter #29313000:   Batch Loss = 7.081318, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.406810760498047, Accuracy = 0.8999404311180115\n",
      "Training iter #29316000:   Batch Loss = 7.067538, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.404932975769043, Accuracy = 0.900536060333252\n",
      "Training iter #29319000:   Batch Loss = 7.013710, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403607368469238, Accuracy = 0.900536060333252\n",
      "Training iter #29322000:   Batch Loss = 7.040856, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403297424316406, Accuracy = 0.9011316299438477\n",
      "Training iter #29325000:   Batch Loss = 7.056461, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403019905090332, Accuracy = 0.9011316299438477\n",
      "Training iter #29328000:   Batch Loss = 7.098633, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4029059410095215, Accuracy = 0.9014294147491455\n",
      "Training iter #29331000:   Batch Loss = 7.038018, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403251647949219, Accuracy = 0.9014294147491455\n",
      "Training iter #29334000:   Batch Loss = 7.016128, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403343200683594, Accuracy = 0.902025043964386\n",
      "Training iter #29337000:   Batch Loss = 7.049679, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403563499450684, Accuracy = 0.9017271995544434\n",
      "Training iter #29340000:   Batch Loss = 7.073893, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4043169021606445, Accuracy = 0.9017271995544434\n",
      "Training iter #29343000:   Batch Loss = 7.059102, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4049391746521, Accuracy = 0.9014294147491455\n",
      "Training iter #29346000:   Batch Loss = 7.007918, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.405324935913086, Accuracy = 0.9017271995544434\n",
      "Training iter #29349000:   Batch Loss = 7.042738, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4054179191589355, Accuracy = 0.9011316299438477\n",
      "Training iter #29352000:   Batch Loss = 7.057226, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4052839279174805, Accuracy = 0.9014294147491455\n",
      "Training iter #29355000:   Batch Loss = 7.095046, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4054059982299805, Accuracy = 0.9008338451385498\n",
      "Training iter #29358000:   Batch Loss = 7.028879, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.404158592224121, Accuracy = 0.9023228287696838\n",
      "Training iter #29361000:   Batch Loss = 7.014555, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403637886047363, Accuracy = 0.902025043964386\n",
      "Training iter #29364000:   Batch Loss = 7.063944, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403414726257324, Accuracy = 0.902025043964386\n",
      "Training iter #29367000:   Batch Loss = 7.074512, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.403127670288086, Accuracy = 0.9023228287696838\n",
      "Training iter #29370000:   Batch Loss = 7.056190, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.402720928192139, Accuracy = 0.9014294147491455\n",
      "Training iter #29373000:   Batch Loss = 7.006378, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.40233039855957, Accuracy = 0.9017271995544434\n",
      "Training iter #29376000:   Batch Loss = 7.040996, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4018635749816895, Accuracy = 0.9017271995544434\n",
      "Training iter #29379000:   Batch Loss = 7.043696, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.401196479797363, Accuracy = 0.9017271995544434\n",
      "Training iter #29382000:   Batch Loss = 7.094248, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.400931358337402, Accuracy = 0.9017271995544434\n",
      "Training iter #29385000:   Batch Loss = 7.027368, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4008588790893555, Accuracy = 0.9017271995544434\n",
      "Training iter #29388000:   Batch Loss = 7.011694, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.400723457336426, Accuracy = 0.9014294147491455\n",
      "Training iter #29391000:   Batch Loss = 7.062817, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.400448799133301, Accuracy = 0.9017271995544434\n",
      "Training iter #29394000:   Batch Loss = 7.068884, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.400117874145508, Accuracy = 0.9017271995544434\n",
      "Training iter #29397000:   Batch Loss = 7.060109, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.399929523468018, Accuracy = 0.902025043964386\n",
      "Training iter #29400000:   Batch Loss = 7.007181, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.400120258331299, Accuracy = 0.9008338451385498\n",
      "Training iter #29403000:   Batch Loss = 7.040024, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.4000115394592285, Accuracy = 0.9014294147491455\n",
      "Training iter #29406000:   Batch Loss = 7.044316, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.399880409240723, Accuracy = 0.9008338451385498\n",
      "Training iter #29409000:   Batch Loss = 7.093977, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.399802207946777, Accuracy = 0.9008338451385498\n",
      "Training iter #29412000:   Batch Loss = 7.014091, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.399661540985107, Accuracy = 0.9008338451385498\n",
      "Training iter #29415000:   Batch Loss = 7.009850, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.399477005004883, Accuracy = 0.9011316299438477\n",
      "Training iter #29418000:   Batch Loss = 7.062784, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.399130344390869, Accuracy = 0.9011316299438477\n",
      "Training iter #29421000:   Batch Loss = 7.069108, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.398845195770264, Accuracy = 0.9011316299438477\n",
      "Training iter #29424000:   Batch Loss = 7.059116, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.398723602294922, Accuracy = 0.9011316299438477\n",
      "Training iter #29427000:   Batch Loss = 7.012295, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.398580551147461, Accuracy = 0.9014294147491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #29430000:   Batch Loss = 7.040534, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3983330726623535, Accuracy = 0.9014294147491455\n",
      "Training iter #29433000:   Batch Loss = 7.039444, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.398192882537842, Accuracy = 0.9011316299438477\n",
      "Training iter #29436000:   Batch Loss = 7.087443, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.398177146911621, Accuracy = 0.9014294147491455\n",
      "Training iter #29439000:   Batch Loss = 7.005654, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.398105144500732, Accuracy = 0.9017271995544434\n",
      "Training iter #29442000:   Batch Loss = 7.003640, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.397949695587158, Accuracy = 0.9017271995544434\n",
      "Training iter #29445000:   Batch Loss = 7.065391, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3976149559021, Accuracy = 0.9017271995544434\n",
      "Training iter #29448000:   Batch Loss = 7.075326, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.397374153137207, Accuracy = 0.9017271995544434\n",
      "Training iter #29451000:   Batch Loss = 7.056726, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.397043704986572, Accuracy = 0.9017271995544434\n",
      "Training iter #29454000:   Batch Loss = 7.013593, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.39656925201416, Accuracy = 0.9017271995544434\n",
      "Training iter #29457000:   Batch Loss = 7.041947, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.396215438842773, Accuracy = 0.9017271995544434\n",
      "Training iter #29460000:   Batch Loss = 7.050348, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.395969390869141, Accuracy = 0.9014294147491455\n",
      "Training iter #29463000:   Batch Loss = 7.078373, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.395941734313965, Accuracy = 0.9014294147491455\n",
      "Training iter #29466000:   Batch Loss = 7.003300, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.395709991455078, Accuracy = 0.9017271995544434\n",
      "Training iter #29469000:   Batch Loss = 7.004987, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.395368576049805, Accuracy = 0.902025043964386\n",
      "Training iter #29472000:   Batch Loss = 7.059666, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.39503812789917, Accuracy = 0.902025043964386\n",
      "Training iter #29475000:   Batch Loss = 7.069582, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.39501428604126, Accuracy = 0.902025043964386\n",
      "Training iter #29478000:   Batch Loss = 7.057745, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.39516544342041, Accuracy = 0.902025043964386\n",
      "Training iter #29481000:   Batch Loss = 7.012530, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.395086288452148, Accuracy = 0.902025043964386\n",
      "Training iter #29484000:   Batch Loss = 7.039581, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.394618988037109, Accuracy = 0.902025043964386\n",
      "Training iter #29487000:   Batch Loss = 7.050312, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.394444465637207, Accuracy = 0.9017271995544434\n",
      "Training iter #29490000:   Batch Loss = 7.069733, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.394367218017578, Accuracy = 0.9017271995544434\n",
      "Training iter #29493000:   Batch Loss = 6.997279, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.394251823425293, Accuracy = 0.9017271995544434\n",
      "Training iter #29496000:   Batch Loss = 7.003725, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.394190788269043, Accuracy = 0.902025043964386\n",
      "Training iter #29499000:   Batch Loss = 7.049072, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.393857479095459, Accuracy = 0.902025043964386\n",
      "Training iter #29502000:   Batch Loss = 7.076151, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.393477916717529, Accuracy = 0.902025043964386\n",
      "Training iter #29505000:   Batch Loss = 7.058902, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3934006690979, Accuracy = 0.9017271995544434\n",
      "Training iter #29508000:   Batch Loss = 7.016158, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.393361568450928, Accuracy = 0.9017271995544434\n",
      "Training iter #29511000:   Batch Loss = 7.043735, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.393128395080566, Accuracy = 0.9017271995544434\n",
      "Training iter #29514000:   Batch Loss = 7.045700, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.393077373504639, Accuracy = 0.9017271995544434\n",
      "Training iter #29517000:   Batch Loss = 7.064595, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.392781734466553, Accuracy = 0.9017271995544434\n",
      "Training iter #29520000:   Batch Loss = 6.992721, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3930792808532715, Accuracy = 0.9017271995544434\n",
      "Training iter #29523000:   Batch Loss = 7.009947, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.393310546875, Accuracy = 0.9014294147491455\n",
      "Training iter #29526000:   Batch Loss = 7.048601, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.393355846405029, Accuracy = 0.9011316299438477\n",
      "Training iter #29529000:   Batch Loss = 7.075209, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.39308500289917, Accuracy = 0.9014294147491455\n",
      "Training iter #29532000:   Batch Loss = 7.049431, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.392631530761719, Accuracy = 0.9014294147491455\n",
      "Training iter #29535000:   Batch Loss = 7.015450, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.392096519470215, Accuracy = 0.902025043964386\n",
      "Training iter #29538000:   Batch Loss = 7.040216, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.391402244567871, Accuracy = 0.9023228287696838\n",
      "Training iter #29541000:   Batch Loss = 7.042326, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3910675048828125, Accuracy = 0.9023228287696838\n",
      "Training iter #29544000:   Batch Loss = 7.057807, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390993595123291, Accuracy = 0.9023228287696838\n",
      "Training iter #29547000:   Batch Loss = 6.996369, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390963554382324, Accuracy = 0.9017271995544434\n",
      "Training iter #29550000:   Batch Loss = 7.011523, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390951633453369, Accuracy = 0.9017271995544434\n",
      "Training iter #29553000:   Batch Loss = 7.046781, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3909406661987305, Accuracy = 0.902025043964386\n",
      "Training iter #29556000:   Batch Loss = 7.076823, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390995025634766, Accuracy = 0.9017271995544434\n",
      "Training iter #29559000:   Batch Loss = 7.041533, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.391031265258789, Accuracy = 0.9014294147491455\n",
      "Training iter #29562000:   Batch Loss = 7.010066, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.391077995300293, Accuracy = 0.9014294147491455\n",
      "Training iter #29565000:   Batch Loss = 7.047756, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.391012668609619, Accuracy = 0.9014294147491455\n",
      "Training iter #29568000:   Batch Loss = 7.046455, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390894412994385, Accuracy = 0.9011316299438477\n",
      "Training iter #29571000:   Batch Loss = 7.062935, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390907287597656, Accuracy = 0.9014294147491455\n",
      "Training iter #29574000:   Batch Loss = 6.993155, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390868186950684, Accuracy = 0.9014294147491455\n",
      "Training iter #29577000:   Batch Loss = 7.008537, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390861511230469, Accuracy = 0.9017271995544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #29580000:   Batch Loss = 7.041046, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390757083892822, Accuracy = 0.9017271995544434\n",
      "Training iter #29583000:   Batch Loss = 7.080066, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390661716461182, Accuracy = 0.9017271995544434\n",
      "Training iter #29586000:   Batch Loss = 7.028719, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390480995178223, Accuracy = 0.9017271995544434\n",
      "Training iter #29589000:   Batch Loss = 7.006337, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3903727531433105, Accuracy = 0.9017271995544434\n",
      "Training iter #29592000:   Batch Loss = 7.049389, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3902082443237305, Accuracy = 0.9011316299438477\n",
      "Training iter #29595000:   Batch Loss = 7.044451, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.390092849731445, Accuracy = 0.9011316299438477\n",
      "Training iter #29598000:   Batch Loss = 7.058524, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.389691352844238, Accuracy = 0.9017271995544434\n",
      "Training iter #29601000:   Batch Loss = 6.996402, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3887529373168945, Accuracy = 0.902025043964386\n",
      "Training iter #29604000:   Batch Loss = 7.009112, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388504981994629, Accuracy = 0.902025043964386\n",
      "Training iter #29607000:   Batch Loss = 7.035886, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388202667236328, Accuracy = 0.902025043964386\n",
      "Training iter #29610000:   Batch Loss = 7.083329, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.38803768157959, Accuracy = 0.902025043964386\n",
      "Training iter #29613000:   Batch Loss = 7.024927, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388022422790527, Accuracy = 0.902025043964386\n",
      "Training iter #29616000:   Batch Loss = 7.005697, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388072967529297, Accuracy = 0.902025043964386\n",
      "Training iter #29619000:   Batch Loss = 7.049336, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388437747955322, Accuracy = 0.9017271995544434\n",
      "Training iter #29622000:   Batch Loss = 7.048977, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388421535491943, Accuracy = 0.9014294147491455\n",
      "Training iter #29625000:   Batch Loss = 7.053762, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3887763023376465, Accuracy = 0.902025043964386\n",
      "Training iter #29628000:   Batch Loss = 6.994055, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388582229614258, Accuracy = 0.902025043964386\n",
      "Training iter #29631000:   Batch Loss = 7.022103, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388931751251221, Accuracy = 0.9014294147491455\n",
      "Training iter #29634000:   Batch Loss = 7.034999, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388896942138672, Accuracy = 0.9014294147491455\n",
      "Training iter #29637000:   Batch Loss = 7.069183, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388598442077637, Accuracy = 0.9011316299438477\n",
      "Training iter #29640000:   Batch Loss = 7.020873, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388433456420898, Accuracy = 0.9011316299438477\n",
      "Training iter #29643000:   Batch Loss = 7.001920, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388429164886475, Accuracy = 0.9008338451385498\n",
      "Training iter #29646000:   Batch Loss = 7.032207, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388461589813232, Accuracy = 0.9002382159233093\n",
      "Training iter #29649000:   Batch Loss = 7.058628, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388472557067871, Accuracy = 0.9002382159233093\n",
      "Training iter #29652000:   Batch Loss = 7.042994, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388289928436279, Accuracy = 0.9014294147491455\n",
      "Training iter #29655000:   Batch Loss = 6.994969, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387720108032227, Accuracy = 0.9014294147491455\n",
      "Training iter #29658000:   Batch Loss = 7.023977, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387880325317383, Accuracy = 0.9011316299438477\n",
      "Training iter #29661000:   Batch Loss = 7.040785, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387702941894531, Accuracy = 0.9014294147491455\n",
      "Training iter #29664000:   Batch Loss = 7.077285, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387377738952637, Accuracy = 0.9017271995544434\n",
      "Training iter #29667000:   Batch Loss = 7.016946, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387171268463135, Accuracy = 0.9017271995544434\n",
      "Training iter #29670000:   Batch Loss = 6.998600, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387110233306885, Accuracy = 0.9017271995544434\n",
      "Training iter #29673000:   Batch Loss = 7.032671, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387516498565674, Accuracy = 0.9008338451385498\n",
      "Training iter #29676000:   Batch Loss = 7.056530, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388049602508545, Accuracy = 0.9011316299438477\n",
      "Training iter #29679000:   Batch Loss = 7.043625, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.388209342956543, Accuracy = 0.9011316299438477\n",
      "Training iter #29682000:   Batch Loss = 6.989425, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387436389923096, Accuracy = 0.9011316299438477\n",
      "Training iter #29685000:   Batch Loss = 7.023649, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387333393096924, Accuracy = 0.9014294147491455\n",
      "Training iter #29688000:   Batch Loss = 7.030852, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.387060165405273, Accuracy = 0.9011316299438477\n",
      "Training iter #29691000:   Batch Loss = 7.075928, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.386516571044922, Accuracy = 0.9008338451385498\n",
      "Training iter #29694000:   Batch Loss = 7.011907, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3859944343566895, Accuracy = 0.9008338451385498\n",
      "Training iter #29697000:   Batch Loss = 6.995339, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.385491371154785, Accuracy = 0.9008338451385498\n",
      "Training iter #29700000:   Batch Loss = 7.045935, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.386250972747803, Accuracy = 0.9008338451385498\n",
      "Training iter #29703000:   Batch Loss = 7.056026, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.385676383972168, Accuracy = 0.9014294147491455\n",
      "Training iter #29706000:   Batch Loss = 7.037847, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.383853912353516, Accuracy = 0.9017271995544434\n",
      "Training iter #29709000:   Batch Loss = 6.989515, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382575035095215, Accuracy = 0.9029183983802795\n",
      "Training iter #29712000:   Batch Loss = 7.024802, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382625579833984, Accuracy = 0.9029183983802795\n",
      "Training iter #29715000:   Batch Loss = 7.026355, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3830037117004395, Accuracy = 0.9026206135749817\n",
      "Training iter #29718000:   Batch Loss = 7.072990, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.383692264556885, Accuracy = 0.9026206135749817\n",
      "Training iter #29721000:   Batch Loss = 7.004753, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382467746734619, Accuracy = 0.9026206135749817\n",
      "Training iter #29724000:   Batch Loss = 6.995045, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382562637329102, Accuracy = 0.9023228287696838\n",
      "Training iter #29727000:   Batch Loss = 7.044682, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3824334144592285, Accuracy = 0.9017271995544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #29730000:   Batch Loss = 7.050047, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382565498352051, Accuracy = 0.9014294147491455\n",
      "Training iter #29733000:   Batch Loss = 7.044425, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382936000823975, Accuracy = 0.9008338451385498\n",
      "Training iter #29736000:   Batch Loss = 6.991213, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382923126220703, Accuracy = 0.9011316299438477\n",
      "Training iter #29739000:   Batch Loss = 7.023289, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.383781909942627, Accuracy = 0.900536060333252\n",
      "Training iter #29742000:   Batch Loss = 7.024300, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3839616775512695, Accuracy = 0.900536060333252\n",
      "Training iter #29745000:   Batch Loss = 7.074525, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.38380241394043, Accuracy = 0.9008338451385498\n",
      "Training iter #29748000:   Batch Loss = 6.992177, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.383698463439941, Accuracy = 0.9008338451385498\n",
      "Training iter #29751000:   Batch Loss = 6.988220, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.383620738983154, Accuracy = 0.9008338451385498\n",
      "Training iter #29754000:   Batch Loss = 7.046327, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3838019371032715, Accuracy = 0.9002382159233093\n",
      "Training iter #29757000:   Batch Loss = 7.052723, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.384461879730225, Accuracy = 0.9002382159233093\n",
      "Training iter #29760000:   Batch Loss = 7.042073, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.384016990661621, Accuracy = 0.900536060333252\n",
      "Training iter #29763000:   Batch Loss = 6.996222, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.38344144821167, Accuracy = 0.9008338451385498\n",
      "Training iter #29766000:   Batch Loss = 7.021396, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.383481502532959, Accuracy = 0.900536060333252\n",
      "Training iter #29769000:   Batch Loss = 7.024813, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3834638595581055, Accuracy = 0.900536060333252\n",
      "Training iter #29772000:   Batch Loss = 7.064616, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.383147239685059, Accuracy = 0.9011316299438477\n",
      "Training iter #29775000:   Batch Loss = 6.988192, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382786273956299, Accuracy = 0.9008338451385498\n",
      "Training iter #29778000:   Batch Loss = 6.986420, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3828840255737305, Accuracy = 0.900536060333252\n",
      "Training iter #29781000:   Batch Loss = 7.043600, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382881164550781, Accuracy = 0.900536060333252\n",
      "Training iter #29784000:   Batch Loss = 7.051240, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382895469665527, Accuracy = 0.900536060333252\n",
      "Training iter #29787000:   Batch Loss = 7.036782, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382928848266602, Accuracy = 0.9008338451385498\n",
      "Training iter #29790000:   Batch Loss = 6.996354, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382200717926025, Accuracy = 0.9014294147491455\n",
      "Training iter #29793000:   Batch Loss = 7.022246, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.381642818450928, Accuracy = 0.9014294147491455\n",
      "Training iter #29796000:   Batch Loss = 7.033978, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3810715675354, Accuracy = 0.9014294147491455\n",
      "Training iter #29799000:   Batch Loss = 7.057250, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.381031036376953, Accuracy = 0.9014294147491455\n",
      "Training iter #29802000:   Batch Loss = 6.983395, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.381459712982178, Accuracy = 0.9008338451385498\n",
      "Training iter #29805000:   Batch Loss = 6.987442, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382381916046143, Accuracy = 0.900536060333252\n",
      "Training iter #29808000:   Batch Loss = 7.037978, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382505416870117, Accuracy = 0.8999404311180115\n",
      "Training iter #29811000:   Batch Loss = 7.054759, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.382009983062744, Accuracy = 0.8999404311180115\n",
      "Training iter #29814000:   Batch Loss = 7.040234, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.381871700286865, Accuracy = 0.8999404311180115\n",
      "Training iter #29817000:   Batch Loss = 7.004409, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.380340576171875, Accuracy = 0.900536060333252\n",
      "Training iter #29820000:   Batch Loss = 7.027393, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.379854679107666, Accuracy = 0.900536060333252\n",
      "Training iter #29823000:   Batch Loss = 7.030443, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.37966251373291, Accuracy = 0.900536060333252\n",
      "Training iter #29826000:   Batch Loss = 7.045897, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.380009174346924, Accuracy = 0.9008338451385498\n",
      "Training iter #29829000:   Batch Loss = 6.977068, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3794121742248535, Accuracy = 0.9008338451385498\n",
      "Training iter #29832000:   Batch Loss = 6.992039, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.379820823669434, Accuracy = 0.900536060333252\n",
      "Training iter #29835000:   Batch Loss = 7.029968, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.379748344421387, Accuracy = 0.900536060333252\n",
      "Training iter #29838000:   Batch Loss = 7.056772, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.379555702209473, Accuracy = 0.900536060333252\n",
      "Training iter #29841000:   Batch Loss = 7.039102, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.379673480987549, Accuracy = 0.8999404311180115\n",
      "Training iter #29844000:   Batch Loss = 7.003170, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.379245758056641, Accuracy = 0.8999404311180115\n",
      "Training iter #29847000:   Batch Loss = 7.023569, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.378565788269043, Accuracy = 0.900536060333252\n",
      "Training iter #29850000:   Batch Loss = 7.026154, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.378166198730469, Accuracy = 0.900536060333252\n",
      "Training iter #29853000:   Batch Loss = 7.046618, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.37776517868042, Accuracy = 0.9011316299438477\n",
      "Training iter #29856000:   Batch Loss = 6.976019, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.377096176147461, Accuracy = 0.9014294147491455\n",
      "Training iter #29859000:   Batch Loss = 6.994327, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.377053260803223, Accuracy = 0.9008338451385498\n",
      "Training iter #29862000:   Batch Loss = 7.031929, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.376904487609863, Accuracy = 0.9011316299438477\n",
      "Training iter #29865000:   Batch Loss = 7.061049, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.376763820648193, Accuracy = 0.9011316299438477\n",
      "Training iter #29868000:   Batch Loss = 7.027483, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3767170906066895, Accuracy = 0.9011316299438477\n",
      "Training iter #29871000:   Batch Loss = 6.995574, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.375981330871582, Accuracy = 0.9014294147491455\n",
      "Training iter #29874000:   Batch Loss = 7.026541, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.375837802886963, Accuracy = 0.9014294147491455\n",
      "Training iter #29877000:   Batch Loss = 7.028140, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.37580680847168, Accuracy = 0.9017271995544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #29880000:   Batch Loss = 7.040751, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.375230312347412, Accuracy = 0.9011316299438477\n",
      "Training iter #29883000:   Batch Loss = 6.978115, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.375036716461182, Accuracy = 0.9014294147491455\n",
      "Training iter #29886000:   Batch Loss = 6.993896, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.375128269195557, Accuracy = 0.9017271995544434\n",
      "Training iter #29889000:   Batch Loss = 7.024710, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.375065803527832, Accuracy = 0.9014294147491455\n",
      "Training iter #29892000:   Batch Loss = 7.060930, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.374693870544434, Accuracy = 0.9017271995544434\n",
      "Training iter #29895000:   Batch Loss = 7.014778, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.374629020690918, Accuracy = 0.902025043964386\n",
      "Training iter #29898000:   Batch Loss = 6.994346, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3744587898254395, Accuracy = 0.9017271995544434\n",
      "Training iter #29901000:   Batch Loss = 7.029757, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373915672302246, Accuracy = 0.902025043964386\n",
      "Training iter #29904000:   Batch Loss = 7.030258, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373663902282715, Accuracy = 0.902025043964386\n",
      "Training iter #29907000:   Batch Loss = 7.041702, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3733906745910645, Accuracy = 0.9023228287696838\n",
      "Training iter #29910000:   Batch Loss = 6.979475, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373088836669922, Accuracy = 0.9026206135749817\n",
      "Training iter #29913000:   Batch Loss = 6.991138, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.372936725616455, Accuracy = 0.9023228287696838\n",
      "Training iter #29916000:   Batch Loss = 7.022576, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373046875, Accuracy = 0.902025043964386\n",
      "Training iter #29919000:   Batch Loss = 7.063200, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373627185821533, Accuracy = 0.9017271995544434\n",
      "Training iter #29922000:   Batch Loss = 7.009912, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373473644256592, Accuracy = 0.902025043964386\n",
      "Training iter #29925000:   Batch Loss = 6.988494, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373321533203125, Accuracy = 0.9023228287696838\n",
      "Training iter #29928000:   Batch Loss = 7.033235, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373290061950684, Accuracy = 0.9023228287696838\n",
      "Training iter #29931000:   Batch Loss = 7.026671, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373241901397705, Accuracy = 0.9023228287696838\n",
      "Training iter #29934000:   Batch Loss = 7.038434, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373217582702637, Accuracy = 0.9023228287696838\n",
      "Training iter #29937000:   Batch Loss = 6.978928, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373780727386475, Accuracy = 0.902025043964386\n",
      "Training iter #29940000:   Batch Loss = 6.989130, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3737874031066895, Accuracy = 0.902025043964386\n",
      "Training iter #29943000:   Batch Loss = 7.017764, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373667240142822, Accuracy = 0.9017271995544434\n",
      "Training iter #29946000:   Batch Loss = 7.063113, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373685359954834, Accuracy = 0.9014294147491455\n",
      "Training iter #29949000:   Batch Loss = 7.006952, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373668193817139, Accuracy = 0.9014294147491455\n",
      "Training iter #29952000:   Batch Loss = 6.988616, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373605728149414, Accuracy = 0.9014294147491455\n",
      "Training iter #29955000:   Batch Loss = 7.014925, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.373291015625, Accuracy = 0.9014294147491455\n",
      "Training iter #29958000:   Batch Loss = 7.031447, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.372823715209961, Accuracy = 0.9017271995544434\n",
      "Training iter #29961000:   Batch Loss = 7.031527, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.372745513916016, Accuracy = 0.902025043964386\n",
      "Training iter #29964000:   Batch Loss = 6.977338, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.372603893280029, Accuracy = 0.902025043964386\n",
      "Training iter #29967000:   Batch Loss = 7.005563, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.372519016265869, Accuracy = 0.902025043964386\n",
      "Training iter #29970000:   Batch Loss = 7.021172, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.372247219085693, Accuracy = 0.902025043964386\n",
      "Training iter #29973000:   Batch Loss = 7.055264, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.371989727020264, Accuracy = 0.902025043964386\n",
      "Training iter #29976000:   Batch Loss = 7.003134, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.37187385559082, Accuracy = 0.9017271995544434\n",
      "Training iter #29979000:   Batch Loss = 6.983408, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.371826648712158, Accuracy = 0.9017271995544434\n",
      "Training iter #29982000:   Batch Loss = 7.015043, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.371655464172363, Accuracy = 0.902025043964386\n",
      "Training iter #29985000:   Batch Loss = 7.037746, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.371448516845703, Accuracy = 0.9023228287696838\n",
      "Training iter #29988000:   Batch Loss = 7.018325, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.371452331542969, Accuracy = 0.9023228287696838\n",
      "Training iter #29991000:   Batch Loss = 6.975140, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370726108551025, Accuracy = 0.902025043964386\n",
      "Training iter #29994000:   Batch Loss = 7.006373, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370471000671387, Accuracy = 0.902025043964386\n",
      "Training iter #29997000:   Batch Loss = 7.021201, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370187282562256, Accuracy = 0.9017271995544434\n",
      "Training iter #30000000:   Batch Loss = 7.057968, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370001792907715, Accuracy = 0.9017271995544434\n",
      "Training iter #30003000:   Batch Loss = 6.995383, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370123386383057, Accuracy = 0.9017271995544434\n",
      "Training iter #30006000:   Batch Loss = 6.981329, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370502471923828, Accuracy = 0.902025043964386\n",
      "Training iter #30009000:   Batch Loss = 7.016585, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370391845703125, Accuracy = 0.9017271995544434\n",
      "Training iter #30012000:   Batch Loss = 7.038375, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370340347290039, Accuracy = 0.9017271995544434\n",
      "Training iter #30015000:   Batch Loss = 7.021867, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.370169162750244, Accuracy = 0.902025043964386\n",
      "Training iter #30018000:   Batch Loss = 6.974528, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.369723796844482, Accuracy = 0.902025043964386\n",
      "Training iter #30021000:   Batch Loss = 7.005918, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.368959426879883, Accuracy = 0.902025043964386\n",
      "Training iter #30024000:   Batch Loss = 7.008063, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.368074893951416, Accuracy = 0.902025043964386\n",
      "Training iter #30027000:   Batch Loss = 7.061479, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.367535591125488, Accuracy = 0.902025043964386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30030000:   Batch Loss = 6.994476, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366472244262695, Accuracy = 0.9026206135749817\n",
      "Training iter #30033000:   Batch Loss = 6.979290, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365579128265381, Accuracy = 0.9032161831855774\n",
      "Training iter #30036000:   Batch Loss = 7.025201, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365357875823975, Accuracy = 0.9035139679908752\n",
      "Training iter #30039000:   Batch Loss = 7.036615, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365744113922119, Accuracy = 0.9029183983802795\n",
      "Training iter #30042000:   Batch Loss = 7.024511, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366664886474609, Accuracy = 0.9023228287696838\n",
      "Training iter #30045000:   Batch Loss = 6.973693, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366969108581543, Accuracy = 0.9026206135749817\n",
      "Training iter #30048000:   Batch Loss = 7.006989, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.367275238037109, Accuracy = 0.902025043964386\n",
      "Training iter #30051000:   Batch Loss = 7.009070, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.367680549621582, Accuracy = 0.9017271995544434\n",
      "Training iter #30054000:   Batch Loss = 7.055570, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366687297821045, Accuracy = 0.9023228287696838\n",
      "Training iter #30057000:   Batch Loss = 6.982301, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365975856781006, Accuracy = 0.902025043964386\n",
      "Training iter #30060000:   Batch Loss = 6.976926, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365486145019531, Accuracy = 0.902025043964386\n",
      "Training iter #30063000:   Batch Loss = 7.028360, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365078449249268, Accuracy = 0.9023228287696838\n",
      "Training iter #30066000:   Batch Loss = 7.042424, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366246223449707, Accuracy = 0.9023228287696838\n",
      "Training iter #30069000:   Batch Loss = 7.025506, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.36672306060791, Accuracy = 0.9026206135749817\n",
      "Training iter #30072000:   Batch Loss = 6.979247, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.367072105407715, Accuracy = 0.902025043964386\n",
      "Training iter #30075000:   Batch Loss = 7.007692, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.367428302764893, Accuracy = 0.9017271995544434\n",
      "Training iter #30078000:   Batch Loss = 7.007442, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.367729187011719, Accuracy = 0.9014294147491455\n",
      "Training iter #30081000:   Batch Loss = 7.054963, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366416931152344, Accuracy = 0.9017271995544434\n",
      "Training iter #30084000:   Batch Loss = 6.972928, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3658976554870605, Accuracy = 0.9017271995544434\n",
      "Training iter #30087000:   Batch Loss = 6.971671, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.36651611328125, Accuracy = 0.9011316299438477\n",
      "Training iter #30090000:   Batch Loss = 7.030731, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366799354553223, Accuracy = 0.9011316299438477\n",
      "Training iter #30093000:   Batch Loss = 7.037295, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.366814613342285, Accuracy = 0.9014294147491455\n",
      "Training iter #30096000:   Batch Loss = 7.023146, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365870952606201, Accuracy = 0.902025043964386\n",
      "Training iter #30099000:   Batch Loss = 6.980119, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365898609161377, Accuracy = 0.9008338451385498\n",
      "Training iter #30102000:   Batch Loss = 7.007481, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365584373474121, Accuracy = 0.9011316299438477\n",
      "Training iter #30105000:   Batch Loss = 7.013307, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.365212917327881, Accuracy = 0.9011316299438477\n",
      "Training iter #30108000:   Batch Loss = 7.041764, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.364737510681152, Accuracy = 0.9011316299438477\n",
      "Training iter #30111000:   Batch Loss = 6.970265, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.363746166229248, Accuracy = 0.9014294147491455\n",
      "Training iter #30114000:   Batch Loss = 6.971764, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362975120544434, Accuracy = 0.9014294147491455\n",
      "Training iter #30117000:   Batch Loss = 7.024875, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362285614013672, Accuracy = 0.9017271995544434\n",
      "Training iter #30120000:   Batch Loss = 7.033017, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362084865570068, Accuracy = 0.9017271995544434\n",
      "Training iter #30123000:   Batch Loss = 7.021401, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362248420715332, Accuracy = 0.9017271995544434\n",
      "Training iter #30126000:   Batch Loss = 6.979306, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362762451171875, Accuracy = 0.902025043964386\n",
      "Training iter #30129000:   Batch Loss = 7.009434, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362843990325928, Accuracy = 0.9017271995544434\n",
      "Training iter #30132000:   Batch Loss = 7.013996, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362690448760986, Accuracy = 0.902025043964386\n",
      "Training iter #30135000:   Batch Loss = 7.034331, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362494468688965, Accuracy = 0.9017271995544434\n",
      "Training iter #30138000:   Batch Loss = 6.964413, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362321376800537, Accuracy = 0.9017271995544434\n",
      "Training iter #30141000:   Batch Loss = 6.969808, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362228870391846, Accuracy = 0.9017271995544434\n",
      "Training iter #30144000:   Batch Loss = 7.014332, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362492084503174, Accuracy = 0.9011316299438477\n",
      "Training iter #30147000:   Batch Loss = 7.039857, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3621978759765625, Accuracy = 0.9011316299438477\n",
      "Training iter #30150000:   Batch Loss = 7.023045, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.362043380737305, Accuracy = 0.9017271995544434\n",
      "Training iter #30153000:   Batch Loss = 6.982474, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361532688140869, Accuracy = 0.9017271995544434\n",
      "Training iter #30156000:   Batch Loss = 7.007482, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.36118221282959, Accuracy = 0.9017271995544434\n",
      "Training iter #30159000:   Batch Loss = 7.008778, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3610758781433105, Accuracy = 0.902025043964386\n",
      "Training iter #30162000:   Batch Loss = 7.027380, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361323356628418, Accuracy = 0.902025043964386\n",
      "Training iter #30165000:   Batch Loss = 6.960261, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361275672912598, Accuracy = 0.902025043964386\n",
      "Training iter #30168000:   Batch Loss = 6.976161, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361398220062256, Accuracy = 0.902025043964386\n",
      "Training iter #30171000:   Batch Loss = 7.012655, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361500263214111, Accuracy = 0.9017271995544434\n",
      "Training iter #30174000:   Batch Loss = 7.038073, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361471176147461, Accuracy = 0.9014294147491455\n",
      "Training iter #30177000:   Batch Loss = 7.017674, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3614420890808105, Accuracy = 0.9011316299438477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30180000:   Batch Loss = 6.981980, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361441612243652, Accuracy = 0.9011316299438477\n",
      "Training iter #30183000:   Batch Loss = 7.004351, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361384868621826, Accuracy = 0.9017271995544434\n",
      "Training iter #30186000:   Batch Loss = 7.007206, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361263275146484, Accuracy = 0.9014294147491455\n",
      "Training iter #30189000:   Batch Loss = 7.027146, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.361118793487549, Accuracy = 0.9011316299438477\n",
      "Training iter #30192000:   Batch Loss = 6.963363, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.360635757446289, Accuracy = 0.9014294147491455\n",
      "Training iter #30195000:   Batch Loss = 6.977298, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3602752685546875, Accuracy = 0.9014294147491455\n",
      "Training iter #30198000:   Batch Loss = 7.007924, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.35975980758667, Accuracy = 0.9014294147491455\n",
      "Training iter #30201000:   Batch Loss = 7.040921, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359495639801025, Accuracy = 0.9014294147491455\n",
      "Training iter #30204000:   Batch Loss = 7.006234, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.36003303527832, Accuracy = 0.9011316299438477\n",
      "Training iter #30207000:   Batch Loss = 6.982091, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.360170364379883, Accuracy = 0.9014294147491455\n",
      "Training iter #30210000:   Batch Loss = 7.012603, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359869003295898, Accuracy = 0.9014294147491455\n",
      "Training iter #30213000:   Batch Loss = 7.008800, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.35943603515625, Accuracy = 0.9014294147491455\n",
      "Training iter #30216000:   Batch Loss = 7.027952, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359289646148682, Accuracy = 0.9014294147491455\n",
      "Training iter #30219000:   Batch Loss = 6.960482, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359170913696289, Accuracy = 0.9014294147491455\n",
      "Training iter #30222000:   Batch Loss = 6.974123, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359314441680908, Accuracy = 0.9017271995544434\n",
      "Training iter #30225000:   Batch Loss = 7.005814, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359334945678711, Accuracy = 0.9014294147491455\n",
      "Training iter #30228000:   Batch Loss = 7.044265, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359533786773682, Accuracy = 0.9011316299438477\n",
      "Training iter #30231000:   Batch Loss = 6.995240, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359441757202148, Accuracy = 0.9014294147491455\n",
      "Training iter #30234000:   Batch Loss = 6.973465, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.359035491943359, Accuracy = 0.9002382159233093\n",
      "Training iter #30237000:   Batch Loss = 7.010377, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.358790397644043, Accuracy = 0.9002382159233093\n",
      "Training iter #30240000:   Batch Loss = 7.007441, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.358715057373047, Accuracy = 0.9002382159233093\n",
      "Training iter #30243000:   Batch Loss = 7.023873, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.358687400817871, Accuracy = 0.9002382159233093\n",
      "Training iter #30246000:   Batch Loss = 6.963395, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.35883903503418, Accuracy = 0.900536060333252\n",
      "Training iter #30249000:   Batch Loss = 6.974106, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.358623027801514, Accuracy = 0.900536060333252\n",
      "Training iter #30252000:   Batch Loss = 7.001684, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.358343601226807, Accuracy = 0.8999404311180115\n",
      "Training iter #30255000:   Batch Loss = 7.044951, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3578200340271, Accuracy = 0.8999404311180115\n",
      "Training iter #30258000:   Batch Loss = 6.990774, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.357479095458984, Accuracy = 0.9002382159233093\n",
      "Training iter #30261000:   Batch Loss = 6.973453, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.356865882873535, Accuracy = 0.9017271995544434\n",
      "Training iter #30264000:   Batch Loss = 7.014727, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.35628604888916, Accuracy = 0.9017271995544434\n",
      "Training iter #30267000:   Batch Loss = 7.009094, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3562140464782715, Accuracy = 0.9017271995544434\n",
      "Training iter #30270000:   Batch Loss = 7.019156, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.355844020843506, Accuracy = 0.902025043964386\n",
      "Training iter #30273000:   Batch Loss = 6.961462, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.356104850769043, Accuracy = 0.9017271995544434\n",
      "Training iter #30276000:   Batch Loss = 6.986074, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.356131553649902, Accuracy = 0.9011316299438477\n",
      "Training iter #30279000:   Batch Loss = 7.001976, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.355895042419434, Accuracy = 0.9008338451385498\n",
      "Training iter #30282000:   Batch Loss = 7.044935, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.355581283569336, Accuracy = 0.9011316299438477\n",
      "Training iter #30285000:   Batch Loss = 6.987275, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3552446365356445, Accuracy = 0.9017271995544434\n",
      "Training iter #30288000:   Batch Loss = 6.971447, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354983806610107, Accuracy = 0.9017271995544434\n",
      "Training iter #30291000:   Batch Loss = 6.997701, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354731559753418, Accuracy = 0.9017271995544434\n",
      "Training iter #30294000:   Batch Loss = 7.020614, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354682922363281, Accuracy = 0.9023228287696838\n",
      "Training iter #30297000:   Batch Loss = 7.009708, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354689598083496, Accuracy = 0.9023228287696838\n",
      "Training iter #30300000:   Batch Loss = 6.962713, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354928970336914, Accuracy = 0.9023228287696838\n",
      "Training iter #30303000:   Batch Loss = 6.988533, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.355127334594727, Accuracy = 0.9017271995544434\n",
      "Training iter #30306000:   Batch Loss = 7.002297, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3551459312438965, Accuracy = 0.9017271995544434\n",
      "Training iter #30309000:   Batch Loss = 7.041060, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354975700378418, Accuracy = 0.902025043964386\n",
      "Training iter #30312000:   Batch Loss = 6.984180, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354614734649658, Accuracy = 0.902025043964386\n",
      "Training iter #30315000:   Batch Loss = 6.965809, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354091167449951, Accuracy = 0.9023228287696838\n",
      "Training iter #30318000:   Batch Loss = 6.996725, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.353137493133545, Accuracy = 0.9023228287696838\n",
      "Training iter #30321000:   Batch Loss = 7.019207, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.352902412414551, Accuracy = 0.9017271995544434\n",
      "Training iter #30324000:   Batch Loss = 7.007222, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.353498935699463, Accuracy = 0.902025043964386\n",
      "Training iter #30327000:   Batch Loss = 6.957614, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.353598117828369, Accuracy = 0.902025043964386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30330000:   Batch Loss = 6.988521, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.353486061096191, Accuracy = 0.902025043964386\n",
      "Training iter #30333000:   Batch Loss = 7.003585, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3532233238220215, Accuracy = 0.9017271995544434\n",
      "Training iter #30336000:   Batch Loss = 7.037822, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.353213310241699, Accuracy = 0.902025043964386\n",
      "Training iter #30339000:   Batch Loss = 6.976744, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.353041172027588, Accuracy = 0.9023228287696838\n",
      "Training iter #30342000:   Batch Loss = 6.962686, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.352835178375244, Accuracy = 0.9017271995544434\n",
      "Training iter #30345000:   Batch Loss = 7.010380, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.352845191955566, Accuracy = 0.9008338451385498\n",
      "Training iter #30348000:   Batch Loss = 7.019519, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.352909564971924, Accuracy = 0.9008338451385498\n",
      "Training iter #30351000:   Batch Loss = 7.006250, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354653835296631, Accuracy = 0.900536060333252\n",
      "Training iter #30354000:   Batch Loss = 6.969025, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.355685710906982, Accuracy = 0.9014294147491455\n",
      "Training iter #30357000:   Batch Loss = 6.992861, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354793071746826, Accuracy = 0.9023228287696838\n",
      "Training iter #30360000:   Batch Loss = 6.992301, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354528427124023, Accuracy = 0.9026206135749817\n",
      "Training iter #30363000:   Batch Loss = 7.037436, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.354542255401611, Accuracy = 0.9011316299438477\n",
      "Training iter #30366000:   Batch Loss = 6.973241, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.353630542755127, Accuracy = 0.9011316299438477\n",
      "Training iter #30369000:   Batch Loss = 6.973199, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.352665901184082, Accuracy = 0.9014294147491455\n",
      "Training iter #30372000:   Batch Loss = 7.009124, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.351891040802002, Accuracy = 0.9023228287696838\n",
      "Training iter #30375000:   Batch Loss = 7.016333, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.351492881774902, Accuracy = 0.9029183983802795\n",
      "Training iter #30378000:   Batch Loss = 7.008455, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.35050106048584, Accuracy = 0.9029183983802795\n",
      "Training iter #30381000:   Batch Loss = 6.962028, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.35001802444458, Accuracy = 0.9029183983802795\n",
      "Training iter #30384000:   Batch Loss = 6.988742, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.349290370941162, Accuracy = 0.9035139679908752\n",
      "Training iter #30387000:   Batch Loss = 6.989145, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.348876953125, Accuracy = 0.9029183983802795\n",
      "Training iter #30390000:   Batch Loss = 7.039504, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.348588466644287, Accuracy = 0.9026206135749817\n",
      "Training iter #30393000:   Batch Loss = 6.963870, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.348388671875, Accuracy = 0.9026206135749817\n",
      "Training iter #30396000:   Batch Loss = 6.959728, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.347847938537598, Accuracy = 0.9032161831855774\n",
      "Training iter #30399000:   Batch Loss = 7.010049, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.348231315612793, Accuracy = 0.9029183983802795\n",
      "Training iter #30402000:   Batch Loss = 7.015239, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.348054885864258, Accuracy = 0.9029183983802795\n",
      "Training iter #30405000:   Batch Loss = 7.007456, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.347440242767334, Accuracy = 0.9032161831855774\n",
      "Training iter #30408000:   Batch Loss = 6.964524, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.347443580627441, Accuracy = 0.9029183983802795\n",
      "Training iter #30411000:   Batch Loss = 6.988524, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.346906661987305, Accuracy = 0.9038118124008179\n",
      "Training iter #30414000:   Batch Loss = 6.987878, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.347201824188232, Accuracy = 0.9032161831855774\n",
      "Training iter #30417000:   Batch Loss = 7.030937, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.347265243530273, Accuracy = 0.9023228287696838\n",
      "Training iter #30420000:   Batch Loss = 6.957847, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3467817306518555, Accuracy = 0.9029183983802795\n",
      "Training iter #30423000:   Batch Loss = 6.957619, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3464131355285645, Accuracy = 0.9017271995544434\n",
      "Training iter #30426000:   Batch Loss = 7.008609, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3465776443481445, Accuracy = 0.9017271995544434\n",
      "Training iter #30429000:   Batch Loss = 7.017269, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.346587657928467, Accuracy = 0.9026206135749817\n",
      "Training iter #30432000:   Batch Loss = 7.003432, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.346220016479492, Accuracy = 0.9026206135749817\n",
      "Training iter #30435000:   Batch Loss = 6.965559, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3459367752075195, Accuracy = 0.9032161831855774\n",
      "Training iter #30438000:   Batch Loss = 6.988752, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345551013946533, Accuracy = 0.9032161831855774\n",
      "Training iter #30441000:   Batch Loss = 6.997787, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345303535461426, Accuracy = 0.9032161831855774\n",
      "Training iter #30444000:   Batch Loss = 7.022234, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345312118530273, Accuracy = 0.9032161831855774\n",
      "Training iter #30447000:   Batch Loss = 6.952350, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.34555721282959, Accuracy = 0.9032161831855774\n",
      "Training iter #30450000:   Batch Loss = 6.957207, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345712661743164, Accuracy = 0.9029183983802795\n",
      "Training iter #30453000:   Batch Loss = 7.005557, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345943927764893, Accuracy = 0.9032161831855774\n",
      "Training iter #30456000:   Batch Loss = 7.016181, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.346166133880615, Accuracy = 0.9029183983802795\n",
      "Training iter #30459000:   Batch Loss = 7.005506, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345839500427246, Accuracy = 0.9035139679908752\n",
      "Training iter #30462000:   Batch Loss = 6.968703, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345551013946533, Accuracy = 0.9032161831855774\n",
      "Training iter #30465000:   Batch Loss = 6.993520, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.34554386138916, Accuracy = 0.9026206135749817\n",
      "Training iter #30468000:   Batch Loss = 6.994403, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345790386199951, Accuracy = 0.9026206135749817\n",
      "Training iter #30471000:   Batch Loss = 7.011676, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3451948165893555, Accuracy = 0.9029183983802795\n",
      "Training iter #30474000:   Batch Loss = 6.945911, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.345072269439697, Accuracy = 0.9032161831855774\n",
      "Training iter #30477000:   Batch Loss = 6.958326, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3449506759643555, Accuracy = 0.9029183983802795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30480000:   Batch Loss = 6.998688, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.344886779785156, Accuracy = 0.9029183983802795\n",
      "Training iter #30483000:   Batch Loss = 7.021822, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3445000648498535, Accuracy = 0.9029183983802795\n",
      "Training iter #30486000:   Batch Loss = 7.005347, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.344259738922119, Accuracy = 0.9029183983802795\n",
      "Training iter #30489000:   Batch Loss = 6.967056, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.344178676605225, Accuracy = 0.9032161831855774\n",
      "Training iter #30492000:   Batch Loss = 6.989680, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.344200611114502, Accuracy = 0.9029183983802795\n",
      "Training iter #30495000:   Batch Loss = 6.990965, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3442158699035645, Accuracy = 0.9032161831855774\n",
      "Training iter #30498000:   Batch Loss = 7.010204, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.343548774719238, Accuracy = 0.9029183983802795\n",
      "Training iter #30501000:   Batch Loss = 6.943622, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.343306541442871, Accuracy = 0.9029183983802795\n",
      "Training iter #30504000:   Batch Loss = 6.960601, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.343141078948975, Accuracy = 0.9032161831855774\n",
      "Training iter #30507000:   Batch Loss = 6.995992, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.343400478363037, Accuracy = 0.9032161831855774\n",
      "Training iter #30510000:   Batch Loss = 7.022003, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.343034744262695, Accuracy = 0.9035139679908752\n",
      "Training iter #30513000:   Batch Loss = 6.995568, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.342239856719971, Accuracy = 0.9035139679908752\n",
      "Training iter #30516000:   Batch Loss = 6.964807, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.341529369354248, Accuracy = 0.9041095972061157\n",
      "Training iter #30519000:   Batch Loss = 6.991715, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.341068744659424, Accuracy = 0.9041095972061157\n",
      "Training iter #30522000:   Batch Loss = 6.989721, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.341189384460449, Accuracy = 0.9038118124008179\n",
      "Training iter #30525000:   Batch Loss = 7.006846, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.34075927734375, Accuracy = 0.9035139679908752\n",
      "Training iter #30528000:   Batch Loss = 6.946396, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340611457824707, Accuracy = 0.9035139679908752\n",
      "Training iter #30531000:   Batch Loss = 6.959091, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3405351638793945, Accuracy = 0.9032161831855774\n",
      "Training iter #30534000:   Batch Loss = 6.992817, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340366840362549, Accuracy = 0.9029183983802795\n",
      "Training iter #30537000:   Batch Loss = 7.024364, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.339587211608887, Accuracy = 0.9029183983802795\n",
      "Training iter #30540000:   Batch Loss = 6.980621, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.339326858520508, Accuracy = 0.9029183983802795\n",
      "Training iter #30543000:   Batch Loss = 6.960793, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.339520454406738, Accuracy = 0.9029183983802795\n",
      "Training iter #30546000:   Batch Loss = 6.996360, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.339589595794678, Accuracy = 0.9023228287696838\n",
      "Training iter #30549000:   Batch Loss = 6.993321, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.339930057525635, Accuracy = 0.902025043964386\n",
      "Training iter #30552000:   Batch Loss = 7.007878, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.34003210067749, Accuracy = 0.9023228287696838\n",
      "Training iter #30555000:   Batch Loss = 6.947208, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340417385101318, Accuracy = 0.9029183983802795\n",
      "Training iter #30558000:   Batch Loss = 6.958000, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340721130371094, Accuracy = 0.9029183983802795\n",
      "Training iter #30561000:   Batch Loss = 6.990098, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340564727783203, Accuracy = 0.9029183983802795\n",
      "Training iter #30564000:   Batch Loss = 7.027200, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340474605560303, Accuracy = 0.9017271995544434\n",
      "Training iter #30567000:   Batch Loss = 6.976422, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340731143951416, Accuracy = 0.902025043964386\n",
      "Training iter #30570000:   Batch Loss = 6.958276, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3412184715271, Accuracy = 0.9023228287696838\n",
      "Training iter #30573000:   Batch Loss = 6.998908, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.341648578643799, Accuracy = 0.9017271995544434\n",
      "Training iter #30576000:   Batch Loss = 6.990289, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.342040061950684, Accuracy = 0.9017271995544434\n",
      "Training iter #30579000:   Batch Loss = 7.004444, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.341777801513672, Accuracy = 0.902025043964386\n",
      "Training iter #30582000:   Batch Loss = 6.947084, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.341731548309326, Accuracy = 0.9023228287696838\n",
      "Training iter #30585000:   Batch Loss = 6.957880, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.341643333435059, Accuracy = 0.9023228287696838\n",
      "Training iter #30588000:   Batch Loss = 6.983650, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340897083282471, Accuracy = 0.9017271995544434\n",
      "Training iter #30591000:   Batch Loss = 7.028112, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.340158939361572, Accuracy = 0.9017271995544434\n",
      "Training iter #30594000:   Batch Loss = 6.972465, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.339816093444824, Accuracy = 0.9023228287696838\n",
      "Training iter #30597000:   Batch Loss = 6.959792, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.339591026306152, Accuracy = 0.9023228287696838\n",
      "Training iter #30600000:   Batch Loss = 6.984002, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3391947746276855, Accuracy = 0.9023228287696838\n",
      "Training iter #30603000:   Batch Loss = 6.993463, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.338824272155762, Accuracy = 0.9026206135749817\n",
      "Training iter #30606000:   Batch Loss = 6.996875, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33817720413208, Accuracy = 0.9029183983802795\n",
      "Training iter #30609000:   Batch Loss = 6.945031, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3383941650390625, Accuracy = 0.9026206135749817\n",
      "Training iter #30612000:   Batch Loss = 6.972385, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3390021324157715, Accuracy = 0.902025043964386\n",
      "Training iter #30615000:   Batch Loss = 6.985303, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.338755130767822, Accuracy = 0.902025043964386\n",
      "Training iter #30618000:   Batch Loss = 7.017799, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.338395118713379, Accuracy = 0.902025043964386\n",
      "Training iter #30621000:   Batch Loss = 6.970385, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.338586807250977, Accuracy = 0.902025043964386\n",
      "Training iter #30624000:   Batch Loss = 6.952940, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3385515213012695, Accuracy = 0.9017271995544434\n",
      "Training iter #30627000:   Batch Loss = 6.983592, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33834171295166, Accuracy = 0.9014294147491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30630000:   Batch Loss = 7.005765, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.337743282318115, Accuracy = 0.9023228287696838\n",
      "Training iter #30633000:   Batch Loss = 6.984701, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336413860321045, Accuracy = 0.9029183983802795\n",
      "Training iter #30636000:   Batch Loss = 6.945935, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336181640625, Accuracy = 0.9029183983802795\n",
      "Training iter #30639000:   Batch Loss = 6.972463, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335887432098389, Accuracy = 0.9032161831855774\n",
      "Training iter #30642000:   Batch Loss = 6.988023, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335942268371582, Accuracy = 0.9029183983802795\n",
      "Training iter #30645000:   Batch Loss = 7.022183, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33632230758667, Accuracy = 0.9029183983802795\n",
      "Training iter #30648000:   Batch Loss = 6.964005, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33640193939209, Accuracy = 0.9029183983802795\n",
      "Training iter #30651000:   Batch Loss = 6.949902, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336581707000732, Accuracy = 0.9029183983802795\n",
      "Training iter #30654000:   Batch Loss = 6.980984, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335899829864502, Accuracy = 0.9029183983802795\n",
      "Training iter #30657000:   Batch Loss = 7.003053, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335789203643799, Accuracy = 0.9029183983802795\n",
      "Training iter #30660000:   Batch Loss = 6.988448, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33583927154541, Accuracy = 0.902025043964386\n",
      "Training iter #30663000:   Batch Loss = 6.940757, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335762023925781, Accuracy = 0.902025043964386\n",
      "Training iter #30666000:   Batch Loss = 6.971941, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3357744216918945, Accuracy = 0.902025043964386\n",
      "Training iter #30669000:   Batch Loss = 6.973162, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336480140686035, Accuracy = 0.9017271995544434\n",
      "Training iter #30672000:   Batch Loss = 7.020666, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336663722991943, Accuracy = 0.902025043964386\n",
      "Training iter #30675000:   Batch Loss = 6.960539, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336704254150391, Accuracy = 0.9023228287696838\n",
      "Training iter #30678000:   Batch Loss = 6.944881, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336861610412598, Accuracy = 0.9023228287696838\n",
      "Training iter #30681000:   Batch Loss = 6.992598, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336769104003906, Accuracy = 0.902025043964386\n",
      "Training iter #30684000:   Batch Loss = 7.000553, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336654186248779, Accuracy = 0.9023228287696838\n",
      "Training iter #30687000:   Batch Loss = 6.985594, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.336276054382324, Accuracy = 0.902025043964386\n",
      "Training iter #30690000:   Batch Loss = 6.941892, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335641384124756, Accuracy = 0.9023228287696838\n",
      "Training iter #30693000:   Batch Loss = 6.972759, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335745811462402, Accuracy = 0.902025043964386\n",
      "Training iter #30696000:   Batch Loss = 6.973490, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3352952003479, Accuracy = 0.902025043964386\n",
      "Training iter #30699000:   Batch Loss = 7.019328, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.334908485412598, Accuracy = 0.902025043964386\n",
      "Training iter #30702000:   Batch Loss = 6.953449, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.334805488586426, Accuracy = 0.902025043964386\n",
      "Training iter #30705000:   Batch Loss = 6.944624, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.334718227386475, Accuracy = 0.9017271995544434\n",
      "Training iter #30708000:   Batch Loss = 6.993151, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335163593292236, Accuracy = 0.902025043964386\n",
      "Training iter #30711000:   Batch Loss = 6.995934, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335225582122803, Accuracy = 0.902025043964386\n",
      "Training iter #30714000:   Batch Loss = 6.988963, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33550500869751, Accuracy = 0.9017271995544434\n",
      "Training iter #30717000:   Batch Loss = 6.946467, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33546781539917, Accuracy = 0.902025043964386\n",
      "Training iter #30720000:   Batch Loss = 6.970918, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3354926109313965, Accuracy = 0.9017271995544434\n",
      "Training iter #30723000:   Batch Loss = 6.972855, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335474967956543, Accuracy = 0.9017271995544434\n",
      "Training iter #30726000:   Batch Loss = 7.018573, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335466384887695, Accuracy = 0.9014294147491455\n",
      "Training iter #30729000:   Batch Loss = 6.941236, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335173606872559, Accuracy = 0.9011316299438477\n",
      "Training iter #30732000:   Batch Loss = 6.939003, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.334684371948242, Accuracy = 0.9011316299438477\n",
      "Training iter #30735000:   Batch Loss = 6.995168, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.334440231323242, Accuracy = 0.9011316299438477\n",
      "Training iter #30738000:   Batch Loss = 7.001438, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.333264350891113, Accuracy = 0.902025043964386\n",
      "Training iter #30741000:   Batch Loss = 6.987173, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.33406400680542, Accuracy = 0.902025043964386\n",
      "Training iter #30744000:   Batch Loss = 6.947527, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.334157943725586, Accuracy = 0.9029183983802795\n",
      "Training iter #30747000:   Batch Loss = 6.970122, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.335043430328369, Accuracy = 0.9023228287696838\n",
      "Training iter #30750000:   Batch Loss = 6.977979, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.334090232849121, Accuracy = 0.9029183983802795\n",
      "Training iter #30753000:   Batch Loss = 7.005766, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3338470458984375, Accuracy = 0.9032161831855774\n",
      "Training iter #30756000:   Batch Loss = 6.938585, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.333048343658447, Accuracy = 0.9035139679908752\n",
      "Training iter #30759000:   Batch Loss = 6.938462, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.332446575164795, Accuracy = 0.9029183983802795\n",
      "Training iter #30762000:   Batch Loss = 6.990250, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.332028388977051, Accuracy = 0.9029183983802795\n",
      "Training iter #30765000:   Batch Loss = 7.001306, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.332322120666504, Accuracy = 0.9029183983802795\n",
      "Training iter #30768000:   Batch Loss = 6.984429, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.332101821899414, Accuracy = 0.9029183983802795\n",
      "Training iter #30771000:   Batch Loss = 6.945995, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.332060813903809, Accuracy = 0.9029183983802795\n",
      "Training iter #30774000:   Batch Loss = 6.971998, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.332096099853516, Accuracy = 0.9035139679908752\n",
      "Training iter #30777000:   Batch Loss = 6.981583, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3317084312438965, Accuracy = 0.9035139679908752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30780000:   Batch Loss = 7.001435, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.331868648529053, Accuracy = 0.9032161831855774\n",
      "Training iter #30783000:   Batch Loss = 6.931914, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.330992698669434, Accuracy = 0.9029183983802795\n",
      "Training iter #30786000:   Batch Loss = 6.936738, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.331327438354492, Accuracy = 0.9029183983802795\n",
      "Training iter #30789000:   Batch Loss = 6.980679, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.330702304840088, Accuracy = 0.9032161831855774\n",
      "Training iter #30792000:   Batch Loss = 7.003409, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.330010414123535, Accuracy = 0.9026206135749817\n",
      "Training iter #30795000:   Batch Loss = 6.986993, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.330407619476318, Accuracy = 0.9026206135749817\n",
      "Training iter #30798000:   Batch Loss = 6.949706, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.330409049987793, Accuracy = 0.9026206135749817\n",
      "Training iter #30801000:   Batch Loss = 6.973616, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.330862522125244, Accuracy = 0.9026206135749817\n",
      "Training iter #30804000:   Batch Loss = 6.975454, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.331265926361084, Accuracy = 0.9023228287696838\n",
      "Training iter #30807000:   Batch Loss = 6.992712, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.330620765686035, Accuracy = 0.902025043964386\n",
      "Training iter #30810000:   Batch Loss = 6.927933, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.329683303833008, Accuracy = 0.902025043964386\n",
      "Training iter #30813000:   Batch Loss = 6.940667, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.329240798950195, Accuracy = 0.9017271995544434\n",
      "Training iter #30816000:   Batch Loss = 6.977205, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328742980957031, Accuracy = 0.9017271995544434\n",
      "Training iter #30819000:   Batch Loss = 7.002449, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328779220581055, Accuracy = 0.9017271995544434\n",
      "Training iter #30822000:   Batch Loss = 6.983153, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3280534744262695, Accuracy = 0.902025043964386\n",
      "Training iter #30825000:   Batch Loss = 6.949028, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328737258911133, Accuracy = 0.902025043964386\n",
      "Training iter #30828000:   Batch Loss = 6.971753, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.329010009765625, Accuracy = 0.9023228287696838\n",
      "Training iter #30831000:   Batch Loss = 6.973530, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328949451446533, Accuracy = 0.9023228287696838\n",
      "Training iter #30834000:   Batch Loss = 6.992902, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328247547149658, Accuracy = 0.9023228287696838\n",
      "Training iter #30837000:   Batch Loss = 6.927297, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328648090362549, Accuracy = 0.9023228287696838\n",
      "Training iter #30840000:   Batch Loss = 6.943507, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328439235687256, Accuracy = 0.9017271995544434\n",
      "Training iter #30843000:   Batch Loss = 6.974326, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328616142272949, Accuracy = 0.902025043964386\n",
      "Training iter #30846000:   Batch Loss = 7.010097, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328611850738525, Accuracy = 0.902025043964386\n",
      "Training iter #30849000:   Batch Loss = 6.975089, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328493595123291, Accuracy = 0.902025043964386\n",
      "Training iter #30852000:   Batch Loss = 6.944437, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328343391418457, Accuracy = 0.902025043964386\n",
      "Training iter #30855000:   Batch Loss = 6.974926, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328206539154053, Accuracy = 0.9023228287696838\n",
      "Training iter #30858000:   Batch Loss = 6.975518, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.328034400939941, Accuracy = 0.9023228287696838\n",
      "Training iter #30861000:   Batch Loss = 6.992091, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.327668190002441, Accuracy = 0.9023228287696838\n",
      "Training iter #30864000:   Batch Loss = 6.928407, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.327437877655029, Accuracy = 0.9023228287696838\n",
      "Training iter #30867000:   Batch Loss = 6.940675, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.327330112457275, Accuracy = 0.9023228287696838\n",
      "Training iter #30870000:   Batch Loss = 6.971706, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.327045917510986, Accuracy = 0.9023228287696838\n",
      "Training iter #30873000:   Batch Loss = 7.005361, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3266215324401855, Accuracy = 0.9023228287696838\n",
      "Training iter #30876000:   Batch Loss = 6.961541, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.326620101928711, Accuracy = 0.9023228287696838\n",
      "Training iter #30879000:   Batch Loss = 6.939392, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3265533447265625, Accuracy = 0.9023228287696838\n",
      "Training iter #30882000:   Batch Loss = 6.976879, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3263397216796875, Accuracy = 0.9023228287696838\n",
      "Training iter #30885000:   Batch Loss = 6.976361, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.325662612915039, Accuracy = 0.9017271995544434\n",
      "Training iter #30888000:   Batch Loss = 6.988702, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3256683349609375, Accuracy = 0.9017271995544434\n",
      "Training iter #30891000:   Batch Loss = 6.930341, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.325878620147705, Accuracy = 0.9017271995544434\n",
      "Training iter #30894000:   Batch Loss = 6.940157, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.32586669921875, Accuracy = 0.902025043964386\n",
      "Training iter #30897000:   Batch Loss = 6.970056, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.325764179229736, Accuracy = 0.902025043964386\n",
      "Training iter #30900000:   Batch Loss = 7.008610, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.325498104095459, Accuracy = 0.9023228287696838\n",
      "Training iter #30903000:   Batch Loss = 6.956288, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.325013160705566, Accuracy = 0.9023228287696838\n",
      "Training iter #30906000:   Batch Loss = 6.937674, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.324702262878418, Accuracy = 0.9023228287696838\n",
      "Training iter #30909000:   Batch Loss = 6.980837, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.324490547180176, Accuracy = 0.9029183983802795\n",
      "Training iter #30912000:   Batch Loss = 6.973900, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.324333667755127, Accuracy = 0.9029183983802795\n",
      "Training iter #30915000:   Batch Loss = 6.983704, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.32354736328125, Accuracy = 0.9026206135749817\n",
      "Training iter #30918000:   Batch Loss = 6.929640, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3233466148376465, Accuracy = 0.9023228287696838\n",
      "Training iter #30921000:   Batch Loss = 6.945549, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.32323694229126, Accuracy = 0.9026206135749817\n",
      "Training iter #30924000:   Batch Loss = 6.967257, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.323118686676025, Accuracy = 0.9029183983802795\n",
      "Training iter #30927000:   Batch Loss = 7.009048, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.323307514190674, Accuracy = 0.9026206135749817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30930000:   Batch Loss = 6.954388, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.323474884033203, Accuracy = 0.9026206135749817\n",
      "Training iter #30933000:   Batch Loss = 6.938223, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.32282829284668, Accuracy = 0.9026206135749817\n",
      "Training iter #30936000:   Batch Loss = 6.963799, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.322696208953857, Accuracy = 0.902025043964386\n",
      "Training iter #30939000:   Batch Loss = 6.992800, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.32249641418457, Accuracy = 0.902025043964386\n",
      "Training iter #30942000:   Batch Loss = 6.979393, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.32157564163208, Accuracy = 0.9026206135749817\n",
      "Training iter #30945000:   Batch Loss = 6.931203, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3207197189331055, Accuracy = 0.9026206135749817\n",
      "Training iter #30948000:   Batch Loss = 6.954191, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.319804668426514, Accuracy = 0.902025043964386\n",
      "Training iter #30951000:   Batch Loss = 6.975997, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.319208145141602, Accuracy = 0.9026206135749817\n",
      "Training iter #30954000:   Batch Loss = 7.008678, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.318414688110352, Accuracy = 0.902025043964386\n",
      "Training iter #30957000:   Batch Loss = 6.951127, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.317438125610352, Accuracy = 0.902025043964386\n",
      "Training iter #30960000:   Batch Loss = 6.934258, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.318997859954834, Accuracy = 0.902025043964386\n",
      "Training iter #30963000:   Batch Loss = 6.966183, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.319067001342773, Accuracy = 0.9023228287696838\n",
      "Training iter #30966000:   Batch Loss = 7.007165, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.318971157073975, Accuracy = 0.9035139679908752\n",
      "Training iter #30969000:   Batch Loss = 6.975465, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.320241928100586, Accuracy = 0.9032161831855774\n",
      "Training iter #30972000:   Batch Loss = 6.926053, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.319830417633057, Accuracy = 0.9029183983802795\n",
      "Training iter #30975000:   Batch Loss = 6.955434, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.318941593170166, Accuracy = 0.9041095972061157\n",
      "Training iter #30978000:   Batch Loss = 6.979502, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.317963600158691, Accuracy = 0.9038118124008179\n",
      "Training iter #30981000:   Batch Loss = 7.008658, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.318867206573486, Accuracy = 0.9035139679908752\n",
      "Training iter #30984000:   Batch Loss = 6.947623, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.318658828735352, Accuracy = 0.9041095972061157\n",
      "Training iter #30987000:   Batch Loss = 6.932691, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31881856918335, Accuracy = 0.9029183983802795\n",
      "Training iter #30990000:   Batch Loss = 6.980463, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3192596435546875, Accuracy = 0.9029183983802795\n",
      "Training iter #30993000:   Batch Loss = 7.027634, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.319629669189453, Accuracy = 0.9023228287696838\n",
      "Training iter #30996000:   Batch Loss = 6.974071, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3177409172058105, Accuracy = 0.9029183983802795\n",
      "Training iter #30999000:   Batch Loss = 6.925268, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.316935062408447, Accuracy = 0.9035139679908752\n",
      "Training iter #31002000:   Batch Loss = 6.955040, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.316795349121094, Accuracy = 0.9032161831855774\n",
      "Training iter #31005000:   Batch Loss = 6.969204, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3165154457092285, Accuracy = 0.9032161831855774\n",
      "Training iter #31008000:   Batch Loss = 7.005960, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.317453384399414, Accuracy = 0.9029183983802795\n",
      "Training iter #31011000:   Batch Loss = 6.945629, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3179612159729, Accuracy = 0.9023228287696838\n",
      "Training iter #31014000:   Batch Loss = 6.929136, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.317296504974365, Accuracy = 0.9032161831855774\n",
      "Training iter #31017000:   Batch Loss = 6.977921, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.316939353942871, Accuracy = 0.9038118124008179\n",
      "Training iter #31020000:   Batch Loss = 7.015519, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.316883563995361, Accuracy = 0.9038118124008179\n",
      "Training iter #31023000:   Batch Loss = 6.975274, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3162760734558105, Accuracy = 0.9041095972061157\n",
      "Training iter #31026000:   Batch Loss = 6.925292, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.315413475036621, Accuracy = 0.9041095972061157\n",
      "Training iter #31029000:   Batch Loss = 6.956173, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312924861907959, Accuracy = 0.9047051668167114\n",
      "Training iter #31032000:   Batch Loss = 6.967216, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3123369216918945, Accuracy = 0.9050029516220093\n",
      "Training iter #31035000:   Batch Loss = 7.007318, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.313559055328369, Accuracy = 0.9053007960319519\n",
      "Training iter #31038000:   Batch Loss = 6.931859, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.314081192016602, Accuracy = 0.9047051668167114\n",
      "Training iter #31041000:   Batch Loss = 6.927601, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3138861656188965, Accuracy = 0.9041095972061157\n",
      "Training iter #31044000:   Batch Loss = 6.977755, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.313693523406982, Accuracy = 0.9041095972061157\n",
      "Training iter #31047000:   Batch Loss = 6.987493, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31421422958374, Accuracy = 0.9038118124008179\n",
      "Training iter #31050000:   Batch Loss = 6.974523, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.314267158508301, Accuracy = 0.9029183983802795\n",
      "Training iter #31053000:   Batch Loss = 6.929801, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3143815994262695, Accuracy = 0.9029183983802795\n",
      "Training iter #31056000:   Batch Loss = 6.956204, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.314727306365967, Accuracy = 0.9029183983802795\n",
      "Training iter #31059000:   Batch Loss = 6.980456, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3148298263549805, Accuracy = 0.9023228287696838\n",
      "Training iter #31062000:   Batch Loss = 6.997706, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.314985752105713, Accuracy = 0.9017271995544434\n",
      "Training iter #31065000:   Batch Loss = 6.923881, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.315077304840088, Accuracy = 0.9029183983802795\n",
      "Training iter #31068000:   Batch Loss = 6.921767, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.315001487731934, Accuracy = 0.9032161831855774\n",
      "Training iter #31071000:   Batch Loss = 6.979191, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31421422958374, Accuracy = 0.9035139679908752\n",
      "Training iter #31074000:   Batch Loss = 6.990092, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.313163757324219, Accuracy = 0.9029183983802795\n",
      "Training iter #31077000:   Batch Loss = 6.971674, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.313614845275879, Accuracy = 0.9032161831855774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #31080000:   Batch Loss = 6.930945, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312834739685059, Accuracy = 0.9035139679908752\n",
      "Training iter #31083000:   Batch Loss = 6.956494, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312564373016357, Accuracy = 0.9038118124008179\n",
      "Training iter #31086000:   Batch Loss = 6.989516, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312605381011963, Accuracy = 0.9029183983802795\n",
      "Training iter #31089000:   Batch Loss = 6.988113, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312795162200928, Accuracy = 0.9029183983802795\n",
      "Training iter #31092000:   Batch Loss = 6.921098, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312782287597656, Accuracy = 0.9029183983802795\n",
      "Training iter #31095000:   Batch Loss = 6.921945, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312962532043457, Accuracy = 0.9029183983802795\n",
      "Training iter #31098000:   Batch Loss = 6.973667, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312862396240234, Accuracy = 0.9029183983802795\n",
      "Training iter #31101000:   Batch Loss = 6.982028, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312748908996582, Accuracy = 0.9029183983802795\n",
      "Training iter #31104000:   Batch Loss = 6.972047, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312915325164795, Accuracy = 0.9029183983802795\n",
      "Training iter #31107000:   Batch Loss = 6.929578, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31302547454834, Accuracy = 0.9026206135749817\n",
      "Training iter #31110000:   Batch Loss = 6.956274, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.313133716583252, Accuracy = 0.9029183983802795\n",
      "Training iter #31113000:   Batch Loss = 6.987753, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.313352108001709, Accuracy = 0.9029183983802795\n",
      "Training iter #31116000:   Batch Loss = 6.978812, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31339693069458, Accuracy = 0.9029183983802795\n",
      "Training iter #31119000:   Batch Loss = 6.915582, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31326961517334, Accuracy = 0.9029183983802795\n",
      "Training iter #31122000:   Batch Loss = 6.920445, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.313048362731934, Accuracy = 0.9032161831855774\n",
      "Training iter #31125000:   Batch Loss = 6.963513, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3122239112854, Accuracy = 0.9026206135749817\n",
      "Training iter #31128000:   Batch Loss = 6.987515, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.311584949493408, Accuracy = 0.9023228287696838\n",
      "Training iter #31131000:   Batch Loss = 6.971914, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312558174133301, Accuracy = 0.9026206135749817\n",
      "Training iter #31134000:   Batch Loss = 6.932696, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.312127113342285, Accuracy = 0.9026206135749817\n",
      "Training iter #31137000:   Batch Loss = 6.959191, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.311870574951172, Accuracy = 0.9026206135749817\n",
      "Training iter #31140000:   Batch Loss = 6.980109, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.311834335327148, Accuracy = 0.9026206135749817\n",
      "Training iter #31143000:   Batch Loss = 6.975957, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.311707496643066, Accuracy = 0.9026206135749817\n",
      "Training iter #31146000:   Batch Loss = 6.911585, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31124210357666, Accuracy = 0.9026206135749817\n",
      "Training iter #31149000:   Batch Loss = 6.925772, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.310896873474121, Accuracy = 0.9029183983802795\n",
      "Training iter #31152000:   Batch Loss = 6.962955, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.311005115509033, Accuracy = 0.9032161831855774\n",
      "Training iter #31155000:   Batch Loss = 6.986276, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.310766220092773, Accuracy = 0.9029183983802795\n",
      "Training iter #31158000:   Batch Loss = 6.962909, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.31053352355957, Accuracy = 0.9029183983802795\n",
      "Training iter #31161000:   Batch Loss = 6.931857, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309229373931885, Accuracy = 0.9035139679908752\n",
      "Training iter #31164000:   Batch Loss = 6.955638, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309553623199463, Accuracy = 0.9035139679908752\n",
      "Training iter #31167000:   Batch Loss = 6.975486, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.310020923614502, Accuracy = 0.9032161831855774\n",
      "Training iter #31170000:   Batch Loss = 6.970159, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3100762367248535, Accuracy = 0.9029183983802795\n",
      "Training iter #31173000:   Batch Loss = 6.914676, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.311088562011719, Accuracy = 0.9029183983802795\n",
      "Training iter #31176000:   Batch Loss = 6.926990, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.310486793518066, Accuracy = 0.9032161831855774\n",
      "Training iter #31179000:   Batch Loss = 6.961722, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3104119300842285, Accuracy = 0.9029183983802795\n",
      "Training iter #31182000:   Batch Loss = 6.987498, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.310414791107178, Accuracy = 0.9023228287696838\n",
      "Training iter #31185000:   Batch Loss = 6.955412, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309980869293213, Accuracy = 0.9023228287696838\n",
      "Training iter #31188000:   Batch Loss = 6.926969, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309600353240967, Accuracy = 0.9029183983802795\n",
      "Training iter #31191000:   Batch Loss = 6.962066, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309215068817139, Accuracy = 0.9029183983802795\n",
      "Training iter #31194000:   Batch Loss = 6.978130, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309078216552734, Accuracy = 0.9026206135749817\n",
      "Training iter #31197000:   Batch Loss = 6.974775, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.30910062789917, Accuracy = 0.9026206135749817\n",
      "Training iter #31200000:   Batch Loss = 6.914027, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.307865142822266, Accuracy = 0.9029183983802795\n",
      "Training iter #31203000:   Batch Loss = 6.924244, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.308238506317139, Accuracy = 0.9029183983802795\n",
      "Training iter #31206000:   Batch Loss = 6.956741, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.308210372924805, Accuracy = 0.9029183983802795\n",
      "Training iter #31209000:   Batch Loss = 6.990681, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.30819034576416, Accuracy = 0.9029183983802795\n",
      "Training iter #31212000:   Batch Loss = 6.943671, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3089094161987305, Accuracy = 0.9029183983802795\n",
      "Training iter #31215000:   Batch Loss = 6.923225, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.308477878570557, Accuracy = 0.9029183983802795\n",
      "Training iter #31218000:   Batch Loss = 6.963582, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309113025665283, Accuracy = 0.9032161831855774\n",
      "Training iter #31221000:   Batch Loss = 6.975468, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309208869934082, Accuracy = 0.9032161831855774\n",
      "Training iter #31224000:   Batch Loss = 6.970214, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.309145450592041, Accuracy = 0.9032161831855774\n",
      "Training iter #31227000:   Batch Loss = 6.914423, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.308897018432617, Accuracy = 0.9029183983802795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #31230000:   Batch Loss = 6.924463, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.308506965637207, Accuracy = 0.9029183983802795\n",
      "Training iter #31233000:   Batch Loss = 6.951200, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.30810022354126, Accuracy = 0.9029183983802795\n",
      "Training iter #31236000:   Batch Loss = 6.993179, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.307654857635498, Accuracy = 0.9029183983802795\n",
      "Training iter #31239000:   Batch Loss = 6.939620, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.307064056396484, Accuracy = 0.9029183983802795\n",
      "Training iter #31242000:   Batch Loss = 6.922757, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.307217121124268, Accuracy = 0.9023228287696838\n",
      "Training iter #31245000:   Batch Loss = 6.963682, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.307509899139404, Accuracy = 0.9026206135749817\n",
      "Training iter #31248000:   Batch Loss = 6.979651, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3070759773254395, Accuracy = 0.9023228287696838\n",
      "Training iter #31251000:   Batch Loss = 6.965733, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.30703592300415, Accuracy = 0.9023228287696838\n",
      "Training iter #31254000:   Batch Loss = 6.912246, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3066253662109375, Accuracy = 0.9026206135749817\n",
      "Training iter #31257000:   Batch Loss = 6.937170, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.306674003601074, Accuracy = 0.9026206135749817\n",
      "Training iter #31260000:   Batch Loss = 6.951404, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.306686878204346, Accuracy = 0.9023228287696838\n",
      "Training iter #31263000:   Batch Loss = 6.979141, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.306759834289551, Accuracy = 0.9023228287696838\n",
      "Training iter #31266000:   Batch Loss = 6.935862, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.306671619415283, Accuracy = 0.9023228287696838\n",
      "Training iter #31269000:   Batch Loss = 6.918814, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.306458950042725, Accuracy = 0.9023228287696838\n",
      "Training iter #31272000:   Batch Loss = 6.947198, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.306163787841797, Accuracy = 0.9032161831855774\n",
      "Training iter #31275000:   Batch Loss = 6.986775, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.305361747741699, Accuracy = 0.9029183983802795\n",
      "Training iter #31278000:   Batch Loss = 6.956047, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.305306434631348, Accuracy = 0.9029183983802795\n",
      "Training iter #31281000:   Batch Loss = 6.913169, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304895877838135, Accuracy = 0.9029183983802795\n",
      "Training iter #31284000:   Batch Loss = 6.938996, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304485321044922, Accuracy = 0.9029183983802795\n",
      "Training iter #31287000:   Batch Loss = 6.956100, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304083824157715, Accuracy = 0.9029183983802795\n",
      "Training iter #31290000:   Batch Loss = 6.986431, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304291725158691, Accuracy = 0.9026206135749817\n",
      "Training iter #31293000:   Batch Loss = 6.931592, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304348468780518, Accuracy = 0.9026206135749817\n",
      "Training iter #31296000:   Batch Loss = 6.916033, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303537368774414, Accuracy = 0.9032161831855774\n",
      "Training iter #31299000:   Batch Loss = 6.947518, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303842544555664, Accuracy = 0.9029183983802795\n",
      "Training iter #31302000:   Batch Loss = 6.986181, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.30403995513916, Accuracy = 0.9029183983802795\n",
      "Training iter #31305000:   Batch Loss = 6.955452, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303666114807129, Accuracy = 0.9026206135749817\n",
      "Training iter #31308000:   Batch Loss = 6.907915, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303149700164795, Accuracy = 0.9032161831855774\n",
      "Training iter #31311000:   Batch Loss = 6.938311, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304104804992676, Accuracy = 0.9026206135749817\n",
      "Training iter #31314000:   Batch Loss = 6.940941, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304089069366455, Accuracy = 0.9026206135749817\n",
      "Training iter #31317000:   Batch Loss = 6.984555, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303676605224609, Accuracy = 0.9026206135749817\n",
      "Training iter #31320000:   Batch Loss = 6.927036, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303018569946289, Accuracy = 0.9026206135749817\n",
      "Training iter #31323000:   Batch Loss = 6.912889, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303920269012451, Accuracy = 0.9026206135749817\n",
      "Training iter #31326000:   Batch Loss = 6.960007, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.304059982299805, Accuracy = 0.9026206135749817\n",
      "Training iter #31329000:   Batch Loss = 6.984587, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303981781005859, Accuracy = 0.9023228287696838\n",
      "Training iter #31332000:   Batch Loss = 6.950101, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303738117218018, Accuracy = 0.9023228287696838\n",
      "Training iter #31335000:   Batch Loss = 6.907938, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303142547607422, Accuracy = 0.9023228287696838\n",
      "Training iter #31338000:   Batch Loss = 6.939623, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3030171394348145, Accuracy = 0.9026206135749817\n",
      "Training iter #31341000:   Batch Loss = 6.941387, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302938461303711, Accuracy = 0.9026206135749817\n",
      "Training iter #31344000:   Batch Loss = 6.981916, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303009033203125, Accuracy = 0.9023228287696838\n",
      "Training iter #31347000:   Batch Loss = 6.921419, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.30286169052124, Accuracy = 0.9029183983802795\n",
      "Training iter #31350000:   Batch Loss = 6.912533, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302787780761719, Accuracy = 0.9029183983802795\n",
      "Training iter #31353000:   Batch Loss = 6.958272, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302831172943115, Accuracy = 0.9029183983802795\n",
      "Training iter #31356000:   Batch Loss = 6.979723, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.303030490875244, Accuracy = 0.9023228287696838\n",
      "Training iter #31359000:   Batch Loss = 6.956406, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3028998374938965, Accuracy = 0.9023228287696838\n",
      "Training iter #31362000:   Batch Loss = 6.909087, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3026018142700195, Accuracy = 0.9023228287696838\n",
      "Training iter #31365000:   Batch Loss = 6.938151, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302386283874512, Accuracy = 0.9023228287696838\n",
      "Training iter #31368000:   Batch Loss = 6.945263, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302263259887695, Accuracy = 0.9026206135749817\n",
      "Training iter #31371000:   Batch Loss = 6.982925, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302540302276611, Accuracy = 0.902025043964386\n",
      "Training iter #31374000:   Batch Loss = 6.909404, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302725791931152, Accuracy = 0.902025043964386\n",
      "Training iter #31377000:   Batch Loss = 6.906081, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3026957511901855, Accuracy = 0.9017271995544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #31380000:   Batch Loss = 6.959567, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.302143573760986, Accuracy = 0.9023228287696838\n",
      "Training iter #31383000:   Batch Loss = 6.964251, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.301313877105713, Accuracy = 0.902025043964386\n",
      "Training iter #31386000:   Batch Loss = 6.954977, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.301659107208252, Accuracy = 0.9017271995544434\n",
      "Training iter #31389000:   Batch Loss = 6.914419, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.300752639770508, Accuracy = 0.9014294147491455\n",
      "Training iter #31392000:   Batch Loss = 6.936541, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3013529777526855, Accuracy = 0.902025043964386\n",
      "Training iter #31395000:   Batch Loss = 6.958308, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.3014678955078125, Accuracy = 0.9023228287696838\n",
      "Training iter #31398000:   Batch Loss = 6.970057, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.301515102386475, Accuracy = 0.902025043964386\n",
      "Training iter #31401000:   Batch Loss = 6.906020, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.301064491271973, Accuracy = 0.902025043964386\n",
      "Training iter #31404000:   Batch Loss = 6.904388, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.30064582824707, Accuracy = 0.902025043964386\n",
      "Training iter #31407000:   Batch Loss = 6.956856, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.300322532653809, Accuracy = 0.902025043964386\n",
      "Training iter #31410000:   Batch Loss = 6.963238, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.300432205200195, Accuracy = 0.9017271995544434\n",
      "Training iter #31413000:   Batch Loss = 6.950318, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.300655364990234, Accuracy = 0.9017271995544434\n",
      "Training iter #31416000:   Batch Loss = 6.913304, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.300171375274658, Accuracy = 0.902025043964386\n",
      "Training iter #31419000:   Batch Loss = 6.937873, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.299248695373535, Accuracy = 0.9023228287696838\n",
      "Training iter #31422000:   Batch Loss = 6.965405, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.299348831176758, Accuracy = 0.9023228287696838\n",
      "Training iter #31425000:   Batch Loss = 6.966165, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.299292087554932, Accuracy = 0.902025043964386\n",
      "Training iter #31428000:   Batch Loss = 6.901500, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.299954891204834, Accuracy = 0.9014294147491455\n",
      "Training iter #31431000:   Batch Loss = 6.904546, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2993855476379395, Accuracy = 0.9008338451385498\n",
      "Training iter #31434000:   Batch Loss = 6.950464, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.298913955688477, Accuracy = 0.9008338451385498\n",
      "Training iter #31437000:   Batch Loss = 6.965882, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.298545837402344, Accuracy = 0.9011316299438477\n",
      "Training iter #31440000:   Batch Loss = 6.952571, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2995524406433105, Accuracy = 0.9011316299438477\n",
      "Training iter #31443000:   Batch Loss = 6.916694, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2997212409973145, Accuracy = 0.9014294147491455\n",
      "Training iter #31446000:   Batch Loss = 6.942881, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.299421310424805, Accuracy = 0.9023228287696838\n",
      "Training iter #31449000:   Batch Loss = 6.962065, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.298417091369629, Accuracy = 0.9023228287696838\n",
      "Training iter #31452000:   Batch Loss = 6.957278, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.298516273498535, Accuracy = 0.9017271995544434\n",
      "Training iter #31455000:   Batch Loss = 6.895963, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2979841232299805, Accuracy = 0.9014294147491455\n",
      "Training iter #31458000:   Batch Loss = 6.907809, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.297948837280273, Accuracy = 0.9011316299438477\n",
      "Training iter #31461000:   Batch Loss = 6.943608, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.297729969024658, Accuracy = 0.9011316299438477\n",
      "Training iter #31464000:   Batch Loss = 6.967332, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.297723293304443, Accuracy = 0.9011316299438477\n",
      "Training iter #31467000:   Batch Loss = 6.951048, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.29730749130249, Accuracy = 0.9017271995544434\n",
      "Training iter #31470000:   Batch Loss = 6.915975, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.296859264373779, Accuracy = 0.902025043964386\n",
      "Training iter #31473000:   Batch Loss = 6.939015, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.296940803527832, Accuracy = 0.9014294147491455\n",
      "Training iter #31476000:   Batch Loss = 6.957975, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.296916961669922, Accuracy = 0.9014294147491455\n",
      "Training iter #31479000:   Batch Loss = 6.957916, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.296761989593506, Accuracy = 0.9014294147491455\n",
      "Training iter #31482000:   Batch Loss = 6.894970, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.296882152557373, Accuracy = 0.9014294147491455\n",
      "Training iter #31485000:   Batch Loss = 6.910337, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295669078826904, Accuracy = 0.9017271995544434\n",
      "Training iter #31488000:   Batch Loss = 6.946161, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.29557991027832, Accuracy = 0.9014294147491455\n",
      "Training iter #31491000:   Batch Loss = 6.971306, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295540809631348, Accuracy = 0.9014294147491455\n",
      "Training iter #31494000:   Batch Loss = 6.940829, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294893264770508, Accuracy = 0.902025043964386\n",
      "Training iter #31497000:   Batch Loss = 6.910708, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295521259307861, Accuracy = 0.9023228287696838\n",
      "Training iter #31500000:   Batch Loss = 6.941286, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2956767082214355, Accuracy = 0.9023228287696838\n",
      "Training iter #31503000:   Batch Loss = 6.959389, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.29583740234375, Accuracy = 0.902025043964386\n",
      "Training iter #31506000:   Batch Loss = 6.953226, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.29545783996582, Accuracy = 0.9023228287696838\n",
      "Training iter #31509000:   Batch Loss = 6.896573, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295079708099365, Accuracy = 0.9023228287696838\n",
      "Training iter #31512000:   Batch Loss = 6.908990, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2955427169799805, Accuracy = 0.902025043964386\n",
      "Training iter #31515000:   Batch Loss = 6.939849, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.296412944793701, Accuracy = 0.902025043964386\n",
      "Training iter #31518000:   Batch Loss = 6.970741, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.296314239501953, Accuracy = 0.9023228287696838\n",
      "Training iter #31521000:   Batch Loss = 6.928775, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295519828796387, Accuracy = 0.9026206135749817\n",
      "Training iter #31524000:   Batch Loss = 6.909859, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295909881591797, Accuracy = 0.9029183983802795\n",
      "Training iter #31527000:   Batch Loss = 6.943955, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294679641723633, Accuracy = 0.9029183983802795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #31530000:   Batch Loss = 6.960581, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294699668884277, Accuracy = 0.9023228287696838\n",
      "Training iter #31533000:   Batch Loss = 6.953843, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295656681060791, Accuracy = 0.902025043964386\n",
      "Training iter #31536000:   Batch Loss = 6.897602, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.295163154602051, Accuracy = 0.9023228287696838\n",
      "Training iter #31539000:   Batch Loss = 6.907418, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294686317443848, Accuracy = 0.9026206135749817\n",
      "Training iter #31542000:   Batch Loss = 6.937525, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294497966766357, Accuracy = 0.9023228287696838\n",
      "Training iter #31545000:   Batch Loss = 6.972333, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294597625732422, Accuracy = 0.9017271995544434\n",
      "Training iter #31548000:   Batch Loss = 6.924211, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294835090637207, Accuracy = 0.9023228287696838\n",
      "Training iter #31551000:   Batch Loss = 6.904975, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294881820678711, Accuracy = 0.902025043964386\n",
      "Training iter #31554000:   Batch Loss = 6.947669, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294785976409912, Accuracy = 0.902025043964386\n",
      "Training iter #31557000:   Batch Loss = 6.957338, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294313430786133, Accuracy = 0.902025043964386\n",
      "Training iter #31560000:   Batch Loss = 6.950312, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.294375419616699, Accuracy = 0.9023228287696838\n",
      "Training iter #31563000:   Batch Loss = 6.897353, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.293932914733887, Accuracy = 0.9017271995544434\n",
      "Training iter #31566000:   Batch Loss = 6.905084, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.293453693389893, Accuracy = 0.9017271995544434\n",
      "Training iter #31569000:   Batch Loss = 6.933101, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.293825626373291, Accuracy = 0.902025043964386\n",
      "Training iter #31572000:   Batch Loss = 6.972991, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.293497085571289, Accuracy = 0.9023228287696838\n",
      "Training iter #31575000:   Batch Loss = 6.922035, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292018413543701, Accuracy = 0.902025043964386\n",
      "Training iter #31578000:   Batch Loss = 6.905612, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292171478271484, Accuracy = 0.9014294147491455\n",
      "Training iter #31581000:   Batch Loss = 6.931824, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292403221130371, Accuracy = 0.9014294147491455\n",
      "Training iter #31584000:   Batch Loss = 6.963173, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292534828186035, Accuracy = 0.9017271995544434\n",
      "Training iter #31587000:   Batch Loss = 6.944275, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.29262113571167, Accuracy = 0.902025043964386\n",
      "Training iter #31590000:   Batch Loss = 6.895766, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292357444763184, Accuracy = 0.902025043964386\n",
      "Training iter #31593000:   Batch Loss = 6.921432, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.29226541519165, Accuracy = 0.9026206135749817\n",
      "Training iter #31596000:   Batch Loss = 6.936217, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292269706726074, Accuracy = 0.9023228287696838\n",
      "Training iter #31599000:   Batch Loss = 6.964642, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292097568511963, Accuracy = 0.902025043964386\n",
      "Training iter #31602000:   Batch Loss = 6.918169, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.291364669799805, Accuracy = 0.9023228287696838\n",
      "Training iter #31605000:   Batch Loss = 6.900445, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.292210578918457, Accuracy = 0.9029183983802795\n",
      "Training iter #31608000:   Batch Loss = 6.930259, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.291959762573242, Accuracy = 0.9029183983802795\n",
      "Training iter #31611000:   Batch Loss = 6.967276, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.291721343994141, Accuracy = 0.9029183983802795\n",
      "Training iter #31614000:   Batch Loss = 6.933251, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2913312911987305, Accuracy = 0.9023228287696838\n",
      "Training iter #31617000:   Batch Loss = 6.893648, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.291075706481934, Accuracy = 0.9026206135749817\n",
      "Training iter #31620000:   Batch Loss = 6.921934, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290821075439453, Accuracy = 0.9029183983802795\n",
      "Training iter #31623000:   Batch Loss = 6.936958, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290465354919434, Accuracy = 0.9026206135749817\n",
      "Training iter #31626000:   Batch Loss = 6.967241, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2902655601501465, Accuracy = 0.9035139679908752\n",
      "Training iter #31629000:   Batch Loss = 6.910969, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290053367614746, Accuracy = 0.9038118124008179\n",
      "Training iter #31632000:   Batch Loss = 6.898951, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.289760112762451, Accuracy = 0.9038118124008179\n",
      "Training iter #31635000:   Batch Loss = 6.931333, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290273666381836, Accuracy = 0.9038118124008179\n",
      "Training iter #31638000:   Batch Loss = 6.968084, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290615558624268, Accuracy = 0.9035139679908752\n",
      "Training iter #31641000:   Batch Loss = 6.934857, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290536880493164, Accuracy = 0.9026206135749817\n",
      "Training iter #31644000:   Batch Loss = 6.892302, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290679454803467, Accuracy = 0.9026206135749817\n",
      "Training iter #31647000:   Batch Loss = 6.920841, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290602684020996, Accuracy = 0.9038118124008179\n",
      "Training iter #31650000:   Batch Loss = 6.923521, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.289918422698975, Accuracy = 0.9032161831855774\n",
      "Training iter #31653000:   Batch Loss = 6.965557, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.288965702056885, Accuracy = 0.9035139679908752\n",
      "Training iter #31656000:   Batch Loss = 6.909919, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.289064884185791, Accuracy = 0.9032161831855774\n",
      "Training iter #31659000:   Batch Loss = 6.896395, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2903852462768555, Accuracy = 0.9035139679908752\n",
      "Training iter #31662000:   Batch Loss = 6.939342, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290376663208008, Accuracy = 0.9032161831855774\n",
      "Training iter #31665000:   Batch Loss = 6.964699, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.290064334869385, Accuracy = 0.9035139679908752\n",
      "Training iter #31668000:   Batch Loss = 6.936440, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.289024829864502, Accuracy = 0.9032161831855774\n",
      "Training iter #31671000:   Batch Loss = 6.891675, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.289278507232666, Accuracy = 0.9035139679908752\n",
      "Training iter #31674000:   Batch Loss = 6.922129, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.289135932922363, Accuracy = 0.9038118124008179\n",
      "Training iter #31677000:   Batch Loss = 6.924288, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.288674831390381, Accuracy = 0.9038118124008179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #31680000:   Batch Loss = 6.964289, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.288590431213379, Accuracy = 0.9029183983802795\n",
      "Training iter #31683000:   Batch Loss = 6.899066, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.288298606872559, Accuracy = 0.9032161831855774\n",
      "Training iter #31686000:   Batch Loss = 6.894741, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.287534713745117, Accuracy = 0.9035139679908752\n",
      "Training iter #31689000:   Batch Loss = 6.940550, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.286203384399414, Accuracy = 0.9029183983802795\n",
      "Training iter #31692000:   Batch Loss = 6.965617, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.286391735076904, Accuracy = 0.9026206135749817\n",
      "Training iter #31695000:   Batch Loss = 6.937398, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.286900520324707, Accuracy = 0.9032161831855774\n",
      "Training iter #31698000:   Batch Loss = 6.896924, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.287057876586914, Accuracy = 0.9032161831855774\n",
      "Training iter #31701000:   Batch Loss = 6.921895, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.28710412979126, Accuracy = 0.9032161831855774\n",
      "Training iter #31704000:   Batch Loss = 6.940259, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.286897659301758, Accuracy = 0.9032161831855774\n",
      "Training iter #31707000:   Batch Loss = 6.963266, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.286783218383789, Accuracy = 0.9032161831855774\n",
      "Training iter #31710000:   Batch Loss = 6.890886, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2858781814575195, Accuracy = 0.9026206135749817\n",
      "Training iter #31713000:   Batch Loss = 6.889914, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.28607177734375, Accuracy = 0.9026206135749817\n",
      "Training iter #31716000:   Batch Loss = 6.943826, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.286050319671631, Accuracy = 0.9023228287696838\n",
      "Training iter #31719000:   Batch Loss = 6.947741, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285770893096924, Accuracy = 0.9023228287696838\n",
      "Training iter #31722000:   Batch Loss = 6.934750, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.286518096923828, Accuracy = 0.9026206135749817\n",
      "Training iter #31725000:   Batch Loss = 6.897602, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285984039306641, Accuracy = 0.9032161831855774\n",
      "Training iter #31728000:   Batch Loss = 6.922752, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285545825958252, Accuracy = 0.9026206135749817\n",
      "Training iter #31731000:   Batch Loss = 6.945168, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285273551940918, Accuracy = 0.9026206135749817\n",
      "Training iter #31734000:   Batch Loss = 6.950994, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285512447357178, Accuracy = 0.9026206135749817\n",
      "Training iter #31737000:   Batch Loss = 6.888432, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285599231719971, Accuracy = 0.9023228287696838\n",
      "Training iter #31740000:   Batch Loss = 6.889466, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285521030426025, Accuracy = 0.9029183983802795\n",
      "Training iter #31743000:   Batch Loss = 6.937925, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.285290241241455, Accuracy = 0.9026206135749817\n",
      "Training iter #31746000:   Batch Loss = 6.945157, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.284725189208984, Accuracy = 0.9023228287696838\n",
      "Training iter #31749000:   Batch Loss = 6.933790, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.284873962402344, Accuracy = 0.9023228287696838\n",
      "Training iter #31752000:   Batch Loss = 6.896866, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.28497838973999, Accuracy = 0.902025043964386\n",
      "Training iter #31755000:   Batch Loss = 6.925490, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.28475284576416, Accuracy = 0.902025043964386\n",
      "Training iter #31758000:   Batch Loss = 6.945709, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.284603118896484, Accuracy = 0.9023228287696838\n",
      "Training iter #31761000:   Batch Loss = 6.944955, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.284408092498779, Accuracy = 0.9023228287696838\n",
      "Training iter #31764000:   Batch Loss = 6.883029, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.284275531768799, Accuracy = 0.902025043964386\n",
      "Training iter #31767000:   Batch Loss = 6.887349, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282965183258057, Accuracy = 0.902025043964386\n",
      "Training iter #31770000:   Batch Loss = 6.928373, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282210826873779, Accuracy = 0.902025043964386\n",
      "Training iter #31773000:   Batch Loss = 6.950855, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282043933868408, Accuracy = 0.9023228287696838\n",
      "Training iter #31776000:   Batch Loss = 6.935395, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282411098480225, Accuracy = 0.9023228287696838\n",
      "Training iter #31779000:   Batch Loss = 6.899442, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.283414363861084, Accuracy = 0.9023228287696838\n",
      "Training iter #31782000:   Batch Loss = 6.925024, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282405853271484, Accuracy = 0.9026206135749817\n",
      "Training iter #31785000:   Batch Loss = 6.940548, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282588481903076, Accuracy = 0.9023228287696838\n",
      "Training iter #31788000:   Batch Loss = 6.939925, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282459259033203, Accuracy = 0.9032161831855774\n",
      "Training iter #31791000:   Batch Loss = 6.879311, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.283245086669922, Accuracy = 0.9029183983802795\n",
      "Training iter #31794000:   Batch Loss = 6.893390, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282935619354248, Accuracy = 0.9023228287696838\n",
      "Training iter #31797000:   Batch Loss = 6.926674, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.282468318939209, Accuracy = 0.9029183983802795\n",
      "Training iter #31800000:   Batch Loss = 6.951169, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.281917095184326, Accuracy = 0.9026206135749817\n",
      "Training iter #31803000:   Batch Loss = 6.931079, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.280667304992676, Accuracy = 0.9026206135749817\n",
      "Training iter #31806000:   Batch Loss = 6.898796, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.280207633972168, Accuracy = 0.9029183983802795\n",
      "Training iter #31809000:   Batch Loss = 6.920537, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.28033447265625, Accuracy = 0.9029183983802795\n",
      "Training iter #31812000:   Batch Loss = 6.938600, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279862403869629, Accuracy = 0.9029183983802795\n",
      "Training iter #31815000:   Batch Loss = 6.939641, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2788472175598145, Accuracy = 0.9032161831855774\n",
      "Training iter #31818000:   Batch Loss = 6.882158, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2796220779418945, Accuracy = 0.9035139679908752\n",
      "Training iter #31821000:   Batch Loss = 6.894263, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279568195343018, Accuracy = 0.9038118124008179\n",
      "Training iter #31824000:   Batch Loss = 6.923079, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279111862182617, Accuracy = 0.9038118124008179\n",
      "Training iter #31827000:   Batch Loss = 6.953347, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279043197631836, Accuracy = 0.9044073820114136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #31830000:   Batch Loss = 6.920722, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.27974271774292, Accuracy = 0.9041095972061157\n",
      "Training iter #31833000:   Batch Loss = 6.894327, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279524803161621, Accuracy = 0.9041095972061157\n",
      "Training iter #31836000:   Batch Loss = 6.927493, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.278884410858154, Accuracy = 0.9038118124008179\n",
      "Training iter #31839000:   Batch Loss = 6.940146, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279311180114746, Accuracy = 0.9032161831855774\n",
      "Training iter #31842000:   Batch Loss = 6.939268, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279018878936768, Accuracy = 0.9035139679908752\n",
      "Training iter #31845000:   Batch Loss = 6.879523, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2789435386657715, Accuracy = 0.9038118124008179\n",
      "Training iter #31848000:   Batch Loss = 6.890964, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.27927303314209, Accuracy = 0.9032161831855774\n",
      "Training iter #31851000:   Batch Loss = 6.921069, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.279030799865723, Accuracy = 0.9035139679908752\n",
      "Training iter #31854000:   Batch Loss = 6.954364, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.278526306152344, Accuracy = 0.9041095972061157\n",
      "Training iter #31857000:   Batch Loss = 6.909944, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.278144836425781, Accuracy = 0.9041095972061157\n",
      "Training iter #31860000:   Batch Loss = 6.890297, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.278026103973389, Accuracy = 0.9041095972061157\n",
      "Training iter #31863000:   Batch Loss = 6.925632, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.278205871582031, Accuracy = 0.9041095972061157\n",
      "Training iter #31866000:   Batch Loss = 6.938840, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.278591156005859, Accuracy = 0.9044073820114136\n",
      "Training iter #31869000:   Batch Loss = 6.935740, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.278111934661865, Accuracy = 0.9041095972061157\n",
      "Training iter #31872000:   Batch Loss = 6.881881, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.277804851531982, Accuracy = 0.9035139679908752\n",
      "Training iter #31875000:   Batch Loss = 6.890840, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.277470588684082, Accuracy = 0.9035139679908752\n",
      "Training iter #31878000:   Batch Loss = 6.917382, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.277275562286377, Accuracy = 0.9035139679908752\n",
      "Training iter #31881000:   Batch Loss = 6.956233, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.277098655700684, Accuracy = 0.9035139679908752\n",
      "Training iter #31884000:   Batch Loss = 6.905840, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276942729949951, Accuracy = 0.9038118124008179\n",
      "Training iter #31887000:   Batch Loss = 6.890339, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2768402099609375, Accuracy = 0.9038118124008179\n",
      "Training iter #31890000:   Batch Loss = 6.929473, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276785373687744, Accuracy = 0.9035139679908752\n",
      "Training iter #31893000:   Batch Loss = 6.940197, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.27697229385376, Accuracy = 0.9035139679908752\n",
      "Training iter #31896000:   Batch Loss = 6.931727, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276894569396973, Accuracy = 0.9035139679908752\n",
      "Training iter #31899000:   Batch Loss = 6.879875, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.27691650390625, Accuracy = 0.9035139679908752\n",
      "Training iter #31902000:   Batch Loss = 6.903903, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276869773864746, Accuracy = 0.9035139679908752\n",
      "Training iter #31905000:   Batch Loss = 6.918370, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276895046234131, Accuracy = 0.9035139679908752\n",
      "Training iter #31908000:   Batch Loss = 6.955150, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2765655517578125, Accuracy = 0.9032161831855774\n",
      "Training iter #31911000:   Batch Loss = 6.902557, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276172637939453, Accuracy = 0.9035139679908752\n",
      "Training iter #31914000:   Batch Loss = 6.888566, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.27579927444458, Accuracy = 0.9035139679908752\n",
      "Training iter #31917000:   Batch Loss = 6.912752, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.274167060852051, Accuracy = 0.9035139679908752\n",
      "Training iter #31920000:   Batch Loss = 6.951762, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.274131774902344, Accuracy = 0.9035139679908752\n",
      "Training iter #31923000:   Batch Loss = 6.922610, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.273911952972412, Accuracy = 0.9038118124008179\n",
      "Training iter #31926000:   Batch Loss = 6.880617, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2743239402771, Accuracy = 0.9041095972061157\n",
      "Training iter #31929000:   Batch Loss = 6.904799, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.275389671325684, Accuracy = 0.9038118124008179\n",
      "Training iter #31932000:   Batch Loss = 6.919934, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2762041091918945, Accuracy = 0.9035139679908752\n",
      "Training iter #31935000:   Batch Loss = 6.951574, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.277161121368408, Accuracy = 0.9032161831855774\n",
      "Training iter #31938000:   Batch Loss = 6.899996, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.277765274047852, Accuracy = 0.9038118124008179\n",
      "Training iter #31941000:   Batch Loss = 6.883929, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276972770690918, Accuracy = 0.9044073820114136\n",
      "Training iter #31944000:   Batch Loss = 6.915551, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276532173156738, Accuracy = 0.9038118124008179\n",
      "Training iter #31947000:   Batch Loss = 6.951651, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.276216983795166, Accuracy = 0.9032161831855774\n",
      "Training iter #31950000:   Batch Loss = 6.921762, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.275979995727539, Accuracy = 0.9035139679908752\n",
      "Training iter #31953000:   Batch Loss = 6.875830, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.275030612945557, Accuracy = 0.9041095972061157\n",
      "Training iter #31956000:   Batch Loss = 6.904806, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.274573802947998, Accuracy = 0.9041095972061157\n",
      "Training iter #31959000:   Batch Loss = 6.919878, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.274105072021484, Accuracy = 0.9050029516220093\n",
      "Training iter #31962000:   Batch Loss = 6.949149, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.273283004760742, Accuracy = 0.9047051668167114\n",
      "Training iter #31965000:   Batch Loss = 6.893658, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.273022651672363, Accuracy = 0.9044073820114136\n",
      "Training iter #31968000:   Batch Loss = 6.881047, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.27273416519165, Accuracy = 0.9047051668167114\n",
      "Training iter #31971000:   Batch Loss = 6.925529, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.272151470184326, Accuracy = 0.9044073820114136\n",
      "Training iter #31974000:   Batch Loss = 6.949653, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.271596908569336, Accuracy = 0.9041095972061157\n",
      "Training iter #31977000:   Batch Loss = 6.916633, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.271541118621826, Accuracy = 0.9038118124008179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #31980000:   Batch Loss = 6.875610, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270339012145996, Accuracy = 0.9038118124008179\n",
      "Training iter #31983000:   Batch Loss = 6.904555, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269754886627197, Accuracy = 0.9038118124008179\n",
      "Training iter #31986000:   Batch Loss = 6.908280, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270296573638916, Accuracy = 0.9044073820114136\n",
      "Training iter #31989000:   Batch Loss = 6.946404, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270852565765381, Accuracy = 0.9047051668167114\n",
      "Training iter #31992000:   Batch Loss = 6.888671, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.271101951599121, Accuracy = 0.9050029516220093\n",
      "Training iter #31995000:   Batch Loss = 6.880669, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270979404449463, Accuracy = 0.9050029516220093\n",
      "Training iter #31998000:   Batch Loss = 6.923470, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270547389984131, Accuracy = 0.9053007960319519\n",
      "Training iter #32001000:   Batch Loss = 6.945063, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269925594329834, Accuracy = 0.9050029516220093\n",
      "Training iter #32004000:   Batch Loss = 6.920359, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269612789154053, Accuracy = 0.9044073820114136\n",
      "Training iter #32007000:   Batch Loss = 6.876627, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269277572631836, Accuracy = 0.9044073820114136\n",
      "Training iter #32010000:   Batch Loss = 6.904390, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.268998622894287, Accuracy = 0.9044073820114136\n",
      "Training iter #32013000:   Batch Loss = 6.905286, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270222187042236, Accuracy = 0.9044073820114136\n",
      "Training iter #32016000:   Batch Loss = 6.948201, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270322322845459, Accuracy = 0.9041095972061157\n",
      "Training iter #32019000:   Batch Loss = 6.881994, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270320892333984, Accuracy = 0.9038118124008179\n",
      "Training iter #32022000:   Batch Loss = 6.876267, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270159721374512, Accuracy = 0.9035139679908752\n",
      "Training iter #32025000:   Batch Loss = 6.923110, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.26984977722168, Accuracy = 0.9041095972061157\n",
      "Training iter #32028000:   Batch Loss = 6.926849, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.268868446350098, Accuracy = 0.9041095972061157\n",
      "Training iter #32031000:   Batch Loss = 6.919794, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269587993621826, Accuracy = 0.9047051668167114\n",
      "Training iter #32034000:   Batch Loss = 6.880450, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270370960235596, Accuracy = 0.9047051668167114\n",
      "Training iter #32037000:   Batch Loss = 6.903474, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270423412322998, Accuracy = 0.9047051668167114\n",
      "Training iter #32040000:   Batch Loss = 6.921565, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270472049713135, Accuracy = 0.9047051668167114\n",
      "Training iter #32043000:   Batch Loss = 6.940245, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270496845245361, Accuracy = 0.9047051668167114\n",
      "Training iter #32046000:   Batch Loss = 6.874046, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2703633308410645, Accuracy = 0.9041095972061157\n",
      "Training iter #32049000:   Batch Loss = 6.875750, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270498752593994, Accuracy = 0.9041095972061157\n",
      "Training iter #32052000:   Batch Loss = 6.921868, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270559310913086, Accuracy = 0.9044073820114136\n",
      "Training iter #32055000:   Batch Loss = 6.929465, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270830154418945, Accuracy = 0.9047051668167114\n",
      "Training iter #32058000:   Batch Loss = 6.915818, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270698547363281, Accuracy = 0.9050029516220093\n",
      "Training iter #32061000:   Batch Loss = 6.881074, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270410060882568, Accuracy = 0.9053007960319519\n",
      "Training iter #32064000:   Batch Loss = 6.905174, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270026206970215, Accuracy = 0.9050029516220093\n",
      "Training iter #32067000:   Batch Loss = 6.929528, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269835948944092, Accuracy = 0.9047051668167114\n",
      "Training iter #32070000:   Batch Loss = 6.933190, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269893169403076, Accuracy = 0.9041095972061157\n",
      "Training iter #32073000:   Batch Loss = 6.873860, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269521713256836, Accuracy = 0.9041095972061157\n",
      "Training iter #32076000:   Batch Loss = 6.874991, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.268902778625488, Accuracy = 0.9050029516220093\n",
      "Training iter #32079000:   Batch Loss = 6.918485, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.268669128417969, Accuracy = 0.9053007960319519\n",
      "Training iter #32082000:   Batch Loss = 6.929008, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.268351078033447, Accuracy = 0.9047051668167114\n",
      "Training iter #32085000:   Batch Loss = 6.918038, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2682271003723145, Accuracy = 0.9050029516220093\n",
      "Training iter #32088000:   Batch Loss = 6.884586, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267932415008545, Accuracy = 0.9050029516220093\n",
      "Training iter #32091000:   Batch Loss = 6.910110, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.26767635345459, Accuracy = 0.9053007960319519\n",
      "Training iter #32094000:   Batch Loss = 6.926423, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267590522766113, Accuracy = 0.9050029516220093\n",
      "Training iter #32097000:   Batch Loss = 6.924155, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267766952514648, Accuracy = 0.9047051668167114\n",
      "Training iter #32100000:   Batch Loss = 6.865744, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267776012420654, Accuracy = 0.9050029516220093\n",
      "Training iter #32103000:   Batch Loss = 6.877531, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2677531242370605, Accuracy = 0.9047051668167114\n",
      "Training iter #32106000:   Batch Loss = 6.912641, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267852306365967, Accuracy = 0.9041095972061157\n",
      "Training iter #32109000:   Batch Loss = 6.932525, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267681121826172, Accuracy = 0.9038118124008179\n",
      "Training iter #32112000:   Batch Loss = 6.917897, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.268706321716309, Accuracy = 0.9029183983802795\n",
      "Training iter #32115000:   Batch Loss = 6.893722, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.26666259765625, Accuracy = 0.9047051668167114\n",
      "Training iter #32118000:   Batch Loss = 6.906255, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.26644229888916, Accuracy = 0.9041095972061157\n",
      "Training iter #32121000:   Batch Loss = 6.923476, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267744541168213, Accuracy = 0.9050029516220093\n",
      "Training iter #32124000:   Batch Loss = 6.924479, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267489433288574, Accuracy = 0.9050029516220093\n",
      "Training iter #32127000:   Batch Loss = 6.871336, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.266539573669434, Accuracy = 0.9053007960319519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #32130000:   Batch Loss = 6.914656, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267364501953125, Accuracy = 0.9053007960319519\n",
      "Training iter #32133000:   Batch Loss = 6.910656, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.266778469085693, Accuracy = 0.9041095972061157\n",
      "Training iter #32136000:   Batch Loss = 6.936617, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.266692161560059, Accuracy = 0.9035139679908752\n",
      "Training iter #32139000:   Batch Loss = 6.910186, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.26706600189209, Accuracy = 0.9032161831855774\n",
      "Training iter #32142000:   Batch Loss = 6.886297, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.266338348388672, Accuracy = 0.9035139679908752\n",
      "Training iter #32145000:   Batch Loss = 6.909269, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.266207695007324, Accuracy = 0.9026206135749817\n",
      "Training iter #32148000:   Batch Loss = 6.923332, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.266256809234619, Accuracy = 0.9026206135749817\n",
      "Training iter #32151000:   Batch Loss = 6.920456, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.266263961791992, Accuracy = 0.9026206135749817\n",
      "Training iter #32154000:   Batch Loss = 6.875354, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.265470504760742, Accuracy = 0.9023228287696838\n",
      "Training iter #32157000:   Batch Loss = 6.878057, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.264425754547119, Accuracy = 0.9038118124008179\n",
      "Training iter #32160000:   Batch Loss = 6.911387, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.264239311218262, Accuracy = 0.9035139679908752\n",
      "Training iter #32163000:   Batch Loss = 6.942393, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.264742374420166, Accuracy = 0.9035139679908752\n",
      "Training iter #32166000:   Batch Loss = 6.897382, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267890930175781, Accuracy = 0.9044073820114136\n",
      "Training iter #32169000:   Batch Loss = 6.881800, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2700934410095215, Accuracy = 0.9041095972061157\n",
      "Training iter #32172000:   Batch Loss = 6.914340, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.271368026733398, Accuracy = 0.9032161831855774\n",
      "Training iter #32175000:   Batch Loss = 6.930388, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.273277759552002, Accuracy = 0.9026206135749817\n",
      "Training iter #32178000:   Batch Loss = 6.923837, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.272603511810303, Accuracy = 0.9026206135749817\n",
      "Training iter #32181000:   Batch Loss = 6.869936, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.270524024963379, Accuracy = 0.9029183983802795\n",
      "Training iter #32184000:   Batch Loss = 6.877026, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269495010375977, Accuracy = 0.9023228287696838\n",
      "Training iter #32187000:   Batch Loss = 6.907658, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269573211669922, Accuracy = 0.9017271995544434\n",
      "Training iter #32190000:   Batch Loss = 6.939151, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.269277572631836, Accuracy = 0.9029183983802795\n",
      "Training iter #32193000:   Batch Loss = 6.895485, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.267951011657715, Accuracy = 0.9023228287696838\n",
      "Training iter #32196000:   Batch Loss = 6.876136, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.265817642211914, Accuracy = 0.9032161831855774\n",
      "Training iter #32199000:   Batch Loss = 6.915061, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.265439987182617, Accuracy = 0.9038118124008179\n",
      "Training iter #32202000:   Batch Loss = 6.924344, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.265961647033691, Accuracy = 0.9047051668167114\n",
      "Training iter #32205000:   Batch Loss = 6.918760, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.265622615814209, Accuracy = 0.9044073820114136\n",
      "Training iter #32208000:   Batch Loss = 6.868107, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.264888286590576, Accuracy = 0.9047051668167114\n",
      "Training iter #32211000:   Batch Loss = 6.876524, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.264214992523193, Accuracy = 0.9055985808372498\n",
      "Training iter #32214000:   Batch Loss = 6.900896, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.263863563537598, Accuracy = 0.9055985808372498\n",
      "Training iter #32217000:   Batch Loss = 6.940459, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.263168811798096, Accuracy = 0.9058963656425476\n",
      "Training iter #32220000:   Batch Loss = 6.890225, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.262359619140625, Accuracy = 0.9058963656425476\n",
      "Training iter #32223000:   Batch Loss = 6.875704, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.261961936950684, Accuracy = 0.9058963656425476\n",
      "Training iter #32226000:   Batch Loss = 6.899115, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2617106437683105, Accuracy = 0.9055985808372498\n",
      "Training iter #32229000:   Batch Loss = 6.924727, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.261916637420654, Accuracy = 0.9044073820114136\n",
      "Training iter #32232000:   Batch Loss = 6.910578, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.262101650238037, Accuracy = 0.9044073820114136\n",
      "Training iter #32235000:   Batch Loss = 6.863758, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.262308120727539, Accuracy = 0.9044073820114136\n",
      "Training iter #32238000:   Batch Loss = 6.889369, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.26224946975708, Accuracy = 0.9047051668167114\n",
      "Training iter #32241000:   Batch Loss = 6.902483, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.26215124130249, Accuracy = 0.9041095972061157\n",
      "Training iter #32244000:   Batch Loss = 6.928177, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.261809349060059, Accuracy = 0.9038118124008179\n",
      "Training iter #32247000:   Batch Loss = 6.886439, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.261474132537842, Accuracy = 0.9032161831855774\n",
      "Training iter #32250000:   Batch Loss = 6.870431, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.261220455169678, Accuracy = 0.9035139679908752\n",
      "Training iter #32253000:   Batch Loss = 6.897207, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.261112213134766, Accuracy = 0.9041095972061157\n",
      "Training iter #32256000:   Batch Loss = 6.934754, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2613420486450195, Accuracy = 0.9038118124008179\n",
      "Training iter #32259000:   Batch Loss = 6.898872, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2612528800964355, Accuracy = 0.9041095972061157\n",
      "Training iter #32262000:   Batch Loss = 6.864563, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.261014938354492, Accuracy = 0.9038118124008179\n",
      "Training iter #32265000:   Batch Loss = 6.890023, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.260669708251953, Accuracy = 0.9038118124008179\n",
      "Training iter #32268000:   Batch Loss = 6.904269, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.260440349578857, Accuracy = 0.9044073820114136\n",
      "Training iter #32271000:   Batch Loss = 6.932019, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.260368824005127, Accuracy = 0.9041095972061157\n",
      "Training iter #32274000:   Batch Loss = 6.880348, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2605414390563965, Accuracy = 0.9041095972061157\n",
      "Training iter #32277000:   Batch Loss = 6.868567, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.260513782501221, Accuracy = 0.9044073820114136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #32280000:   Batch Loss = 6.896461, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.260284423828125, Accuracy = 0.9041095972061157\n",
      "Training iter #32283000:   Batch Loss = 6.933260, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.259282112121582, Accuracy = 0.9044073820114136\n",
      "Training iter #32286000:   Batch Loss = 6.901613, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.259953498840332, Accuracy = 0.9038118124008179\n",
      "Training iter #32289000:   Batch Loss = 6.859861, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.259759902954102, Accuracy = 0.9041095972061157\n",
      "Training iter #32292000:   Batch Loss = 6.888481, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.259316921234131, Accuracy = 0.9038118124008179\n",
      "Training iter #32295000:   Batch Loss = 6.889822, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.258964538574219, Accuracy = 0.9038118124008179\n",
      "Training iter #32298000:   Batch Loss = 6.929835, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.258803844451904, Accuracy = 0.9035139679908752\n",
      "Training iter #32301000:   Batch Loss = 6.877186, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.258681297302246, Accuracy = 0.9035139679908752\n",
      "Training iter #32304000:   Batch Loss = 6.865431, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.25825834274292, Accuracy = 0.9038118124008179\n",
      "Training iter #32307000:   Batch Loss = 6.907341, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.257927417755127, Accuracy = 0.9038118124008179\n",
      "Training iter #32310000:   Batch Loss = 6.930581, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.257627010345459, Accuracy = 0.9038118124008179\n",
      "Training iter #32313000:   Batch Loss = 6.899203, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.258126258850098, Accuracy = 0.9041095972061157\n",
      "Training iter #32316000:   Batch Loss = 6.859606, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.257905960083008, Accuracy = 0.9041095972061157\n",
      "Training iter #32319000:   Batch Loss = 6.889470, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.257353782653809, Accuracy = 0.9038118124008179\n",
      "Training iter #32322000:   Batch Loss = 6.890340, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2568888664245605, Accuracy = 0.9035139679908752\n",
      "Training iter #32325000:   Batch Loss = 6.928882, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.256799221038818, Accuracy = 0.9044073820114136\n",
      "Training iter #32328000:   Batch Loss = 6.870606, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2567291259765625, Accuracy = 0.9041095972061157\n",
      "Training iter #32331000:   Batch Loss = 6.865138, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.255881309509277, Accuracy = 0.9050029516220093\n",
      "Training iter #32334000:   Batch Loss = 6.907693, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.255980968475342, Accuracy = 0.9053007960319519\n",
      "Training iter #32337000:   Batch Loss = 6.926424, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2556328773498535, Accuracy = 0.9055985808372498\n",
      "Training iter #32340000:   Batch Loss = 6.902055, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.255612850189209, Accuracy = 0.9050029516220093\n",
      "Training iter #32343000:   Batch Loss = 6.864169, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.25541353225708, Accuracy = 0.9047051668167114\n",
      "Training iter #32346000:   Batch Loss = 6.887835, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2551398277282715, Accuracy = 0.9041095972061157\n",
      "Training iter #32349000:   Batch Loss = 6.905527, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.255000114440918, Accuracy = 0.9041095972061157\n",
      "Training iter #32352000:   Batch Loss = 6.928162, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.255357265472412, Accuracy = 0.9038118124008179\n",
      "Training iter #32355000:   Batch Loss = 6.859327, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2553815841674805, Accuracy = 0.9035139679908752\n",
      "Training iter #32358000:   Batch Loss = 6.859905, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.255083084106445, Accuracy = 0.9035139679908752\n",
      "Training iter #32361000:   Batch Loss = 6.908926, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.254835605621338, Accuracy = 0.9035139679908752\n",
      "Training iter #32364000:   Batch Loss = 6.912949, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.254850387573242, Accuracy = 0.9035139679908752\n",
      "Training iter #32367000:   Batch Loss = 6.900816, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.254629135131836, Accuracy = 0.9038118124008179\n",
      "Training iter #32370000:   Batch Loss = 6.865250, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.253568172454834, Accuracy = 0.9041095972061157\n",
      "Training iter #32373000:   Batch Loss = 6.887724, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.253190994262695, Accuracy = 0.9047051668167114\n",
      "Training iter #32376000:   Batch Loss = 6.909649, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2531609535217285, Accuracy = 0.9050029516220093\n",
      "Training iter #32379000:   Batch Loss = 6.916100, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.253077030181885, Accuracy = 0.9047051668167114\n",
      "Training iter #32382000:   Batch Loss = 6.856689, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2528462409973145, Accuracy = 0.9047051668167114\n",
      "Training iter #32385000:   Batch Loss = 6.858916, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252630233764648, Accuracy = 0.9050029516220093\n",
      "Training iter #32388000:   Batch Loss = 6.904093, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2529802322387695, Accuracy = 0.9047051668167114\n",
      "Training iter #32391000:   Batch Loss = 6.910116, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.253128528594971, Accuracy = 0.9047051668167114\n",
      "Training iter #32394000:   Batch Loss = 6.897470, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252930641174316, Accuracy = 0.9047051668167114\n",
      "Training iter #32397000:   Batch Loss = 6.863801, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252598762512207, Accuracy = 0.9047051668167114\n",
      "Training iter #32400000:   Batch Loss = 6.888490, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252198696136475, Accuracy = 0.9047051668167114\n",
      "Training iter #32403000:   Batch Loss = 6.910754, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2519917488098145, Accuracy = 0.9047051668167114\n",
      "Training iter #32406000:   Batch Loss = 6.912000, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2530837059021, Accuracy = 0.9047051668167114\n",
      "Training iter #32409000:   Batch Loss = 6.851227, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252987384796143, Accuracy = 0.9047051668167114\n",
      "Training iter #32412000:   Batch Loss = 6.857264, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252748966217041, Accuracy = 0.9047051668167114\n",
      "Training iter #32415000:   Batch Loss = 6.894913, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252567291259766, Accuracy = 0.9047051668167114\n",
      "Training iter #32418000:   Batch Loss = 6.914671, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.25253438949585, Accuracy = 0.9047051668167114\n",
      "Training iter #32421000:   Batch Loss = 6.899844, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252377510070801, Accuracy = 0.9047051668167114\n",
      "Training iter #32424000:   Batch Loss = 6.866794, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.252159118652344, Accuracy = 0.9047051668167114\n",
      "Training iter #32427000:   Batch Loss = 6.889960, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.251947402954102, Accuracy = 0.9044073820114136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #32430000:   Batch Loss = 6.905032, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.251791954040527, Accuracy = 0.9044073820114136\n",
      "Training iter #32433000:   Batch Loss = 6.904627, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.251587867736816, Accuracy = 0.9047051668167114\n",
      "Training iter #32436000:   Batch Loss = 6.847601, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.251384258270264, Accuracy = 0.9044073820114136\n",
      "Training iter #32439000:   Batch Loss = 6.861204, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.251185894012451, Accuracy = 0.9047051668167114\n",
      "Training iter #32442000:   Batch Loss = 6.892528, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.251161098480225, Accuracy = 0.9047051668167114\n",
      "Training iter #32445000:   Batch Loss = 6.913414, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.25127649307251, Accuracy = 0.9047051668167114\n",
      "Training iter #32448000:   Batch Loss = 6.896211, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2511491775512695, Accuracy = 0.9047051668167114\n",
      "Training iter #32451000:   Batch Loss = 6.867906, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.250951766967773, Accuracy = 0.9047051668167114\n",
      "Training iter #32454000:   Batch Loss = 6.887217, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.250730037689209, Accuracy = 0.9044073820114136\n",
      "Training iter #32457000:   Batch Loss = 6.903568, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.250636577606201, Accuracy = 0.9047051668167114\n",
      "Training iter #32460000:   Batch Loss = 6.904619, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.250731945037842, Accuracy = 0.9047051668167114\n",
      "Training iter #32463000:   Batch Loss = 6.846995, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.250699043273926, Accuracy = 0.9047051668167114\n",
      "Training iter #32466000:   Batch Loss = 6.861263, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.250441551208496, Accuracy = 0.9047051668167114\n",
      "Training iter #32469000:   Batch Loss = 6.889863, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.249307632446289, Accuracy = 0.9053007960319519\n",
      "Training iter #32472000:   Batch Loss = 6.920533, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.249371528625488, Accuracy = 0.9050029516220093\n",
      "Training iter #32475000:   Batch Loss = 6.885491, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.249997615814209, Accuracy = 0.9050029516220093\n",
      "Training iter #32478000:   Batch Loss = 6.863531, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.249359607696533, Accuracy = 0.9050029516220093\n",
      "Training iter #32481000:   Batch Loss = 6.891001, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248779773712158, Accuracy = 0.9047051668167114\n",
      "Training iter #32484000:   Batch Loss = 6.904741, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248588562011719, Accuracy = 0.9047051668167114\n",
      "Training iter #32487000:   Batch Loss = 6.904699, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248466968536377, Accuracy = 0.9047051668167114\n",
      "Training iter #32490000:   Batch Loss = 6.847483, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248641490936279, Accuracy = 0.9047051668167114\n",
      "Training iter #32493000:   Batch Loss = 6.858727, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248857021331787, Accuracy = 0.9047051668167114\n",
      "Training iter #32496000:   Batch Loss = 6.887525, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248843669891357, Accuracy = 0.9050029516220093\n",
      "Training iter #32499000:   Batch Loss = 6.917876, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248258113861084, Accuracy = 0.9047051668167114\n",
      "Training iter #32502000:   Batch Loss = 6.876551, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248900890350342, Accuracy = 0.9047051668167114\n",
      "Training iter #32505000:   Batch Loss = 6.858697, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.248617649078369, Accuracy = 0.9047051668167114\n",
      "Training iter #32508000:   Batch Loss = 6.892264, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.247884273529053, Accuracy = 0.9047051668167114\n",
      "Training iter #32511000:   Batch Loss = 6.905919, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.247570037841797, Accuracy = 0.9044073820114136\n",
      "Training iter #32514000:   Batch Loss = 6.901200, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.247708797454834, Accuracy = 0.9047051668167114\n",
      "Training iter #32517000:   Batch Loss = 6.849506, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.247237205505371, Accuracy = 0.9047051668167114\n",
      "Training iter #32520000:   Batch Loss = 6.858227, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246801376342773, Accuracy = 0.9047051668167114\n",
      "Training iter #32523000:   Batch Loss = 6.884910, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246469020843506, Accuracy = 0.9050029516220093\n",
      "Training iter #32526000:   Batch Loss = 6.920299, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246774673461914, Accuracy = 0.9053007960319519\n",
      "Training iter #32529000:   Batch Loss = 6.871814, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246789455413818, Accuracy = 0.9053007960319519\n",
      "Training iter #32532000:   Batch Loss = 6.856524, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.245819091796875, Accuracy = 0.9050029516220093\n",
      "Training iter #32535000:   Batch Loss = 6.896108, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246042728424072, Accuracy = 0.9050029516220093\n",
      "Training iter #32538000:   Batch Loss = 6.903704, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246137619018555, Accuracy = 0.9050029516220093\n",
      "Training iter #32541000:   Batch Loss = 6.896358, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246156692504883, Accuracy = 0.9050029516220093\n",
      "Training iter #32544000:   Batch Loss = 6.848819, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246067047119141, Accuracy = 0.9047051668167114\n",
      "Training iter #32547000:   Batch Loss = 6.869494, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.247013092041016, Accuracy = 0.9044073820114136\n",
      "Training iter #32550000:   Batch Loss = 6.882151, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246945858001709, Accuracy = 0.9038118124008179\n",
      "Training iter #32553000:   Batch Loss = 6.919192, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.246876239776611, Accuracy = 0.9038118124008179\n",
      "Training iter #32556000:   Batch Loss = 6.869336, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2463274002075195, Accuracy = 0.9038118124008179\n",
      "Training iter #32559000:   Batch Loss = 6.856129, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.245725631713867, Accuracy = 0.9041095972061157\n",
      "Training iter #32562000:   Batch Loss = 6.879534, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.245410919189453, Accuracy = 0.9041095972061157\n",
      "Training iter #32565000:   Batch Loss = 6.918394, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.245100021362305, Accuracy = 0.9041095972061157\n",
      "Training iter #32568000:   Batch Loss = 6.891197, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2454752922058105, Accuracy = 0.9041095972061157\n",
      "Training iter #32571000:   Batch Loss = 6.849812, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.245526313781738, Accuracy = 0.9044073820114136\n",
      "Training iter #32574000:   Batch Loss = 6.871352, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.244300842285156, Accuracy = 0.9044073820114136\n",
      "Training iter #32577000:   Batch Loss = 6.884220, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.244462966918945, Accuracy = 0.9044073820114136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #32580000:   Batch Loss = 6.915111, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.244450569152832, Accuracy = 0.9047051668167114\n",
      "Training iter #32583000:   Batch Loss = 6.865800, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.244022846221924, Accuracy = 0.9047051668167114\n",
      "Training iter #32586000:   Batch Loss = 6.851235, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2441301345825195, Accuracy = 0.9047051668167114\n",
      "Training iter #32589000:   Batch Loss = 6.878537, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.244622230529785, Accuracy = 0.9044073820114136\n",
      "Training iter #32592000:   Batch Loss = 6.913652, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.244460582733154, Accuracy = 0.9047051668167114\n",
      "Training iter #32595000:   Batch Loss = 6.884508, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2439751625061035, Accuracy = 0.9047051668167114\n",
      "Training iter #32598000:   Batch Loss = 6.844890, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.243457317352295, Accuracy = 0.9047051668167114\n",
      "Training iter #32601000:   Batch Loss = 6.871748, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.243476867675781, Accuracy = 0.9044073820114136\n",
      "Training iter #32604000:   Batch Loss = 6.884727, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2436957359313965, Accuracy = 0.9041095972061157\n",
      "Training iter #32607000:   Batch Loss = 6.911184, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.242976188659668, Accuracy = 0.9047051668167114\n",
      "Training iter #32610000:   Batch Loss = 6.860051, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.243108749389648, Accuracy = 0.9047051668167114\n",
      "Training iter #32613000:   Batch Loss = 6.849601, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.243347644805908, Accuracy = 0.9047051668167114\n",
      "Training iter #32616000:   Batch Loss = 6.890935, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.242081642150879, Accuracy = 0.9050029516220093\n",
      "Training iter #32619000:   Batch Loss = 6.914321, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.242325305938721, Accuracy = 0.9047051668167114\n",
      "Training iter #32622000:   Batch Loss = 6.881966, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2422966957092285, Accuracy = 0.9047051668167114\n",
      "Training iter #32625000:   Batch Loss = 6.843568, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2420654296875, Accuracy = 0.9047051668167114\n",
      "Training iter #32628000:   Batch Loss = 6.870814, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241811275482178, Accuracy = 0.9047051668167114\n",
      "Training iter #32631000:   Batch Loss = 6.872827, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241457462310791, Accuracy = 0.9047051668167114\n",
      "Training iter #32634000:   Batch Loss = 6.910532, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.242386341094971, Accuracy = 0.9047051668167114\n",
      "Training iter #32637000:   Batch Loss = 6.858701, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241281509399414, Accuracy = 0.9047051668167114\n",
      "Training iter #32640000:   Batch Loss = 6.847494, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241661548614502, Accuracy = 0.9047051668167114\n",
      "Training iter #32643000:   Batch Loss = 6.889613, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2414703369140625, Accuracy = 0.9050029516220093\n",
      "Training iter #32646000:   Batch Loss = 6.910301, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241511344909668, Accuracy = 0.9047051668167114\n",
      "Training iter #32649000:   Batch Loss = 6.883867, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241186141967773, Accuracy = 0.9047051668167114\n",
      "Training iter #32652000:   Batch Loss = 6.843282, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241013050079346, Accuracy = 0.9044073820114136\n",
      "Training iter #32655000:   Batch Loss = 6.870608, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241518974304199, Accuracy = 0.9047051668167114\n",
      "Training iter #32658000:   Batch Loss = 6.872183, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.241794109344482, Accuracy = 0.9047051668167114\n",
      "Training iter #32661000:   Batch Loss = 6.911304, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.240424156188965, Accuracy = 0.9050029516220093\n",
      "Training iter #32664000:   Batch Loss = 6.848008, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.240212917327881, Accuracy = 0.9047051668167114\n",
      "Training iter #32667000:   Batch Loss = 6.845809, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.240026950836182, Accuracy = 0.9047051668167114\n",
      "Training iter #32670000:   Batch Loss = 6.889114, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.240108013153076, Accuracy = 0.9047051668167114\n",
      "Training iter #32673000:   Batch Loss = 6.893330, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.239711284637451, Accuracy = 0.9050029516220093\n",
      "Training iter #32676000:   Batch Loss = 6.884758, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.240366458892822, Accuracy = 0.9047051668167114\n",
      "Training iter #32679000:   Batch Loss = 6.847712, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2401580810546875, Accuracy = 0.9047051668167114\n",
      "Training iter #32682000:   Batch Loss = 6.871210, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.239787578582764, Accuracy = 0.9047051668167114\n",
      "Training iter #32685000:   Batch Loss = 6.884739, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.239659786224365, Accuracy = 0.9047051668167114\n",
      "Training iter #32688000:   Batch Loss = 6.905130, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.239782810211182, Accuracy = 0.9047051668167114\n",
      "Training iter #32691000:   Batch Loss = 6.841293, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.239668369293213, Accuracy = 0.9044073820114136\n",
      "Training iter #32694000:   Batch Loss = 6.840312, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.238426685333252, Accuracy = 0.9047051668167114\n",
      "Training iter #32697000:   Batch Loss = 6.890522, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.238577365875244, Accuracy = 0.9044073820114136\n",
      "Training iter #32700000:   Batch Loss = 6.898723, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.238340377807617, Accuracy = 0.9044073820114136\n",
      "Training iter #32703000:   Batch Loss = 6.881456, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.238907814025879, Accuracy = 0.9047051668167114\n",
      "Training iter #32706000:   Batch Loss = 6.848296, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.237985610961914, Accuracy = 0.9053007960319519\n",
      "Training iter #32709000:   Batch Loss = 6.871152, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.237865924835205, Accuracy = 0.9050029516220093\n",
      "Training iter #32712000:   Batch Loss = 6.893120, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.237983226776123, Accuracy = 0.9050029516220093\n",
      "Training iter #32715000:   Batch Loss = 6.897686, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.23757791519165, Accuracy = 0.9047051668167114\n",
      "Training iter #32718000:   Batch Loss = 6.839276, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.23734712600708, Accuracy = 0.9044073820114136\n",
      "Training iter #32721000:   Batch Loss = 6.840189, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2374043464660645, Accuracy = 0.9038118124008179\n",
      "Training iter #32724000:   Batch Loss = 6.885733, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.237616539001465, Accuracy = 0.9041095972061157\n",
      "Training iter #32727000:   Batch Loss = 6.894116, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.237392425537109, Accuracy = 0.9044073820114136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #32730000:   Batch Loss = 6.883084, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.236794471740723, Accuracy = 0.9044073820114136\n",
      "Training iter #32733000:   Batch Loss = 6.847011, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2361836433410645, Accuracy = 0.9044073820114136\n",
      "Training iter #32736000:   Batch Loss = 6.872178, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.236147403717041, Accuracy = 0.9044073820114136\n",
      "Training iter #32739000:   Batch Loss = 6.892553, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2359843254089355, Accuracy = 0.9047051668167114\n",
      "Training iter #32742000:   Batch Loss = 6.890044, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235537528991699, Accuracy = 0.9050029516220093\n",
      "Training iter #32745000:   Batch Loss = 6.834441, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.236628532409668, Accuracy = 0.9050029516220093\n",
      "Training iter #32748000:   Batch Loss = 6.839334, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.236027717590332, Accuracy = 0.9047051668167114\n",
      "Training iter #32751000:   Batch Loss = 6.876858, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235602855682373, Accuracy = 0.9047051668167114\n",
      "Training iter #32754000:   Batch Loss = 6.899201, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235379695892334, Accuracy = 0.9047051668167114\n",
      "Training iter #32757000:   Batch Loss = 6.883182, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235405445098877, Accuracy = 0.9050029516220093\n",
      "Training iter #32760000:   Batch Loss = 6.849647, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235552787780762, Accuracy = 0.9047051668167114\n",
      "Training iter #32763000:   Batch Loss = 6.874478, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235855579376221, Accuracy = 0.9047051668167114\n",
      "Training iter #32766000:   Batch Loss = 6.887530, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2347331047058105, Accuracy = 0.9047051668167114\n",
      "Training iter #32769000:   Batch Loss = 6.888510, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.234665393829346, Accuracy = 0.9047051668167114\n",
      "Training iter #32772000:   Batch Loss = 6.830802, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235469341278076, Accuracy = 0.9047051668167114\n",
      "Training iter #32775000:   Batch Loss = 6.844069, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235575199127197, Accuracy = 0.9041095972061157\n",
      "Training iter #32778000:   Batch Loss = 6.876789, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235449314117432, Accuracy = 0.9041095972061157\n",
      "Training iter #32781000:   Batch Loss = 6.896766, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.23538064956665, Accuracy = 0.9044073820114136\n",
      "Training iter #32784000:   Batch Loss = 6.875273, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235344886779785, Accuracy = 0.9044073820114136\n",
      "Training iter #32787000:   Batch Loss = 6.848703, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.235034465789795, Accuracy = 0.9041095972061157\n",
      "Training iter #32790000:   Batch Loss = 6.871330, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.234714031219482, Accuracy = 0.9041095972061157\n",
      "Training iter #32793000:   Batch Loss = 6.884758, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.234307289123535, Accuracy = 0.9044073820114136\n",
      "Training iter #32796000:   Batch Loss = 6.883569, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.234449863433838, Accuracy = 0.9044073820114136\n",
      "Training iter #32799000:   Batch Loss = 6.833330, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.234040260314941, Accuracy = 0.9038118124008179\n",
      "Training iter #32802000:   Batch Loss = 6.844687, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.233149528503418, Accuracy = 0.9038118124008179\n",
      "Training iter #32805000:   Batch Loss = 6.875574, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2329936027526855, Accuracy = 0.9038118124008179\n",
      "Training iter #32808000:   Batch Loss = 6.898950, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.232792377471924, Accuracy = 0.9044073820114136\n",
      "Training iter #32811000:   Batch Loss = 6.868591, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.232219219207764, Accuracy = 0.9044073820114136\n",
      "Training iter #32814000:   Batch Loss = 6.844571, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2315673828125, Accuracy = 0.9044073820114136\n",
      "Training iter #32817000:   Batch Loss = 6.876891, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.232559680938721, Accuracy = 0.9044073820114136\n",
      "Training iter #32820000:   Batch Loss = 6.887553, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.232435703277588, Accuracy = 0.9044073820114136\n",
      "Training iter #32823000:   Batch Loss = 6.887069, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.232298851013184, Accuracy = 0.9047051668167114\n",
      "Training iter #32826000:   Batch Loss = 6.834014, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.23187255859375, Accuracy = 0.9041095972061157\n",
      "Training iter #32829000:   Batch Loss = 6.842004, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2315354347229, Accuracy = 0.9038118124008179\n",
      "Training iter #32832000:   Batch Loss = 6.870960, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.231147289276123, Accuracy = 0.9038118124008179\n",
      "Training iter #32835000:   Batch Loss = 6.902133, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.230630397796631, Accuracy = 0.9044073820114136\n",
      "Training iter #32838000:   Batch Loss = 6.858263, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.230860233306885, Accuracy = 0.9047051668167114\n",
      "Training iter #32841000:   Batch Loss = 6.840592, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.229916572570801, Accuracy = 0.9053007960319519\n",
      "Training iter #32844000:   Batch Loss = 6.878549, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.229775905609131, Accuracy = 0.9050029516220093\n",
      "Training iter #32847000:   Batch Loss = 6.886087, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2296576499938965, Accuracy = 0.9047051668167114\n",
      "Training iter #32850000:   Batch Loss = 6.883148, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.229253768920898, Accuracy = 0.9047051668167114\n",
      "Training iter #32853000:   Batch Loss = 6.833029, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228930473327637, Accuracy = 0.9047051668167114\n",
      "Training iter #32856000:   Batch Loss = 6.842212, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228159427642822, Accuracy = 0.9047051668167114\n",
      "Training iter #32859000:   Batch Loss = 6.864732, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227537155151367, Accuracy = 0.9050029516220093\n",
      "Training iter #32862000:   Batch Loss = 6.903682, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227935314178467, Accuracy = 0.9044073820114136\n",
      "Training iter #32865000:   Batch Loss = 6.854812, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.226519584655762, Accuracy = 0.9047051668167114\n",
      "Training iter #32868000:   Batch Loss = 6.840305, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.226690292358398, Accuracy = 0.9047051668167114\n",
      "Training iter #32871000:   Batch Loss = 6.878784, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228269100189209, Accuracy = 0.9050029516220093\n",
      "Training iter #32874000:   Batch Loss = 6.890319, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228081703186035, Accuracy = 0.9050029516220093\n",
      "Training iter #32877000:   Batch Loss = 6.880277, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227537155151367, Accuracy = 0.9050029516220093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #32880000:   Batch Loss = 6.831132, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227859020233154, Accuracy = 0.9050029516220093\n",
      "Training iter #32883000:   Batch Loss = 6.854477, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228199481964111, Accuracy = 0.9055985808372498\n",
      "Training iter #32886000:   Batch Loss = 6.864186, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228087425231934, Accuracy = 0.9055985808372498\n",
      "Training iter #32889000:   Batch Loss = 6.889050, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2271857261657715, Accuracy = 0.9053007960319519\n",
      "Training iter #32892000:   Batch Loss = 6.851114, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2271294593811035, Accuracy = 0.9053007960319519\n",
      "Training iter #32895000:   Batch Loss = 6.836508, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227205753326416, Accuracy = 0.9050029516220093\n",
      "Training iter #32898000:   Batch Loss = 6.862669, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227327823638916, Accuracy = 0.9047051668167114\n",
      "Training iter #32901000:   Batch Loss = 6.898137, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227465629577637, Accuracy = 0.9050029516220093\n",
      "Training iter #32904000:   Batch Loss = 6.868174, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228724956512451, Accuracy = 0.9047051668167114\n",
      "Training iter #32907000:   Batch Loss = 6.831767, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2285614013671875, Accuracy = 0.9047051668167114\n",
      "Training iter #32910000:   Batch Loss = 6.855896, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.228023529052734, Accuracy = 0.9044073820114136\n",
      "Training iter #32913000:   Batch Loss = 6.868806, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227502346038818, Accuracy = 0.9044073820114136\n",
      "Training iter #32916000:   Batch Loss = 6.895171, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.22784948348999, Accuracy = 0.9047051668167114\n",
      "Training iter #32919000:   Batch Loss = 6.846382, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.227921009063721, Accuracy = 0.9050029516220093\n",
      "Training iter #32922000:   Batch Loss = 6.834048, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.226912021636963, Accuracy = 0.9050029516220093\n",
      "Training iter #32925000:   Batch Loss = 6.862026, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.226844310760498, Accuracy = 0.9050029516220093\n",
      "Training iter #32928000:   Batch Loss = 6.897618, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.226626873016357, Accuracy = 0.9050029516220093\n",
      "Training iter #32931000:   Batch Loss = 6.869296, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225965976715088, Accuracy = 0.9047051668167114\n",
      "Training iter #32934000:   Batch Loss = 6.826914, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.226553440093994, Accuracy = 0.9053007960319519\n",
      "Training iter #32937000:   Batch Loss = 6.855659, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225884914398193, Accuracy = 0.9053007960319519\n",
      "Training iter #32940000:   Batch Loss = 6.854632, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.22486686706543, Accuracy = 0.9050029516220093\n",
      "Training iter #32943000:   Batch Loss = 6.893620, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2245354652404785, Accuracy = 0.9050029516220093\n",
      "Training iter #32946000:   Batch Loss = 6.843237, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.224404811859131, Accuracy = 0.9053007960319519\n",
      "Training iter #32949000:   Batch Loss = 6.831060, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.223996162414551, Accuracy = 0.9053007960319519\n",
      "Training iter #32952000:   Batch Loss = 6.873714, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225147724151611, Accuracy = 0.9050029516220093\n",
      "Training iter #32955000:   Batch Loss = 6.895966, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225468635559082, Accuracy = 0.9050029516220093\n",
      "Training iter #32958000:   Batch Loss = 6.864557, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225562572479248, Accuracy = 0.9047051668167114\n",
      "Training iter #32961000:   Batch Loss = 6.826602, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225778579711914, Accuracy = 0.9044073820114136\n",
      "Training iter #32964000:   Batch Loss = 6.855937, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225749492645264, Accuracy = 0.9044073820114136\n",
      "Training iter #32967000:   Batch Loss = 6.854899, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2257080078125, Accuracy = 0.9044073820114136\n",
      "Training iter #32970000:   Batch Loss = 6.891186, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225548267364502, Accuracy = 0.9044073820114136\n",
      "Training iter #32973000:   Batch Loss = 6.838111, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225155353546143, Accuracy = 0.9047051668167114\n",
      "Training iter #32976000:   Batch Loss = 6.831010, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.224357604980469, Accuracy = 0.9047051668167114\n",
      "Training iter #32979000:   Batch Loss = 6.871882, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.223416328430176, Accuracy = 0.9053007960319519\n",
      "Training iter #32982000:   Batch Loss = 6.891573, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.223493576049805, Accuracy = 0.9053007960319519\n",
      "Training iter #32985000:   Batch Loss = 6.869004, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.222856521606445, Accuracy = 0.9055985808372498\n",
      "Training iter #32988000:   Batch Loss = 6.827451, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.223962783813477, Accuracy = 0.9050029516220093\n",
      "Training iter #32991000:   Batch Loss = 6.854576, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.224860668182373, Accuracy = 0.9047051668167114\n",
      "Training iter #32994000:   Batch Loss = 6.864071, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225419044494629, Accuracy = 0.9044073820114136\n",
      "Training iter #32997000:   Batch Loss = 6.892439, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.225190162658691, Accuracy = 0.9044073820114136\n",
      "Training iter #33000000:   Batch Loss = 6.827094, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.224833965301514, Accuracy = 0.9047051668167114\n",
      "Training iter #33003000:   Batch Loss = 6.824782, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.224673271179199, Accuracy = 0.9047051668167114\n",
      "Training iter #33006000:   Batch Loss = 6.873720, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.224221229553223, Accuracy = 0.9044073820114136\n",
      "Training iter #33009000:   Batch Loss = 6.877876, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.224019527435303, Accuracy = 0.9041095972061157\n",
      "Training iter #33012000:   Batch Loss = 6.867633, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.22383975982666, Accuracy = 0.9038118124008179\n",
      "Training iter #33015000:   Batch Loss = 6.832285, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.222384452819824, Accuracy = 0.9041095972061157\n",
      "Training iter #33018000:   Batch Loss = 6.853498, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.222389221191406, Accuracy = 0.9041095972061157\n",
      "Training iter #33021000:   Batch Loss = 6.870107, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.222014427185059, Accuracy = 0.9044073820114136\n",
      "Training iter #33024000:   Batch Loss = 6.880716, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.222564697265625, Accuracy = 0.9044073820114136\n",
      "Training iter #33027000:   Batch Loss = 6.824321, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2222113609313965, Accuracy = 0.9047051668167114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #33030000:   Batch Loss = 6.823133, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.221584796905518, Accuracy = 0.9047051668167114\n",
      "Training iter #33033000:   Batch Loss = 6.870212, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.221491813659668, Accuracy = 0.9047051668167114\n",
      "Training iter #33036000:   Batch Loss = 6.876289, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2212700843811035, Accuracy = 0.9047051668167114\n",
      "Training iter #33039000:   Batch Loss = 6.863206, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.221199989318848, Accuracy = 0.9050029516220093\n",
      "Training iter #33042000:   Batch Loss = 6.831174, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.22153902053833, Accuracy = 0.9050029516220093\n",
      "Training iter #33045000:   Batch Loss = 6.854321, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.221982955932617, Accuracy = 0.9053007960319519\n",
      "Training iter #33048000:   Batch Loss = 6.875396, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.22119140625, Accuracy = 0.9053007960319519\n",
      "Training iter #33051000:   Batch Loss = 6.877685, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.22112512588501, Accuracy = 0.9053007960319519\n",
      "Training iter #33054000:   Batch Loss = 6.819808, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.221301555633545, Accuracy = 0.9050029516220093\n",
      "Training iter #33057000:   Batch Loss = 6.823138, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2219390869140625, Accuracy = 0.9047051668167114\n",
      "Training iter #33060000:   Batch Loss = 6.864172, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.221798896789551, Accuracy = 0.9047051668167114\n",
      "Training iter #33063000:   Batch Loss = 6.878041, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220821857452393, Accuracy = 0.9050029516220093\n",
      "Training iter #33066000:   Batch Loss = 6.864619, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.221269607543945, Accuracy = 0.9050029516220093\n",
      "Training iter #33069000:   Batch Loss = 6.833724, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220847129821777, Accuracy = 0.9050029516220093\n",
      "Training iter #33072000:   Batch Loss = 6.858873, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220459461212158, Accuracy = 0.9050029516220093\n",
      "Training iter #33075000:   Batch Loss = 6.872655, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220399856567383, Accuracy = 0.9050029516220093\n",
      "Training iter #33078000:   Batch Loss = 6.870404, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220430374145508, Accuracy = 0.9050029516220093\n",
      "Training iter #33081000:   Batch Loss = 6.815420, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.22026252746582, Accuracy = 0.9047051668167114\n",
      "Training iter #33084000:   Batch Loss = 6.826330, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.219925880432129, Accuracy = 0.9047051668167114\n",
      "Training iter #33087000:   Batch Loss = 6.858525, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2201080322265625, Accuracy = 0.9044073820114136\n",
      "Training iter #33090000:   Batch Loss = 6.878943, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220191955566406, Accuracy = 0.9047051668167114\n",
      "Training iter #33093000:   Batch Loss = 6.863048, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.219603061676025, Accuracy = 0.9050029516220093\n",
      "Training iter #33096000:   Batch Loss = 6.832852, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220518112182617, Accuracy = 0.9047051668167114\n",
      "Training iter #33099000:   Batch Loss = 6.855072, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.220202922821045, Accuracy = 0.9047051668167114\n",
      "Training iter #33102000:   Batch Loss = 6.868108, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.219675540924072, Accuracy = 0.9047051668167114\n",
      "Training iter #33105000:   Batch Loss = 6.871135, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21882963180542, Accuracy = 0.9050029516220093\n",
      "Training iter #33108000:   Batch Loss = 6.814364, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.217841148376465, Accuracy = 0.9055985808372498\n",
      "Training iter #33111000:   Batch Loss = 6.828017, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21774959564209, Accuracy = 0.9044073820114136\n",
      "Training iter #33114000:   Batch Loss = 6.858819, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21818733215332, Accuracy = 0.9047051668167114\n",
      "Training iter #33117000:   Batch Loss = 6.882128, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.218094825744629, Accuracy = 0.9044073820114136\n",
      "Training iter #33120000:   Batch Loss = 6.853953, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2181010246276855, Accuracy = 0.9047051668167114\n",
      "Training iter #33123000:   Batch Loss = 6.828091, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21933650970459, Accuracy = 0.9044073820114136\n",
      "Training iter #33126000:   Batch Loss = 6.857137, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2189202308654785, Accuracy = 0.9041095972061157\n",
      "Training iter #33129000:   Batch Loss = 6.869361, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.218689918518066, Accuracy = 0.9047051668167114\n",
      "Training iter #33132000:   Batch Loss = 6.866251, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.219024181365967, Accuracy = 0.9047051668167114\n",
      "Training iter #33135000:   Batch Loss = 6.815457, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.218999862670898, Accuracy = 0.9047051668167114\n",
      "Training iter #33138000:   Batch Loss = 6.826812, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.217343807220459, Accuracy = 0.9044073820114136\n",
      "Training iter #33141000:   Batch Loss = 6.854691, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216894149780273, Accuracy = 0.9047051668167114\n",
      "Training iter #33144000:   Batch Loss = 6.882117, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216850757598877, Accuracy = 0.9047051668167114\n",
      "Training iter #33147000:   Batch Loss = 6.843539, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21653413772583, Accuracy = 0.9047051668167114\n",
      "Training iter #33150000:   Batch Loss = 6.827296, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.215969085693359, Accuracy = 0.9044073820114136\n",
      "Training iter #33153000:   Batch Loss = 6.859421, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216494083404541, Accuracy = 0.9041095972061157\n",
      "Training iter #33156000:   Batch Loss = 6.871627, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216372013092041, Accuracy = 0.9038118124008179\n",
      "Training iter #33159000:   Batch Loss = 6.866600, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.215464115142822, Accuracy = 0.9041095972061157\n",
      "Training iter #33162000:   Batch Loss = 6.816463, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21513557434082, Accuracy = 0.9044073820114136\n",
      "Training iter #33165000:   Batch Loss = 6.825101, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216865062713623, Accuracy = 0.9044073820114136\n",
      "Training iter #33168000:   Batch Loss = 6.851397, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.217206954956055, Accuracy = 0.9044073820114136\n",
      "Training iter #33171000:   Batch Loss = 6.883848, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216189861297607, Accuracy = 0.9050029516220093\n",
      "Training iter #33174000:   Batch Loss = 6.839076, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216245651245117, Accuracy = 0.9050029516220093\n",
      "Training iter #33177000:   Batch Loss = 6.822679, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.216341495513916, Accuracy = 0.9047051668167114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #33180000:   Batch Loss = 6.862596, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2161865234375, Accuracy = 0.9053007960319519\n",
      "Training iter #33183000:   Batch Loss = 6.868701, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2149338722229, Accuracy = 0.9050029516220093\n",
      "Training iter #33186000:   Batch Loss = 6.863943, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.215245723724365, Accuracy = 0.9050029516220093\n",
      "Training iter #33189000:   Batch Loss = 6.815876, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.214864253997803, Accuracy = 0.9047051668167114\n",
      "Training iter #33192000:   Batch Loss = 6.822527, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.214442253112793, Accuracy = 0.9044073820114136\n",
      "Training iter #33195000:   Batch Loss = 6.847055, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.214052200317383, Accuracy = 0.9044073820114136\n",
      "Training iter #33198000:   Batch Loss = 6.884031, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.213781833648682, Accuracy = 0.9047051668167114\n",
      "Training iter #33201000:   Batch Loss = 6.836814, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2126359939575195, Accuracy = 0.9047051668167114\n",
      "Training iter #33204000:   Batch Loss = 6.823273, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.212486743927002, Accuracy = 0.9050029516220093\n",
      "Training iter #33207000:   Batch Loss = 6.847786, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.212197303771973, Accuracy = 0.9044073820114136\n",
      "Training iter #33210000:   Batch Loss = 6.874579, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.211939334869385, Accuracy = 0.9041095972061157\n",
      "Training iter #33213000:   Batch Loss = 6.858688, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21229362487793, Accuracy = 0.9041095972061157\n",
      "Training iter #33216000:   Batch Loss = 6.815575, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21112060546875, Accuracy = 0.9041095972061157\n",
      "Training iter #33219000:   Batch Loss = 6.838242, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.211192607879639, Accuracy = 0.9041095972061157\n",
      "Training iter #33222000:   Batch Loss = 6.858306, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21127986907959, Accuracy = 0.9044073820114136\n",
      "Training iter #33225000:   Batch Loss = 6.874969, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2097601890563965, Accuracy = 0.9047051668167114\n",
      "Training iter #33228000:   Batch Loss = 6.833420, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2098565101623535, Accuracy = 0.9044073820114136\n",
      "Training iter #33231000:   Batch Loss = 6.818641, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.210749626159668, Accuracy = 0.9050029516220093\n",
      "Training iter #33234000:   Batch Loss = 6.847654, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.211800575256348, Accuracy = 0.9053007960319519\n",
      "Training iter #33237000:   Batch Loss = 6.886800, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.212449073791504, Accuracy = 0.9050029516220093\n",
      "Training iter #33240000:   Batch Loss = 6.848999, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.213321685791016, Accuracy = 0.9038118124008179\n",
      "Training iter #33243000:   Batch Loss = 6.812586, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.214704513549805, Accuracy = 0.9038118124008179\n",
      "Training iter #33246000:   Batch Loss = 6.838710, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.215404987335205, Accuracy = 0.9041095972061157\n",
      "Training iter #33249000:   Batch Loss = 6.852287, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.214975833892822, Accuracy = 0.9044073820114136\n",
      "Training iter #33252000:   Batch Loss = 6.877698, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.214845180511475, Accuracy = 0.9047051668167114\n",
      "Training iter #33255000:   Batch Loss = 6.827175, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.213339805603027, Accuracy = 0.9058963656425476\n",
      "Training iter #33258000:   Batch Loss = 6.817289, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.212677955627441, Accuracy = 0.9055985808372498\n",
      "Training iter #33261000:   Batch Loss = 6.845969, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2122578620910645, Accuracy = 0.9055985808372498\n",
      "Training iter #33264000:   Batch Loss = 6.881712, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.211345672607422, Accuracy = 0.9050029516220093\n",
      "Training iter #33267000:   Batch Loss = 6.850877, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.210383415222168, Accuracy = 0.9053007960319519\n",
      "Training iter #33270000:   Batch Loss = 6.811496, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.209096431732178, Accuracy = 0.9061941504478455\n",
      "Training iter #33273000:   Batch Loss = 6.837821, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.208574295043945, Accuracy = 0.9053007960319519\n",
      "Training iter #33276000:   Batch Loss = 6.867528, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.208707809448242, Accuracy = 0.9055985808372498\n",
      "Training iter #33279000:   Batch Loss = 6.876612, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.21019172668457, Accuracy = 0.9058963656425476\n",
      "Training iter #33282000:   Batch Loss = 6.826596, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.211002349853516, Accuracy = 0.9050029516220093\n",
      "Training iter #33285000:   Batch Loss = 6.815822, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.212608337402344, Accuracy = 0.9064919352531433\n",
      "Training iter #33288000:   Batch Loss = 6.856128, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.213842391967773, Accuracy = 0.9053007960319519\n",
      "Training iter #33291000:   Batch Loss = 6.882244, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.210528373718262, Accuracy = 0.9050029516220093\n",
      "Training iter #33294000:   Batch Loss = 6.852768, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.210017204284668, Accuracy = 0.9055985808372498\n",
      "Training iter #33297000:   Batch Loss = 6.810513, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.208675384521484, Accuracy = 0.9053007960319519\n",
      "Training iter #33300000:   Batch Loss = 6.840736, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.209108829498291, Accuracy = 0.9061941504478455\n",
      "Training iter #33303000:   Batch Loss = 6.841698, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.208817481994629, Accuracy = 0.9061941504478455\n",
      "Training iter #33306000:   Batch Loss = 6.877537, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2077202796936035, Accuracy = 0.9061941504478455\n",
      "Training iter #33309000:   Batch Loss = 6.819273, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.207616806030273, Accuracy = 0.9053007960319519\n",
      "Training iter #33312000:   Batch Loss = 6.813759, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.20806884765625, Accuracy = 0.9058963656425476\n",
      "Training iter #33315000:   Batch Loss = 6.857409, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.207136154174805, Accuracy = 0.9047051668167114\n",
      "Training iter #33318000:   Batch Loss = 6.887444, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.207528591156006, Accuracy = 0.9038118124008179\n",
      "Training iter #33321000:   Batch Loss = 6.854660, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2064008712768555, Accuracy = 0.9035139679908752\n",
      "Training iter #33324000:   Batch Loss = 6.816316, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.206668376922607, Accuracy = 0.9038118124008179\n",
      "Training iter #33327000:   Batch Loss = 6.841821, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.207270622253418, Accuracy = 0.9047051668167114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #33330000:   Batch Loss = 6.858223, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.208969593048096, Accuracy = 0.9047051668167114\n",
      "Training iter #33333000:   Batch Loss = 6.875525, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.209472179412842, Accuracy = 0.9047051668167114\n",
      "Training iter #33336000:   Batch Loss = 6.811902, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.209854602813721, Accuracy = 0.9050029516220093\n",
      "Training iter #33339000:   Batch Loss = 6.811250, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.209184169769287, Accuracy = 0.9050029516220093\n",
      "Training iter #33342000:   Batch Loss = 6.860511, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.208195209503174, Accuracy = 0.9050029516220093\n",
      "Training iter #33345000:   Batch Loss = 6.869783, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.207560062408447, Accuracy = 0.9050029516220093\n",
      "Training iter #33348000:   Batch Loss = 6.853046, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.206636428833008, Accuracy = 0.9061941504478455\n",
      "Training iter #33351000:   Batch Loss = 6.817524, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.204998016357422, Accuracy = 0.9058963656425476\n",
      "Training iter #33354000:   Batch Loss = 6.843143, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.205641269683838, Accuracy = 0.9058963656425476\n",
      "Training iter #33357000:   Batch Loss = 6.860468, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.20622444152832, Accuracy = 0.9055985808372498\n",
      "Training iter #33360000:   Batch Loss = 6.868698, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.206377029418945, Accuracy = 0.9053007960319519\n",
      "Training iter #33363000:   Batch Loss = 6.811088, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.206361293792725, Accuracy = 0.9047051668167114\n",
      "Training iter #33366000:   Batch Loss = 6.809331, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.204629421234131, Accuracy = 0.9050029516220093\n",
      "Training iter #33369000:   Batch Loss = 6.856477, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.204399108886719, Accuracy = 0.9044073820114136\n",
      "Training iter #33372000:   Batch Loss = 6.863342, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.204510688781738, Accuracy = 0.9038118124008179\n",
      "Training iter #33375000:   Batch Loss = 6.855174, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.205341815948486, Accuracy = 0.9035139679908752\n",
      "Training iter #33378000:   Batch Loss = 6.815751, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.205619812011719, Accuracy = 0.9035139679908752\n",
      "Training iter #33381000:   Batch Loss = 6.847430, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.20550012588501, Accuracy = 0.9038118124008179\n",
      "Training iter #33384000:   Batch Loss = 6.860703, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.2038726806640625, Accuracy = 0.9047051668167114\n",
      "Training iter #33387000:   Batch Loss = 6.860142, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.203337669372559, Accuracy = 0.9055985808372498\n",
      "Training iter #33390000:   Batch Loss = 6.802777, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.203953266143799, Accuracy = 0.9067897796630859\n",
      "Training iter #33393000:   Batch Loss = 6.807068, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.203190803527832, Accuracy = 0.9073853492736816\n",
      "Training iter #33396000:   Batch Loss = 6.846401, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.202096939086914, Accuracy = 0.9073853492736816\n",
      "Training iter #33399000:   Batch Loss = 6.868632, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200461387634277, Accuracy = 0.9076831340789795\n",
      "Training iter #33402000:   Batch Loss = 6.858915, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200046062469482, Accuracy = 0.9067897796630859\n",
      "Training iter #33405000:   Batch Loss = 6.817237, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.20088005065918, Accuracy = 0.9070875644683838\n",
      "Training iter #33408000:   Batch Loss = 6.846950, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200664520263672, Accuracy = 0.9070875644683838\n",
      "Training iter #33411000:   Batch Loss = 6.852954, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200274467468262, Accuracy = 0.9064919352531433\n",
      "Training iter #33414000:   Batch Loss = 6.853688, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.199258804321289, Accuracy = 0.9061941504478455\n",
      "Training iter #33417000:   Batch Loss = 6.802007, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.198958396911621, Accuracy = 0.9058963656425476\n",
      "Training iter #33420000:   Batch Loss = 6.811953, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.199301719665527, Accuracy = 0.9058963656425476\n",
      "Training iter #33423000:   Batch Loss = 6.846268, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1998209953308105, Accuracy = 0.9053007960319519\n",
      "Training iter #33426000:   Batch Loss = 6.864283, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200344562530518, Accuracy = 0.9053007960319519\n",
      "Training iter #33429000:   Batch Loss = 6.849048, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200919151306152, Accuracy = 0.9050029516220093\n",
      "Training iter #33432000:   Batch Loss = 6.816615, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1999382972717285, Accuracy = 0.9050029516220093\n",
      "Training iter #33435000:   Batch Loss = 6.839761, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200491905212402, Accuracy = 0.9050029516220093\n",
      "Training iter #33438000:   Batch Loss = 6.858158, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200265884399414, Accuracy = 0.9050029516220093\n",
      "Training iter #33441000:   Batch Loss = 6.853099, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.199869632720947, Accuracy = 0.9053007960319519\n",
      "Training iter #33444000:   Batch Loss = 6.804325, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200780868530273, Accuracy = 0.9053007960319519\n",
      "Training iter #33447000:   Batch Loss = 6.812336, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.20134973526001, Accuracy = 0.9044073820114136\n",
      "Training iter #33450000:   Batch Loss = 6.841351, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.201258659362793, Accuracy = 0.9053007960319519\n",
      "Training iter #33453000:   Batch Loss = 6.869229, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200370788574219, Accuracy = 0.9050029516220093\n",
      "Training iter #33456000:   Batch Loss = 6.839767, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.200187683105469, Accuracy = 0.9050029516220093\n",
      "Training iter #33459000:   Batch Loss = 6.812476, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.199895858764648, Accuracy = 0.9053007960319519\n",
      "Training iter #33462000:   Batch Loss = 6.845429, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.199571132659912, Accuracy = 0.9053007960319519\n",
      "Training iter #33465000:   Batch Loss = 6.859546, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.199369430541992, Accuracy = 0.9053007960319519\n",
      "Training iter #33468000:   Batch Loss = 6.853343, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.198657512664795, Accuracy = 0.9053007960319519\n",
      "Training iter #33471000:   Batch Loss = 6.799316, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1981587409973145, Accuracy = 0.9053007960319519\n",
      "Training iter #33474000:   Batch Loss = 6.809448, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.199156284332275, Accuracy = 0.9050029516220093\n",
      "Training iter #33477000:   Batch Loss = 6.840430, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.19890832901001, Accuracy = 0.9050029516220093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #33480000:   Batch Loss = 6.869297, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.198843002319336, Accuracy = 0.9053007960319519\n",
      "Training iter #33483000:   Batch Loss = 6.827055, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.198559284210205, Accuracy = 0.9047051668167114\n",
      "Training iter #33486000:   Batch Loss = 6.808204, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.198503017425537, Accuracy = 0.9050029516220093\n",
      "Training iter #33489000:   Batch Loss = 6.847511, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.198573589324951, Accuracy = 0.9044073820114136\n",
      "Training iter #33492000:   Batch Loss = 6.852288, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.197841167449951, Accuracy = 0.9047051668167114\n",
      "Training iter #33495000:   Batch Loss = 6.850950, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.197048187255859, Accuracy = 0.9053007960319519\n",
      "Training iter #33498000:   Batch Loss = 6.801133, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.196671009063721, Accuracy = 0.9058963656425476\n",
      "Training iter #33501000:   Batch Loss = 6.808794, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.19650936126709, Accuracy = 0.9058963656425476\n",
      "Training iter #33504000:   Batch Loss = 6.832188, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.196599960327148, Accuracy = 0.9055985808372498\n",
      "Training iter #33507000:   Batch Loss = 6.870672, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.196305751800537, Accuracy = 0.9055985808372498\n",
      "Training iter #33510000:   Batch Loss = 6.822829, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.195910930633545, Accuracy = 0.9061941504478455\n",
      "Training iter #33513000:   Batch Loss = 6.808647, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.19577693939209, Accuracy = 0.9061941504478455\n",
      "Training iter #33516000:   Batch Loss = 6.847396, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1957292556762695, Accuracy = 0.9061941504478455\n",
      "Training iter #33519000:   Batch Loss = 6.852388, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.195518493652344, Accuracy = 0.9055985808372498\n",
      "Training iter #33522000:   Batch Loss = 6.845439, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.19498348236084, Accuracy = 0.9055985808372498\n",
      "Training iter #33525000:   Batch Loss = 6.799132, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.195362091064453, Accuracy = 0.9047051668167114\n",
      "Training iter #33528000:   Batch Loss = 6.821659, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.195440292358398, Accuracy = 0.9050029516220093\n",
      "Training iter #33531000:   Batch Loss = 6.833067, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1952924728393555, Accuracy = 0.9050029516220093\n",
      "Training iter #33534000:   Batch Loss = 6.868108, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.195424556732178, Accuracy = 0.9053007960319519\n",
      "Training iter #33537000:   Batch Loss = 6.818616, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.195312023162842, Accuracy = 0.9050029516220093\n",
      "Training iter #33540000:   Batch Loss = 6.806811, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.195139408111572, Accuracy = 0.9053007960319519\n",
      "Training iter #33543000:   Batch Loss = 6.829962, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.194943428039551, Accuracy = 0.9053007960319519\n",
      "Training iter #33546000:   Batch Loss = 6.864960, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.194734573364258, Accuracy = 0.9053007960319519\n",
      "Training iter #33549000:   Batch Loss = 6.836979, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.194169521331787, Accuracy = 0.9055985808372498\n",
      "Training iter #33552000:   Batch Loss = 6.799861, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.194124698638916, Accuracy = 0.9050029516220093\n",
      "Training iter #33555000:   Batch Loss = 6.822737, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.194157123565674, Accuracy = 0.9044073820114136\n",
      "Training iter #33558000:   Batch Loss = 6.833687, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.194326400756836, Accuracy = 0.9044073820114136\n",
      "Training iter #33561000:   Batch Loss = 6.861880, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.193770885467529, Accuracy = 0.9047051668167114\n",
      "Training iter #33564000:   Batch Loss = 6.816033, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.193664073944092, Accuracy = 0.9047051668167114\n",
      "Training iter #33567000:   Batch Loss = 6.802341, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1935505867004395, Accuracy = 0.9050029516220093\n",
      "Training iter #33570000:   Batch Loss = 6.828790, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.193839073181152, Accuracy = 0.9053007960319519\n",
      "Training iter #33573000:   Batch Loss = 6.862822, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.194006443023682, Accuracy = 0.9050029516220093\n",
      "Training iter #33576000:   Batch Loss = 6.834513, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1939520835876465, Accuracy = 0.9047051668167114\n",
      "Training iter #33579000:   Batch Loss = 6.795342, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.193807125091553, Accuracy = 0.9050029516220093\n",
      "Training iter #33582000:   Batch Loss = 6.821805, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.193482875823975, Accuracy = 0.9050029516220093\n",
      "Training iter #33585000:   Batch Loss = 6.834114, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.193172454833984, Accuracy = 0.9050029516220093\n",
      "Training iter #33588000:   Batch Loss = 6.858778, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.192728519439697, Accuracy = 0.9053007960319519\n",
      "Training iter #33591000:   Batch Loss = 6.809796, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.192440986633301, Accuracy = 0.9055985808372498\n",
      "Training iter #33594000:   Batch Loss = 6.799435, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 7.1921067237854, Accuracy = 0.9058963656425476\n",
      "Optimization Finished!\n",
      "FINAL RESULT: Batch Loss = 7.1921067237854, Accuracy = 0.9058963656425476\n"
     ]
    }
   ],
   "source": [
    "# To keep track of training's performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training is good, but having visual insight is even better:\n",
    "\n",
    "Okay, let's plot this simply in the notebook for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, the multi-class confusion matrix and metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 90.58963656425476%\n",
      "\n",
      "Precision: 90.82682830765256%\n",
      "Recall: 90.58963668850507%\n",
      "f1_score: 90.5540030913397%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35  0  0 ...  0  0  0]\n",
      " [ 0 27  0 ...  0  0  0]\n",
      " [ 0  0 34 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 29  0  0]\n",
      " [ 0  0  0 ...  0 30  0]\n",
      " [ 0  0  0 ...  0  0 30]]\n",
      "\n",
      "Confusion matrix (normalised to % of total test data):\n",
      "[[1.0422871  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.80404997 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.0125074  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.86360925 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.893389   0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.893389  ]]\n",
      "Note: training and testing data is not equally distributed amongst classes, \n",
      "so it is normal that more than a 6th of the data is correctly classifier in the last category.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 20\n",
    "height = 20\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Results:\n",
    "width = 20\n",
    "height = 20\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix,\n",
    "    interpolation='nearest',\n",
    "    cmap=plt.cm.Blues\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "model_path = \"log_1_2_3/123model.ckpt\"\n",
    "saver=tf.train.Saver()\n",
    "tf.add_to_collection('pred_network', pred)\n",
    "saver.save(sess, model_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
